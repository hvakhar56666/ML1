{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb132150",
   "metadata": {},
   "source": [
    "# Implementing Gradient Descent from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a560c55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>airline</th>\n",
       "      <th>flight</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>SpiceJet</td>\n",
       "      <td>SG-8709</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Evening</td>\n",
       "      <td>zero</td>\n",
       "      <td>Night</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SpiceJet</td>\n",
       "      <td>SG-8157</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AirAsia</td>\n",
       "      <td>I5-764</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-995</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1</td>\n",
       "      <td>5955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-963</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300148</th>\n",
       "      <td>300148</td>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-822</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Business</td>\n",
       "      <td>10.08</td>\n",
       "      <td>49</td>\n",
       "      <td>69265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300149</th>\n",
       "      <td>300149</td>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-826</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>one</td>\n",
       "      <td>Night</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Business</td>\n",
       "      <td>10.42</td>\n",
       "      <td>49</td>\n",
       "      <td>77105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300150</th>\n",
       "      <td>300150</td>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-832</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Night</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Business</td>\n",
       "      <td>13.83</td>\n",
       "      <td>49</td>\n",
       "      <td>79099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300151</th>\n",
       "      <td>300151</td>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-828</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Business</td>\n",
       "      <td>10.00</td>\n",
       "      <td>49</td>\n",
       "      <td>81585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300152</th>\n",
       "      <td>300152</td>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-822</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Business</td>\n",
       "      <td>10.08</td>\n",
       "      <td>49</td>\n",
       "      <td>81585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300153 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0   airline   flight source_city departure_time stops  \\\n",
       "0                0  SpiceJet  SG-8709       Delhi        Evening  zero   \n",
       "1                1  SpiceJet  SG-8157       Delhi  Early_Morning  zero   \n",
       "2                2   AirAsia   I5-764       Delhi  Early_Morning  zero   \n",
       "3                3   Vistara   UK-995       Delhi        Morning  zero   \n",
       "4                4   Vistara   UK-963       Delhi        Morning  zero   \n",
       "...            ...       ...      ...         ...            ...   ...   \n",
       "300148      300148   Vistara   UK-822     Chennai        Morning   one   \n",
       "300149      300149   Vistara   UK-826     Chennai      Afternoon   one   \n",
       "300150      300150   Vistara   UK-832     Chennai  Early_Morning   one   \n",
       "300151      300151   Vistara   UK-828     Chennai  Early_Morning   one   \n",
       "300152      300152   Vistara   UK-822     Chennai        Morning   one   \n",
       "\n",
       "         arrival_time destination_city     class  duration  days_left  price  \n",
       "0               Night           Mumbai   Economy      2.17          1   5953  \n",
       "1             Morning           Mumbai   Economy      2.33          1   5953  \n",
       "2       Early_Morning           Mumbai   Economy      2.17          1   5956  \n",
       "3           Afternoon           Mumbai   Economy      2.25          1   5955  \n",
       "4             Morning           Mumbai   Economy      2.33          1   5955  \n",
       "...               ...              ...       ...       ...        ...    ...  \n",
       "300148        Evening        Hyderabad  Business     10.08         49  69265  \n",
       "300149          Night        Hyderabad  Business     10.42         49  77105  \n",
       "300150          Night        Hyderabad  Business     13.83         49  79099  \n",
       "300151        Evening        Hyderabad  Business     10.00         49  81585  \n",
       "300152        Evening        Hyderabad  Business     10.08         49  81585  \n",
       "\n",
       "[300153 rows x 12 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the flight dataset from the folder and the required libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df=pd.read_csv(r\"C:\\Users\\hasan\\Downloads\\datasets\\datasets\\flight_price_prediction.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "07f46bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the flight column\n",
    "\n",
    "df.drop(\"flight\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b8c84007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1</td>\n",
       "      <td>5955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300148</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10.08</td>\n",
       "      <td>49</td>\n",
       "      <td>69265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300149</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10.42</td>\n",
       "      <td>49</td>\n",
       "      <td>77105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300150</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>13.83</td>\n",
       "      <td>49</td>\n",
       "      <td>79099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300151</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10.00</td>\n",
       "      <td>49</td>\n",
       "      <td>81585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300152</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10.08</td>\n",
       "      <td>49</td>\n",
       "      <td>81585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300153 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       airline source_city departure_time stops arrival_time destination_city  \\\n",
       "0            1           1              1     1            1                1   \n",
       "1            1           1              2     1            2                1   \n",
       "2            2           1              2     1            3                1   \n",
       "3            3           1              3     1            4                1   \n",
       "4            3           1              3     1            2                1   \n",
       "...        ...         ...            ...   ...          ...              ...   \n",
       "300148       3           6              3     2            5                4   \n",
       "300149       3           6              4     2            1                4   \n",
       "300150       3           6              2     2            1                4   \n",
       "300151       3           6              2     2            5                4   \n",
       "300152       3           6              3     2            5                4   \n",
       "\n",
       "       class  duration  days_left  price  \n",
       "0          1      2.17          1   5953  \n",
       "1          1      2.33          1   5953  \n",
       "2          1      2.17          1   5956  \n",
       "3          1      2.25          1   5955  \n",
       "4          1      2.33          1   5955  \n",
       "...      ...       ...        ...    ...  \n",
       "300148     2     10.08         49  69265  \n",
       "300149     2     10.42         49  77105  \n",
       "300150     2     13.83         49  79099  \n",
       "300151     2     10.00         49  81585  \n",
       "300152     2     10.08         49  81585  \n",
       "\n",
       "[300153 rows x 10 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performing the label encoding for all the categorical column\n",
    "\n",
    "cat_col=(\"airline\",\"source_city\",\"departure_time\",\"stops\",\"arrival_time\",\"destination_city\",\"class\")\n",
    "for i in cat_col:\n",
    "    unique_value=df[i].unique()\n",
    "    \n",
    "    n=1\n",
    "    for j in unique_value:\n",
    "        df.loc[df[i]==j, i] = n\n",
    "        n+=1\n",
    "df.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "023e6f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "      <th>stop_class_</th>\n",
       "      <th>dur_stop_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "      <td>1</td>\n",
       "      <td>0.774727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "      <td>1</td>\n",
       "      <td>0.845868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5956</td>\n",
       "      <td>1</td>\n",
       "      <td>0.774727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1</td>\n",
       "      <td>5955</td>\n",
       "      <td>1</td>\n",
       "      <td>0.81093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5955</td>\n",
       "      <td>1</td>\n",
       "      <td>0.845868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300148</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10.08</td>\n",
       "      <td>49</td>\n",
       "      <td>69265</td>\n",
       "      <td>4</td>\n",
       "      <td>4.621107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300149</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10.42</td>\n",
       "      <td>49</td>\n",
       "      <td>77105</td>\n",
       "      <td>4</td>\n",
       "      <td>4.687454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300150</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>13.83</td>\n",
       "      <td>49</td>\n",
       "      <td>79099</td>\n",
       "      <td>4</td>\n",
       "      <td>5.25368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300151</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10.00</td>\n",
       "      <td>49</td>\n",
       "      <td>81585</td>\n",
       "      <td>4</td>\n",
       "      <td>4.60517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300152</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10.08</td>\n",
       "      <td>49</td>\n",
       "      <td>81585</td>\n",
       "      <td>4</td>\n",
       "      <td>4.621107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300153 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       airline source_city departure_time stops arrival_time destination_city  \\\n",
       "0            1           1              1     1            1                1   \n",
       "1            1           1              2     1            2                1   \n",
       "2            2           1              2     1            3                1   \n",
       "3            3           1              3     1            4                1   \n",
       "4            3           1              3     1            2                1   \n",
       "...        ...         ...            ...   ...          ...              ...   \n",
       "300148       3           6              3     2            5                4   \n",
       "300149       3           6              4     2            1                4   \n",
       "300150       3           6              2     2            1                4   \n",
       "300151       3           6              2     2            5                4   \n",
       "300152       3           6              3     2            5                4   \n",
       "\n",
       "       class  duration  days_left  price stop_class_ dur_stop_  \n",
       "0          1      2.17          1   5953           1  0.774727  \n",
       "1          1      2.33          1   5953           1  0.845868  \n",
       "2          1      2.17          1   5956           1  0.774727  \n",
       "3          1      2.25          1   5955           1   0.81093  \n",
       "4          1      2.33          1   5955           1  0.845868  \n",
       "...      ...       ...        ...    ...         ...       ...  \n",
       "300148     2     10.08         49  69265           4  4.621107  \n",
       "300149     2     10.42         49  77105           4  4.687454  \n",
       "300150     2     13.83         49  79099           4   5.25368  \n",
       "300151     2     10.00         49  81585           4   4.60517  \n",
       "300152     2     10.08         49  81585           4  4.621107  \n",
       "\n",
       "[300153 rows x 12 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding new feature to the dataset based on the previous data analysis which helped to improve the mse\n",
    "\n",
    "df[\"stop_class_\"]=df[\"stops\"]*df[\"class\"]\n",
    "df[\"dur_stop_\"]=np.log(df[\"duration\"])*df[\"stops\"]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b1b257ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline             int32\n",
       "source_city         int32\n",
       "departure_time      int32\n",
       "stops               int32\n",
       "arrival_time        int32\n",
       "destination_city    int32\n",
       "class               int32\n",
       "duration            int32\n",
       "days_left           int64\n",
       "price               int64\n",
       "stop_class_         int32\n",
       "dur_stop_           int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting different datatype like object to int for defined columns\n",
    "\n",
    "cat_col=(\"duration\",\"dur_stop_\",\"stop_class_\",\"airline\",\"source_city\",\"departure_time\",\"stops\",\"arrival_time\",\"destination_city\",\"class\")\n",
    "\n",
    "for i in cat_col:\n",
    "    df[i] = df[i].astype(\"int\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3e490c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the denormalization function for the price column to get the final number in the same range\n",
    "\n",
    "price_max=df[\"price\"].max()\n",
    "price_min=df[\"price\"].min()\n",
    "def denorm(x):\n",
    "    return (x * (price_max-price_min) + price_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e2777d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "      <th>stop_class_</th>\n",
       "      <th>dur_stop_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039749</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039749</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039765</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039765</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300148</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.558844</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300149</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.623124</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300150</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.639473</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300151</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.659856</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300152</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.659856</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300153 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        airline  source_city  departure_time  stops  arrival_time  \\\n",
       "0             1            1               1      1             1   \n",
       "1             1            1               2      1             2   \n",
       "2             2            1               2      1             3   \n",
       "3             3            1               3      1             4   \n",
       "4             3            1               3      1             2   \n",
       "...         ...          ...             ...    ...           ...   \n",
       "300148        3            6               3      2             5   \n",
       "300149        3            6               4      2             1   \n",
       "300150        3            6               2      2             1   \n",
       "300151        3            6               2      2             5   \n",
       "300152        3            6               3      2             5   \n",
       "\n",
       "        destination_city  class  duration  days_left     price  stop_class_  \\\n",
       "0                      1      1  0.040816        0.0  0.039749            1   \n",
       "1                      1      1  0.040816        0.0  0.039749            1   \n",
       "2                      1      1  0.040816        0.0  0.039773            1   \n",
       "3                      1      1  0.040816        0.0  0.039765            1   \n",
       "4                      1      1  0.040816        0.0  0.039765            1   \n",
       "...                  ...    ...       ...        ...       ...          ...   \n",
       "300148                 4      2  0.204082        1.0  0.558844            4   \n",
       "300149                 4      2  0.204082        1.0  0.623124            4   \n",
       "300150                 4      2  0.265306        1.0  0.639473            4   \n",
       "300151                 4      2  0.204082        1.0  0.659856            4   \n",
       "300152                 4      2  0.204082        1.0  0.659856            4   \n",
       "\n",
       "        dur_stop_  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "...           ...  \n",
       "300148          4  \n",
       "300149          4  \n",
       "300150          5  \n",
       "300151          4  \n",
       "300152          4  \n",
       "\n",
       "[300153 rows x 12 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing the required column\n",
    "\n",
    "independen_variable=[\"duration\",\"days_left\",\"price\"]\n",
    "for i in independen_variable:\n",
    "    df[i]=(df[i]-df[i].min())/(df[i].max()-df[i].min())\n",
    "#df.drop(columns=[\"arrival_time\",\"stops\",\"destination_city\",\"stop_class_\"],inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6be6d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns=[\"arrival_time\",\"destination_city\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "317389e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking Price at the target variable\n",
    "# defining the train and test split function \n",
    "\n",
    "def train_test_split(df):\n",
    "    train_index = np.random.rand(len(df)) < 0.8\n",
    "    train_data = df[train_index]\n",
    "    test_data = df[~train_index]\n",
    "    train_x=train_data.drop(\"price\",axis=1)\n",
    "    test_x=test_data.drop(\"price\",axis=1)\n",
    "    train_y=train_data[\"price\"]\n",
    "    test_y=test_data[\"price\"]\n",
    "    return(train_x,train_y,test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3ff5bcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239751, 11)\n",
      "(239751,)\n",
      "(60402, 11)\n",
      "(60402,)\n"
     ]
    }
   ],
   "source": [
    "# printing the shape of train and test datasets\n",
    "\n",
    "train_x,train_y,test_x,test_y=train_test_split(df)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "060f25be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ridge_GD:\n",
    "    \n",
    "    def __init__(self, itr, learning_rate,lamda):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.itr = itr\n",
    "        self.lamda=lamda\n",
    "        self.weights = None\n",
    "        self.losss = []\n",
    "        self.we = []\n",
    "\n",
    "    def loss(self,test_x,test_y):\n",
    "        predicted=self.predict(test_x)\n",
    "        mse=.5*np.mean((test_y-predicted)**2) + ((self.lamda/2)*(np.dot(self.weights.T,self.weights)))\n",
    "        return mse\n",
    "\n",
    "\n",
    "    def gradient_descent(self,x_train,train_y, y_predicted):\n",
    "        delta = y_predicted- train_y\n",
    "        dW=(2*(np.dot(x_train.T,delta)) + (2*self.lamda*self.weights))/x_train.shape[0]\n",
    "        return (dW)\n",
    "\n",
    "    def fit(self, x_train, train_y):\n",
    "        self.weights=np.ones(x_train.shape[1])\n",
    "        for i in range(self.itr):\n",
    "            z = np.dot(x_train,self.weights.T) \n",
    "            y_predicted= z\n",
    "            dW= self.gradient_descent(x_train,train_y, y_predicted)\n",
    "            self.weights=self.weights-(self.learning_rate*dW)\n",
    "            loss=self.loss(x_train, train_y)\n",
    "            \n",
    "            self.losss.append(loss)\n",
    "            print(f\"For Iteration {i} the Loss is {round(self.losss[i],4)}.\")\n",
    "            self.we.append(self.weights)\n",
    "\n",
    "    def predict(self, x_train):\n",
    "        z=np.dot(x_train,self.weights.T)\n",
    "        y_predicted=z\n",
    "        \n",
    "            \n",
    "        return(y_predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9fbb7134",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 0 the Loss is 255.5171.\n",
      "For Iteration 1 the Loss is 174.2093.\n",
      "For Iteration 2 the Loss is 118.7978.\n",
      "For Iteration 3 the Loss is 81.0347.\n",
      "For Iteration 4 the Loss is 55.2989.\n",
      "For Iteration 5 the Loss is 37.7596.\n",
      "For Iteration 6 the Loss is 25.8063.\n",
      "For Iteration 7 the Loss is 17.6598.\n",
      "For Iteration 8 the Loss is 12.1076.\n",
      "For Iteration 9 the Loss is 8.3234.\n",
      "For Iteration 10 the Loss is 5.7442.\n",
      "For Iteration 11 the Loss is 3.9861.\n",
      "For Iteration 12 the Loss is 2.7876.\n",
      "For Iteration 13 the Loss is 1.9705.\n",
      "For Iteration 14 the Loss is 1.4133.\n",
      "For Iteration 15 the Loss is 1.0332.\n",
      "For Iteration 16 the Loss is 0.7738.\n",
      "For Iteration 17 the Loss is 0.5967.\n",
      "For Iteration 18 the Loss is 0.4757.\n",
      "For Iteration 19 the Loss is 0.3929.\n",
      "For Iteration 20 the Loss is 0.3362.\n",
      "For Iteration 21 the Loss is 0.2972.\n",
      "For Iteration 22 the Loss is 0.2703.\n",
      "For Iteration 23 the Loss is 0.2516.\n",
      "For Iteration 24 the Loss is 0.2386.\n",
      "For Iteration 25 the Loss is 0.2294.\n",
      "For Iteration 26 the Loss is 0.2228.\n",
      "For Iteration 27 the Loss is 0.2181.\n",
      "For Iteration 28 the Loss is 0.2145.\n",
      "For Iteration 29 the Loss is 0.2118.\n",
      "For Iteration 30 the Loss is 0.2096.\n",
      "For Iteration 31 the Loss is 0.2078.\n",
      "For Iteration 32 the Loss is 0.2063.\n",
      "For Iteration 33 the Loss is 0.205.\n",
      "For Iteration 34 the Loss is 0.2038.\n",
      "For Iteration 35 the Loss is 0.2027.\n",
      "For Iteration 36 the Loss is 0.2017.\n",
      "For Iteration 37 the Loss is 0.2007.\n",
      "For Iteration 38 the Loss is 0.1997.\n",
      "For Iteration 39 the Loss is 0.1988.\n",
      "For Iteration 40 the Loss is 0.1979.\n",
      "For Iteration 41 the Loss is 0.197.\n",
      "For Iteration 42 the Loss is 0.1961.\n",
      "For Iteration 43 the Loss is 0.1952.\n",
      "For Iteration 44 the Loss is 0.1944.\n",
      "For Iteration 45 the Loss is 0.1935.\n",
      "For Iteration 46 the Loss is 0.1927.\n",
      "For Iteration 47 the Loss is 0.1918.\n",
      "For Iteration 48 the Loss is 0.191.\n",
      "For Iteration 49 the Loss is 0.1902.\n",
      "For Iteration 50 the Loss is 0.1894.\n",
      "For Iteration 51 the Loss is 0.1885.\n",
      "For Iteration 52 the Loss is 0.1877.\n",
      "For Iteration 53 the Loss is 0.1869.\n",
      "For Iteration 54 the Loss is 0.1861.\n",
      "For Iteration 55 the Loss is 0.1854.\n",
      "For Iteration 56 the Loss is 0.1846.\n",
      "For Iteration 57 the Loss is 0.1838.\n",
      "For Iteration 58 the Loss is 0.183.\n",
      "For Iteration 59 the Loss is 0.1823.\n",
      "For Iteration 60 the Loss is 0.1815.\n",
      "For Iteration 61 the Loss is 0.1808.\n",
      "For Iteration 62 the Loss is 0.18.\n",
      "For Iteration 63 the Loss is 0.1793.\n",
      "For Iteration 64 the Loss is 0.1785.\n",
      "For Iteration 65 the Loss is 0.1778.\n",
      "For Iteration 66 the Loss is 0.1771.\n",
      "For Iteration 67 the Loss is 0.1764.\n",
      "For Iteration 68 the Loss is 0.1757.\n",
      "For Iteration 69 the Loss is 0.1749.\n",
      "For Iteration 70 the Loss is 0.1742.\n",
      "For Iteration 71 the Loss is 0.1735.\n",
      "For Iteration 72 the Loss is 0.1729.\n",
      "For Iteration 73 the Loss is 0.1722.\n",
      "For Iteration 74 the Loss is 0.1715.\n",
      "For Iteration 75 the Loss is 0.1708.\n",
      "For Iteration 76 the Loss is 0.1701.\n",
      "For Iteration 77 the Loss is 0.1695.\n",
      "For Iteration 78 the Loss is 0.1688.\n",
      "For Iteration 79 the Loss is 0.1682.\n",
      "For Iteration 80 the Loss is 0.1675.\n",
      "For Iteration 81 the Loss is 0.1669.\n",
      "For Iteration 82 the Loss is 0.1662.\n",
      "For Iteration 83 the Loss is 0.1656.\n",
      "For Iteration 84 the Loss is 0.1649.\n",
      "For Iteration 85 the Loss is 0.1643.\n",
      "For Iteration 86 the Loss is 0.1637.\n",
      "For Iteration 87 the Loss is 0.1631.\n",
      "For Iteration 88 the Loss is 0.1625.\n",
      "For Iteration 89 the Loss is 0.1618.\n",
      "For Iteration 90 the Loss is 0.1612.\n",
      "For Iteration 91 the Loss is 0.1606.\n",
      "For Iteration 92 the Loss is 0.16.\n",
      "For Iteration 93 the Loss is 0.1595.\n",
      "For Iteration 94 the Loss is 0.1589.\n",
      "For Iteration 95 the Loss is 0.1583.\n",
      "For Iteration 96 the Loss is 0.1577.\n",
      "For Iteration 97 the Loss is 0.1571.\n",
      "For Iteration 98 the Loss is 0.1566.\n",
      "For Iteration 99 the Loss is 0.156.\n",
      "For Iteration 100 the Loss is 0.1554.\n",
      "For Iteration 101 the Loss is 0.1549.\n",
      "For Iteration 102 the Loss is 0.1543.\n",
      "For Iteration 103 the Loss is 0.1538.\n",
      "For Iteration 104 the Loss is 0.1532.\n",
      "For Iteration 105 the Loss is 0.1527.\n",
      "For Iteration 106 the Loss is 0.1521.\n",
      "For Iteration 107 the Loss is 0.1516.\n",
      "For Iteration 108 the Loss is 0.1511.\n",
      "For Iteration 109 the Loss is 0.1505.\n",
      "For Iteration 110 the Loss is 0.15.\n",
      "For Iteration 111 the Loss is 0.1495.\n",
      "For Iteration 112 the Loss is 0.149.\n",
      "For Iteration 113 the Loss is 0.1485.\n",
      "For Iteration 114 the Loss is 0.1479.\n",
      "For Iteration 115 the Loss is 0.1474.\n",
      "For Iteration 116 the Loss is 0.1469.\n",
      "For Iteration 117 the Loss is 0.1464.\n",
      "For Iteration 118 the Loss is 0.1459.\n",
      "For Iteration 119 the Loss is 0.1454.\n",
      "For Iteration 120 the Loss is 0.145.\n",
      "For Iteration 121 the Loss is 0.1445.\n",
      "For Iteration 122 the Loss is 0.144.\n",
      "For Iteration 123 the Loss is 0.1435.\n",
      "For Iteration 124 the Loss is 0.143.\n",
      "For Iteration 125 the Loss is 0.1426.\n",
      "For Iteration 126 the Loss is 0.1421.\n",
      "For Iteration 127 the Loss is 0.1416.\n",
      "For Iteration 128 the Loss is 0.1412.\n",
      "For Iteration 129 the Loss is 0.1407.\n",
      "For Iteration 130 the Loss is 0.1402.\n",
      "For Iteration 131 the Loss is 0.1398.\n",
      "For Iteration 132 the Loss is 0.1393.\n",
      "For Iteration 133 the Loss is 0.1389.\n",
      "For Iteration 134 the Loss is 0.1384.\n",
      "For Iteration 135 the Loss is 0.138.\n",
      "For Iteration 136 the Loss is 0.1376.\n",
      "For Iteration 137 the Loss is 0.1371.\n",
      "For Iteration 138 the Loss is 0.1367.\n",
      "For Iteration 139 the Loss is 0.1363.\n",
      "For Iteration 140 the Loss is 0.1358.\n",
      "For Iteration 141 the Loss is 0.1354.\n",
      "For Iteration 142 the Loss is 0.135.\n",
      "For Iteration 143 the Loss is 0.1346.\n",
      "For Iteration 144 the Loss is 0.1342.\n",
      "For Iteration 145 the Loss is 0.1337.\n",
      "For Iteration 146 the Loss is 0.1333.\n",
      "For Iteration 147 the Loss is 0.1329.\n",
      "For Iteration 148 the Loss is 0.1325.\n",
      "For Iteration 149 the Loss is 0.1321.\n",
      "For Iteration 150 the Loss is 0.1317.\n",
      "For Iteration 151 the Loss is 0.1313.\n",
      "For Iteration 152 the Loss is 0.1309.\n",
      "For Iteration 153 the Loss is 0.1305.\n",
      "For Iteration 154 the Loss is 0.1302.\n",
      "For Iteration 155 the Loss is 0.1298.\n",
      "For Iteration 156 the Loss is 0.1294.\n",
      "For Iteration 157 the Loss is 0.129.\n",
      "For Iteration 158 the Loss is 0.1286.\n",
      "For Iteration 159 the Loss is 0.1282.\n",
      "For Iteration 160 the Loss is 0.1279.\n",
      "For Iteration 161 the Loss is 0.1275.\n",
      "For Iteration 162 the Loss is 0.1271.\n",
      "For Iteration 163 the Loss is 0.1268.\n",
      "For Iteration 164 the Loss is 0.1264.\n",
      "For Iteration 165 the Loss is 0.126.\n",
      "For Iteration 166 the Loss is 0.1257.\n",
      "For Iteration 167 the Loss is 0.1253.\n",
      "For Iteration 168 the Loss is 0.125.\n",
      "For Iteration 169 the Loss is 0.1246.\n",
      "For Iteration 170 the Loss is 0.1243.\n",
      "For Iteration 171 the Loss is 0.1239.\n",
      "For Iteration 172 the Loss is 0.1236.\n",
      "For Iteration 173 the Loss is 0.1232.\n",
      "For Iteration 174 the Loss is 0.1229.\n",
      "For Iteration 175 the Loss is 0.1225.\n",
      "For Iteration 176 the Loss is 0.1222.\n",
      "For Iteration 177 the Loss is 0.1219.\n",
      "For Iteration 178 the Loss is 0.1215.\n",
      "For Iteration 179 the Loss is 0.1212.\n",
      "For Iteration 180 the Loss is 0.1209.\n",
      "For Iteration 181 the Loss is 0.1205.\n",
      "For Iteration 182 the Loss is 0.1202.\n",
      "For Iteration 183 the Loss is 0.1199.\n",
      "For Iteration 184 the Loss is 0.1196.\n",
      "For Iteration 185 the Loss is 0.1192.\n",
      "For Iteration 186 the Loss is 0.1189.\n",
      "For Iteration 187 the Loss is 0.1186.\n",
      "For Iteration 188 the Loss is 0.1183.\n",
      "For Iteration 189 the Loss is 0.118.\n",
      "For Iteration 190 the Loss is 0.1177.\n",
      "For Iteration 191 the Loss is 0.1174.\n",
      "For Iteration 192 the Loss is 0.1171.\n",
      "For Iteration 193 the Loss is 0.1168.\n",
      "For Iteration 194 the Loss is 0.1165.\n",
      "For Iteration 195 the Loss is 0.1162.\n",
      "For Iteration 196 the Loss is 0.1159.\n",
      "For Iteration 197 the Loss is 0.1156.\n",
      "For Iteration 198 the Loss is 0.1153.\n",
      "For Iteration 199 the Loss is 0.115.\n",
      "For Iteration 200 the Loss is 0.1147.\n",
      "For Iteration 201 the Loss is 0.1144.\n",
      "For Iteration 202 the Loss is 0.1141.\n",
      "For Iteration 203 the Loss is 0.1138.\n",
      "For Iteration 204 the Loss is 0.1135.\n",
      "For Iteration 205 the Loss is 0.1132.\n",
      "For Iteration 206 the Loss is 0.113.\n",
      "For Iteration 207 the Loss is 0.1127.\n",
      "For Iteration 208 the Loss is 0.1124.\n",
      "For Iteration 209 the Loss is 0.1121.\n",
      "For Iteration 210 the Loss is 0.1119.\n",
      "For Iteration 211 the Loss is 0.1116.\n",
      "For Iteration 212 the Loss is 0.1113.\n",
      "For Iteration 213 the Loss is 0.111.\n",
      "For Iteration 214 the Loss is 0.1108.\n",
      "For Iteration 215 the Loss is 0.1105.\n",
      "For Iteration 216 the Loss is 0.1102.\n",
      "For Iteration 217 the Loss is 0.11.\n",
      "For Iteration 218 the Loss is 0.1097.\n",
      "For Iteration 219 the Loss is 0.1094.\n",
      "For Iteration 220 the Loss is 0.1092.\n",
      "For Iteration 221 the Loss is 0.1089.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 222 the Loss is 0.1087.\n",
      "For Iteration 223 the Loss is 0.1084.\n",
      "For Iteration 224 the Loss is 0.1082.\n",
      "For Iteration 225 the Loss is 0.1079.\n",
      "For Iteration 226 the Loss is 0.1077.\n",
      "For Iteration 227 the Loss is 0.1074.\n",
      "For Iteration 228 the Loss is 0.1072.\n",
      "For Iteration 229 the Loss is 0.1069.\n",
      "For Iteration 230 the Loss is 0.1067.\n",
      "For Iteration 231 the Loss is 0.1064.\n",
      "For Iteration 232 the Loss is 0.1062.\n",
      "For Iteration 233 the Loss is 0.106.\n",
      "For Iteration 234 the Loss is 0.1057.\n",
      "For Iteration 235 the Loss is 0.1055.\n",
      "For Iteration 236 the Loss is 0.1052.\n",
      "For Iteration 237 the Loss is 0.105.\n",
      "For Iteration 238 the Loss is 0.1048.\n",
      "For Iteration 239 the Loss is 0.1045.\n",
      "For Iteration 240 the Loss is 0.1043.\n",
      "For Iteration 241 the Loss is 0.1041.\n",
      "For Iteration 242 the Loss is 0.1038.\n",
      "For Iteration 243 the Loss is 0.1036.\n",
      "For Iteration 244 the Loss is 0.1034.\n",
      "For Iteration 245 the Loss is 0.1032.\n",
      "For Iteration 246 the Loss is 0.1029.\n",
      "For Iteration 247 the Loss is 0.1027.\n",
      "For Iteration 248 the Loss is 0.1025.\n",
      "For Iteration 249 the Loss is 0.1023.\n",
      "For Iteration 250 the Loss is 0.1021.\n",
      "For Iteration 251 the Loss is 0.1018.\n",
      "For Iteration 252 the Loss is 0.1016.\n",
      "For Iteration 253 the Loss is 0.1014.\n",
      "For Iteration 254 the Loss is 0.1012.\n",
      "For Iteration 255 the Loss is 0.101.\n",
      "For Iteration 256 the Loss is 0.1008.\n",
      "For Iteration 257 the Loss is 0.1006.\n",
      "For Iteration 258 the Loss is 0.1004.\n",
      "For Iteration 259 the Loss is 0.1001.\n",
      "For Iteration 260 the Loss is 0.0999.\n",
      "For Iteration 261 the Loss is 0.0997.\n",
      "For Iteration 262 the Loss is 0.0995.\n",
      "For Iteration 263 the Loss is 0.0993.\n",
      "For Iteration 264 the Loss is 0.0991.\n",
      "For Iteration 265 the Loss is 0.0989.\n",
      "For Iteration 266 the Loss is 0.0987.\n",
      "For Iteration 267 the Loss is 0.0985.\n",
      "For Iteration 268 the Loss is 0.0983.\n",
      "For Iteration 269 the Loss is 0.0981.\n",
      "For Iteration 270 the Loss is 0.0979.\n",
      "For Iteration 271 the Loss is 0.0977.\n",
      "For Iteration 272 the Loss is 0.0975.\n",
      "For Iteration 273 the Loss is 0.0973.\n",
      "For Iteration 274 the Loss is 0.0972.\n",
      "For Iteration 275 the Loss is 0.097.\n",
      "For Iteration 276 the Loss is 0.0968.\n",
      "For Iteration 277 the Loss is 0.0966.\n",
      "For Iteration 278 the Loss is 0.0964.\n",
      "For Iteration 279 the Loss is 0.0962.\n",
      "For Iteration 280 the Loss is 0.096.\n",
      "For Iteration 281 the Loss is 0.0958.\n",
      "For Iteration 282 the Loss is 0.0957.\n",
      "For Iteration 283 the Loss is 0.0955.\n",
      "For Iteration 284 the Loss is 0.0953.\n",
      "For Iteration 285 the Loss is 0.0951.\n",
      "For Iteration 286 the Loss is 0.0949.\n",
      "For Iteration 287 the Loss is 0.0947.\n",
      "For Iteration 288 the Loss is 0.0946.\n",
      "For Iteration 289 the Loss is 0.0944.\n",
      "For Iteration 290 the Loss is 0.0942.\n",
      "For Iteration 291 the Loss is 0.094.\n",
      "For Iteration 292 the Loss is 0.0939.\n",
      "For Iteration 293 the Loss is 0.0937.\n",
      "For Iteration 294 the Loss is 0.0935.\n",
      "For Iteration 295 the Loss is 0.0933.\n",
      "For Iteration 296 the Loss is 0.0932.\n",
      "For Iteration 297 the Loss is 0.093.\n",
      "For Iteration 298 the Loss is 0.0928.\n",
      "For Iteration 299 the Loss is 0.0927.\n",
      "For Iteration 300 the Loss is 0.0925.\n",
      "For Iteration 301 the Loss is 0.0923.\n",
      "For Iteration 302 the Loss is 0.0922.\n",
      "For Iteration 303 the Loss is 0.092.\n",
      "For Iteration 304 the Loss is 0.0918.\n",
      "For Iteration 305 the Loss is 0.0917.\n",
      "For Iteration 306 the Loss is 0.0915.\n",
      "For Iteration 307 the Loss is 0.0913.\n",
      "For Iteration 308 the Loss is 0.0912.\n",
      "For Iteration 309 the Loss is 0.091.\n",
      "For Iteration 310 the Loss is 0.0909.\n",
      "For Iteration 311 the Loss is 0.0907.\n",
      "For Iteration 312 the Loss is 0.0905.\n",
      "For Iteration 313 the Loss is 0.0904.\n",
      "For Iteration 314 the Loss is 0.0902.\n",
      "For Iteration 315 the Loss is 0.0901.\n",
      "For Iteration 316 the Loss is 0.0899.\n",
      "For Iteration 317 the Loss is 0.0898.\n",
      "For Iteration 318 the Loss is 0.0896.\n",
      "For Iteration 319 the Loss is 0.0895.\n",
      "For Iteration 320 the Loss is 0.0893.\n",
      "For Iteration 321 the Loss is 0.0891.\n",
      "For Iteration 322 the Loss is 0.089.\n",
      "For Iteration 323 the Loss is 0.0888.\n",
      "For Iteration 324 the Loss is 0.0887.\n",
      "For Iteration 325 the Loss is 0.0886.\n",
      "For Iteration 326 the Loss is 0.0884.\n",
      "For Iteration 327 the Loss is 0.0883.\n",
      "For Iteration 328 the Loss is 0.0881.\n",
      "For Iteration 329 the Loss is 0.088.\n",
      "For Iteration 330 the Loss is 0.0878.\n",
      "For Iteration 331 the Loss is 0.0877.\n",
      "For Iteration 332 the Loss is 0.0875.\n",
      "For Iteration 333 the Loss is 0.0874.\n",
      "For Iteration 334 the Loss is 0.0872.\n",
      "For Iteration 335 the Loss is 0.0871.\n",
      "For Iteration 336 the Loss is 0.087.\n",
      "For Iteration 337 the Loss is 0.0868.\n",
      "For Iteration 338 the Loss is 0.0867.\n",
      "For Iteration 339 the Loss is 0.0865.\n",
      "For Iteration 340 the Loss is 0.0864.\n",
      "For Iteration 341 the Loss is 0.0863.\n",
      "For Iteration 342 the Loss is 0.0861.\n",
      "For Iteration 343 the Loss is 0.086.\n",
      "For Iteration 344 the Loss is 0.0859.\n",
      "For Iteration 345 the Loss is 0.0857.\n",
      "For Iteration 346 the Loss is 0.0856.\n",
      "For Iteration 347 the Loss is 0.0855.\n",
      "For Iteration 348 the Loss is 0.0853.\n",
      "For Iteration 349 the Loss is 0.0852.\n",
      "For Iteration 350 the Loss is 0.0851.\n",
      "For Iteration 351 the Loss is 0.0849.\n",
      "For Iteration 352 the Loss is 0.0848.\n",
      "For Iteration 353 the Loss is 0.0847.\n",
      "For Iteration 354 the Loss is 0.0845.\n",
      "For Iteration 355 the Loss is 0.0844.\n",
      "For Iteration 356 the Loss is 0.0843.\n",
      "For Iteration 357 the Loss is 0.0841.\n",
      "For Iteration 358 the Loss is 0.084.\n",
      "For Iteration 359 the Loss is 0.0839.\n",
      "For Iteration 360 the Loss is 0.0838.\n",
      "For Iteration 361 the Loss is 0.0836.\n",
      "For Iteration 362 the Loss is 0.0835.\n",
      "For Iteration 363 the Loss is 0.0834.\n",
      "For Iteration 364 the Loss is 0.0833.\n",
      "For Iteration 365 the Loss is 0.0831.\n",
      "For Iteration 366 the Loss is 0.083.\n",
      "For Iteration 367 the Loss is 0.0829.\n",
      "For Iteration 368 the Loss is 0.0828.\n",
      "For Iteration 369 the Loss is 0.0827.\n",
      "For Iteration 370 the Loss is 0.0825.\n",
      "For Iteration 371 the Loss is 0.0824.\n",
      "For Iteration 372 the Loss is 0.0823.\n",
      "For Iteration 373 the Loss is 0.0822.\n",
      "For Iteration 374 the Loss is 0.0821.\n",
      "For Iteration 375 the Loss is 0.0819.\n",
      "For Iteration 376 the Loss is 0.0818.\n",
      "For Iteration 377 the Loss is 0.0817.\n",
      "For Iteration 378 the Loss is 0.0816.\n",
      "For Iteration 379 the Loss is 0.0815.\n",
      "For Iteration 380 the Loss is 0.0814.\n",
      "For Iteration 381 the Loss is 0.0812.\n",
      "For Iteration 382 the Loss is 0.0811.\n",
      "For Iteration 383 the Loss is 0.081.\n",
      "For Iteration 384 the Loss is 0.0809.\n",
      "For Iteration 385 the Loss is 0.0808.\n",
      "For Iteration 386 the Loss is 0.0807.\n",
      "For Iteration 387 the Loss is 0.0806.\n",
      "For Iteration 388 the Loss is 0.0805.\n",
      "For Iteration 389 the Loss is 0.0803.\n",
      "For Iteration 390 the Loss is 0.0802.\n",
      "For Iteration 391 the Loss is 0.0801.\n",
      "For Iteration 392 the Loss is 0.08.\n",
      "For Iteration 393 the Loss is 0.0799.\n",
      "For Iteration 394 the Loss is 0.0798.\n",
      "For Iteration 395 the Loss is 0.0797.\n",
      "For Iteration 396 the Loss is 0.0796.\n",
      "For Iteration 397 the Loss is 0.0795.\n",
      "For Iteration 398 the Loss is 0.0794.\n",
      "For Iteration 399 the Loss is 0.0793.\n",
      "For Iteration 400 the Loss is 0.0792.\n",
      "For Iteration 401 the Loss is 0.079.\n",
      "For Iteration 402 the Loss is 0.0789.\n",
      "For Iteration 403 the Loss is 0.0788.\n",
      "For Iteration 404 the Loss is 0.0787.\n",
      "For Iteration 405 the Loss is 0.0786.\n",
      "For Iteration 406 the Loss is 0.0785.\n",
      "For Iteration 407 the Loss is 0.0784.\n",
      "For Iteration 408 the Loss is 0.0783.\n",
      "For Iteration 409 the Loss is 0.0782.\n",
      "For Iteration 410 the Loss is 0.0781.\n",
      "For Iteration 411 the Loss is 0.078.\n",
      "For Iteration 412 the Loss is 0.0779.\n",
      "For Iteration 413 the Loss is 0.0778.\n",
      "For Iteration 414 the Loss is 0.0777.\n",
      "For Iteration 415 the Loss is 0.0776.\n",
      "For Iteration 416 the Loss is 0.0775.\n",
      "For Iteration 417 the Loss is 0.0774.\n",
      "For Iteration 418 the Loss is 0.0773.\n",
      "For Iteration 419 the Loss is 0.0772.\n",
      "For Iteration 420 the Loss is 0.0771.\n",
      "For Iteration 421 the Loss is 0.077.\n",
      "For Iteration 422 the Loss is 0.0769.\n",
      "For Iteration 423 the Loss is 0.0768.\n",
      "For Iteration 424 the Loss is 0.0767.\n",
      "For Iteration 425 the Loss is 0.0766.\n",
      "For Iteration 426 the Loss is 0.0766.\n",
      "For Iteration 427 the Loss is 0.0765.\n",
      "For Iteration 428 the Loss is 0.0764.\n",
      "For Iteration 429 the Loss is 0.0763.\n",
      "For Iteration 430 the Loss is 0.0762.\n",
      "For Iteration 431 the Loss is 0.0761.\n",
      "For Iteration 432 the Loss is 0.076.\n",
      "For Iteration 433 the Loss is 0.0759.\n",
      "For Iteration 434 the Loss is 0.0758.\n",
      "For Iteration 435 the Loss is 0.0757.\n",
      "For Iteration 436 the Loss is 0.0756.\n",
      "For Iteration 437 the Loss is 0.0755.\n",
      "For Iteration 438 the Loss is 0.0754.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 439 the Loss is 0.0753.\n",
      "For Iteration 440 the Loss is 0.0753.\n",
      "For Iteration 441 the Loss is 0.0752.\n",
      "For Iteration 442 the Loss is 0.0751.\n",
      "For Iteration 443 the Loss is 0.075.\n",
      "For Iteration 444 the Loss is 0.0749.\n",
      "For Iteration 445 the Loss is 0.0748.\n",
      "For Iteration 446 the Loss is 0.0747.\n",
      "For Iteration 447 the Loss is 0.0746.\n",
      "For Iteration 448 the Loss is 0.0746.\n",
      "For Iteration 449 the Loss is 0.0745.\n",
      "For Iteration 450 the Loss is 0.0744.\n",
      "For Iteration 451 the Loss is 0.0743.\n",
      "For Iteration 452 the Loss is 0.0742.\n",
      "For Iteration 453 the Loss is 0.0741.\n",
      "For Iteration 454 the Loss is 0.074.\n",
      "For Iteration 455 the Loss is 0.074.\n",
      "For Iteration 456 the Loss is 0.0739.\n",
      "For Iteration 457 the Loss is 0.0738.\n",
      "For Iteration 458 the Loss is 0.0737.\n",
      "For Iteration 459 the Loss is 0.0736.\n",
      "For Iteration 460 the Loss is 0.0735.\n",
      "For Iteration 461 the Loss is 0.0734.\n",
      "For Iteration 462 the Loss is 0.0734.\n",
      "For Iteration 463 the Loss is 0.0733.\n",
      "For Iteration 464 the Loss is 0.0732.\n",
      "For Iteration 465 the Loss is 0.0731.\n",
      "For Iteration 466 the Loss is 0.073.\n",
      "For Iteration 467 the Loss is 0.073.\n",
      "For Iteration 468 the Loss is 0.0729.\n",
      "For Iteration 469 the Loss is 0.0728.\n",
      "For Iteration 470 the Loss is 0.0727.\n",
      "For Iteration 471 the Loss is 0.0726.\n",
      "For Iteration 472 the Loss is 0.0726.\n",
      "For Iteration 473 the Loss is 0.0725.\n",
      "For Iteration 474 the Loss is 0.0724.\n",
      "For Iteration 475 the Loss is 0.0723.\n",
      "For Iteration 476 the Loss is 0.0722.\n",
      "For Iteration 477 the Loss is 0.0722.\n",
      "For Iteration 478 the Loss is 0.0721.\n",
      "For Iteration 479 the Loss is 0.072.\n",
      "For Iteration 480 the Loss is 0.0719.\n",
      "For Iteration 481 the Loss is 0.0719.\n",
      "For Iteration 482 the Loss is 0.0718.\n",
      "For Iteration 483 the Loss is 0.0717.\n",
      "For Iteration 484 the Loss is 0.0716.\n",
      "For Iteration 485 the Loss is 0.0715.\n",
      "For Iteration 486 the Loss is 0.0715.\n",
      "For Iteration 487 the Loss is 0.0714.\n",
      "For Iteration 488 the Loss is 0.0713.\n",
      "For Iteration 489 the Loss is 0.0712.\n",
      "For Iteration 490 the Loss is 0.0712.\n",
      "For Iteration 491 the Loss is 0.0711.\n",
      "For Iteration 492 the Loss is 0.071.\n",
      "For Iteration 493 the Loss is 0.071.\n",
      "For Iteration 494 the Loss is 0.0709.\n",
      "For Iteration 495 the Loss is 0.0708.\n",
      "For Iteration 496 the Loss is 0.0707.\n",
      "For Iteration 497 the Loss is 0.0707.\n",
      "For Iteration 498 the Loss is 0.0706.\n",
      "For Iteration 499 the Loss is 0.0705.\n",
      "For Iteration 500 the Loss is 0.0704.\n",
      "For Iteration 501 the Loss is 0.0704.\n",
      "For Iteration 502 the Loss is 0.0703.\n",
      "For Iteration 503 the Loss is 0.0702.\n",
      "For Iteration 504 the Loss is 0.0702.\n",
      "For Iteration 505 the Loss is 0.0701.\n",
      "For Iteration 506 the Loss is 0.07.\n",
      "For Iteration 507 the Loss is 0.0699.\n",
      "For Iteration 508 the Loss is 0.0699.\n",
      "For Iteration 509 the Loss is 0.0698.\n",
      "For Iteration 510 the Loss is 0.0697.\n",
      "For Iteration 511 the Loss is 0.0697.\n",
      "For Iteration 512 the Loss is 0.0696.\n",
      "For Iteration 513 the Loss is 0.0695.\n",
      "For Iteration 514 the Loss is 0.0695.\n",
      "For Iteration 515 the Loss is 0.0694.\n",
      "For Iteration 516 the Loss is 0.0693.\n",
      "For Iteration 517 the Loss is 0.0693.\n",
      "For Iteration 518 the Loss is 0.0692.\n",
      "For Iteration 519 the Loss is 0.0691.\n",
      "For Iteration 520 the Loss is 0.0691.\n",
      "For Iteration 521 the Loss is 0.069.\n",
      "For Iteration 522 the Loss is 0.0689.\n",
      "For Iteration 523 the Loss is 0.0689.\n",
      "For Iteration 524 the Loss is 0.0688.\n",
      "For Iteration 525 the Loss is 0.0687.\n",
      "For Iteration 526 the Loss is 0.0687.\n",
      "For Iteration 527 the Loss is 0.0686.\n",
      "For Iteration 528 the Loss is 0.0685.\n",
      "For Iteration 529 the Loss is 0.0685.\n",
      "For Iteration 530 the Loss is 0.0684.\n",
      "For Iteration 531 the Loss is 0.0683.\n",
      "For Iteration 532 the Loss is 0.0683.\n",
      "For Iteration 533 the Loss is 0.0682.\n",
      "For Iteration 534 the Loss is 0.0681.\n",
      "For Iteration 535 the Loss is 0.0681.\n",
      "For Iteration 536 the Loss is 0.068.\n",
      "For Iteration 537 the Loss is 0.068.\n",
      "For Iteration 538 the Loss is 0.0679.\n",
      "For Iteration 539 the Loss is 0.0678.\n",
      "For Iteration 540 the Loss is 0.0678.\n",
      "For Iteration 541 the Loss is 0.0677.\n",
      "For Iteration 542 the Loss is 0.0676.\n",
      "For Iteration 543 the Loss is 0.0676.\n",
      "For Iteration 544 the Loss is 0.0675.\n",
      "For Iteration 545 the Loss is 0.0675.\n",
      "For Iteration 546 the Loss is 0.0674.\n",
      "For Iteration 547 the Loss is 0.0673.\n",
      "For Iteration 548 the Loss is 0.0673.\n",
      "For Iteration 549 the Loss is 0.0672.\n",
      "For Iteration 550 the Loss is 0.0671.\n",
      "For Iteration 551 the Loss is 0.0671.\n",
      "For Iteration 552 the Loss is 0.067.\n",
      "For Iteration 553 the Loss is 0.067.\n",
      "For Iteration 554 the Loss is 0.0669.\n",
      "For Iteration 555 the Loss is 0.0668.\n",
      "For Iteration 556 the Loss is 0.0668.\n",
      "For Iteration 557 the Loss is 0.0667.\n",
      "For Iteration 558 the Loss is 0.0667.\n",
      "For Iteration 559 the Loss is 0.0666.\n",
      "For Iteration 560 the Loss is 0.0666.\n",
      "For Iteration 561 the Loss is 0.0665.\n",
      "For Iteration 562 the Loss is 0.0664.\n",
      "For Iteration 563 the Loss is 0.0664.\n",
      "For Iteration 564 the Loss is 0.0663.\n",
      "For Iteration 565 the Loss is 0.0663.\n",
      "For Iteration 566 the Loss is 0.0662.\n",
      "For Iteration 567 the Loss is 0.0661.\n",
      "For Iteration 568 the Loss is 0.0661.\n",
      "For Iteration 569 the Loss is 0.066.\n",
      "For Iteration 570 the Loss is 0.066.\n",
      "For Iteration 571 the Loss is 0.0659.\n",
      "For Iteration 572 the Loss is 0.0659.\n",
      "For Iteration 573 the Loss is 0.0658.\n",
      "For Iteration 574 the Loss is 0.0657.\n",
      "For Iteration 575 the Loss is 0.0657.\n",
      "For Iteration 576 the Loss is 0.0656.\n",
      "For Iteration 577 the Loss is 0.0656.\n",
      "For Iteration 578 the Loss is 0.0655.\n",
      "For Iteration 579 the Loss is 0.0655.\n",
      "For Iteration 580 the Loss is 0.0654.\n",
      "For Iteration 581 the Loss is 0.0654.\n",
      "For Iteration 582 the Loss is 0.0653.\n",
      "For Iteration 583 the Loss is 0.0653.\n",
      "For Iteration 584 the Loss is 0.0652.\n",
      "For Iteration 585 the Loss is 0.0651.\n",
      "For Iteration 586 the Loss is 0.0651.\n",
      "For Iteration 587 the Loss is 0.065.\n",
      "For Iteration 588 the Loss is 0.065.\n",
      "For Iteration 589 the Loss is 0.0649.\n",
      "For Iteration 590 the Loss is 0.0649.\n",
      "For Iteration 591 the Loss is 0.0648.\n",
      "For Iteration 592 the Loss is 0.0648.\n",
      "For Iteration 593 the Loss is 0.0647.\n",
      "For Iteration 594 the Loss is 0.0647.\n",
      "For Iteration 595 the Loss is 0.0646.\n",
      "For Iteration 596 the Loss is 0.0646.\n",
      "For Iteration 597 the Loss is 0.0645.\n",
      "For Iteration 598 the Loss is 0.0645.\n",
      "For Iteration 599 the Loss is 0.0644.\n",
      "For Iteration 600 the Loss is 0.0643.\n",
      "For Iteration 601 the Loss is 0.0643.\n",
      "For Iteration 602 the Loss is 0.0642.\n",
      "For Iteration 603 the Loss is 0.0642.\n",
      "For Iteration 604 the Loss is 0.0641.\n",
      "For Iteration 605 the Loss is 0.0641.\n",
      "For Iteration 606 the Loss is 0.064.\n",
      "For Iteration 607 the Loss is 0.064.\n",
      "For Iteration 608 the Loss is 0.0639.\n",
      "For Iteration 609 the Loss is 0.0639.\n",
      "For Iteration 610 the Loss is 0.0638.\n",
      "For Iteration 611 the Loss is 0.0638.\n",
      "For Iteration 612 the Loss is 0.0637.\n",
      "For Iteration 613 the Loss is 0.0637.\n",
      "For Iteration 614 the Loss is 0.0636.\n",
      "For Iteration 615 the Loss is 0.0636.\n",
      "For Iteration 616 the Loss is 0.0635.\n",
      "For Iteration 617 the Loss is 0.0635.\n",
      "For Iteration 618 the Loss is 0.0634.\n",
      "For Iteration 619 the Loss is 0.0634.\n",
      "For Iteration 620 the Loss is 0.0633.\n",
      "For Iteration 621 the Loss is 0.0633.\n",
      "For Iteration 622 the Loss is 0.0632.\n",
      "For Iteration 623 the Loss is 0.0632.\n",
      "For Iteration 624 the Loss is 0.0631.\n",
      "For Iteration 625 the Loss is 0.0631.\n",
      "For Iteration 626 the Loss is 0.063.\n",
      "For Iteration 627 the Loss is 0.063.\n",
      "For Iteration 628 the Loss is 0.063.\n",
      "For Iteration 629 the Loss is 0.0629.\n",
      "For Iteration 630 the Loss is 0.0629.\n",
      "For Iteration 631 the Loss is 0.0628.\n",
      "For Iteration 632 the Loss is 0.0628.\n",
      "For Iteration 633 the Loss is 0.0627.\n",
      "For Iteration 634 the Loss is 0.0627.\n",
      "For Iteration 635 the Loss is 0.0626.\n",
      "For Iteration 636 the Loss is 0.0626.\n",
      "For Iteration 637 the Loss is 0.0625.\n",
      "For Iteration 638 the Loss is 0.0625.\n",
      "For Iteration 639 the Loss is 0.0624.\n",
      "For Iteration 640 the Loss is 0.0624.\n",
      "For Iteration 641 the Loss is 0.0623.\n",
      "For Iteration 642 the Loss is 0.0623.\n",
      "For Iteration 643 the Loss is 0.0623.\n",
      "For Iteration 644 the Loss is 0.0622.\n",
      "For Iteration 645 the Loss is 0.0622.\n",
      "For Iteration 646 the Loss is 0.0621.\n",
      "For Iteration 647 the Loss is 0.0621.\n",
      "For Iteration 648 the Loss is 0.062.\n",
      "For Iteration 649 the Loss is 0.062.\n",
      "For Iteration 650 the Loss is 0.0619.\n",
      "For Iteration 651 the Loss is 0.0619.\n",
      "For Iteration 652 the Loss is 0.0618.\n",
      "For Iteration 653 the Loss is 0.0618.\n",
      "For Iteration 654 the Loss is 0.0618.\n",
      "For Iteration 655 the Loss is 0.0617.\n",
      "For Iteration 656 the Loss is 0.0617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 657 the Loss is 0.0616.\n",
      "For Iteration 658 the Loss is 0.0616.\n",
      "For Iteration 659 the Loss is 0.0615.\n",
      "For Iteration 660 the Loss is 0.0615.\n",
      "For Iteration 661 the Loss is 0.0614.\n",
      "For Iteration 662 the Loss is 0.0614.\n",
      "For Iteration 663 the Loss is 0.0614.\n",
      "For Iteration 664 the Loss is 0.0613.\n",
      "For Iteration 665 the Loss is 0.0613.\n",
      "For Iteration 666 the Loss is 0.0612.\n",
      "For Iteration 667 the Loss is 0.0612.\n",
      "For Iteration 668 the Loss is 0.0611.\n",
      "For Iteration 669 the Loss is 0.0611.\n",
      "For Iteration 670 the Loss is 0.0611.\n",
      "For Iteration 671 the Loss is 0.061.\n",
      "For Iteration 672 the Loss is 0.061.\n",
      "For Iteration 673 the Loss is 0.0609.\n",
      "For Iteration 674 the Loss is 0.0609.\n",
      "For Iteration 675 the Loss is 0.0608.\n",
      "For Iteration 676 the Loss is 0.0608.\n",
      "For Iteration 677 the Loss is 0.0608.\n",
      "For Iteration 678 the Loss is 0.0607.\n",
      "For Iteration 679 the Loss is 0.0607.\n",
      "For Iteration 680 the Loss is 0.0606.\n",
      "For Iteration 681 the Loss is 0.0606.\n",
      "For Iteration 682 the Loss is 0.0606.\n",
      "For Iteration 683 the Loss is 0.0605.\n",
      "For Iteration 684 the Loss is 0.0605.\n",
      "For Iteration 685 the Loss is 0.0604.\n",
      "For Iteration 686 the Loss is 0.0604.\n",
      "For Iteration 687 the Loss is 0.0603.\n",
      "For Iteration 688 the Loss is 0.0603.\n",
      "For Iteration 689 the Loss is 0.0603.\n",
      "For Iteration 690 the Loss is 0.0602.\n",
      "For Iteration 691 the Loss is 0.0602.\n",
      "For Iteration 692 the Loss is 0.0601.\n",
      "For Iteration 693 the Loss is 0.0601.\n",
      "For Iteration 694 the Loss is 0.0601.\n",
      "For Iteration 695 the Loss is 0.06.\n",
      "For Iteration 696 the Loss is 0.06.\n",
      "For Iteration 697 the Loss is 0.0599.\n",
      "For Iteration 698 the Loss is 0.0599.\n",
      "For Iteration 699 the Loss is 0.0599.\n",
      "For Iteration 700 the Loss is 0.0598.\n",
      "For Iteration 701 the Loss is 0.0598.\n",
      "For Iteration 702 the Loss is 0.0597.\n",
      "For Iteration 703 the Loss is 0.0597.\n",
      "For Iteration 704 the Loss is 0.0597.\n",
      "For Iteration 705 the Loss is 0.0596.\n",
      "For Iteration 706 the Loss is 0.0596.\n",
      "For Iteration 707 the Loss is 0.0596.\n",
      "For Iteration 708 the Loss is 0.0595.\n",
      "For Iteration 709 the Loss is 0.0595.\n",
      "For Iteration 710 the Loss is 0.0594.\n",
      "For Iteration 711 the Loss is 0.0594.\n",
      "For Iteration 712 the Loss is 0.0594.\n",
      "For Iteration 713 the Loss is 0.0593.\n",
      "For Iteration 714 the Loss is 0.0593.\n",
      "For Iteration 715 the Loss is 0.0592.\n",
      "For Iteration 716 the Loss is 0.0592.\n",
      "For Iteration 717 the Loss is 0.0592.\n",
      "For Iteration 718 the Loss is 0.0591.\n",
      "For Iteration 719 the Loss is 0.0591.\n",
      "For Iteration 720 the Loss is 0.0591.\n",
      "For Iteration 721 the Loss is 0.059.\n",
      "For Iteration 722 the Loss is 0.059.\n",
      "For Iteration 723 the Loss is 0.0589.\n",
      "For Iteration 724 the Loss is 0.0589.\n",
      "For Iteration 725 the Loss is 0.0589.\n",
      "For Iteration 726 the Loss is 0.0588.\n",
      "For Iteration 727 the Loss is 0.0588.\n",
      "For Iteration 728 the Loss is 0.0588.\n",
      "For Iteration 729 the Loss is 0.0587.\n",
      "For Iteration 730 the Loss is 0.0587.\n",
      "For Iteration 731 the Loss is 0.0586.\n",
      "For Iteration 732 the Loss is 0.0586.\n",
      "For Iteration 733 the Loss is 0.0586.\n",
      "For Iteration 734 the Loss is 0.0585.\n",
      "For Iteration 735 the Loss is 0.0585.\n",
      "For Iteration 736 the Loss is 0.0585.\n",
      "For Iteration 737 the Loss is 0.0584.\n",
      "For Iteration 738 the Loss is 0.0584.\n",
      "For Iteration 739 the Loss is 0.0584.\n",
      "For Iteration 740 the Loss is 0.0583.\n",
      "For Iteration 741 the Loss is 0.0583.\n",
      "For Iteration 742 the Loss is 0.0582.\n",
      "For Iteration 743 the Loss is 0.0582.\n",
      "For Iteration 744 the Loss is 0.0582.\n",
      "For Iteration 745 the Loss is 0.0581.\n",
      "For Iteration 746 the Loss is 0.0581.\n",
      "For Iteration 747 the Loss is 0.0581.\n",
      "For Iteration 748 the Loss is 0.058.\n",
      "For Iteration 749 the Loss is 0.058.\n",
      "For Iteration 750 the Loss is 0.058.\n",
      "For Iteration 751 the Loss is 0.0579.\n",
      "For Iteration 752 the Loss is 0.0579.\n",
      "For Iteration 753 the Loss is 0.0579.\n",
      "For Iteration 754 the Loss is 0.0578.\n",
      "For Iteration 755 the Loss is 0.0578.\n",
      "For Iteration 756 the Loss is 0.0578.\n",
      "For Iteration 757 the Loss is 0.0577.\n",
      "For Iteration 758 the Loss is 0.0577.\n",
      "For Iteration 759 the Loss is 0.0577.\n",
      "For Iteration 760 the Loss is 0.0576.\n",
      "For Iteration 761 the Loss is 0.0576.\n",
      "For Iteration 762 the Loss is 0.0575.\n",
      "For Iteration 763 the Loss is 0.0575.\n",
      "For Iteration 764 the Loss is 0.0575.\n",
      "For Iteration 765 the Loss is 0.0574.\n",
      "For Iteration 766 the Loss is 0.0574.\n",
      "For Iteration 767 the Loss is 0.0574.\n",
      "For Iteration 768 the Loss is 0.0573.\n",
      "For Iteration 769 the Loss is 0.0573.\n",
      "For Iteration 770 the Loss is 0.0573.\n",
      "For Iteration 771 the Loss is 0.0572.\n",
      "For Iteration 772 the Loss is 0.0572.\n",
      "For Iteration 773 the Loss is 0.0572.\n",
      "For Iteration 774 the Loss is 0.0571.\n",
      "For Iteration 775 the Loss is 0.0571.\n",
      "For Iteration 776 the Loss is 0.0571.\n",
      "For Iteration 777 the Loss is 0.057.\n",
      "For Iteration 778 the Loss is 0.057.\n",
      "For Iteration 779 the Loss is 0.057.\n",
      "For Iteration 780 the Loss is 0.0569.\n",
      "For Iteration 781 the Loss is 0.0569.\n",
      "For Iteration 782 the Loss is 0.0569.\n",
      "For Iteration 783 the Loss is 0.0568.\n",
      "For Iteration 784 the Loss is 0.0568.\n",
      "For Iteration 785 the Loss is 0.0568.\n",
      "For Iteration 786 the Loss is 0.0567.\n",
      "For Iteration 787 the Loss is 0.0567.\n",
      "For Iteration 788 the Loss is 0.0567.\n",
      "For Iteration 789 the Loss is 0.0567.\n",
      "For Iteration 790 the Loss is 0.0566.\n",
      "For Iteration 791 the Loss is 0.0566.\n",
      "For Iteration 792 the Loss is 0.0566.\n",
      "For Iteration 793 the Loss is 0.0565.\n",
      "For Iteration 794 the Loss is 0.0565.\n",
      "For Iteration 795 the Loss is 0.0565.\n",
      "For Iteration 796 the Loss is 0.0564.\n",
      "For Iteration 797 the Loss is 0.0564.\n",
      "For Iteration 798 the Loss is 0.0564.\n",
      "For Iteration 799 the Loss is 0.0563.\n",
      "For Iteration 800 the Loss is 0.0563.\n",
      "For Iteration 801 the Loss is 0.0563.\n",
      "For Iteration 802 the Loss is 0.0562.\n",
      "For Iteration 803 the Loss is 0.0562.\n",
      "For Iteration 804 the Loss is 0.0562.\n",
      "For Iteration 805 the Loss is 0.0561.\n",
      "For Iteration 806 the Loss is 0.0561.\n",
      "For Iteration 807 the Loss is 0.0561.\n",
      "For Iteration 808 the Loss is 0.0561.\n",
      "For Iteration 809 the Loss is 0.056.\n",
      "For Iteration 810 the Loss is 0.056.\n",
      "For Iteration 811 the Loss is 0.056.\n",
      "For Iteration 812 the Loss is 0.0559.\n",
      "For Iteration 813 the Loss is 0.0559.\n",
      "For Iteration 814 the Loss is 0.0559.\n",
      "For Iteration 815 the Loss is 0.0558.\n",
      "For Iteration 816 the Loss is 0.0558.\n",
      "For Iteration 817 the Loss is 0.0558.\n",
      "For Iteration 818 the Loss is 0.0557.\n",
      "For Iteration 819 the Loss is 0.0557.\n",
      "For Iteration 820 the Loss is 0.0557.\n",
      "For Iteration 821 the Loss is 0.0557.\n",
      "For Iteration 822 the Loss is 0.0556.\n",
      "For Iteration 823 the Loss is 0.0556.\n",
      "For Iteration 824 the Loss is 0.0556.\n",
      "For Iteration 825 the Loss is 0.0555.\n",
      "For Iteration 826 the Loss is 0.0555.\n",
      "For Iteration 827 the Loss is 0.0555.\n",
      "For Iteration 828 the Loss is 0.0554.\n",
      "For Iteration 829 the Loss is 0.0554.\n",
      "For Iteration 830 the Loss is 0.0554.\n",
      "For Iteration 831 the Loss is 0.0554.\n",
      "For Iteration 832 the Loss is 0.0553.\n",
      "For Iteration 833 the Loss is 0.0553.\n",
      "For Iteration 834 the Loss is 0.0553.\n",
      "For Iteration 835 the Loss is 0.0552.\n",
      "For Iteration 836 the Loss is 0.0552.\n",
      "For Iteration 837 the Loss is 0.0552.\n",
      "For Iteration 838 the Loss is 0.0551.\n",
      "For Iteration 839 the Loss is 0.0551.\n",
      "For Iteration 840 the Loss is 0.0551.\n",
      "For Iteration 841 the Loss is 0.0551.\n",
      "For Iteration 842 the Loss is 0.055.\n",
      "For Iteration 843 the Loss is 0.055.\n",
      "For Iteration 844 the Loss is 0.055.\n",
      "For Iteration 845 the Loss is 0.0549.\n",
      "For Iteration 846 the Loss is 0.0549.\n",
      "For Iteration 847 the Loss is 0.0549.\n",
      "For Iteration 848 the Loss is 0.0549.\n",
      "For Iteration 849 the Loss is 0.0548.\n",
      "For Iteration 850 the Loss is 0.0548.\n",
      "For Iteration 851 the Loss is 0.0548.\n",
      "For Iteration 852 the Loss is 0.0547.\n",
      "For Iteration 853 the Loss is 0.0547.\n",
      "For Iteration 854 the Loss is 0.0547.\n",
      "For Iteration 855 the Loss is 0.0547.\n",
      "For Iteration 856 the Loss is 0.0546.\n",
      "For Iteration 857 the Loss is 0.0546.\n",
      "For Iteration 858 the Loss is 0.0546.\n",
      "For Iteration 859 the Loss is 0.0545.\n",
      "For Iteration 860 the Loss is 0.0545.\n",
      "For Iteration 861 the Loss is 0.0545.\n",
      "For Iteration 862 the Loss is 0.0545.\n",
      "For Iteration 863 the Loss is 0.0544.\n",
      "For Iteration 864 the Loss is 0.0544.\n",
      "For Iteration 865 the Loss is 0.0544.\n",
      "For Iteration 866 the Loss is 0.0543.\n",
      "For Iteration 867 the Loss is 0.0543.\n",
      "For Iteration 868 the Loss is 0.0543.\n",
      "For Iteration 869 the Loss is 0.0543.\n",
      "For Iteration 870 the Loss is 0.0542.\n",
      "For Iteration 871 the Loss is 0.0542.\n",
      "For Iteration 872 the Loss is 0.0542.\n",
      "For Iteration 873 the Loss is 0.0542.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 874 the Loss is 0.0541.\n",
      "For Iteration 875 the Loss is 0.0541.\n",
      "For Iteration 876 the Loss is 0.0541.\n",
      "For Iteration 877 the Loss is 0.054.\n",
      "For Iteration 878 the Loss is 0.054.\n",
      "For Iteration 879 the Loss is 0.054.\n",
      "For Iteration 880 the Loss is 0.054.\n",
      "For Iteration 881 the Loss is 0.0539.\n",
      "For Iteration 882 the Loss is 0.0539.\n",
      "For Iteration 883 the Loss is 0.0539.\n",
      "For Iteration 884 the Loss is 0.0539.\n",
      "For Iteration 885 the Loss is 0.0538.\n",
      "For Iteration 886 the Loss is 0.0538.\n",
      "For Iteration 887 the Loss is 0.0538.\n",
      "For Iteration 888 the Loss is 0.0537.\n",
      "For Iteration 889 the Loss is 0.0537.\n",
      "For Iteration 890 the Loss is 0.0537.\n",
      "For Iteration 891 the Loss is 0.0537.\n",
      "For Iteration 892 the Loss is 0.0536.\n",
      "For Iteration 893 the Loss is 0.0536.\n",
      "For Iteration 894 the Loss is 0.0536.\n",
      "For Iteration 895 the Loss is 0.0536.\n",
      "For Iteration 896 the Loss is 0.0535.\n",
      "For Iteration 897 the Loss is 0.0535.\n",
      "For Iteration 898 the Loss is 0.0535.\n",
      "For Iteration 899 the Loss is 0.0535.\n",
      "For Iteration 900 the Loss is 0.0534.\n",
      "For Iteration 901 the Loss is 0.0534.\n",
      "For Iteration 902 the Loss is 0.0534.\n",
      "For Iteration 903 the Loss is 0.0534.\n",
      "For Iteration 904 the Loss is 0.0533.\n",
      "For Iteration 905 the Loss is 0.0533.\n",
      "For Iteration 906 the Loss is 0.0533.\n",
      "For Iteration 907 the Loss is 0.0532.\n",
      "For Iteration 908 the Loss is 0.0532.\n",
      "For Iteration 909 the Loss is 0.0532.\n",
      "For Iteration 910 the Loss is 0.0532.\n",
      "For Iteration 911 the Loss is 0.0531.\n",
      "For Iteration 912 the Loss is 0.0531.\n",
      "For Iteration 913 the Loss is 0.0531.\n",
      "For Iteration 914 the Loss is 0.0531.\n",
      "For Iteration 915 the Loss is 0.053.\n",
      "For Iteration 916 the Loss is 0.053.\n",
      "For Iteration 917 the Loss is 0.053.\n",
      "For Iteration 918 the Loss is 0.053.\n",
      "For Iteration 919 the Loss is 0.0529.\n",
      "For Iteration 920 the Loss is 0.0529.\n",
      "For Iteration 921 the Loss is 0.0529.\n",
      "For Iteration 922 the Loss is 0.0529.\n",
      "For Iteration 923 the Loss is 0.0528.\n",
      "For Iteration 924 the Loss is 0.0528.\n",
      "For Iteration 925 the Loss is 0.0528.\n",
      "For Iteration 926 the Loss is 0.0528.\n",
      "For Iteration 927 the Loss is 0.0527.\n",
      "For Iteration 928 the Loss is 0.0527.\n",
      "For Iteration 929 the Loss is 0.0527.\n",
      "For Iteration 930 the Loss is 0.0527.\n",
      "For Iteration 931 the Loss is 0.0526.\n",
      "For Iteration 932 the Loss is 0.0526.\n",
      "For Iteration 933 the Loss is 0.0526.\n",
      "For Iteration 934 the Loss is 0.0526.\n",
      "For Iteration 935 the Loss is 0.0525.\n",
      "For Iteration 936 the Loss is 0.0525.\n",
      "For Iteration 937 the Loss is 0.0525.\n",
      "For Iteration 938 the Loss is 0.0525.\n",
      "For Iteration 939 the Loss is 0.0524.\n",
      "For Iteration 940 the Loss is 0.0524.\n",
      "For Iteration 941 the Loss is 0.0524.\n",
      "For Iteration 942 the Loss is 0.0524.\n",
      "For Iteration 943 the Loss is 0.0523.\n",
      "For Iteration 944 the Loss is 0.0523.\n",
      "For Iteration 945 the Loss is 0.0523.\n",
      "For Iteration 946 the Loss is 0.0523.\n",
      "For Iteration 947 the Loss is 0.0522.\n",
      "For Iteration 948 the Loss is 0.0522.\n",
      "For Iteration 949 the Loss is 0.0522.\n",
      "For Iteration 950 the Loss is 0.0522.\n",
      "For Iteration 951 the Loss is 0.0522.\n",
      "For Iteration 952 the Loss is 0.0521.\n",
      "For Iteration 953 the Loss is 0.0521.\n",
      "For Iteration 954 the Loss is 0.0521.\n",
      "For Iteration 955 the Loss is 0.0521.\n",
      "For Iteration 956 the Loss is 0.052.\n",
      "For Iteration 957 the Loss is 0.052.\n",
      "For Iteration 958 the Loss is 0.052.\n",
      "For Iteration 959 the Loss is 0.052.\n",
      "For Iteration 960 the Loss is 0.0519.\n",
      "For Iteration 961 the Loss is 0.0519.\n",
      "For Iteration 962 the Loss is 0.0519.\n",
      "For Iteration 963 the Loss is 0.0519.\n",
      "For Iteration 964 the Loss is 0.0518.\n",
      "For Iteration 965 the Loss is 0.0518.\n",
      "For Iteration 966 the Loss is 0.0518.\n",
      "For Iteration 967 the Loss is 0.0518.\n",
      "For Iteration 968 the Loss is 0.0517.\n",
      "For Iteration 969 the Loss is 0.0517.\n",
      "For Iteration 970 the Loss is 0.0517.\n",
      "For Iteration 971 the Loss is 0.0517.\n",
      "For Iteration 972 the Loss is 0.0517.\n",
      "For Iteration 973 the Loss is 0.0516.\n",
      "For Iteration 974 the Loss is 0.0516.\n",
      "For Iteration 975 the Loss is 0.0516.\n",
      "For Iteration 976 the Loss is 0.0516.\n",
      "For Iteration 977 the Loss is 0.0515.\n",
      "For Iteration 978 the Loss is 0.0515.\n",
      "For Iteration 979 the Loss is 0.0515.\n",
      "For Iteration 980 the Loss is 0.0515.\n",
      "For Iteration 981 the Loss is 0.0514.\n",
      "For Iteration 982 the Loss is 0.0514.\n",
      "For Iteration 983 the Loss is 0.0514.\n",
      "For Iteration 984 the Loss is 0.0514.\n",
      "For Iteration 985 the Loss is 0.0514.\n",
      "For Iteration 986 the Loss is 0.0513.\n",
      "For Iteration 987 the Loss is 0.0513.\n",
      "For Iteration 988 the Loss is 0.0513.\n",
      "For Iteration 989 the Loss is 0.0513.\n",
      "For Iteration 990 the Loss is 0.0512.\n",
      "For Iteration 991 the Loss is 0.0512.\n",
      "For Iteration 992 the Loss is 0.0512.\n",
      "For Iteration 993 the Loss is 0.0512.\n",
      "For Iteration 994 the Loss is 0.0511.\n",
      "For Iteration 995 the Loss is 0.0511.\n",
      "For Iteration 996 the Loss is 0.0511.\n",
      "For Iteration 997 the Loss is 0.0511.\n",
      "For Iteration 998 the Loss is 0.0511.\n",
      "For Iteration 999 the Loss is 0.051.\n",
      "For Iteration 1000 the Loss is 0.051.\n",
      "For Iteration 1001 the Loss is 0.051.\n",
      "For Iteration 1002 the Loss is 0.051.\n",
      "For Iteration 1003 the Loss is 0.0509.\n",
      "For Iteration 1004 the Loss is 0.0509.\n",
      "For Iteration 1005 the Loss is 0.0509.\n",
      "For Iteration 1006 the Loss is 0.0509.\n",
      "For Iteration 1007 the Loss is 0.0509.\n",
      "For Iteration 1008 the Loss is 0.0508.\n",
      "For Iteration 1009 the Loss is 0.0508.\n",
      "For Iteration 1010 the Loss is 0.0508.\n",
      "For Iteration 1011 the Loss is 0.0508.\n",
      "For Iteration 1012 the Loss is 0.0507.\n",
      "For Iteration 1013 the Loss is 0.0507.\n",
      "For Iteration 1014 the Loss is 0.0507.\n",
      "For Iteration 1015 the Loss is 0.0507.\n",
      "For Iteration 1016 the Loss is 0.0507.\n",
      "For Iteration 1017 the Loss is 0.0506.\n",
      "For Iteration 1018 the Loss is 0.0506.\n",
      "For Iteration 1019 the Loss is 0.0506.\n",
      "For Iteration 1020 the Loss is 0.0506.\n",
      "For Iteration 1021 the Loss is 0.0505.\n",
      "For Iteration 1022 the Loss is 0.0505.\n",
      "For Iteration 1023 the Loss is 0.0505.\n",
      "For Iteration 1024 the Loss is 0.0505.\n",
      "For Iteration 1025 the Loss is 0.0505.\n",
      "For Iteration 1026 the Loss is 0.0504.\n",
      "For Iteration 1027 the Loss is 0.0504.\n",
      "For Iteration 1028 the Loss is 0.0504.\n",
      "For Iteration 1029 the Loss is 0.0504.\n",
      "For Iteration 1030 the Loss is 0.0503.\n",
      "For Iteration 1031 the Loss is 0.0503.\n",
      "For Iteration 1032 the Loss is 0.0503.\n",
      "For Iteration 1033 the Loss is 0.0503.\n",
      "For Iteration 1034 the Loss is 0.0503.\n",
      "For Iteration 1035 the Loss is 0.0502.\n",
      "For Iteration 1036 the Loss is 0.0502.\n",
      "For Iteration 1037 the Loss is 0.0502.\n",
      "For Iteration 1038 the Loss is 0.0502.\n",
      "For Iteration 1039 the Loss is 0.0502.\n",
      "For Iteration 1040 the Loss is 0.0501.\n",
      "For Iteration 1041 the Loss is 0.0501.\n",
      "For Iteration 1042 the Loss is 0.0501.\n",
      "For Iteration 1043 the Loss is 0.0501.\n",
      "For Iteration 1044 the Loss is 0.0501.\n",
      "For Iteration 1045 the Loss is 0.05.\n",
      "For Iteration 1046 the Loss is 0.05.\n",
      "For Iteration 1047 the Loss is 0.05.\n",
      "For Iteration 1048 the Loss is 0.05.\n",
      "For Iteration 1049 the Loss is 0.0499.\n",
      "For Iteration 1050 the Loss is 0.0499.\n",
      "For Iteration 1051 the Loss is 0.0499.\n",
      "For Iteration 1052 the Loss is 0.0499.\n",
      "For Iteration 1053 the Loss is 0.0499.\n",
      "For Iteration 1054 the Loss is 0.0498.\n",
      "For Iteration 1055 the Loss is 0.0498.\n",
      "For Iteration 1056 the Loss is 0.0498.\n",
      "For Iteration 1057 the Loss is 0.0498.\n",
      "For Iteration 1058 the Loss is 0.0498.\n",
      "For Iteration 1059 the Loss is 0.0497.\n",
      "For Iteration 1060 the Loss is 0.0497.\n",
      "For Iteration 1061 the Loss is 0.0497.\n",
      "For Iteration 1062 the Loss is 0.0497.\n",
      "For Iteration 1063 the Loss is 0.0497.\n",
      "For Iteration 1064 the Loss is 0.0496.\n",
      "For Iteration 1065 the Loss is 0.0496.\n",
      "For Iteration 1066 the Loss is 0.0496.\n",
      "For Iteration 1067 the Loss is 0.0496.\n",
      "For Iteration 1068 the Loss is 0.0495.\n",
      "For Iteration 1069 the Loss is 0.0495.\n",
      "For Iteration 1070 the Loss is 0.0495.\n",
      "For Iteration 1071 the Loss is 0.0495.\n",
      "For Iteration 1072 the Loss is 0.0495.\n",
      "For Iteration 1073 the Loss is 0.0494.\n",
      "For Iteration 1074 the Loss is 0.0494.\n",
      "For Iteration 1075 the Loss is 0.0494.\n",
      "For Iteration 1076 the Loss is 0.0494.\n",
      "For Iteration 1077 the Loss is 0.0494.\n",
      "For Iteration 1078 the Loss is 0.0493.\n",
      "For Iteration 1079 the Loss is 0.0493.\n",
      "For Iteration 1080 the Loss is 0.0493.\n",
      "For Iteration 1081 the Loss is 0.0493.\n",
      "For Iteration 1082 the Loss is 0.0493.\n",
      "For Iteration 1083 the Loss is 0.0492.\n",
      "For Iteration 1084 the Loss is 0.0492.\n",
      "For Iteration 1085 the Loss is 0.0492.\n",
      "For Iteration 1086 the Loss is 0.0492.\n",
      "For Iteration 1087 the Loss is 0.0492.\n",
      "For Iteration 1088 the Loss is 0.0491.\n",
      "For Iteration 1089 the Loss is 0.0491.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1090 the Loss is 0.0491.\n",
      "For Iteration 1091 the Loss is 0.0491.\n",
      "For Iteration 1092 the Loss is 0.0491.\n",
      "For Iteration 1093 the Loss is 0.049.\n",
      "For Iteration 1094 the Loss is 0.049.\n",
      "For Iteration 1095 the Loss is 0.049.\n",
      "For Iteration 1096 the Loss is 0.049.\n",
      "For Iteration 1097 the Loss is 0.049.\n",
      "For Iteration 1098 the Loss is 0.0489.\n",
      "For Iteration 1099 the Loss is 0.0489.\n",
      "For Iteration 1100 the Loss is 0.0489.\n",
      "For Iteration 1101 the Loss is 0.0489.\n",
      "For Iteration 1102 the Loss is 0.0489.\n",
      "For Iteration 1103 the Loss is 0.0488.\n",
      "For Iteration 1104 the Loss is 0.0488.\n",
      "For Iteration 1105 the Loss is 0.0488.\n",
      "For Iteration 1106 the Loss is 0.0488.\n",
      "For Iteration 1107 the Loss is 0.0488.\n",
      "For Iteration 1108 the Loss is 0.0487.\n",
      "For Iteration 1109 the Loss is 0.0487.\n",
      "For Iteration 1110 the Loss is 0.0487.\n",
      "For Iteration 1111 the Loss is 0.0487.\n",
      "For Iteration 1112 the Loss is 0.0487.\n",
      "For Iteration 1113 the Loss is 0.0486.\n",
      "For Iteration 1114 the Loss is 0.0486.\n",
      "For Iteration 1115 the Loss is 0.0486.\n",
      "For Iteration 1116 the Loss is 0.0486.\n",
      "For Iteration 1117 the Loss is 0.0486.\n",
      "For Iteration 1118 the Loss is 0.0486.\n",
      "For Iteration 1119 the Loss is 0.0485.\n",
      "For Iteration 1120 the Loss is 0.0485.\n",
      "For Iteration 1121 the Loss is 0.0485.\n",
      "For Iteration 1122 the Loss is 0.0485.\n",
      "For Iteration 1123 the Loss is 0.0485.\n",
      "For Iteration 1124 the Loss is 0.0484.\n",
      "For Iteration 1125 the Loss is 0.0484.\n",
      "For Iteration 1126 the Loss is 0.0484.\n",
      "For Iteration 1127 the Loss is 0.0484.\n",
      "For Iteration 1128 the Loss is 0.0484.\n",
      "For Iteration 1129 the Loss is 0.0483.\n",
      "For Iteration 1130 the Loss is 0.0483.\n",
      "For Iteration 1131 the Loss is 0.0483.\n",
      "For Iteration 1132 the Loss is 0.0483.\n",
      "For Iteration 1133 the Loss is 0.0483.\n",
      "For Iteration 1134 the Loss is 0.0482.\n",
      "For Iteration 1135 the Loss is 0.0482.\n",
      "For Iteration 1136 the Loss is 0.0482.\n",
      "For Iteration 1137 the Loss is 0.0482.\n",
      "For Iteration 1138 the Loss is 0.0482.\n",
      "For Iteration 1139 the Loss is 0.0481.\n",
      "For Iteration 1140 the Loss is 0.0481.\n",
      "For Iteration 1141 the Loss is 0.0481.\n",
      "For Iteration 1142 the Loss is 0.0481.\n",
      "For Iteration 1143 the Loss is 0.0481.\n",
      "For Iteration 1144 the Loss is 0.0481.\n",
      "For Iteration 1145 the Loss is 0.048.\n",
      "For Iteration 1146 the Loss is 0.048.\n",
      "For Iteration 1147 the Loss is 0.048.\n",
      "For Iteration 1148 the Loss is 0.048.\n",
      "For Iteration 1149 the Loss is 0.048.\n",
      "For Iteration 1150 the Loss is 0.0479.\n",
      "For Iteration 1151 the Loss is 0.0479.\n",
      "For Iteration 1152 the Loss is 0.0479.\n",
      "For Iteration 1153 the Loss is 0.0479.\n",
      "For Iteration 1154 the Loss is 0.0479.\n",
      "For Iteration 1155 the Loss is 0.0478.\n",
      "For Iteration 1156 the Loss is 0.0478.\n",
      "For Iteration 1157 the Loss is 0.0478.\n",
      "For Iteration 1158 the Loss is 0.0478.\n",
      "For Iteration 1159 the Loss is 0.0478.\n",
      "For Iteration 1160 the Loss is 0.0478.\n",
      "For Iteration 1161 the Loss is 0.0477.\n",
      "For Iteration 1162 the Loss is 0.0477.\n",
      "For Iteration 1163 the Loss is 0.0477.\n",
      "For Iteration 1164 the Loss is 0.0477.\n",
      "For Iteration 1165 the Loss is 0.0477.\n",
      "For Iteration 1166 the Loss is 0.0476.\n",
      "For Iteration 1167 the Loss is 0.0476.\n",
      "For Iteration 1168 the Loss is 0.0476.\n",
      "For Iteration 1169 the Loss is 0.0476.\n",
      "For Iteration 1170 the Loss is 0.0476.\n",
      "For Iteration 1171 the Loss is 0.0476.\n",
      "For Iteration 1172 the Loss is 0.0475.\n",
      "For Iteration 1173 the Loss is 0.0475.\n",
      "For Iteration 1174 the Loss is 0.0475.\n",
      "For Iteration 1175 the Loss is 0.0475.\n",
      "For Iteration 1176 the Loss is 0.0475.\n",
      "For Iteration 1177 the Loss is 0.0474.\n",
      "For Iteration 1178 the Loss is 0.0474.\n",
      "For Iteration 1179 the Loss is 0.0474.\n",
      "For Iteration 1180 the Loss is 0.0474.\n",
      "For Iteration 1181 the Loss is 0.0474.\n",
      "For Iteration 1182 the Loss is 0.0473.\n",
      "For Iteration 1183 the Loss is 0.0473.\n",
      "For Iteration 1184 the Loss is 0.0473.\n",
      "For Iteration 1185 the Loss is 0.0473.\n",
      "For Iteration 1186 the Loss is 0.0473.\n",
      "For Iteration 1187 the Loss is 0.0473.\n",
      "For Iteration 1188 the Loss is 0.0472.\n",
      "For Iteration 1189 the Loss is 0.0472.\n",
      "For Iteration 1190 the Loss is 0.0472.\n",
      "For Iteration 1191 the Loss is 0.0472.\n",
      "For Iteration 1192 the Loss is 0.0472.\n",
      "For Iteration 1193 the Loss is 0.0472.\n",
      "For Iteration 1194 the Loss is 0.0471.\n",
      "For Iteration 1195 the Loss is 0.0471.\n",
      "For Iteration 1196 the Loss is 0.0471.\n",
      "For Iteration 1197 the Loss is 0.0471.\n",
      "For Iteration 1198 the Loss is 0.0471.\n",
      "For Iteration 1199 the Loss is 0.047.\n",
      "For Iteration 1200 the Loss is 0.047.\n",
      "For Iteration 1201 the Loss is 0.047.\n",
      "For Iteration 1202 the Loss is 0.047.\n",
      "For Iteration 1203 the Loss is 0.047.\n",
      "For Iteration 1204 the Loss is 0.047.\n",
      "For Iteration 1205 the Loss is 0.0469.\n",
      "For Iteration 1206 the Loss is 0.0469.\n",
      "For Iteration 1207 the Loss is 0.0469.\n",
      "For Iteration 1208 the Loss is 0.0469.\n",
      "For Iteration 1209 the Loss is 0.0469.\n",
      "For Iteration 1210 the Loss is 0.0468.\n",
      "For Iteration 1211 the Loss is 0.0468.\n",
      "For Iteration 1212 the Loss is 0.0468.\n",
      "For Iteration 1213 the Loss is 0.0468.\n",
      "For Iteration 1214 the Loss is 0.0468.\n",
      "For Iteration 1215 the Loss is 0.0468.\n",
      "For Iteration 1216 the Loss is 0.0467.\n",
      "For Iteration 1217 the Loss is 0.0467.\n",
      "For Iteration 1218 the Loss is 0.0467.\n",
      "For Iteration 1219 the Loss is 0.0467.\n",
      "For Iteration 1220 the Loss is 0.0467.\n",
      "For Iteration 1221 the Loss is 0.0467.\n",
      "For Iteration 1222 the Loss is 0.0466.\n",
      "For Iteration 1223 the Loss is 0.0466.\n",
      "For Iteration 1224 the Loss is 0.0466.\n",
      "For Iteration 1225 the Loss is 0.0466.\n",
      "For Iteration 1226 the Loss is 0.0466.\n",
      "For Iteration 1227 the Loss is 0.0466.\n",
      "For Iteration 1228 the Loss is 0.0465.\n",
      "For Iteration 1229 the Loss is 0.0465.\n",
      "For Iteration 1230 the Loss is 0.0465.\n",
      "For Iteration 1231 the Loss is 0.0465.\n",
      "For Iteration 1232 the Loss is 0.0465.\n",
      "For Iteration 1233 the Loss is 0.0464.\n",
      "For Iteration 1234 the Loss is 0.0464.\n",
      "For Iteration 1235 the Loss is 0.0464.\n",
      "For Iteration 1236 the Loss is 0.0464.\n",
      "For Iteration 1237 the Loss is 0.0464.\n",
      "For Iteration 1238 the Loss is 0.0464.\n",
      "For Iteration 1239 the Loss is 0.0463.\n",
      "For Iteration 1240 the Loss is 0.0463.\n",
      "For Iteration 1241 the Loss is 0.0463.\n",
      "For Iteration 1242 the Loss is 0.0463.\n",
      "For Iteration 1243 the Loss is 0.0463.\n",
      "For Iteration 1244 the Loss is 0.0463.\n",
      "For Iteration 1245 the Loss is 0.0462.\n",
      "For Iteration 1246 the Loss is 0.0462.\n",
      "For Iteration 1247 the Loss is 0.0462.\n",
      "For Iteration 1248 the Loss is 0.0462.\n",
      "For Iteration 1249 the Loss is 0.0462.\n",
      "For Iteration 1250 the Loss is 0.0462.\n",
      "For Iteration 1251 the Loss is 0.0461.\n",
      "For Iteration 1252 the Loss is 0.0461.\n",
      "For Iteration 1253 the Loss is 0.0461.\n",
      "For Iteration 1254 the Loss is 0.0461.\n",
      "For Iteration 1255 the Loss is 0.0461.\n",
      "For Iteration 1256 the Loss is 0.0461.\n",
      "For Iteration 1257 the Loss is 0.046.\n",
      "For Iteration 1258 the Loss is 0.046.\n",
      "For Iteration 1259 the Loss is 0.046.\n",
      "For Iteration 1260 the Loss is 0.046.\n",
      "For Iteration 1261 the Loss is 0.046.\n",
      "For Iteration 1262 the Loss is 0.046.\n",
      "For Iteration 1263 the Loss is 0.0459.\n",
      "For Iteration 1264 the Loss is 0.0459.\n",
      "For Iteration 1265 the Loss is 0.0459.\n",
      "For Iteration 1266 the Loss is 0.0459.\n",
      "For Iteration 1267 the Loss is 0.0459.\n",
      "For Iteration 1268 the Loss is 0.0459.\n",
      "For Iteration 1269 the Loss is 0.0458.\n",
      "For Iteration 1270 the Loss is 0.0458.\n",
      "For Iteration 1271 the Loss is 0.0458.\n",
      "For Iteration 1272 the Loss is 0.0458.\n",
      "For Iteration 1273 the Loss is 0.0458.\n",
      "For Iteration 1274 the Loss is 0.0457.\n",
      "For Iteration 1275 the Loss is 0.0457.\n",
      "For Iteration 1276 the Loss is 0.0457.\n",
      "For Iteration 1277 the Loss is 0.0457.\n",
      "For Iteration 1278 the Loss is 0.0457.\n",
      "For Iteration 1279 the Loss is 0.0457.\n",
      "For Iteration 1280 the Loss is 0.0456.\n",
      "For Iteration 1281 the Loss is 0.0456.\n",
      "For Iteration 1282 the Loss is 0.0456.\n",
      "For Iteration 1283 the Loss is 0.0456.\n",
      "For Iteration 1284 the Loss is 0.0456.\n",
      "For Iteration 1285 the Loss is 0.0456.\n",
      "For Iteration 1286 the Loss is 0.0456.\n",
      "For Iteration 1287 the Loss is 0.0455.\n",
      "For Iteration 1288 the Loss is 0.0455.\n",
      "For Iteration 1289 the Loss is 0.0455.\n",
      "For Iteration 1290 the Loss is 0.0455.\n",
      "For Iteration 1291 the Loss is 0.0455.\n",
      "For Iteration 1292 the Loss is 0.0455.\n",
      "For Iteration 1293 the Loss is 0.0454.\n",
      "For Iteration 1294 the Loss is 0.0454.\n",
      "For Iteration 1295 the Loss is 0.0454.\n",
      "For Iteration 1296 the Loss is 0.0454.\n",
      "For Iteration 1297 the Loss is 0.0454.\n",
      "For Iteration 1298 the Loss is 0.0454.\n",
      "For Iteration 1299 the Loss is 0.0453.\n",
      "For Iteration 1300 the Loss is 0.0453.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1301 the Loss is 0.0453.\n",
      "For Iteration 1302 the Loss is 0.0453.\n",
      "For Iteration 1303 the Loss is 0.0453.\n",
      "For Iteration 1304 the Loss is 0.0453.\n",
      "For Iteration 1305 the Loss is 0.0452.\n",
      "For Iteration 1306 the Loss is 0.0452.\n",
      "For Iteration 1307 the Loss is 0.0452.\n",
      "For Iteration 1308 the Loss is 0.0452.\n",
      "For Iteration 1309 the Loss is 0.0452.\n",
      "For Iteration 1310 the Loss is 0.0452.\n",
      "For Iteration 1311 the Loss is 0.0451.\n",
      "For Iteration 1312 the Loss is 0.0451.\n",
      "For Iteration 1313 the Loss is 0.0451.\n",
      "For Iteration 1314 the Loss is 0.0451.\n",
      "For Iteration 1315 the Loss is 0.0451.\n",
      "For Iteration 1316 the Loss is 0.0451.\n",
      "For Iteration 1317 the Loss is 0.045.\n",
      "For Iteration 1318 the Loss is 0.045.\n",
      "For Iteration 1319 the Loss is 0.045.\n",
      "For Iteration 1320 the Loss is 0.045.\n",
      "For Iteration 1321 the Loss is 0.045.\n",
      "For Iteration 1322 the Loss is 0.045.\n",
      "For Iteration 1323 the Loss is 0.0449.\n",
      "For Iteration 1324 the Loss is 0.0449.\n",
      "For Iteration 1325 the Loss is 0.0449.\n",
      "For Iteration 1326 the Loss is 0.0449.\n",
      "For Iteration 1327 the Loss is 0.0449.\n",
      "For Iteration 1328 the Loss is 0.0449.\n",
      "For Iteration 1329 the Loss is 0.0449.\n",
      "For Iteration 1330 the Loss is 0.0448.\n",
      "For Iteration 1331 the Loss is 0.0448.\n",
      "For Iteration 1332 the Loss is 0.0448.\n",
      "For Iteration 1333 the Loss is 0.0448.\n",
      "For Iteration 1334 the Loss is 0.0448.\n",
      "For Iteration 1335 the Loss is 0.0448.\n",
      "For Iteration 1336 the Loss is 0.0447.\n",
      "For Iteration 1337 the Loss is 0.0447.\n",
      "For Iteration 1338 the Loss is 0.0447.\n",
      "For Iteration 1339 the Loss is 0.0447.\n",
      "For Iteration 1340 the Loss is 0.0447.\n",
      "For Iteration 1341 the Loss is 0.0447.\n",
      "For Iteration 1342 the Loss is 0.0446.\n",
      "For Iteration 1343 the Loss is 0.0446.\n",
      "For Iteration 1344 the Loss is 0.0446.\n",
      "For Iteration 1345 the Loss is 0.0446.\n",
      "For Iteration 1346 the Loss is 0.0446.\n",
      "For Iteration 1347 the Loss is 0.0446.\n",
      "For Iteration 1348 the Loss is 0.0446.\n",
      "For Iteration 1349 the Loss is 0.0445.\n",
      "For Iteration 1350 the Loss is 0.0445.\n",
      "For Iteration 1351 the Loss is 0.0445.\n",
      "For Iteration 1352 the Loss is 0.0445.\n",
      "For Iteration 1353 the Loss is 0.0445.\n",
      "For Iteration 1354 the Loss is 0.0445.\n",
      "For Iteration 1355 the Loss is 0.0444.\n",
      "For Iteration 1356 the Loss is 0.0444.\n",
      "For Iteration 1357 the Loss is 0.0444.\n",
      "For Iteration 1358 the Loss is 0.0444.\n",
      "For Iteration 1359 the Loss is 0.0444.\n",
      "For Iteration 1360 the Loss is 0.0444.\n",
      "For Iteration 1361 the Loss is 0.0443.\n",
      "For Iteration 1362 the Loss is 0.0443.\n",
      "For Iteration 1363 the Loss is 0.0443.\n",
      "For Iteration 1364 the Loss is 0.0443.\n",
      "For Iteration 1365 the Loss is 0.0443.\n",
      "For Iteration 1366 the Loss is 0.0443.\n",
      "For Iteration 1367 the Loss is 0.0443.\n",
      "For Iteration 1368 the Loss is 0.0442.\n",
      "For Iteration 1369 the Loss is 0.0442.\n",
      "For Iteration 1370 the Loss is 0.0442.\n",
      "For Iteration 1371 the Loss is 0.0442.\n",
      "For Iteration 1372 the Loss is 0.0442.\n",
      "For Iteration 1373 the Loss is 0.0442.\n",
      "For Iteration 1374 the Loss is 0.0441.\n",
      "For Iteration 1375 the Loss is 0.0441.\n",
      "For Iteration 1376 the Loss is 0.0441.\n",
      "For Iteration 1377 the Loss is 0.0441.\n",
      "For Iteration 1378 the Loss is 0.0441.\n",
      "For Iteration 1379 the Loss is 0.0441.\n",
      "For Iteration 1380 the Loss is 0.0441.\n",
      "For Iteration 1381 the Loss is 0.044.\n",
      "For Iteration 1382 the Loss is 0.044.\n",
      "For Iteration 1383 the Loss is 0.044.\n",
      "For Iteration 1384 the Loss is 0.044.\n",
      "For Iteration 1385 the Loss is 0.044.\n",
      "For Iteration 1386 the Loss is 0.044.\n",
      "For Iteration 1387 the Loss is 0.0439.\n",
      "For Iteration 1388 the Loss is 0.0439.\n",
      "For Iteration 1389 the Loss is 0.0439.\n",
      "For Iteration 1390 the Loss is 0.0439.\n",
      "For Iteration 1391 the Loss is 0.0439.\n",
      "For Iteration 1392 the Loss is 0.0439.\n",
      "For Iteration 1393 the Loss is 0.0439.\n",
      "For Iteration 1394 the Loss is 0.0438.\n",
      "For Iteration 1395 the Loss is 0.0438.\n",
      "For Iteration 1396 the Loss is 0.0438.\n",
      "For Iteration 1397 the Loss is 0.0438.\n",
      "For Iteration 1398 the Loss is 0.0438.\n",
      "For Iteration 1399 the Loss is 0.0438.\n",
      "For Iteration 1400 the Loss is 0.0437.\n",
      "For Iteration 1401 the Loss is 0.0437.\n",
      "For Iteration 1402 the Loss is 0.0437.\n",
      "For Iteration 1403 the Loss is 0.0437.\n",
      "For Iteration 1404 the Loss is 0.0437.\n",
      "For Iteration 1405 the Loss is 0.0437.\n",
      "For Iteration 1406 the Loss is 0.0437.\n",
      "For Iteration 1407 the Loss is 0.0436.\n",
      "For Iteration 1408 the Loss is 0.0436.\n",
      "For Iteration 1409 the Loss is 0.0436.\n",
      "For Iteration 1410 the Loss is 0.0436.\n",
      "For Iteration 1411 the Loss is 0.0436.\n",
      "For Iteration 1412 the Loss is 0.0436.\n",
      "For Iteration 1413 the Loss is 0.0436.\n",
      "For Iteration 1414 the Loss is 0.0435.\n",
      "For Iteration 1415 the Loss is 0.0435.\n",
      "For Iteration 1416 the Loss is 0.0435.\n",
      "For Iteration 1417 the Loss is 0.0435.\n",
      "For Iteration 1418 the Loss is 0.0435.\n",
      "For Iteration 1419 the Loss is 0.0435.\n",
      "For Iteration 1420 the Loss is 0.0434.\n",
      "For Iteration 1421 the Loss is 0.0434.\n",
      "For Iteration 1422 the Loss is 0.0434.\n",
      "For Iteration 1423 the Loss is 0.0434.\n",
      "For Iteration 1424 the Loss is 0.0434.\n",
      "For Iteration 1425 the Loss is 0.0434.\n",
      "For Iteration 1426 the Loss is 0.0434.\n",
      "For Iteration 1427 the Loss is 0.0433.\n",
      "For Iteration 1428 the Loss is 0.0433.\n",
      "For Iteration 1429 the Loss is 0.0433.\n",
      "For Iteration 1430 the Loss is 0.0433.\n",
      "For Iteration 1431 the Loss is 0.0433.\n",
      "For Iteration 1432 the Loss is 0.0433.\n",
      "For Iteration 1433 the Loss is 0.0433.\n",
      "For Iteration 1434 the Loss is 0.0432.\n",
      "For Iteration 1435 the Loss is 0.0432.\n",
      "For Iteration 1436 the Loss is 0.0432.\n",
      "For Iteration 1437 the Loss is 0.0432.\n",
      "For Iteration 1438 the Loss is 0.0432.\n",
      "For Iteration 1439 the Loss is 0.0432.\n",
      "For Iteration 1440 the Loss is 0.0432.\n",
      "For Iteration 1441 the Loss is 0.0431.\n",
      "For Iteration 1442 the Loss is 0.0431.\n",
      "For Iteration 1443 the Loss is 0.0431.\n",
      "For Iteration 1444 the Loss is 0.0431.\n",
      "For Iteration 1445 the Loss is 0.0431.\n",
      "For Iteration 1446 the Loss is 0.0431.\n",
      "For Iteration 1447 the Loss is 0.043.\n",
      "For Iteration 1448 the Loss is 0.043.\n",
      "For Iteration 1449 the Loss is 0.043.\n",
      "For Iteration 1450 the Loss is 0.043.\n",
      "For Iteration 1451 the Loss is 0.043.\n",
      "For Iteration 1452 the Loss is 0.043.\n",
      "For Iteration 1453 the Loss is 0.043.\n",
      "For Iteration 1454 the Loss is 0.0429.\n",
      "For Iteration 1455 the Loss is 0.0429.\n",
      "For Iteration 1456 the Loss is 0.0429.\n",
      "For Iteration 1457 the Loss is 0.0429.\n",
      "For Iteration 1458 the Loss is 0.0429.\n",
      "For Iteration 1459 the Loss is 0.0429.\n",
      "For Iteration 1460 the Loss is 0.0429.\n",
      "For Iteration 1461 the Loss is 0.0428.\n",
      "For Iteration 1462 the Loss is 0.0428.\n",
      "For Iteration 1463 the Loss is 0.0428.\n",
      "For Iteration 1464 the Loss is 0.0428.\n",
      "For Iteration 1465 the Loss is 0.0428.\n",
      "For Iteration 1466 the Loss is 0.0428.\n",
      "For Iteration 1467 the Loss is 0.0428.\n",
      "For Iteration 1468 the Loss is 0.0427.\n",
      "For Iteration 1469 the Loss is 0.0427.\n",
      "For Iteration 1470 the Loss is 0.0427.\n",
      "For Iteration 1471 the Loss is 0.0427.\n",
      "For Iteration 1472 the Loss is 0.0427.\n",
      "For Iteration 1473 the Loss is 0.0427.\n",
      "For Iteration 1474 the Loss is 0.0427.\n",
      "For Iteration 1475 the Loss is 0.0426.\n",
      "For Iteration 1476 the Loss is 0.0426.\n",
      "For Iteration 1477 the Loss is 0.0426.\n",
      "For Iteration 1478 the Loss is 0.0426.\n",
      "For Iteration 1479 the Loss is 0.0426.\n",
      "For Iteration 1480 the Loss is 0.0426.\n",
      "For Iteration 1481 the Loss is 0.0426.\n",
      "For Iteration 1482 the Loss is 0.0425.\n",
      "For Iteration 1483 the Loss is 0.0425.\n",
      "For Iteration 1484 the Loss is 0.0425.\n",
      "For Iteration 1485 the Loss is 0.0425.\n",
      "For Iteration 1486 the Loss is 0.0425.\n",
      "For Iteration 1487 the Loss is 0.0425.\n",
      "For Iteration 1488 the Loss is 0.0425.\n",
      "For Iteration 1489 the Loss is 0.0424.\n",
      "For Iteration 1490 the Loss is 0.0424.\n",
      "For Iteration 1491 the Loss is 0.0424.\n",
      "For Iteration 1492 the Loss is 0.0424.\n",
      "For Iteration 1493 the Loss is 0.0424.\n",
      "For Iteration 1494 the Loss is 0.0424.\n",
      "For Iteration 1495 the Loss is 0.0424.\n",
      "For Iteration 1496 the Loss is 0.0423.\n",
      "For Iteration 1497 the Loss is 0.0423.\n",
      "For Iteration 1498 the Loss is 0.0423.\n",
      "For Iteration 1499 the Loss is 0.0423.\n",
      "For Iteration 1500 the Loss is 0.0423.\n",
      "For Iteration 1501 the Loss is 0.0423.\n",
      "For Iteration 1502 the Loss is 0.0423.\n",
      "For Iteration 1503 the Loss is 0.0422.\n",
      "For Iteration 1504 the Loss is 0.0422.\n",
      "For Iteration 1505 the Loss is 0.0422.\n",
      "For Iteration 1506 the Loss is 0.0422.\n",
      "For Iteration 1507 the Loss is 0.0422.\n",
      "For Iteration 1508 the Loss is 0.0422.\n",
      "For Iteration 1509 the Loss is 0.0422.\n",
      "For Iteration 1510 the Loss is 0.0421.\n",
      "For Iteration 1511 the Loss is 0.0421.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1512 the Loss is 0.0421.\n",
      "For Iteration 1513 the Loss is 0.0421.\n",
      "For Iteration 1514 the Loss is 0.0421.\n",
      "For Iteration 1515 the Loss is 0.0421.\n",
      "For Iteration 1516 the Loss is 0.0421.\n",
      "For Iteration 1517 the Loss is 0.042.\n",
      "For Iteration 1518 the Loss is 0.042.\n",
      "For Iteration 1519 the Loss is 0.042.\n",
      "For Iteration 1520 the Loss is 0.042.\n",
      "For Iteration 1521 the Loss is 0.042.\n",
      "For Iteration 1522 the Loss is 0.042.\n",
      "For Iteration 1523 the Loss is 0.042.\n",
      "For Iteration 1524 the Loss is 0.0419.\n",
      "For Iteration 1525 the Loss is 0.0419.\n",
      "For Iteration 1526 the Loss is 0.0419.\n",
      "For Iteration 1527 the Loss is 0.0419.\n",
      "For Iteration 1528 the Loss is 0.0419.\n",
      "For Iteration 1529 the Loss is 0.0419.\n",
      "For Iteration 1530 the Loss is 0.0419.\n",
      "For Iteration 1531 the Loss is 0.0418.\n",
      "For Iteration 1532 the Loss is 0.0418.\n",
      "For Iteration 1533 the Loss is 0.0418.\n",
      "For Iteration 1534 the Loss is 0.0418.\n",
      "For Iteration 1535 the Loss is 0.0418.\n",
      "For Iteration 1536 the Loss is 0.0418.\n",
      "For Iteration 1537 the Loss is 0.0418.\n",
      "For Iteration 1538 the Loss is 0.0417.\n",
      "For Iteration 1539 the Loss is 0.0417.\n",
      "For Iteration 1540 the Loss is 0.0417.\n",
      "For Iteration 1541 the Loss is 0.0417.\n",
      "For Iteration 1542 the Loss is 0.0417.\n",
      "For Iteration 1543 the Loss is 0.0417.\n",
      "For Iteration 1544 the Loss is 0.0417.\n",
      "For Iteration 1545 the Loss is 0.0417.\n",
      "For Iteration 1546 the Loss is 0.0416.\n",
      "For Iteration 1547 the Loss is 0.0416.\n",
      "For Iteration 1548 the Loss is 0.0416.\n",
      "For Iteration 1549 the Loss is 0.0416.\n",
      "For Iteration 1550 the Loss is 0.0416.\n",
      "For Iteration 1551 the Loss is 0.0416.\n",
      "For Iteration 1552 the Loss is 0.0416.\n",
      "For Iteration 1553 the Loss is 0.0415.\n",
      "For Iteration 1554 the Loss is 0.0415.\n",
      "For Iteration 1555 the Loss is 0.0415.\n",
      "For Iteration 1556 the Loss is 0.0415.\n",
      "For Iteration 1557 the Loss is 0.0415.\n",
      "For Iteration 1558 the Loss is 0.0415.\n",
      "For Iteration 1559 the Loss is 0.0415.\n",
      "For Iteration 1560 the Loss is 0.0414.\n",
      "For Iteration 1561 the Loss is 0.0414.\n",
      "For Iteration 1562 the Loss is 0.0414.\n",
      "For Iteration 1563 the Loss is 0.0414.\n",
      "For Iteration 1564 the Loss is 0.0414.\n",
      "For Iteration 1565 the Loss is 0.0414.\n",
      "For Iteration 1566 the Loss is 0.0414.\n",
      "For Iteration 1567 the Loss is 0.0414.\n",
      "For Iteration 1568 the Loss is 0.0413.\n",
      "For Iteration 1569 the Loss is 0.0413.\n",
      "For Iteration 1570 the Loss is 0.0413.\n",
      "For Iteration 1571 the Loss is 0.0413.\n",
      "For Iteration 1572 the Loss is 0.0413.\n",
      "For Iteration 1573 the Loss is 0.0413.\n",
      "For Iteration 1574 the Loss is 0.0413.\n",
      "For Iteration 1575 the Loss is 0.0412.\n",
      "For Iteration 1576 the Loss is 0.0412.\n",
      "For Iteration 1577 the Loss is 0.0412.\n",
      "For Iteration 1578 the Loss is 0.0412.\n",
      "For Iteration 1579 the Loss is 0.0412.\n",
      "For Iteration 1580 the Loss is 0.0412.\n",
      "For Iteration 1581 the Loss is 0.0412.\n",
      "For Iteration 1582 the Loss is 0.0411.\n",
      "For Iteration 1583 the Loss is 0.0411.\n",
      "For Iteration 1584 the Loss is 0.0411.\n",
      "For Iteration 1585 the Loss is 0.0411.\n",
      "For Iteration 1586 the Loss is 0.0411.\n",
      "For Iteration 1587 the Loss is 0.0411.\n",
      "For Iteration 1588 the Loss is 0.0411.\n",
      "For Iteration 1589 the Loss is 0.0411.\n",
      "For Iteration 1590 the Loss is 0.041.\n",
      "For Iteration 1591 the Loss is 0.041.\n",
      "For Iteration 1592 the Loss is 0.041.\n",
      "For Iteration 1593 the Loss is 0.041.\n",
      "For Iteration 1594 the Loss is 0.041.\n",
      "For Iteration 1595 the Loss is 0.041.\n",
      "For Iteration 1596 the Loss is 0.041.\n",
      "For Iteration 1597 the Loss is 0.0409.\n",
      "For Iteration 1598 the Loss is 0.0409.\n",
      "For Iteration 1599 the Loss is 0.0409.\n",
      "For Iteration 1600 the Loss is 0.0409.\n",
      "For Iteration 1601 the Loss is 0.0409.\n",
      "For Iteration 1602 the Loss is 0.0409.\n",
      "For Iteration 1603 the Loss is 0.0409.\n",
      "For Iteration 1604 the Loss is 0.0409.\n",
      "For Iteration 1605 the Loss is 0.0408.\n",
      "For Iteration 1606 the Loss is 0.0408.\n",
      "For Iteration 1607 the Loss is 0.0408.\n",
      "For Iteration 1608 the Loss is 0.0408.\n",
      "For Iteration 1609 the Loss is 0.0408.\n",
      "For Iteration 1610 the Loss is 0.0408.\n",
      "For Iteration 1611 the Loss is 0.0408.\n",
      "For Iteration 1612 the Loss is 0.0407.\n",
      "For Iteration 1613 the Loss is 0.0407.\n",
      "For Iteration 1614 the Loss is 0.0407.\n",
      "For Iteration 1615 the Loss is 0.0407.\n",
      "For Iteration 1616 the Loss is 0.0407.\n",
      "For Iteration 1617 the Loss is 0.0407.\n",
      "For Iteration 1618 the Loss is 0.0407.\n",
      "For Iteration 1619 the Loss is 0.0407.\n",
      "For Iteration 1620 the Loss is 0.0406.\n",
      "For Iteration 1621 the Loss is 0.0406.\n",
      "For Iteration 1622 the Loss is 0.0406.\n",
      "For Iteration 1623 the Loss is 0.0406.\n",
      "For Iteration 1624 the Loss is 0.0406.\n",
      "For Iteration 1625 the Loss is 0.0406.\n",
      "For Iteration 1626 the Loss is 0.0406.\n",
      "For Iteration 1627 the Loss is 0.0405.\n",
      "For Iteration 1628 the Loss is 0.0405.\n",
      "For Iteration 1629 the Loss is 0.0405.\n",
      "For Iteration 1630 the Loss is 0.0405.\n",
      "For Iteration 1631 the Loss is 0.0405.\n",
      "For Iteration 1632 the Loss is 0.0405.\n",
      "For Iteration 1633 the Loss is 0.0405.\n",
      "For Iteration 1634 the Loss is 0.0405.\n",
      "For Iteration 1635 the Loss is 0.0404.\n",
      "For Iteration 1636 the Loss is 0.0404.\n",
      "For Iteration 1637 the Loss is 0.0404.\n",
      "For Iteration 1638 the Loss is 0.0404.\n",
      "For Iteration 1639 the Loss is 0.0404.\n",
      "For Iteration 1640 the Loss is 0.0404.\n",
      "For Iteration 1641 the Loss is 0.0404.\n",
      "For Iteration 1642 the Loss is 0.0403.\n",
      "For Iteration 1643 the Loss is 0.0403.\n",
      "For Iteration 1644 the Loss is 0.0403.\n",
      "For Iteration 1645 the Loss is 0.0403.\n",
      "For Iteration 1646 the Loss is 0.0403.\n",
      "For Iteration 1647 the Loss is 0.0403.\n",
      "For Iteration 1648 the Loss is 0.0403.\n",
      "For Iteration 1649 the Loss is 0.0403.\n",
      "For Iteration 1650 the Loss is 0.0402.\n",
      "For Iteration 1651 the Loss is 0.0402.\n",
      "For Iteration 1652 the Loss is 0.0402.\n",
      "For Iteration 1653 the Loss is 0.0402.\n",
      "For Iteration 1654 the Loss is 0.0402.\n",
      "For Iteration 1655 the Loss is 0.0402.\n",
      "For Iteration 1656 the Loss is 0.0402.\n",
      "For Iteration 1657 the Loss is 0.0402.\n",
      "For Iteration 1658 the Loss is 0.0401.\n",
      "For Iteration 1659 the Loss is 0.0401.\n",
      "For Iteration 1660 the Loss is 0.0401.\n",
      "For Iteration 1661 the Loss is 0.0401.\n",
      "For Iteration 1662 the Loss is 0.0401.\n",
      "For Iteration 1663 the Loss is 0.0401.\n",
      "For Iteration 1664 the Loss is 0.0401.\n",
      "For Iteration 1665 the Loss is 0.04.\n",
      "For Iteration 1666 the Loss is 0.04.\n",
      "For Iteration 1667 the Loss is 0.04.\n",
      "For Iteration 1668 the Loss is 0.04.\n",
      "For Iteration 1669 the Loss is 0.04.\n",
      "For Iteration 1670 the Loss is 0.04.\n",
      "For Iteration 1671 the Loss is 0.04.\n",
      "For Iteration 1672 the Loss is 0.04.\n",
      "For Iteration 1673 the Loss is 0.0399.\n",
      "For Iteration 1674 the Loss is 0.0399.\n",
      "For Iteration 1675 the Loss is 0.0399.\n",
      "For Iteration 1676 the Loss is 0.0399.\n",
      "For Iteration 1677 the Loss is 0.0399.\n",
      "For Iteration 1678 the Loss is 0.0399.\n",
      "For Iteration 1679 the Loss is 0.0399.\n",
      "For Iteration 1680 the Loss is 0.0399.\n",
      "For Iteration 1681 the Loss is 0.0398.\n",
      "For Iteration 1682 the Loss is 0.0398.\n",
      "For Iteration 1683 the Loss is 0.0398.\n",
      "For Iteration 1684 the Loss is 0.0398.\n",
      "For Iteration 1685 the Loss is 0.0398.\n",
      "For Iteration 1686 the Loss is 0.0398.\n",
      "For Iteration 1687 the Loss is 0.0398.\n",
      "For Iteration 1688 the Loss is 0.0398.\n",
      "For Iteration 1689 the Loss is 0.0397.\n",
      "For Iteration 1690 the Loss is 0.0397.\n",
      "For Iteration 1691 the Loss is 0.0397.\n",
      "For Iteration 1692 the Loss is 0.0397.\n",
      "For Iteration 1693 the Loss is 0.0397.\n",
      "For Iteration 1694 the Loss is 0.0397.\n",
      "For Iteration 1695 the Loss is 0.0397.\n",
      "For Iteration 1696 the Loss is 0.0397.\n",
      "For Iteration 1697 the Loss is 0.0396.\n",
      "For Iteration 1698 the Loss is 0.0396.\n",
      "For Iteration 1699 the Loss is 0.0396.\n",
      "For Iteration 1700 the Loss is 0.0396.\n",
      "For Iteration 1701 the Loss is 0.0396.\n",
      "For Iteration 1702 the Loss is 0.0396.\n",
      "For Iteration 1703 the Loss is 0.0396.\n",
      "For Iteration 1704 the Loss is 0.0396.\n",
      "For Iteration 1705 the Loss is 0.0395.\n",
      "For Iteration 1706 the Loss is 0.0395.\n",
      "For Iteration 1707 the Loss is 0.0395.\n",
      "For Iteration 1708 the Loss is 0.0395.\n",
      "For Iteration 1709 the Loss is 0.0395.\n",
      "For Iteration 1710 the Loss is 0.0395.\n",
      "For Iteration 1711 the Loss is 0.0395.\n",
      "For Iteration 1712 the Loss is 0.0395.\n",
      "For Iteration 1713 the Loss is 0.0394.\n",
      "For Iteration 1714 the Loss is 0.0394.\n",
      "For Iteration 1715 the Loss is 0.0394.\n",
      "For Iteration 1716 the Loss is 0.0394.\n",
      "For Iteration 1717 the Loss is 0.0394.\n",
      "For Iteration 1718 the Loss is 0.0394.\n",
      "For Iteration 1719 the Loss is 0.0394.\n",
      "For Iteration 1720 the Loss is 0.0393.\n",
      "For Iteration 1721 the Loss is 0.0393.\n",
      "For Iteration 1722 the Loss is 0.0393.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1723 the Loss is 0.0393.\n",
      "For Iteration 1724 the Loss is 0.0393.\n",
      "For Iteration 1725 the Loss is 0.0393.\n",
      "For Iteration 1726 the Loss is 0.0393.\n",
      "For Iteration 1727 the Loss is 0.0393.\n",
      "For Iteration 1728 the Loss is 0.0392.\n",
      "For Iteration 1729 the Loss is 0.0392.\n",
      "For Iteration 1730 the Loss is 0.0392.\n",
      "For Iteration 1731 the Loss is 0.0392.\n",
      "For Iteration 1732 the Loss is 0.0392.\n",
      "For Iteration 1733 the Loss is 0.0392.\n",
      "For Iteration 1734 the Loss is 0.0392.\n",
      "For Iteration 1735 the Loss is 0.0392.\n",
      "For Iteration 1736 the Loss is 0.0391.\n",
      "For Iteration 1737 the Loss is 0.0391.\n",
      "For Iteration 1738 the Loss is 0.0391.\n",
      "For Iteration 1739 the Loss is 0.0391.\n",
      "For Iteration 1740 the Loss is 0.0391.\n",
      "For Iteration 1741 the Loss is 0.0391.\n",
      "For Iteration 1742 the Loss is 0.0391.\n",
      "For Iteration 1743 the Loss is 0.0391.\n",
      "For Iteration 1744 the Loss is 0.0391.\n",
      "For Iteration 1745 the Loss is 0.039.\n",
      "For Iteration 1746 the Loss is 0.039.\n",
      "For Iteration 1747 the Loss is 0.039.\n",
      "For Iteration 1748 the Loss is 0.039.\n",
      "For Iteration 1749 the Loss is 0.039.\n",
      "For Iteration 1750 the Loss is 0.039.\n",
      "For Iteration 1751 the Loss is 0.039.\n",
      "For Iteration 1752 the Loss is 0.039.\n",
      "For Iteration 1753 the Loss is 0.0389.\n",
      "For Iteration 1754 the Loss is 0.0389.\n",
      "For Iteration 1755 the Loss is 0.0389.\n",
      "For Iteration 1756 the Loss is 0.0389.\n",
      "For Iteration 1757 the Loss is 0.0389.\n",
      "For Iteration 1758 the Loss is 0.0389.\n",
      "For Iteration 1759 the Loss is 0.0389.\n",
      "For Iteration 1760 the Loss is 0.0389.\n",
      "For Iteration 1761 the Loss is 0.0388.\n",
      "For Iteration 1762 the Loss is 0.0388.\n",
      "For Iteration 1763 the Loss is 0.0388.\n",
      "For Iteration 1764 the Loss is 0.0388.\n",
      "For Iteration 1765 the Loss is 0.0388.\n",
      "For Iteration 1766 the Loss is 0.0388.\n",
      "For Iteration 1767 the Loss is 0.0388.\n",
      "For Iteration 1768 the Loss is 0.0388.\n",
      "For Iteration 1769 the Loss is 0.0387.\n",
      "For Iteration 1770 the Loss is 0.0387.\n",
      "For Iteration 1771 the Loss is 0.0387.\n",
      "For Iteration 1772 the Loss is 0.0387.\n",
      "For Iteration 1773 the Loss is 0.0387.\n",
      "For Iteration 1774 the Loss is 0.0387.\n",
      "For Iteration 1775 the Loss is 0.0387.\n",
      "For Iteration 1776 the Loss is 0.0387.\n",
      "For Iteration 1777 the Loss is 0.0386.\n",
      "For Iteration 1778 the Loss is 0.0386.\n",
      "For Iteration 1779 the Loss is 0.0386.\n",
      "For Iteration 1780 the Loss is 0.0386.\n",
      "For Iteration 1781 the Loss is 0.0386.\n",
      "For Iteration 1782 the Loss is 0.0386.\n",
      "For Iteration 1783 the Loss is 0.0386.\n",
      "For Iteration 1784 the Loss is 0.0386.\n",
      "For Iteration 1785 the Loss is 0.0385.\n",
      "For Iteration 1786 the Loss is 0.0385.\n",
      "For Iteration 1787 the Loss is 0.0385.\n",
      "For Iteration 1788 the Loss is 0.0385.\n",
      "For Iteration 1789 the Loss is 0.0385.\n",
      "For Iteration 1790 the Loss is 0.0385.\n",
      "For Iteration 1791 the Loss is 0.0385.\n",
      "For Iteration 1792 the Loss is 0.0385.\n",
      "For Iteration 1793 the Loss is 0.0384.\n",
      "For Iteration 1794 the Loss is 0.0384.\n",
      "For Iteration 1795 the Loss is 0.0384.\n",
      "For Iteration 1796 the Loss is 0.0384.\n",
      "For Iteration 1797 the Loss is 0.0384.\n",
      "For Iteration 1798 the Loss is 0.0384.\n",
      "For Iteration 1799 the Loss is 0.0384.\n",
      "For Iteration 1800 the Loss is 0.0384.\n",
      "For Iteration 1801 the Loss is 0.0384.\n",
      "For Iteration 1802 the Loss is 0.0383.\n",
      "For Iteration 1803 the Loss is 0.0383.\n",
      "For Iteration 1804 the Loss is 0.0383.\n",
      "For Iteration 1805 the Loss is 0.0383.\n",
      "For Iteration 1806 the Loss is 0.0383.\n",
      "For Iteration 1807 the Loss is 0.0383.\n",
      "For Iteration 1808 the Loss is 0.0383.\n",
      "For Iteration 1809 the Loss is 0.0383.\n",
      "For Iteration 1810 the Loss is 0.0382.\n",
      "For Iteration 1811 the Loss is 0.0382.\n",
      "For Iteration 1812 the Loss is 0.0382.\n",
      "For Iteration 1813 the Loss is 0.0382.\n",
      "For Iteration 1814 the Loss is 0.0382.\n",
      "For Iteration 1815 the Loss is 0.0382.\n",
      "For Iteration 1816 the Loss is 0.0382.\n",
      "For Iteration 1817 the Loss is 0.0382.\n",
      "For Iteration 1818 the Loss is 0.0381.\n",
      "For Iteration 1819 the Loss is 0.0381.\n",
      "For Iteration 1820 the Loss is 0.0381.\n",
      "For Iteration 1821 the Loss is 0.0381.\n",
      "For Iteration 1822 the Loss is 0.0381.\n",
      "For Iteration 1823 the Loss is 0.0381.\n",
      "For Iteration 1824 the Loss is 0.0381.\n",
      "For Iteration 1825 the Loss is 0.0381.\n",
      "For Iteration 1826 the Loss is 0.0381.\n",
      "For Iteration 1827 the Loss is 0.038.\n",
      "For Iteration 1828 the Loss is 0.038.\n",
      "For Iteration 1829 the Loss is 0.038.\n",
      "For Iteration 1830 the Loss is 0.038.\n",
      "For Iteration 1831 the Loss is 0.038.\n",
      "For Iteration 1832 the Loss is 0.038.\n",
      "For Iteration 1833 the Loss is 0.038.\n",
      "For Iteration 1834 the Loss is 0.038.\n",
      "For Iteration 1835 the Loss is 0.0379.\n",
      "For Iteration 1836 the Loss is 0.0379.\n",
      "For Iteration 1837 the Loss is 0.0379.\n",
      "For Iteration 1838 the Loss is 0.0379.\n",
      "For Iteration 1839 the Loss is 0.0379.\n",
      "For Iteration 1840 the Loss is 0.0379.\n",
      "For Iteration 1841 the Loss is 0.0379.\n",
      "For Iteration 1842 the Loss is 0.0379.\n",
      "For Iteration 1843 the Loss is 0.0379.\n",
      "For Iteration 1844 the Loss is 0.0378.\n",
      "For Iteration 1845 the Loss is 0.0378.\n",
      "For Iteration 1846 the Loss is 0.0378.\n",
      "For Iteration 1847 the Loss is 0.0378.\n",
      "For Iteration 1848 the Loss is 0.0378.\n",
      "For Iteration 1849 the Loss is 0.0378.\n",
      "For Iteration 1850 the Loss is 0.0378.\n",
      "For Iteration 1851 the Loss is 0.0378.\n",
      "For Iteration 1852 the Loss is 0.0377.\n",
      "For Iteration 1853 the Loss is 0.0377.\n",
      "For Iteration 1854 the Loss is 0.0377.\n",
      "For Iteration 1855 the Loss is 0.0377.\n",
      "For Iteration 1856 the Loss is 0.0377.\n",
      "For Iteration 1857 the Loss is 0.0377.\n",
      "For Iteration 1858 the Loss is 0.0377.\n",
      "For Iteration 1859 the Loss is 0.0377.\n",
      "For Iteration 1860 the Loss is 0.0377.\n",
      "For Iteration 1861 the Loss is 0.0376.\n",
      "For Iteration 1862 the Loss is 0.0376.\n",
      "For Iteration 1863 the Loss is 0.0376.\n",
      "For Iteration 1864 the Loss is 0.0376.\n",
      "For Iteration 1865 the Loss is 0.0376.\n",
      "For Iteration 1866 the Loss is 0.0376.\n",
      "For Iteration 1867 the Loss is 0.0376.\n",
      "For Iteration 1868 the Loss is 0.0376.\n",
      "For Iteration 1869 the Loss is 0.0375.\n",
      "For Iteration 1870 the Loss is 0.0375.\n",
      "For Iteration 1871 the Loss is 0.0375.\n",
      "For Iteration 1872 the Loss is 0.0375.\n",
      "For Iteration 1873 the Loss is 0.0375.\n",
      "For Iteration 1874 the Loss is 0.0375.\n",
      "For Iteration 1875 the Loss is 0.0375.\n",
      "For Iteration 1876 the Loss is 0.0375.\n",
      "For Iteration 1877 the Loss is 0.0375.\n",
      "For Iteration 1878 the Loss is 0.0374.\n",
      "For Iteration 1879 the Loss is 0.0374.\n",
      "For Iteration 1880 the Loss is 0.0374.\n",
      "For Iteration 1881 the Loss is 0.0374.\n",
      "For Iteration 1882 the Loss is 0.0374.\n",
      "For Iteration 1883 the Loss is 0.0374.\n",
      "For Iteration 1884 the Loss is 0.0374.\n",
      "For Iteration 1885 the Loss is 0.0374.\n",
      "For Iteration 1886 the Loss is 0.0373.\n",
      "For Iteration 1887 the Loss is 0.0373.\n",
      "For Iteration 1888 the Loss is 0.0373.\n",
      "For Iteration 1889 the Loss is 0.0373.\n",
      "For Iteration 1890 the Loss is 0.0373.\n",
      "For Iteration 1891 the Loss is 0.0373.\n",
      "For Iteration 1892 the Loss is 0.0373.\n",
      "For Iteration 1893 the Loss is 0.0373.\n",
      "For Iteration 1894 the Loss is 0.0373.\n",
      "For Iteration 1895 the Loss is 0.0372.\n",
      "For Iteration 1896 the Loss is 0.0372.\n",
      "For Iteration 1897 the Loss is 0.0372.\n",
      "For Iteration 1898 the Loss is 0.0372.\n",
      "For Iteration 1899 the Loss is 0.0372.\n",
      "For Iteration 1900 the Loss is 0.0372.\n",
      "For Iteration 1901 the Loss is 0.0372.\n",
      "For Iteration 1902 the Loss is 0.0372.\n",
      "For Iteration 1903 the Loss is 0.0372.\n",
      "For Iteration 1904 the Loss is 0.0371.\n",
      "For Iteration 1905 the Loss is 0.0371.\n",
      "For Iteration 1906 the Loss is 0.0371.\n",
      "For Iteration 1907 the Loss is 0.0371.\n",
      "For Iteration 1908 the Loss is 0.0371.\n",
      "For Iteration 1909 the Loss is 0.0371.\n",
      "For Iteration 1910 the Loss is 0.0371.\n",
      "For Iteration 1911 the Loss is 0.0371.\n",
      "For Iteration 1912 the Loss is 0.037.\n",
      "For Iteration 1913 the Loss is 0.037.\n",
      "For Iteration 1914 the Loss is 0.037.\n",
      "For Iteration 1915 the Loss is 0.037.\n",
      "For Iteration 1916 the Loss is 0.037.\n",
      "For Iteration 1917 the Loss is 0.037.\n",
      "For Iteration 1918 the Loss is 0.037.\n",
      "For Iteration 1919 the Loss is 0.037.\n",
      "For Iteration 1920 the Loss is 0.037.\n",
      "For Iteration 1921 the Loss is 0.0369.\n",
      "For Iteration 1922 the Loss is 0.0369.\n",
      "For Iteration 1923 the Loss is 0.0369.\n",
      "For Iteration 1924 the Loss is 0.0369.\n",
      "For Iteration 1925 the Loss is 0.0369.\n",
      "For Iteration 1926 the Loss is 0.0369.\n",
      "For Iteration 1927 the Loss is 0.0369.\n",
      "For Iteration 1928 the Loss is 0.0369.\n",
      "For Iteration 1929 the Loss is 0.0369.\n",
      "For Iteration 1930 the Loss is 0.0368.\n",
      "For Iteration 1931 the Loss is 0.0368.\n",
      "For Iteration 1932 the Loss is 0.0368.\n",
      "For Iteration 1933 the Loss is 0.0368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1934 the Loss is 0.0368.\n",
      "For Iteration 1935 the Loss is 0.0368.\n",
      "For Iteration 1936 the Loss is 0.0368.\n",
      "For Iteration 1937 the Loss is 0.0368.\n",
      "For Iteration 1938 the Loss is 0.0368.\n",
      "For Iteration 1939 the Loss is 0.0367.\n",
      "For Iteration 1940 the Loss is 0.0367.\n",
      "For Iteration 1941 the Loss is 0.0367.\n",
      "For Iteration 1942 the Loss is 0.0367.\n",
      "For Iteration 1943 the Loss is 0.0367.\n",
      "For Iteration 1944 the Loss is 0.0367.\n",
      "For Iteration 1945 the Loss is 0.0367.\n",
      "For Iteration 1946 the Loss is 0.0367.\n",
      "For Iteration 1947 the Loss is 0.0367.\n",
      "For Iteration 1948 the Loss is 0.0366.\n",
      "For Iteration 1949 the Loss is 0.0366.\n",
      "For Iteration 1950 the Loss is 0.0366.\n",
      "For Iteration 1951 the Loss is 0.0366.\n",
      "For Iteration 1952 the Loss is 0.0366.\n",
      "For Iteration 1953 the Loss is 0.0366.\n",
      "For Iteration 1954 the Loss is 0.0366.\n",
      "For Iteration 1955 the Loss is 0.0366.\n",
      "For Iteration 1956 the Loss is 0.0365.\n",
      "For Iteration 1957 the Loss is 0.0365.\n",
      "For Iteration 1958 the Loss is 0.0365.\n",
      "For Iteration 1959 the Loss is 0.0365.\n",
      "For Iteration 1960 the Loss is 0.0365.\n",
      "For Iteration 1961 the Loss is 0.0365.\n",
      "For Iteration 1962 the Loss is 0.0365.\n",
      "For Iteration 1963 the Loss is 0.0365.\n",
      "For Iteration 1964 the Loss is 0.0365.\n",
      "For Iteration 1965 the Loss is 0.0364.\n",
      "For Iteration 1966 the Loss is 0.0364.\n",
      "For Iteration 1967 the Loss is 0.0364.\n",
      "For Iteration 1968 the Loss is 0.0364.\n",
      "For Iteration 1969 the Loss is 0.0364.\n",
      "For Iteration 1970 the Loss is 0.0364.\n",
      "For Iteration 1971 the Loss is 0.0364.\n",
      "For Iteration 1972 the Loss is 0.0364.\n",
      "For Iteration 1973 the Loss is 0.0364.\n",
      "For Iteration 1974 the Loss is 0.0363.\n",
      "For Iteration 1975 the Loss is 0.0363.\n",
      "For Iteration 1976 the Loss is 0.0363.\n",
      "For Iteration 1977 the Loss is 0.0363.\n",
      "For Iteration 1978 the Loss is 0.0363.\n",
      "For Iteration 1979 the Loss is 0.0363.\n",
      "For Iteration 1980 the Loss is 0.0363.\n",
      "For Iteration 1981 the Loss is 0.0363.\n",
      "For Iteration 1982 the Loss is 0.0363.\n",
      "For Iteration 1983 the Loss is 0.0362.\n",
      "For Iteration 1984 the Loss is 0.0362.\n",
      "For Iteration 1985 the Loss is 0.0362.\n",
      "For Iteration 1986 the Loss is 0.0362.\n",
      "For Iteration 1987 the Loss is 0.0362.\n",
      "For Iteration 1988 the Loss is 0.0362.\n",
      "For Iteration 1989 the Loss is 0.0362.\n",
      "For Iteration 1990 the Loss is 0.0362.\n",
      "For Iteration 1991 the Loss is 0.0362.\n",
      "For Iteration 1992 the Loss is 0.0361.\n",
      "For Iteration 1993 the Loss is 0.0361.\n",
      "For Iteration 1994 the Loss is 0.0361.\n",
      "For Iteration 1995 the Loss is 0.0361.\n",
      "For Iteration 1996 the Loss is 0.0361.\n",
      "For Iteration 1997 the Loss is 0.0361.\n",
      "For Iteration 1998 the Loss is 0.0361.\n",
      "For Iteration 1999 the Loss is 0.0361.\n",
      "For Iteration 2000 the Loss is 0.0361.\n",
      "For Iteration 2001 the Loss is 0.036.\n",
      "For Iteration 2002 the Loss is 0.036.\n",
      "For Iteration 2003 the Loss is 0.036.\n",
      "For Iteration 2004 the Loss is 0.036.\n",
      "For Iteration 2005 the Loss is 0.036.\n",
      "For Iteration 2006 the Loss is 0.036.\n",
      "For Iteration 2007 the Loss is 0.036.\n",
      "For Iteration 2008 the Loss is 0.036.\n",
      "For Iteration 2009 the Loss is 0.036.\n",
      "For Iteration 2010 the Loss is 0.036.\n",
      "For Iteration 2011 the Loss is 0.0359.\n",
      "For Iteration 2012 the Loss is 0.0359.\n",
      "For Iteration 2013 the Loss is 0.0359.\n",
      "For Iteration 2014 the Loss is 0.0359.\n",
      "For Iteration 2015 the Loss is 0.0359.\n",
      "For Iteration 2016 the Loss is 0.0359.\n",
      "For Iteration 2017 the Loss is 0.0359.\n",
      "For Iteration 2018 the Loss is 0.0359.\n",
      "For Iteration 2019 the Loss is 0.0359.\n",
      "For Iteration 2020 the Loss is 0.0358.\n",
      "For Iteration 2021 the Loss is 0.0358.\n",
      "For Iteration 2022 the Loss is 0.0358.\n",
      "For Iteration 2023 the Loss is 0.0358.\n",
      "For Iteration 2024 the Loss is 0.0358.\n",
      "For Iteration 2025 the Loss is 0.0358.\n",
      "For Iteration 2026 the Loss is 0.0358.\n",
      "For Iteration 2027 the Loss is 0.0358.\n",
      "For Iteration 2028 the Loss is 0.0358.\n",
      "For Iteration 2029 the Loss is 0.0357.\n",
      "For Iteration 2030 the Loss is 0.0357.\n",
      "For Iteration 2031 the Loss is 0.0357.\n",
      "For Iteration 2032 the Loss is 0.0357.\n",
      "For Iteration 2033 the Loss is 0.0357.\n",
      "For Iteration 2034 the Loss is 0.0357.\n",
      "For Iteration 2035 the Loss is 0.0357.\n",
      "For Iteration 2036 the Loss is 0.0357.\n",
      "For Iteration 2037 the Loss is 0.0357.\n",
      "For Iteration 2038 the Loss is 0.0356.\n",
      "For Iteration 2039 the Loss is 0.0356.\n",
      "For Iteration 2040 the Loss is 0.0356.\n",
      "For Iteration 2041 the Loss is 0.0356.\n",
      "For Iteration 2042 the Loss is 0.0356.\n",
      "For Iteration 2043 the Loss is 0.0356.\n",
      "For Iteration 2044 the Loss is 0.0356.\n",
      "For Iteration 2045 the Loss is 0.0356.\n",
      "For Iteration 2046 the Loss is 0.0356.\n",
      "For Iteration 2047 the Loss is 0.0355.\n",
      "For Iteration 2048 the Loss is 0.0355.\n",
      "For Iteration 2049 the Loss is 0.0355.\n",
      "For Iteration 2050 the Loss is 0.0355.\n",
      "For Iteration 2051 the Loss is 0.0355.\n",
      "For Iteration 2052 the Loss is 0.0355.\n",
      "For Iteration 2053 the Loss is 0.0355.\n",
      "For Iteration 2054 the Loss is 0.0355.\n",
      "For Iteration 2055 the Loss is 0.0355.\n",
      "For Iteration 2056 the Loss is 0.0355.\n",
      "For Iteration 2057 the Loss is 0.0354.\n",
      "For Iteration 2058 the Loss is 0.0354.\n",
      "For Iteration 2059 the Loss is 0.0354.\n",
      "For Iteration 2060 the Loss is 0.0354.\n",
      "For Iteration 2061 the Loss is 0.0354.\n",
      "For Iteration 2062 the Loss is 0.0354.\n",
      "For Iteration 2063 the Loss is 0.0354.\n",
      "For Iteration 2064 the Loss is 0.0354.\n",
      "For Iteration 2065 the Loss is 0.0354.\n",
      "For Iteration 2066 the Loss is 0.0353.\n",
      "For Iteration 2067 the Loss is 0.0353.\n",
      "For Iteration 2068 the Loss is 0.0353.\n",
      "For Iteration 2069 the Loss is 0.0353.\n",
      "For Iteration 2070 the Loss is 0.0353.\n",
      "For Iteration 2071 the Loss is 0.0353.\n",
      "For Iteration 2072 the Loss is 0.0353.\n",
      "For Iteration 2073 the Loss is 0.0353.\n",
      "For Iteration 2074 the Loss is 0.0353.\n",
      "For Iteration 2075 the Loss is 0.0352.\n",
      "For Iteration 2076 the Loss is 0.0352.\n",
      "For Iteration 2077 the Loss is 0.0352.\n",
      "For Iteration 2078 the Loss is 0.0352.\n",
      "For Iteration 2079 the Loss is 0.0352.\n",
      "For Iteration 2080 the Loss is 0.0352.\n",
      "For Iteration 2081 the Loss is 0.0352.\n",
      "For Iteration 2082 the Loss is 0.0352.\n",
      "For Iteration 2083 the Loss is 0.0352.\n",
      "For Iteration 2084 the Loss is 0.0352.\n",
      "For Iteration 2085 the Loss is 0.0351.\n",
      "For Iteration 2086 the Loss is 0.0351.\n",
      "For Iteration 2087 the Loss is 0.0351.\n",
      "For Iteration 2088 the Loss is 0.0351.\n",
      "For Iteration 2089 the Loss is 0.0351.\n",
      "For Iteration 2090 the Loss is 0.0351.\n",
      "For Iteration 2091 the Loss is 0.0351.\n",
      "For Iteration 2092 the Loss is 0.0351.\n",
      "For Iteration 2093 the Loss is 0.0351.\n",
      "For Iteration 2094 the Loss is 0.035.\n",
      "For Iteration 2095 the Loss is 0.035.\n",
      "For Iteration 2096 the Loss is 0.035.\n",
      "For Iteration 2097 the Loss is 0.035.\n",
      "For Iteration 2098 the Loss is 0.035.\n",
      "For Iteration 2099 the Loss is 0.035.\n",
      "For Iteration 2100 the Loss is 0.035.\n",
      "For Iteration 2101 the Loss is 0.035.\n",
      "For Iteration 2102 the Loss is 0.035.\n",
      "For Iteration 2103 the Loss is 0.035.\n",
      "For Iteration 2104 the Loss is 0.0349.\n",
      "For Iteration 2105 the Loss is 0.0349.\n",
      "For Iteration 2106 the Loss is 0.0349.\n",
      "For Iteration 2107 the Loss is 0.0349.\n",
      "For Iteration 2108 the Loss is 0.0349.\n",
      "For Iteration 2109 the Loss is 0.0349.\n",
      "For Iteration 2110 the Loss is 0.0349.\n",
      "For Iteration 2111 the Loss is 0.0349.\n",
      "For Iteration 2112 the Loss is 0.0349.\n",
      "For Iteration 2113 the Loss is 0.0348.\n",
      "For Iteration 2114 the Loss is 0.0348.\n",
      "For Iteration 2115 the Loss is 0.0348.\n",
      "For Iteration 2116 the Loss is 0.0348.\n",
      "For Iteration 2117 the Loss is 0.0348.\n",
      "For Iteration 2118 the Loss is 0.0348.\n",
      "For Iteration 2119 the Loss is 0.0348.\n",
      "For Iteration 2120 the Loss is 0.0348.\n",
      "For Iteration 2121 the Loss is 0.0348.\n",
      "For Iteration 2122 the Loss is 0.0348.\n",
      "For Iteration 2123 the Loss is 0.0347.\n",
      "For Iteration 2124 the Loss is 0.0347.\n",
      "For Iteration 2125 the Loss is 0.0347.\n",
      "For Iteration 2126 the Loss is 0.0347.\n",
      "For Iteration 2127 the Loss is 0.0347.\n",
      "For Iteration 2128 the Loss is 0.0347.\n",
      "For Iteration 2129 the Loss is 0.0347.\n",
      "For Iteration 2130 the Loss is 0.0347.\n",
      "For Iteration 2131 the Loss is 0.0347.\n",
      "For Iteration 2132 the Loss is 0.0346.\n",
      "For Iteration 2133 the Loss is 0.0346.\n",
      "For Iteration 2134 the Loss is 0.0346.\n",
      "For Iteration 2135 the Loss is 0.0346.\n",
      "For Iteration 2136 the Loss is 0.0346.\n",
      "For Iteration 2137 the Loss is 0.0346.\n",
      "For Iteration 2138 the Loss is 0.0346.\n",
      "For Iteration 2139 the Loss is 0.0346.\n",
      "For Iteration 2140 the Loss is 0.0346.\n",
      "For Iteration 2141 the Loss is 0.0346.\n",
      "For Iteration 2142 the Loss is 0.0345.\n",
      "For Iteration 2143 the Loss is 0.0345.\n",
      "For Iteration 2144 the Loss is 0.0345.\n",
      "For Iteration 2145 the Loss is 0.0345.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 2146 the Loss is 0.0345.\n",
      "For Iteration 2147 the Loss is 0.0345.\n",
      "For Iteration 2148 the Loss is 0.0345.\n",
      "For Iteration 2149 the Loss is 0.0345.\n",
      "For Iteration 2150 the Loss is 0.0345.\n",
      "For Iteration 2151 the Loss is 0.0345.\n",
      "For Iteration 2152 the Loss is 0.0344.\n",
      "For Iteration 2153 the Loss is 0.0344.\n",
      "For Iteration 2154 the Loss is 0.0344.\n",
      "For Iteration 2155 the Loss is 0.0344.\n",
      "For Iteration 2156 the Loss is 0.0344.\n",
      "For Iteration 2157 the Loss is 0.0344.\n",
      "For Iteration 2158 the Loss is 0.0344.\n",
      "For Iteration 2159 the Loss is 0.0344.\n",
      "For Iteration 2160 the Loss is 0.0344.\n",
      "For Iteration 2161 the Loss is 0.0344.\n",
      "For Iteration 2162 the Loss is 0.0343.\n",
      "For Iteration 2163 the Loss is 0.0343.\n",
      "For Iteration 2164 the Loss is 0.0343.\n",
      "For Iteration 2165 the Loss is 0.0343.\n",
      "For Iteration 2166 the Loss is 0.0343.\n",
      "For Iteration 2167 the Loss is 0.0343.\n",
      "For Iteration 2168 the Loss is 0.0343.\n",
      "For Iteration 2169 the Loss is 0.0343.\n",
      "For Iteration 2170 the Loss is 0.0343.\n",
      "For Iteration 2171 the Loss is 0.0342.\n",
      "For Iteration 2172 the Loss is 0.0342.\n",
      "For Iteration 2173 the Loss is 0.0342.\n",
      "For Iteration 2174 the Loss is 0.0342.\n",
      "For Iteration 2175 the Loss is 0.0342.\n",
      "For Iteration 2176 the Loss is 0.0342.\n",
      "For Iteration 2177 the Loss is 0.0342.\n",
      "For Iteration 2178 the Loss is 0.0342.\n",
      "For Iteration 2179 the Loss is 0.0342.\n",
      "For Iteration 2180 the Loss is 0.0342.\n",
      "For Iteration 2181 the Loss is 0.0341.\n",
      "For Iteration 2182 the Loss is 0.0341.\n",
      "For Iteration 2183 the Loss is 0.0341.\n",
      "For Iteration 2184 the Loss is 0.0341.\n",
      "For Iteration 2185 the Loss is 0.0341.\n",
      "For Iteration 2186 the Loss is 0.0341.\n",
      "For Iteration 2187 the Loss is 0.0341.\n",
      "For Iteration 2188 the Loss is 0.0341.\n",
      "For Iteration 2189 the Loss is 0.0341.\n",
      "For Iteration 2190 the Loss is 0.0341.\n",
      "For Iteration 2191 the Loss is 0.034.\n",
      "For Iteration 2192 the Loss is 0.034.\n",
      "For Iteration 2193 the Loss is 0.034.\n",
      "For Iteration 2194 the Loss is 0.034.\n",
      "For Iteration 2195 the Loss is 0.034.\n",
      "For Iteration 2196 the Loss is 0.034.\n",
      "For Iteration 2197 the Loss is 0.034.\n",
      "For Iteration 2198 the Loss is 0.034.\n",
      "For Iteration 2199 the Loss is 0.034.\n",
      "For Iteration 2200 the Loss is 0.034.\n",
      "For Iteration 2201 the Loss is 0.0339.\n",
      "For Iteration 2202 the Loss is 0.0339.\n",
      "For Iteration 2203 the Loss is 0.0339.\n",
      "For Iteration 2204 the Loss is 0.0339.\n",
      "For Iteration 2205 the Loss is 0.0339.\n",
      "For Iteration 2206 the Loss is 0.0339.\n",
      "For Iteration 2207 the Loss is 0.0339.\n",
      "For Iteration 2208 the Loss is 0.0339.\n",
      "For Iteration 2209 the Loss is 0.0339.\n",
      "For Iteration 2210 the Loss is 0.0339.\n",
      "For Iteration 2211 the Loss is 0.0338.\n",
      "For Iteration 2212 the Loss is 0.0338.\n",
      "For Iteration 2213 the Loss is 0.0338.\n",
      "For Iteration 2214 the Loss is 0.0338.\n",
      "For Iteration 2215 the Loss is 0.0338.\n",
      "For Iteration 2216 the Loss is 0.0338.\n",
      "For Iteration 2217 the Loss is 0.0338.\n",
      "For Iteration 2218 the Loss is 0.0338.\n",
      "For Iteration 2219 the Loss is 0.0338.\n",
      "For Iteration 2220 the Loss is 0.0338.\n",
      "For Iteration 2221 the Loss is 0.0337.\n",
      "For Iteration 2222 the Loss is 0.0337.\n",
      "For Iteration 2223 the Loss is 0.0337.\n",
      "For Iteration 2224 the Loss is 0.0337.\n",
      "For Iteration 2225 the Loss is 0.0337.\n",
      "For Iteration 2226 the Loss is 0.0337.\n",
      "For Iteration 2227 the Loss is 0.0337.\n",
      "For Iteration 2228 the Loss is 0.0337.\n",
      "For Iteration 2229 the Loss is 0.0337.\n",
      "For Iteration 2230 the Loss is 0.0337.\n",
      "For Iteration 2231 the Loss is 0.0336.\n",
      "For Iteration 2232 the Loss is 0.0336.\n",
      "For Iteration 2233 the Loss is 0.0336.\n",
      "For Iteration 2234 the Loss is 0.0336.\n",
      "For Iteration 2235 the Loss is 0.0336.\n",
      "For Iteration 2236 the Loss is 0.0336.\n",
      "For Iteration 2237 the Loss is 0.0336.\n",
      "For Iteration 2238 the Loss is 0.0336.\n",
      "For Iteration 2239 the Loss is 0.0336.\n",
      "For Iteration 2240 the Loss is 0.0336.\n",
      "For Iteration 2241 the Loss is 0.0335.\n",
      "For Iteration 2242 the Loss is 0.0335.\n",
      "For Iteration 2243 the Loss is 0.0335.\n",
      "For Iteration 2244 the Loss is 0.0335.\n",
      "For Iteration 2245 the Loss is 0.0335.\n",
      "For Iteration 2246 the Loss is 0.0335.\n",
      "For Iteration 2247 the Loss is 0.0335.\n",
      "For Iteration 2248 the Loss is 0.0335.\n",
      "For Iteration 2249 the Loss is 0.0335.\n",
      "For Iteration 2250 the Loss is 0.0335.\n",
      "For Iteration 2251 the Loss is 0.0334.\n",
      "For Iteration 2252 the Loss is 0.0334.\n",
      "For Iteration 2253 the Loss is 0.0334.\n",
      "For Iteration 2254 the Loss is 0.0334.\n",
      "For Iteration 2255 the Loss is 0.0334.\n",
      "For Iteration 2256 the Loss is 0.0334.\n",
      "For Iteration 2257 the Loss is 0.0334.\n",
      "For Iteration 2258 the Loss is 0.0334.\n",
      "For Iteration 2259 the Loss is 0.0334.\n",
      "For Iteration 2260 the Loss is 0.0334.\n",
      "For Iteration 2261 the Loss is 0.0333.\n",
      "For Iteration 2262 the Loss is 0.0333.\n",
      "For Iteration 2263 the Loss is 0.0333.\n",
      "For Iteration 2264 the Loss is 0.0333.\n",
      "For Iteration 2265 the Loss is 0.0333.\n",
      "For Iteration 2266 the Loss is 0.0333.\n",
      "For Iteration 2267 the Loss is 0.0333.\n",
      "For Iteration 2268 the Loss is 0.0333.\n",
      "For Iteration 2269 the Loss is 0.0333.\n",
      "For Iteration 2270 the Loss is 0.0333.\n",
      "For Iteration 2271 the Loss is 0.0332.\n",
      "For Iteration 2272 the Loss is 0.0332.\n",
      "For Iteration 2273 the Loss is 0.0332.\n",
      "For Iteration 2274 the Loss is 0.0332.\n",
      "For Iteration 2275 the Loss is 0.0332.\n",
      "For Iteration 2276 the Loss is 0.0332.\n",
      "For Iteration 2277 the Loss is 0.0332.\n",
      "For Iteration 2278 the Loss is 0.0332.\n",
      "For Iteration 2279 the Loss is 0.0332.\n",
      "For Iteration 2280 the Loss is 0.0332.\n",
      "For Iteration 2281 the Loss is 0.0331.\n",
      "For Iteration 2282 the Loss is 0.0331.\n",
      "For Iteration 2283 the Loss is 0.0331.\n",
      "For Iteration 2284 the Loss is 0.0331.\n",
      "For Iteration 2285 the Loss is 0.0331.\n",
      "For Iteration 2286 the Loss is 0.0331.\n",
      "For Iteration 2287 the Loss is 0.0331.\n",
      "For Iteration 2288 the Loss is 0.0331.\n",
      "For Iteration 2289 the Loss is 0.0331.\n",
      "For Iteration 2290 the Loss is 0.0331.\n",
      "For Iteration 2291 the Loss is 0.0331.\n",
      "For Iteration 2292 the Loss is 0.033.\n",
      "For Iteration 2293 the Loss is 0.033.\n",
      "For Iteration 2294 the Loss is 0.033.\n",
      "For Iteration 2295 the Loss is 0.033.\n",
      "For Iteration 2296 the Loss is 0.033.\n",
      "For Iteration 2297 the Loss is 0.033.\n",
      "For Iteration 2298 the Loss is 0.033.\n",
      "For Iteration 2299 the Loss is 0.033.\n",
      "For Iteration 2300 the Loss is 0.033.\n",
      "For Iteration 2301 the Loss is 0.033.\n",
      "For Iteration 2302 the Loss is 0.0329.\n",
      "For Iteration 2303 the Loss is 0.0329.\n",
      "For Iteration 2304 the Loss is 0.0329.\n",
      "For Iteration 2305 the Loss is 0.0329.\n",
      "For Iteration 2306 the Loss is 0.0329.\n",
      "For Iteration 2307 the Loss is 0.0329.\n",
      "For Iteration 2308 the Loss is 0.0329.\n",
      "For Iteration 2309 the Loss is 0.0329.\n",
      "For Iteration 2310 the Loss is 0.0329.\n",
      "For Iteration 2311 the Loss is 0.0329.\n",
      "For Iteration 2312 the Loss is 0.0328.\n",
      "For Iteration 2313 the Loss is 0.0328.\n",
      "For Iteration 2314 the Loss is 0.0328.\n",
      "For Iteration 2315 the Loss is 0.0328.\n",
      "For Iteration 2316 the Loss is 0.0328.\n",
      "For Iteration 2317 the Loss is 0.0328.\n",
      "For Iteration 2318 the Loss is 0.0328.\n",
      "For Iteration 2319 the Loss is 0.0328.\n",
      "For Iteration 2320 the Loss is 0.0328.\n",
      "For Iteration 2321 the Loss is 0.0328.\n",
      "For Iteration 2322 the Loss is 0.0328.\n",
      "For Iteration 2323 the Loss is 0.0327.\n",
      "For Iteration 2324 the Loss is 0.0327.\n",
      "For Iteration 2325 the Loss is 0.0327.\n",
      "For Iteration 2326 the Loss is 0.0327.\n",
      "For Iteration 2327 the Loss is 0.0327.\n",
      "For Iteration 2328 the Loss is 0.0327.\n",
      "For Iteration 2329 the Loss is 0.0327.\n",
      "For Iteration 2330 the Loss is 0.0327.\n",
      "For Iteration 2331 the Loss is 0.0327.\n",
      "For Iteration 2332 the Loss is 0.0327.\n",
      "For Iteration 2333 the Loss is 0.0326.\n",
      "For Iteration 2334 the Loss is 0.0326.\n",
      "For Iteration 2335 the Loss is 0.0326.\n",
      "For Iteration 2336 the Loss is 0.0326.\n",
      "For Iteration 2337 the Loss is 0.0326.\n",
      "For Iteration 2338 the Loss is 0.0326.\n",
      "For Iteration 2339 the Loss is 0.0326.\n",
      "For Iteration 2340 the Loss is 0.0326.\n",
      "For Iteration 2341 the Loss is 0.0326.\n",
      "For Iteration 2342 the Loss is 0.0326.\n",
      "For Iteration 2343 the Loss is 0.0326.\n",
      "For Iteration 2344 the Loss is 0.0325.\n",
      "For Iteration 2345 the Loss is 0.0325.\n",
      "For Iteration 2346 the Loss is 0.0325.\n",
      "For Iteration 2347 the Loss is 0.0325.\n",
      "For Iteration 2348 the Loss is 0.0325.\n",
      "For Iteration 2349 the Loss is 0.0325.\n",
      "For Iteration 2350 the Loss is 0.0325.\n",
      "For Iteration 2351 the Loss is 0.0325.\n",
      "For Iteration 2352 the Loss is 0.0325.\n",
      "For Iteration 2353 the Loss is 0.0325.\n",
      "For Iteration 2354 the Loss is 0.0324.\n",
      "For Iteration 2355 the Loss is 0.0324.\n",
      "For Iteration 2356 the Loss is 0.0324.\n",
      "For Iteration 2357 the Loss is 0.0324.\n",
      "For Iteration 2358 the Loss is 0.0324.\n",
      "For Iteration 2359 the Loss is 0.0324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 2360 the Loss is 0.0324.\n",
      "For Iteration 2361 the Loss is 0.0324.\n",
      "For Iteration 2362 the Loss is 0.0324.\n",
      "For Iteration 2363 the Loss is 0.0324.\n",
      "For Iteration 2364 the Loss is 0.0324.\n",
      "For Iteration 2365 the Loss is 0.0323.\n",
      "For Iteration 2366 the Loss is 0.0323.\n",
      "For Iteration 2367 the Loss is 0.0323.\n",
      "For Iteration 2368 the Loss is 0.0323.\n",
      "For Iteration 2369 the Loss is 0.0323.\n",
      "For Iteration 2370 the Loss is 0.0323.\n",
      "For Iteration 2371 the Loss is 0.0323.\n",
      "For Iteration 2372 the Loss is 0.0323.\n",
      "For Iteration 2373 the Loss is 0.0323.\n",
      "For Iteration 2374 the Loss is 0.0323.\n",
      "For Iteration 2375 the Loss is 0.0323.\n",
      "For Iteration 2376 the Loss is 0.0322.\n",
      "For Iteration 2377 the Loss is 0.0322.\n",
      "For Iteration 2378 the Loss is 0.0322.\n",
      "For Iteration 2379 the Loss is 0.0322.\n",
      "For Iteration 2380 the Loss is 0.0322.\n",
      "For Iteration 2381 the Loss is 0.0322.\n",
      "For Iteration 2382 the Loss is 0.0322.\n",
      "For Iteration 2383 the Loss is 0.0322.\n",
      "For Iteration 2384 the Loss is 0.0322.\n",
      "For Iteration 2385 the Loss is 0.0322.\n",
      "For Iteration 2386 the Loss is 0.0321.\n",
      "For Iteration 2387 the Loss is 0.0321.\n",
      "For Iteration 2388 the Loss is 0.0321.\n",
      "For Iteration 2389 the Loss is 0.0321.\n",
      "For Iteration 2390 the Loss is 0.0321.\n",
      "For Iteration 2391 the Loss is 0.0321.\n",
      "For Iteration 2392 the Loss is 0.0321.\n",
      "For Iteration 2393 the Loss is 0.0321.\n",
      "For Iteration 2394 the Loss is 0.0321.\n",
      "For Iteration 2395 the Loss is 0.0321.\n",
      "For Iteration 2396 the Loss is 0.0321.\n",
      "For Iteration 2397 the Loss is 0.032.\n",
      "For Iteration 2398 the Loss is 0.032.\n",
      "For Iteration 2399 the Loss is 0.032.\n",
      "For Iteration 2400 the Loss is 0.032.\n",
      "For Iteration 2401 the Loss is 0.032.\n",
      "For Iteration 2402 the Loss is 0.032.\n",
      "For Iteration 2403 the Loss is 0.032.\n",
      "For Iteration 2404 the Loss is 0.032.\n",
      "For Iteration 2405 the Loss is 0.032.\n",
      "For Iteration 2406 the Loss is 0.032.\n",
      "For Iteration 2407 the Loss is 0.032.\n",
      "For Iteration 2408 the Loss is 0.0319.\n",
      "For Iteration 2409 the Loss is 0.0319.\n",
      "For Iteration 2410 the Loss is 0.0319.\n",
      "For Iteration 2411 the Loss is 0.0319.\n",
      "For Iteration 2412 the Loss is 0.0319.\n",
      "For Iteration 2413 the Loss is 0.0319.\n",
      "For Iteration 2414 the Loss is 0.0319.\n",
      "For Iteration 2415 the Loss is 0.0319.\n",
      "For Iteration 2416 the Loss is 0.0319.\n",
      "For Iteration 2417 the Loss is 0.0319.\n",
      "For Iteration 2418 the Loss is 0.0319.\n",
      "For Iteration 2419 the Loss is 0.0318.\n",
      "For Iteration 2420 the Loss is 0.0318.\n",
      "For Iteration 2421 the Loss is 0.0318.\n",
      "For Iteration 2422 the Loss is 0.0318.\n",
      "For Iteration 2423 the Loss is 0.0318.\n",
      "For Iteration 2424 the Loss is 0.0318.\n",
      "For Iteration 2425 the Loss is 0.0318.\n",
      "For Iteration 2426 the Loss is 0.0318.\n",
      "For Iteration 2427 the Loss is 0.0318.\n",
      "For Iteration 2428 the Loss is 0.0318.\n",
      "For Iteration 2429 the Loss is 0.0318.\n",
      "For Iteration 2430 the Loss is 0.0317.\n",
      "For Iteration 2431 the Loss is 0.0317.\n",
      "For Iteration 2432 the Loss is 0.0317.\n",
      "For Iteration 2433 the Loss is 0.0317.\n",
      "For Iteration 2434 the Loss is 0.0317.\n",
      "For Iteration 2435 the Loss is 0.0317.\n",
      "For Iteration 2436 the Loss is 0.0317.\n",
      "For Iteration 2437 the Loss is 0.0317.\n",
      "For Iteration 2438 the Loss is 0.0317.\n",
      "For Iteration 2439 the Loss is 0.0317.\n",
      "For Iteration 2440 the Loss is 0.0316.\n",
      "For Iteration 2441 the Loss is 0.0316.\n",
      "For Iteration 2442 the Loss is 0.0316.\n",
      "For Iteration 2443 the Loss is 0.0316.\n",
      "For Iteration 2444 the Loss is 0.0316.\n",
      "For Iteration 2445 the Loss is 0.0316.\n",
      "For Iteration 2446 the Loss is 0.0316.\n",
      "For Iteration 2447 the Loss is 0.0316.\n",
      "For Iteration 2448 the Loss is 0.0316.\n",
      "For Iteration 2449 the Loss is 0.0316.\n",
      "For Iteration 2450 the Loss is 0.0316.\n",
      "For Iteration 2451 the Loss is 0.0315.\n",
      "For Iteration 2452 the Loss is 0.0315.\n",
      "For Iteration 2453 the Loss is 0.0315.\n",
      "For Iteration 2454 the Loss is 0.0315.\n",
      "For Iteration 2455 the Loss is 0.0315.\n",
      "For Iteration 2456 the Loss is 0.0315.\n",
      "For Iteration 2457 the Loss is 0.0315.\n",
      "For Iteration 2458 the Loss is 0.0315.\n",
      "For Iteration 2459 the Loss is 0.0315.\n",
      "For Iteration 2460 the Loss is 0.0315.\n",
      "For Iteration 2461 the Loss is 0.0315.\n",
      "For Iteration 2462 the Loss is 0.0314.\n",
      "For Iteration 2463 the Loss is 0.0314.\n",
      "For Iteration 2464 the Loss is 0.0314.\n",
      "For Iteration 2465 the Loss is 0.0314.\n",
      "For Iteration 2466 the Loss is 0.0314.\n",
      "For Iteration 2467 the Loss is 0.0314.\n",
      "For Iteration 2468 the Loss is 0.0314.\n",
      "For Iteration 2469 the Loss is 0.0314.\n",
      "For Iteration 2470 the Loss is 0.0314.\n",
      "For Iteration 2471 the Loss is 0.0314.\n",
      "For Iteration 2472 the Loss is 0.0314.\n",
      "For Iteration 2473 the Loss is 0.0314.\n",
      "For Iteration 2474 the Loss is 0.0313.\n",
      "For Iteration 2475 the Loss is 0.0313.\n",
      "For Iteration 2476 the Loss is 0.0313.\n",
      "For Iteration 2477 the Loss is 0.0313.\n",
      "For Iteration 2478 the Loss is 0.0313.\n",
      "For Iteration 2479 the Loss is 0.0313.\n",
      "For Iteration 2480 the Loss is 0.0313.\n",
      "For Iteration 2481 the Loss is 0.0313.\n",
      "For Iteration 2482 the Loss is 0.0313.\n",
      "For Iteration 2483 the Loss is 0.0313.\n",
      "For Iteration 2484 the Loss is 0.0313.\n",
      "For Iteration 2485 the Loss is 0.0312.\n",
      "For Iteration 2486 the Loss is 0.0312.\n",
      "For Iteration 2487 the Loss is 0.0312.\n",
      "For Iteration 2488 the Loss is 0.0312.\n",
      "For Iteration 2489 the Loss is 0.0312.\n",
      "For Iteration 2490 the Loss is 0.0312.\n",
      "For Iteration 2491 the Loss is 0.0312.\n",
      "For Iteration 2492 the Loss is 0.0312.\n",
      "For Iteration 2493 the Loss is 0.0312.\n",
      "For Iteration 2494 the Loss is 0.0312.\n",
      "For Iteration 2495 the Loss is 0.0312.\n",
      "For Iteration 2496 the Loss is 0.0311.\n",
      "For Iteration 2497 the Loss is 0.0311.\n",
      "For Iteration 2498 the Loss is 0.0311.\n",
      "For Iteration 2499 the Loss is 0.0311.\n",
      "For Iteration 2500 the Loss is 0.0311.\n",
      "For Iteration 2501 the Loss is 0.0311.\n",
      "For Iteration 2502 the Loss is 0.0311.\n",
      "For Iteration 2503 the Loss is 0.0311.\n",
      "For Iteration 2504 the Loss is 0.0311.\n",
      "For Iteration 2505 the Loss is 0.0311.\n",
      "For Iteration 2506 the Loss is 0.0311.\n",
      "For Iteration 2507 the Loss is 0.031.\n",
      "For Iteration 2508 the Loss is 0.031.\n",
      "For Iteration 2509 the Loss is 0.031.\n",
      "For Iteration 2510 the Loss is 0.031.\n",
      "For Iteration 2511 the Loss is 0.031.\n",
      "For Iteration 2512 the Loss is 0.031.\n",
      "For Iteration 2513 the Loss is 0.031.\n",
      "For Iteration 2514 the Loss is 0.031.\n",
      "For Iteration 2515 the Loss is 0.031.\n",
      "For Iteration 2516 the Loss is 0.031.\n",
      "For Iteration 2517 the Loss is 0.031.\n",
      "For Iteration 2518 the Loss is 0.0309.\n",
      "For Iteration 2519 the Loss is 0.0309.\n",
      "For Iteration 2520 the Loss is 0.0309.\n",
      "For Iteration 2521 the Loss is 0.0309.\n",
      "For Iteration 2522 the Loss is 0.0309.\n",
      "For Iteration 2523 the Loss is 0.0309.\n",
      "For Iteration 2524 the Loss is 0.0309.\n",
      "For Iteration 2525 the Loss is 0.0309.\n",
      "For Iteration 2526 the Loss is 0.0309.\n",
      "For Iteration 2527 the Loss is 0.0309.\n",
      "For Iteration 2528 the Loss is 0.0309.\n",
      "For Iteration 2529 the Loss is 0.0309.\n",
      "For Iteration 2530 the Loss is 0.0308.\n",
      "For Iteration 2531 the Loss is 0.0308.\n",
      "For Iteration 2532 the Loss is 0.0308.\n",
      "For Iteration 2533 the Loss is 0.0308.\n",
      "For Iteration 2534 the Loss is 0.0308.\n",
      "For Iteration 2535 the Loss is 0.0308.\n",
      "For Iteration 2536 the Loss is 0.0308.\n",
      "For Iteration 2537 the Loss is 0.0308.\n",
      "For Iteration 2538 the Loss is 0.0308.\n",
      "For Iteration 2539 the Loss is 0.0308.\n",
      "For Iteration 2540 the Loss is 0.0308.\n",
      "For Iteration 2541 the Loss is 0.0307.\n",
      "For Iteration 2542 the Loss is 0.0307.\n",
      "For Iteration 2543 the Loss is 0.0307.\n",
      "For Iteration 2544 the Loss is 0.0307.\n",
      "For Iteration 2545 the Loss is 0.0307.\n",
      "For Iteration 2546 the Loss is 0.0307.\n",
      "For Iteration 2547 the Loss is 0.0307.\n",
      "For Iteration 2548 the Loss is 0.0307.\n",
      "For Iteration 2549 the Loss is 0.0307.\n",
      "For Iteration 2550 the Loss is 0.0307.\n",
      "For Iteration 2551 the Loss is 0.0307.\n",
      "For Iteration 2552 the Loss is 0.0306.\n",
      "For Iteration 2553 the Loss is 0.0306.\n",
      "For Iteration 2554 the Loss is 0.0306.\n",
      "For Iteration 2555 the Loss is 0.0306.\n",
      "For Iteration 2556 the Loss is 0.0306.\n",
      "For Iteration 2557 the Loss is 0.0306.\n",
      "For Iteration 2558 the Loss is 0.0306.\n",
      "For Iteration 2559 the Loss is 0.0306.\n",
      "For Iteration 2560 the Loss is 0.0306.\n",
      "For Iteration 2561 the Loss is 0.0306.\n",
      "For Iteration 2562 the Loss is 0.0306.\n",
      "For Iteration 2563 the Loss is 0.0306.\n",
      "For Iteration 2564 the Loss is 0.0305.\n",
      "For Iteration 2565 the Loss is 0.0305.\n",
      "For Iteration 2566 the Loss is 0.0305.\n",
      "For Iteration 2567 the Loss is 0.0305.\n",
      "For Iteration 2568 the Loss is 0.0305.\n",
      "For Iteration 2569 the Loss is 0.0305.\n",
      "For Iteration 2570 the Loss is 0.0305.\n",
      "For Iteration 2571 the Loss is 0.0305.\n",
      "For Iteration 2572 the Loss is 0.0305.\n",
      "For Iteration 2573 the Loss is 0.0305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 2574 the Loss is 0.0305.\n",
      "For Iteration 2575 the Loss is 0.0305.\n",
      "For Iteration 2576 the Loss is 0.0304.\n",
      "For Iteration 2577 the Loss is 0.0304.\n",
      "For Iteration 2578 the Loss is 0.0304.\n",
      "For Iteration 2579 the Loss is 0.0304.\n",
      "For Iteration 2580 the Loss is 0.0304.\n",
      "For Iteration 2581 the Loss is 0.0304.\n",
      "For Iteration 2582 the Loss is 0.0304.\n",
      "For Iteration 2583 the Loss is 0.0304.\n",
      "For Iteration 2584 the Loss is 0.0304.\n",
      "For Iteration 2585 the Loss is 0.0304.\n",
      "For Iteration 2586 the Loss is 0.0304.\n",
      "For Iteration 2587 the Loss is 0.0303.\n",
      "For Iteration 2588 the Loss is 0.0303.\n",
      "For Iteration 2589 the Loss is 0.0303.\n",
      "For Iteration 2590 the Loss is 0.0303.\n",
      "For Iteration 2591 the Loss is 0.0303.\n",
      "For Iteration 2592 the Loss is 0.0303.\n",
      "For Iteration 2593 the Loss is 0.0303.\n",
      "For Iteration 2594 the Loss is 0.0303.\n",
      "For Iteration 2595 the Loss is 0.0303.\n",
      "For Iteration 2596 the Loss is 0.0303.\n",
      "For Iteration 2597 the Loss is 0.0303.\n",
      "For Iteration 2598 the Loss is 0.0303.\n",
      "For Iteration 2599 the Loss is 0.0302.\n",
      "For Iteration 2600 the Loss is 0.0302.\n",
      "For Iteration 2601 the Loss is 0.0302.\n",
      "For Iteration 2602 the Loss is 0.0302.\n",
      "For Iteration 2603 the Loss is 0.0302.\n",
      "For Iteration 2604 the Loss is 0.0302.\n",
      "For Iteration 2605 the Loss is 0.0302.\n",
      "For Iteration 2606 the Loss is 0.0302.\n",
      "For Iteration 2607 the Loss is 0.0302.\n",
      "For Iteration 2608 the Loss is 0.0302.\n",
      "For Iteration 2609 the Loss is 0.0302.\n",
      "For Iteration 2610 the Loss is 0.0301.\n",
      "For Iteration 2611 the Loss is 0.0301.\n",
      "For Iteration 2612 the Loss is 0.0301.\n",
      "For Iteration 2613 the Loss is 0.0301.\n",
      "For Iteration 2614 the Loss is 0.0301.\n",
      "For Iteration 2615 the Loss is 0.0301.\n",
      "For Iteration 2616 the Loss is 0.0301.\n",
      "For Iteration 2617 the Loss is 0.0301.\n",
      "For Iteration 2618 the Loss is 0.0301.\n",
      "For Iteration 2619 the Loss is 0.0301.\n",
      "For Iteration 2620 the Loss is 0.0301.\n",
      "For Iteration 2621 the Loss is 0.0301.\n",
      "For Iteration 2622 the Loss is 0.03.\n",
      "For Iteration 2623 the Loss is 0.03.\n",
      "For Iteration 2624 the Loss is 0.03.\n",
      "For Iteration 2625 the Loss is 0.03.\n",
      "For Iteration 2626 the Loss is 0.03.\n",
      "For Iteration 2627 the Loss is 0.03.\n",
      "For Iteration 2628 the Loss is 0.03.\n",
      "For Iteration 2629 the Loss is 0.03.\n",
      "For Iteration 2630 the Loss is 0.03.\n",
      "For Iteration 2631 the Loss is 0.03.\n",
      "For Iteration 2632 the Loss is 0.03.\n",
      "For Iteration 2633 the Loss is 0.03.\n",
      "For Iteration 2634 the Loss is 0.0299.\n",
      "For Iteration 2635 the Loss is 0.0299.\n",
      "For Iteration 2636 the Loss is 0.0299.\n",
      "For Iteration 2637 the Loss is 0.0299.\n",
      "For Iteration 2638 the Loss is 0.0299.\n",
      "For Iteration 2639 the Loss is 0.0299.\n",
      "For Iteration 2640 the Loss is 0.0299.\n",
      "For Iteration 2641 the Loss is 0.0299.\n",
      "For Iteration 2642 the Loss is 0.0299.\n",
      "For Iteration 2643 the Loss is 0.0299.\n",
      "For Iteration 2644 the Loss is 0.0299.\n",
      "For Iteration 2645 the Loss is 0.0299.\n",
      "For Iteration 2646 the Loss is 0.0298.\n",
      "For Iteration 2647 the Loss is 0.0298.\n",
      "For Iteration 2648 the Loss is 0.0298.\n",
      "For Iteration 2649 the Loss is 0.0298.\n",
      "For Iteration 2650 the Loss is 0.0298.\n",
      "For Iteration 2651 the Loss is 0.0298.\n",
      "For Iteration 2652 the Loss is 0.0298.\n",
      "For Iteration 2653 the Loss is 0.0298.\n",
      "For Iteration 2654 the Loss is 0.0298.\n",
      "For Iteration 2655 the Loss is 0.0298.\n",
      "For Iteration 2656 the Loss is 0.0298.\n",
      "For Iteration 2657 the Loss is 0.0298.\n",
      "For Iteration 2658 the Loss is 0.0297.\n",
      "For Iteration 2659 the Loss is 0.0297.\n",
      "For Iteration 2660 the Loss is 0.0297.\n",
      "For Iteration 2661 the Loss is 0.0297.\n",
      "For Iteration 2662 the Loss is 0.0297.\n",
      "For Iteration 2663 the Loss is 0.0297.\n",
      "For Iteration 2664 the Loss is 0.0297.\n",
      "For Iteration 2665 the Loss is 0.0297.\n",
      "For Iteration 2666 the Loss is 0.0297.\n",
      "For Iteration 2667 the Loss is 0.0297.\n",
      "For Iteration 2668 the Loss is 0.0297.\n",
      "For Iteration 2669 the Loss is 0.0297.\n",
      "For Iteration 2670 the Loss is 0.0296.\n",
      "For Iteration 2671 the Loss is 0.0296.\n",
      "For Iteration 2672 the Loss is 0.0296.\n",
      "For Iteration 2673 the Loss is 0.0296.\n",
      "For Iteration 2674 the Loss is 0.0296.\n",
      "For Iteration 2675 the Loss is 0.0296.\n",
      "For Iteration 2676 the Loss is 0.0296.\n",
      "For Iteration 2677 the Loss is 0.0296.\n",
      "For Iteration 2678 the Loss is 0.0296.\n",
      "For Iteration 2679 the Loss is 0.0296.\n",
      "For Iteration 2680 the Loss is 0.0296.\n",
      "For Iteration 2681 the Loss is 0.0296.\n",
      "For Iteration 2682 the Loss is 0.0295.\n",
      "For Iteration 2683 the Loss is 0.0295.\n",
      "For Iteration 2684 the Loss is 0.0295.\n",
      "For Iteration 2685 the Loss is 0.0295.\n",
      "For Iteration 2686 the Loss is 0.0295.\n",
      "For Iteration 2687 the Loss is 0.0295.\n",
      "For Iteration 2688 the Loss is 0.0295.\n",
      "For Iteration 2689 the Loss is 0.0295.\n",
      "For Iteration 2690 the Loss is 0.0295.\n",
      "For Iteration 2691 the Loss is 0.0295.\n",
      "For Iteration 2692 the Loss is 0.0295.\n",
      "For Iteration 2693 the Loss is 0.0295.\n",
      "For Iteration 2694 the Loss is 0.0294.\n",
      "For Iteration 2695 the Loss is 0.0294.\n",
      "For Iteration 2696 the Loss is 0.0294.\n",
      "For Iteration 2697 the Loss is 0.0294.\n",
      "For Iteration 2698 the Loss is 0.0294.\n",
      "For Iteration 2699 the Loss is 0.0294.\n",
      "For Iteration 2700 the Loss is 0.0294.\n",
      "For Iteration 2701 the Loss is 0.0294.\n",
      "For Iteration 2702 the Loss is 0.0294.\n",
      "For Iteration 2703 the Loss is 0.0294.\n",
      "For Iteration 2704 the Loss is 0.0294.\n",
      "For Iteration 2705 the Loss is 0.0294.\n",
      "For Iteration 2706 the Loss is 0.0293.\n",
      "For Iteration 2707 the Loss is 0.0293.\n",
      "For Iteration 2708 the Loss is 0.0293.\n",
      "For Iteration 2709 the Loss is 0.0293.\n",
      "For Iteration 2710 the Loss is 0.0293.\n",
      "For Iteration 2711 the Loss is 0.0293.\n",
      "For Iteration 2712 the Loss is 0.0293.\n",
      "For Iteration 2713 the Loss is 0.0293.\n",
      "For Iteration 2714 the Loss is 0.0293.\n",
      "For Iteration 2715 the Loss is 0.0293.\n",
      "For Iteration 2716 the Loss is 0.0293.\n",
      "For Iteration 2717 the Loss is 0.0293.\n",
      "For Iteration 2718 the Loss is 0.0292.\n",
      "For Iteration 2719 the Loss is 0.0292.\n",
      "For Iteration 2720 the Loss is 0.0292.\n",
      "For Iteration 2721 the Loss is 0.0292.\n",
      "For Iteration 2722 the Loss is 0.0292.\n",
      "For Iteration 2723 the Loss is 0.0292.\n",
      "For Iteration 2724 the Loss is 0.0292.\n",
      "For Iteration 2725 the Loss is 0.0292.\n",
      "For Iteration 2726 the Loss is 0.0292.\n",
      "For Iteration 2727 the Loss is 0.0292.\n",
      "For Iteration 2728 the Loss is 0.0292.\n",
      "For Iteration 2729 the Loss is 0.0292.\n",
      "For Iteration 2730 the Loss is 0.0292.\n",
      "For Iteration 2731 the Loss is 0.0291.\n",
      "For Iteration 2732 the Loss is 0.0291.\n",
      "For Iteration 2733 the Loss is 0.0291.\n",
      "For Iteration 2734 the Loss is 0.0291.\n",
      "For Iteration 2735 the Loss is 0.0291.\n",
      "For Iteration 2736 the Loss is 0.0291.\n",
      "For Iteration 2737 the Loss is 0.0291.\n",
      "For Iteration 2738 the Loss is 0.0291.\n",
      "For Iteration 2739 the Loss is 0.0291.\n",
      "For Iteration 2740 the Loss is 0.0291.\n",
      "For Iteration 2741 the Loss is 0.0291.\n",
      "For Iteration 2742 the Loss is 0.0291.\n",
      "For Iteration 2743 the Loss is 0.029.\n",
      "For Iteration 2744 the Loss is 0.029.\n",
      "For Iteration 2745 the Loss is 0.029.\n",
      "For Iteration 2746 the Loss is 0.029.\n",
      "For Iteration 2747 the Loss is 0.029.\n",
      "For Iteration 2748 the Loss is 0.029.\n",
      "For Iteration 2749 the Loss is 0.029.\n",
      "For Iteration 2750 the Loss is 0.029.\n",
      "For Iteration 2751 the Loss is 0.029.\n",
      "For Iteration 2752 the Loss is 0.029.\n",
      "For Iteration 2753 the Loss is 0.029.\n",
      "For Iteration 2754 the Loss is 0.029.\n",
      "For Iteration 2755 the Loss is 0.0289.\n",
      "For Iteration 2756 the Loss is 0.0289.\n",
      "For Iteration 2757 the Loss is 0.0289.\n",
      "For Iteration 2758 the Loss is 0.0289.\n",
      "For Iteration 2759 the Loss is 0.0289.\n",
      "For Iteration 2760 the Loss is 0.0289.\n",
      "For Iteration 2761 the Loss is 0.0289.\n",
      "For Iteration 2762 the Loss is 0.0289.\n",
      "For Iteration 2763 the Loss is 0.0289.\n",
      "For Iteration 2764 the Loss is 0.0289.\n",
      "For Iteration 2765 the Loss is 0.0289.\n",
      "For Iteration 2766 the Loss is 0.0289.\n",
      "For Iteration 2767 the Loss is 0.0289.\n",
      "For Iteration 2768 the Loss is 0.0288.\n",
      "For Iteration 2769 the Loss is 0.0288.\n",
      "For Iteration 2770 the Loss is 0.0288.\n",
      "For Iteration 2771 the Loss is 0.0288.\n",
      "For Iteration 2772 the Loss is 0.0288.\n",
      "For Iteration 2773 the Loss is 0.0288.\n",
      "For Iteration 2774 the Loss is 0.0288.\n",
      "For Iteration 2775 the Loss is 0.0288.\n",
      "For Iteration 2776 the Loss is 0.0288.\n",
      "For Iteration 2777 the Loss is 0.0288.\n",
      "For Iteration 2778 the Loss is 0.0288.\n",
      "For Iteration 2779 the Loss is 0.0288.\n",
      "For Iteration 2780 the Loss is 0.0288.\n",
      "For Iteration 2781 the Loss is 0.0287.\n",
      "For Iteration 2782 the Loss is 0.0287.\n",
      "For Iteration 2783 the Loss is 0.0287.\n",
      "For Iteration 2784 the Loss is 0.0287.\n",
      "For Iteration 2785 the Loss is 0.0287.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 2786 the Loss is 0.0287.\n",
      "For Iteration 2787 the Loss is 0.0287.\n",
      "For Iteration 2788 the Loss is 0.0287.\n",
      "For Iteration 2789 the Loss is 0.0287.\n",
      "For Iteration 2790 the Loss is 0.0287.\n",
      "For Iteration 2791 the Loss is 0.0287.\n",
      "For Iteration 2792 the Loss is 0.0287.\n",
      "For Iteration 2793 the Loss is 0.0286.\n",
      "For Iteration 2794 the Loss is 0.0286.\n",
      "For Iteration 2795 the Loss is 0.0286.\n",
      "For Iteration 2796 the Loss is 0.0286.\n",
      "For Iteration 2797 the Loss is 0.0286.\n",
      "For Iteration 2798 the Loss is 0.0286.\n",
      "For Iteration 2799 the Loss is 0.0286.\n",
      "For Iteration 2800 the Loss is 0.0286.\n",
      "For Iteration 2801 the Loss is 0.0286.\n",
      "For Iteration 2802 the Loss is 0.0286.\n",
      "For Iteration 2803 the Loss is 0.0286.\n",
      "For Iteration 2804 the Loss is 0.0286.\n",
      "For Iteration 2805 the Loss is 0.0286.\n",
      "For Iteration 2806 the Loss is 0.0285.\n",
      "For Iteration 2807 the Loss is 0.0285.\n",
      "For Iteration 2808 the Loss is 0.0285.\n",
      "For Iteration 2809 the Loss is 0.0285.\n",
      "For Iteration 2810 the Loss is 0.0285.\n",
      "For Iteration 2811 the Loss is 0.0285.\n",
      "For Iteration 2812 the Loss is 0.0285.\n",
      "For Iteration 2813 the Loss is 0.0285.\n",
      "For Iteration 2814 the Loss is 0.0285.\n",
      "For Iteration 2815 the Loss is 0.0285.\n",
      "For Iteration 2816 the Loss is 0.0285.\n",
      "For Iteration 2817 the Loss is 0.0285.\n",
      "For Iteration 2818 the Loss is 0.0285.\n",
      "For Iteration 2819 the Loss is 0.0284.\n",
      "For Iteration 2820 the Loss is 0.0284.\n",
      "For Iteration 2821 the Loss is 0.0284.\n",
      "For Iteration 2822 the Loss is 0.0284.\n",
      "For Iteration 2823 the Loss is 0.0284.\n",
      "For Iteration 2824 the Loss is 0.0284.\n",
      "For Iteration 2825 the Loss is 0.0284.\n",
      "For Iteration 2826 the Loss is 0.0284.\n",
      "For Iteration 2827 the Loss is 0.0284.\n",
      "For Iteration 2828 the Loss is 0.0284.\n",
      "For Iteration 2829 the Loss is 0.0284.\n",
      "For Iteration 2830 the Loss is 0.0284.\n",
      "For Iteration 2831 the Loss is 0.0283.\n",
      "For Iteration 2832 the Loss is 0.0283.\n",
      "For Iteration 2833 the Loss is 0.0283.\n",
      "For Iteration 2834 the Loss is 0.0283.\n",
      "For Iteration 2835 the Loss is 0.0283.\n",
      "For Iteration 2836 the Loss is 0.0283.\n",
      "For Iteration 2837 the Loss is 0.0283.\n",
      "For Iteration 2838 the Loss is 0.0283.\n",
      "For Iteration 2839 the Loss is 0.0283.\n",
      "For Iteration 2840 the Loss is 0.0283.\n",
      "For Iteration 2841 the Loss is 0.0283.\n",
      "For Iteration 2842 the Loss is 0.0283.\n",
      "For Iteration 2843 the Loss is 0.0283.\n",
      "For Iteration 2844 the Loss is 0.0282.\n",
      "For Iteration 2845 the Loss is 0.0282.\n",
      "For Iteration 2846 the Loss is 0.0282.\n",
      "For Iteration 2847 the Loss is 0.0282.\n",
      "For Iteration 2848 the Loss is 0.0282.\n",
      "For Iteration 2849 the Loss is 0.0282.\n",
      "For Iteration 2850 the Loss is 0.0282.\n",
      "For Iteration 2851 the Loss is 0.0282.\n",
      "For Iteration 2852 the Loss is 0.0282.\n",
      "For Iteration 2853 the Loss is 0.0282.\n",
      "For Iteration 2854 the Loss is 0.0282.\n",
      "For Iteration 2855 the Loss is 0.0282.\n",
      "For Iteration 2856 the Loss is 0.0282.\n",
      "For Iteration 2857 the Loss is 0.0281.\n",
      "For Iteration 2858 the Loss is 0.0281.\n",
      "For Iteration 2859 the Loss is 0.0281.\n",
      "For Iteration 2860 the Loss is 0.0281.\n",
      "For Iteration 2861 the Loss is 0.0281.\n",
      "For Iteration 2862 the Loss is 0.0281.\n",
      "For Iteration 2863 the Loss is 0.0281.\n",
      "For Iteration 2864 the Loss is 0.0281.\n",
      "For Iteration 2865 the Loss is 0.0281.\n",
      "For Iteration 2866 the Loss is 0.0281.\n",
      "For Iteration 2867 the Loss is 0.0281.\n",
      "For Iteration 2868 the Loss is 0.0281.\n",
      "For Iteration 2869 the Loss is 0.0281.\n",
      "For Iteration 2870 the Loss is 0.028.\n",
      "For Iteration 2871 the Loss is 0.028.\n",
      "For Iteration 2872 the Loss is 0.028.\n",
      "For Iteration 2873 the Loss is 0.028.\n",
      "For Iteration 2874 the Loss is 0.028.\n",
      "For Iteration 2875 the Loss is 0.028.\n",
      "For Iteration 2876 the Loss is 0.028.\n",
      "For Iteration 2877 the Loss is 0.028.\n",
      "For Iteration 2878 the Loss is 0.028.\n",
      "For Iteration 2879 the Loss is 0.028.\n",
      "For Iteration 2880 the Loss is 0.028.\n",
      "For Iteration 2881 the Loss is 0.028.\n",
      "For Iteration 2882 the Loss is 0.028.\n",
      "For Iteration 2883 the Loss is 0.0279.\n",
      "For Iteration 2884 the Loss is 0.0279.\n",
      "For Iteration 2885 the Loss is 0.0279.\n",
      "For Iteration 2886 the Loss is 0.0279.\n",
      "For Iteration 2887 the Loss is 0.0279.\n",
      "For Iteration 2888 the Loss is 0.0279.\n",
      "For Iteration 2889 the Loss is 0.0279.\n",
      "For Iteration 2890 the Loss is 0.0279.\n",
      "For Iteration 2891 the Loss is 0.0279.\n",
      "For Iteration 2892 the Loss is 0.0279.\n",
      "For Iteration 2893 the Loss is 0.0279.\n",
      "For Iteration 2894 the Loss is 0.0279.\n",
      "For Iteration 2895 the Loss is 0.0279.\n",
      "For Iteration 2896 the Loss is 0.0278.\n",
      "For Iteration 2897 the Loss is 0.0278.\n",
      "For Iteration 2898 the Loss is 0.0278.\n",
      "For Iteration 2899 the Loss is 0.0278.\n",
      "For Iteration 2900 the Loss is 0.0278.\n",
      "For Iteration 2901 the Loss is 0.0278.\n",
      "For Iteration 2902 the Loss is 0.0278.\n",
      "For Iteration 2903 the Loss is 0.0278.\n",
      "For Iteration 2904 the Loss is 0.0278.\n",
      "For Iteration 2905 the Loss is 0.0278.\n",
      "For Iteration 2906 the Loss is 0.0278.\n",
      "For Iteration 2907 the Loss is 0.0278.\n",
      "For Iteration 2908 the Loss is 0.0278.\n",
      "For Iteration 2909 the Loss is 0.0277.\n",
      "For Iteration 2910 the Loss is 0.0277.\n",
      "For Iteration 2911 the Loss is 0.0277.\n",
      "For Iteration 2912 the Loss is 0.0277.\n",
      "For Iteration 2913 the Loss is 0.0277.\n",
      "For Iteration 2914 the Loss is 0.0277.\n",
      "For Iteration 2915 the Loss is 0.0277.\n",
      "For Iteration 2916 the Loss is 0.0277.\n",
      "For Iteration 2917 the Loss is 0.0277.\n",
      "For Iteration 2918 the Loss is 0.0277.\n",
      "For Iteration 2919 the Loss is 0.0277.\n",
      "For Iteration 2920 the Loss is 0.0277.\n",
      "For Iteration 2921 the Loss is 0.0277.\n",
      "For Iteration 2922 the Loss is 0.0277.\n",
      "For Iteration 2923 the Loss is 0.0276.\n",
      "For Iteration 2924 the Loss is 0.0276.\n",
      "For Iteration 2925 the Loss is 0.0276.\n",
      "For Iteration 2926 the Loss is 0.0276.\n",
      "For Iteration 2927 the Loss is 0.0276.\n",
      "For Iteration 2928 the Loss is 0.0276.\n",
      "For Iteration 2929 the Loss is 0.0276.\n",
      "For Iteration 2930 the Loss is 0.0276.\n",
      "For Iteration 2931 the Loss is 0.0276.\n",
      "For Iteration 2932 the Loss is 0.0276.\n",
      "For Iteration 2933 the Loss is 0.0276.\n",
      "For Iteration 2934 the Loss is 0.0276.\n",
      "For Iteration 2935 the Loss is 0.0276.\n",
      "For Iteration 2936 the Loss is 0.0275.\n",
      "For Iteration 2937 the Loss is 0.0275.\n",
      "For Iteration 2938 the Loss is 0.0275.\n",
      "For Iteration 2939 the Loss is 0.0275.\n",
      "For Iteration 2940 the Loss is 0.0275.\n",
      "For Iteration 2941 the Loss is 0.0275.\n",
      "For Iteration 2942 the Loss is 0.0275.\n",
      "For Iteration 2943 the Loss is 0.0275.\n",
      "For Iteration 2944 the Loss is 0.0275.\n",
      "For Iteration 2945 the Loss is 0.0275.\n",
      "For Iteration 2946 the Loss is 0.0275.\n",
      "For Iteration 2947 the Loss is 0.0275.\n",
      "For Iteration 2948 the Loss is 0.0275.\n",
      "For Iteration 2949 the Loss is 0.0275.\n",
      "For Iteration 2950 the Loss is 0.0274.\n",
      "For Iteration 2951 the Loss is 0.0274.\n",
      "For Iteration 2952 the Loss is 0.0274.\n",
      "For Iteration 2953 the Loss is 0.0274.\n",
      "For Iteration 2954 the Loss is 0.0274.\n",
      "For Iteration 2955 the Loss is 0.0274.\n",
      "For Iteration 2956 the Loss is 0.0274.\n",
      "For Iteration 2957 the Loss is 0.0274.\n",
      "For Iteration 2958 the Loss is 0.0274.\n",
      "For Iteration 2959 the Loss is 0.0274.\n",
      "For Iteration 2960 the Loss is 0.0274.\n",
      "For Iteration 2961 the Loss is 0.0274.\n",
      "For Iteration 2962 the Loss is 0.0274.\n",
      "For Iteration 2963 the Loss is 0.0273.\n",
      "For Iteration 2964 the Loss is 0.0273.\n",
      "For Iteration 2965 the Loss is 0.0273.\n",
      "For Iteration 2966 the Loss is 0.0273.\n",
      "For Iteration 2967 the Loss is 0.0273.\n",
      "For Iteration 2968 the Loss is 0.0273.\n",
      "For Iteration 2969 the Loss is 0.0273.\n",
      "For Iteration 2970 the Loss is 0.0273.\n",
      "For Iteration 2971 the Loss is 0.0273.\n",
      "For Iteration 2972 the Loss is 0.0273.\n",
      "For Iteration 2973 the Loss is 0.0273.\n",
      "For Iteration 2974 the Loss is 0.0273.\n",
      "For Iteration 2975 the Loss is 0.0273.\n",
      "For Iteration 2976 the Loss is 0.0273.\n",
      "For Iteration 2977 the Loss is 0.0272.\n",
      "For Iteration 2978 the Loss is 0.0272.\n",
      "For Iteration 2979 the Loss is 0.0272.\n",
      "For Iteration 2980 the Loss is 0.0272.\n",
      "For Iteration 2981 the Loss is 0.0272.\n",
      "For Iteration 2982 the Loss is 0.0272.\n",
      "For Iteration 2983 the Loss is 0.0272.\n",
      "For Iteration 2984 the Loss is 0.0272.\n",
      "For Iteration 2985 the Loss is 0.0272.\n",
      "For Iteration 2986 the Loss is 0.0272.\n",
      "For Iteration 2987 the Loss is 0.0272.\n",
      "For Iteration 2988 the Loss is 0.0272.\n",
      "For Iteration 2989 the Loss is 0.0272.\n",
      "For Iteration 2990 the Loss is 0.0271.\n",
      "For Iteration 2991 the Loss is 0.0271.\n",
      "For Iteration 2992 the Loss is 0.0271.\n",
      "For Iteration 2993 the Loss is 0.0271.\n",
      "For Iteration 2994 the Loss is 0.0271.\n",
      "For Iteration 2995 the Loss is 0.0271.\n",
      "For Iteration 2996 the Loss is 0.0271.\n",
      "For Iteration 2997 the Loss is 0.0271.\n",
      "For Iteration 2998 the Loss is 0.0271.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 2999 the Loss is 0.0271.\n",
      "For Iteration 3000 the Loss is 0.0271.\n",
      "For Iteration 3001 the Loss is 0.0271.\n",
      "For Iteration 3002 the Loss is 0.0271.\n",
      "For Iteration 3003 the Loss is 0.0271.\n",
      "For Iteration 3004 the Loss is 0.027.\n",
      "For Iteration 3005 the Loss is 0.027.\n",
      "For Iteration 3006 the Loss is 0.027.\n",
      "For Iteration 3007 the Loss is 0.027.\n",
      "For Iteration 3008 the Loss is 0.027.\n",
      "For Iteration 3009 the Loss is 0.027.\n",
      "For Iteration 3010 the Loss is 0.027.\n",
      "For Iteration 3011 the Loss is 0.027.\n",
      "For Iteration 3012 the Loss is 0.027.\n",
      "For Iteration 3013 the Loss is 0.027.\n",
      "For Iteration 3014 the Loss is 0.027.\n",
      "For Iteration 3015 the Loss is 0.027.\n",
      "For Iteration 3016 the Loss is 0.027.\n",
      "For Iteration 3017 the Loss is 0.027.\n",
      "For Iteration 3018 the Loss is 0.0269.\n",
      "For Iteration 3019 the Loss is 0.0269.\n",
      "For Iteration 3020 the Loss is 0.0269.\n",
      "For Iteration 3021 the Loss is 0.0269.\n",
      "For Iteration 3022 the Loss is 0.0269.\n",
      "For Iteration 3023 the Loss is 0.0269.\n",
      "For Iteration 3024 the Loss is 0.0269.\n",
      "For Iteration 3025 the Loss is 0.0269.\n",
      "For Iteration 3026 the Loss is 0.0269.\n",
      "For Iteration 3027 the Loss is 0.0269.\n",
      "For Iteration 3028 the Loss is 0.0269.\n",
      "For Iteration 3029 the Loss is 0.0269.\n",
      "For Iteration 3030 the Loss is 0.0269.\n",
      "For Iteration 3031 the Loss is 0.0269.\n",
      "For Iteration 3032 the Loss is 0.0268.\n",
      "For Iteration 3033 the Loss is 0.0268.\n",
      "For Iteration 3034 the Loss is 0.0268.\n",
      "For Iteration 3035 the Loss is 0.0268.\n",
      "For Iteration 3036 the Loss is 0.0268.\n",
      "For Iteration 3037 the Loss is 0.0268.\n",
      "For Iteration 3038 the Loss is 0.0268.\n",
      "For Iteration 3039 the Loss is 0.0268.\n",
      "For Iteration 3040 the Loss is 0.0268.\n",
      "For Iteration 3041 the Loss is 0.0268.\n",
      "For Iteration 3042 the Loss is 0.0268.\n",
      "For Iteration 3043 the Loss is 0.0268.\n",
      "For Iteration 3044 the Loss is 0.0268.\n",
      "For Iteration 3045 the Loss is 0.0267.\n",
      "For Iteration 3046 the Loss is 0.0267.\n",
      "For Iteration 3047 the Loss is 0.0267.\n",
      "For Iteration 3048 the Loss is 0.0267.\n",
      "For Iteration 3049 the Loss is 0.0267.\n",
      "For Iteration 3050 the Loss is 0.0267.\n",
      "For Iteration 3051 the Loss is 0.0267.\n",
      "For Iteration 3052 the Loss is 0.0267.\n",
      "For Iteration 3053 the Loss is 0.0267.\n",
      "For Iteration 3054 the Loss is 0.0267.\n",
      "For Iteration 3055 the Loss is 0.0267.\n",
      "For Iteration 3056 the Loss is 0.0267.\n",
      "For Iteration 3057 the Loss is 0.0267.\n",
      "For Iteration 3058 the Loss is 0.0267.\n",
      "For Iteration 3059 the Loss is 0.0267.\n",
      "For Iteration 3060 the Loss is 0.0266.\n",
      "For Iteration 3061 the Loss is 0.0266.\n",
      "For Iteration 3062 the Loss is 0.0266.\n",
      "For Iteration 3063 the Loss is 0.0266.\n",
      "For Iteration 3064 the Loss is 0.0266.\n",
      "For Iteration 3065 the Loss is 0.0266.\n",
      "For Iteration 3066 the Loss is 0.0266.\n",
      "For Iteration 3067 the Loss is 0.0266.\n",
      "For Iteration 3068 the Loss is 0.0266.\n",
      "For Iteration 3069 the Loss is 0.0266.\n",
      "For Iteration 3070 the Loss is 0.0266.\n",
      "For Iteration 3071 the Loss is 0.0266.\n",
      "For Iteration 3072 the Loss is 0.0266.\n",
      "For Iteration 3073 the Loss is 0.0266.\n",
      "For Iteration 3074 the Loss is 0.0265.\n",
      "For Iteration 3075 the Loss is 0.0265.\n",
      "For Iteration 3076 the Loss is 0.0265.\n",
      "For Iteration 3077 the Loss is 0.0265.\n",
      "For Iteration 3078 the Loss is 0.0265.\n",
      "For Iteration 3079 the Loss is 0.0265.\n",
      "For Iteration 3080 the Loss is 0.0265.\n",
      "For Iteration 3081 the Loss is 0.0265.\n",
      "For Iteration 3082 the Loss is 0.0265.\n",
      "For Iteration 3083 the Loss is 0.0265.\n",
      "For Iteration 3084 the Loss is 0.0265.\n",
      "For Iteration 3085 the Loss is 0.0265.\n",
      "For Iteration 3086 the Loss is 0.0265.\n",
      "For Iteration 3087 the Loss is 0.0265.\n",
      "For Iteration 3088 the Loss is 0.0264.\n",
      "For Iteration 3089 the Loss is 0.0264.\n",
      "For Iteration 3090 the Loss is 0.0264.\n",
      "For Iteration 3091 the Loss is 0.0264.\n",
      "For Iteration 3092 the Loss is 0.0264.\n",
      "For Iteration 3093 the Loss is 0.0264.\n",
      "For Iteration 3094 the Loss is 0.0264.\n",
      "For Iteration 3095 the Loss is 0.0264.\n",
      "For Iteration 3096 the Loss is 0.0264.\n",
      "For Iteration 3097 the Loss is 0.0264.\n",
      "For Iteration 3098 the Loss is 0.0264.\n",
      "For Iteration 3099 the Loss is 0.0264.\n",
      "For Iteration 3100 the Loss is 0.0264.\n",
      "For Iteration 3101 the Loss is 0.0264.\n",
      "For Iteration 3102 the Loss is 0.0263.\n",
      "For Iteration 3103 the Loss is 0.0263.\n",
      "For Iteration 3104 the Loss is 0.0263.\n",
      "For Iteration 3105 the Loss is 0.0263.\n",
      "For Iteration 3106 the Loss is 0.0263.\n",
      "For Iteration 3107 the Loss is 0.0263.\n",
      "For Iteration 3108 the Loss is 0.0263.\n",
      "For Iteration 3109 the Loss is 0.0263.\n",
      "For Iteration 3110 the Loss is 0.0263.\n",
      "For Iteration 3111 the Loss is 0.0263.\n",
      "For Iteration 3112 the Loss is 0.0263.\n",
      "For Iteration 3113 the Loss is 0.0263.\n",
      "For Iteration 3114 the Loss is 0.0263.\n",
      "For Iteration 3115 the Loss is 0.0263.\n",
      "For Iteration 3116 the Loss is 0.0262.\n",
      "For Iteration 3117 the Loss is 0.0262.\n",
      "For Iteration 3118 the Loss is 0.0262.\n",
      "For Iteration 3119 the Loss is 0.0262.\n",
      "For Iteration 3120 the Loss is 0.0262.\n",
      "For Iteration 3121 the Loss is 0.0262.\n",
      "For Iteration 3122 the Loss is 0.0262.\n",
      "For Iteration 3123 the Loss is 0.0262.\n",
      "For Iteration 3124 the Loss is 0.0262.\n",
      "For Iteration 3125 the Loss is 0.0262.\n",
      "For Iteration 3126 the Loss is 0.0262.\n",
      "For Iteration 3127 the Loss is 0.0262.\n",
      "For Iteration 3128 the Loss is 0.0262.\n",
      "For Iteration 3129 the Loss is 0.0262.\n",
      "For Iteration 3130 the Loss is 0.0262.\n",
      "For Iteration 3131 the Loss is 0.0261.\n",
      "For Iteration 3132 the Loss is 0.0261.\n",
      "For Iteration 3133 the Loss is 0.0261.\n",
      "For Iteration 3134 the Loss is 0.0261.\n",
      "For Iteration 3135 the Loss is 0.0261.\n",
      "For Iteration 3136 the Loss is 0.0261.\n",
      "For Iteration 3137 the Loss is 0.0261.\n",
      "For Iteration 3138 the Loss is 0.0261.\n",
      "For Iteration 3139 the Loss is 0.0261.\n",
      "For Iteration 3140 the Loss is 0.0261.\n",
      "For Iteration 3141 the Loss is 0.0261.\n",
      "For Iteration 3142 the Loss is 0.0261.\n",
      "For Iteration 3143 the Loss is 0.0261.\n",
      "For Iteration 3144 the Loss is 0.0261.\n",
      "For Iteration 3145 the Loss is 0.026.\n",
      "For Iteration 3146 the Loss is 0.026.\n",
      "For Iteration 3147 the Loss is 0.026.\n",
      "For Iteration 3148 the Loss is 0.026.\n",
      "For Iteration 3149 the Loss is 0.026.\n",
      "For Iteration 3150 the Loss is 0.026.\n",
      "For Iteration 3151 the Loss is 0.026.\n",
      "For Iteration 3152 the Loss is 0.026.\n",
      "For Iteration 3153 the Loss is 0.026.\n",
      "For Iteration 3154 the Loss is 0.026.\n",
      "For Iteration 3155 the Loss is 0.026.\n",
      "For Iteration 3156 the Loss is 0.026.\n",
      "For Iteration 3157 the Loss is 0.026.\n",
      "For Iteration 3158 the Loss is 0.026.\n",
      "For Iteration 3159 the Loss is 0.026.\n",
      "For Iteration 3160 the Loss is 0.0259.\n",
      "For Iteration 3161 the Loss is 0.0259.\n",
      "For Iteration 3162 the Loss is 0.0259.\n",
      "For Iteration 3163 the Loss is 0.0259.\n",
      "For Iteration 3164 the Loss is 0.0259.\n",
      "For Iteration 3165 the Loss is 0.0259.\n",
      "For Iteration 3166 the Loss is 0.0259.\n",
      "For Iteration 3167 the Loss is 0.0259.\n",
      "For Iteration 3168 the Loss is 0.0259.\n",
      "For Iteration 3169 the Loss is 0.0259.\n",
      "For Iteration 3170 the Loss is 0.0259.\n",
      "For Iteration 3171 the Loss is 0.0259.\n",
      "For Iteration 3172 the Loss is 0.0259.\n",
      "For Iteration 3173 the Loss is 0.0259.\n",
      "For Iteration 3174 the Loss is 0.0259.\n",
      "For Iteration 3175 the Loss is 0.0258.\n",
      "For Iteration 3176 the Loss is 0.0258.\n",
      "For Iteration 3177 the Loss is 0.0258.\n",
      "For Iteration 3178 the Loss is 0.0258.\n",
      "For Iteration 3179 the Loss is 0.0258.\n",
      "For Iteration 3180 the Loss is 0.0258.\n",
      "For Iteration 3181 the Loss is 0.0258.\n",
      "For Iteration 3182 the Loss is 0.0258.\n",
      "For Iteration 3183 the Loss is 0.0258.\n",
      "For Iteration 3184 the Loss is 0.0258.\n",
      "For Iteration 3185 the Loss is 0.0258.\n",
      "For Iteration 3186 the Loss is 0.0258.\n",
      "For Iteration 3187 the Loss is 0.0258.\n",
      "For Iteration 3188 the Loss is 0.0258.\n",
      "For Iteration 3189 the Loss is 0.0257.\n",
      "For Iteration 3190 the Loss is 0.0257.\n",
      "For Iteration 3191 the Loss is 0.0257.\n",
      "For Iteration 3192 the Loss is 0.0257.\n",
      "For Iteration 3193 the Loss is 0.0257.\n",
      "For Iteration 3194 the Loss is 0.0257.\n",
      "For Iteration 3195 the Loss is 0.0257.\n",
      "For Iteration 3196 the Loss is 0.0257.\n",
      "For Iteration 3197 the Loss is 0.0257.\n",
      "For Iteration 3198 the Loss is 0.0257.\n",
      "For Iteration 3199 the Loss is 0.0257.\n",
      "For Iteration 3200 the Loss is 0.0257.\n",
      "For Iteration 3201 the Loss is 0.0257.\n",
      "For Iteration 3202 the Loss is 0.0257.\n",
      "For Iteration 3203 the Loss is 0.0257.\n",
      "For Iteration 3204 the Loss is 0.0256.\n",
      "For Iteration 3205 the Loss is 0.0256.\n",
      "For Iteration 3206 the Loss is 0.0256.\n",
      "For Iteration 3207 the Loss is 0.0256.\n",
      "For Iteration 3208 the Loss is 0.0256.\n",
      "For Iteration 3209 the Loss is 0.0256.\n",
      "For Iteration 3210 the Loss is 0.0256.\n",
      "For Iteration 3211 the Loss is 0.0256.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 3212 the Loss is 0.0256.\n",
      "For Iteration 3213 the Loss is 0.0256.\n",
      "For Iteration 3214 the Loss is 0.0256.\n",
      "For Iteration 3215 the Loss is 0.0256.\n",
      "For Iteration 3216 the Loss is 0.0256.\n",
      "For Iteration 3217 the Loss is 0.0256.\n",
      "For Iteration 3218 the Loss is 0.0256.\n",
      "For Iteration 3219 the Loss is 0.0255.\n",
      "For Iteration 3220 the Loss is 0.0255.\n",
      "For Iteration 3221 the Loss is 0.0255.\n",
      "For Iteration 3222 the Loss is 0.0255.\n",
      "For Iteration 3223 the Loss is 0.0255.\n",
      "For Iteration 3224 the Loss is 0.0255.\n",
      "For Iteration 3225 the Loss is 0.0255.\n",
      "For Iteration 3226 the Loss is 0.0255.\n",
      "For Iteration 3227 the Loss is 0.0255.\n",
      "For Iteration 3228 the Loss is 0.0255.\n",
      "For Iteration 3229 the Loss is 0.0255.\n",
      "For Iteration 3230 the Loss is 0.0255.\n",
      "For Iteration 3231 the Loss is 0.0255.\n",
      "For Iteration 3232 the Loss is 0.0255.\n",
      "For Iteration 3233 the Loss is 0.0255.\n",
      "For Iteration 3234 the Loss is 0.0254.\n",
      "For Iteration 3235 the Loss is 0.0254.\n",
      "For Iteration 3236 the Loss is 0.0254.\n",
      "For Iteration 3237 the Loss is 0.0254.\n",
      "For Iteration 3238 the Loss is 0.0254.\n",
      "For Iteration 3239 the Loss is 0.0254.\n",
      "For Iteration 3240 the Loss is 0.0254.\n",
      "For Iteration 3241 the Loss is 0.0254.\n",
      "For Iteration 3242 the Loss is 0.0254.\n",
      "For Iteration 3243 the Loss is 0.0254.\n",
      "For Iteration 3244 the Loss is 0.0254.\n",
      "For Iteration 3245 the Loss is 0.0254.\n",
      "For Iteration 3246 the Loss is 0.0254.\n",
      "For Iteration 3247 the Loss is 0.0254.\n",
      "For Iteration 3248 the Loss is 0.0254.\n",
      "For Iteration 3249 the Loss is 0.0253.\n",
      "For Iteration 3250 the Loss is 0.0253.\n",
      "For Iteration 3251 the Loss is 0.0253.\n",
      "For Iteration 3252 the Loss is 0.0253.\n",
      "For Iteration 3253 the Loss is 0.0253.\n",
      "For Iteration 3254 the Loss is 0.0253.\n",
      "For Iteration 3255 the Loss is 0.0253.\n",
      "For Iteration 3256 the Loss is 0.0253.\n",
      "For Iteration 3257 the Loss is 0.0253.\n",
      "For Iteration 3258 the Loss is 0.0253.\n",
      "For Iteration 3259 the Loss is 0.0253.\n",
      "For Iteration 3260 the Loss is 0.0253.\n",
      "For Iteration 3261 the Loss is 0.0253.\n",
      "For Iteration 3262 the Loss is 0.0253.\n",
      "For Iteration 3263 the Loss is 0.0253.\n",
      "For Iteration 3264 the Loss is 0.0252.\n",
      "For Iteration 3265 the Loss is 0.0252.\n",
      "For Iteration 3266 the Loss is 0.0252.\n",
      "For Iteration 3267 the Loss is 0.0252.\n",
      "For Iteration 3268 the Loss is 0.0252.\n",
      "For Iteration 3269 the Loss is 0.0252.\n",
      "For Iteration 3270 the Loss is 0.0252.\n",
      "For Iteration 3271 the Loss is 0.0252.\n",
      "For Iteration 3272 the Loss is 0.0252.\n",
      "For Iteration 3273 the Loss is 0.0252.\n",
      "For Iteration 3274 the Loss is 0.0252.\n",
      "For Iteration 3275 the Loss is 0.0252.\n",
      "For Iteration 3276 the Loss is 0.0252.\n",
      "For Iteration 3277 the Loss is 0.0252.\n",
      "For Iteration 3278 the Loss is 0.0252.\n",
      "For Iteration 3279 the Loss is 0.0252.\n",
      "For Iteration 3280 the Loss is 0.0251.\n",
      "For Iteration 3281 the Loss is 0.0251.\n",
      "For Iteration 3282 the Loss is 0.0251.\n",
      "For Iteration 3283 the Loss is 0.0251.\n",
      "For Iteration 3284 the Loss is 0.0251.\n",
      "For Iteration 3285 the Loss is 0.0251.\n",
      "For Iteration 3286 the Loss is 0.0251.\n",
      "For Iteration 3287 the Loss is 0.0251.\n",
      "For Iteration 3288 the Loss is 0.0251.\n",
      "For Iteration 3289 the Loss is 0.0251.\n",
      "For Iteration 3290 the Loss is 0.0251.\n",
      "For Iteration 3291 the Loss is 0.0251.\n",
      "For Iteration 3292 the Loss is 0.0251.\n",
      "For Iteration 3293 the Loss is 0.0251.\n",
      "For Iteration 3294 the Loss is 0.0251.\n",
      "For Iteration 3295 the Loss is 0.025.\n",
      "For Iteration 3296 the Loss is 0.025.\n",
      "For Iteration 3297 the Loss is 0.025.\n",
      "For Iteration 3298 the Loss is 0.025.\n",
      "For Iteration 3299 the Loss is 0.025.\n",
      "For Iteration 3300 the Loss is 0.025.\n",
      "For Iteration 3301 the Loss is 0.025.\n",
      "For Iteration 3302 the Loss is 0.025.\n",
      "For Iteration 3303 the Loss is 0.025.\n",
      "For Iteration 3304 the Loss is 0.025.\n",
      "For Iteration 3305 the Loss is 0.025.\n",
      "For Iteration 3306 the Loss is 0.025.\n",
      "For Iteration 3307 the Loss is 0.025.\n",
      "For Iteration 3308 the Loss is 0.025.\n",
      "For Iteration 3309 the Loss is 0.025.\n",
      "For Iteration 3310 the Loss is 0.0249.\n",
      "For Iteration 3311 the Loss is 0.0249.\n",
      "For Iteration 3312 the Loss is 0.0249.\n",
      "For Iteration 3313 the Loss is 0.0249.\n",
      "For Iteration 3314 the Loss is 0.0249.\n",
      "For Iteration 3315 the Loss is 0.0249.\n",
      "For Iteration 3316 the Loss is 0.0249.\n",
      "For Iteration 3317 the Loss is 0.0249.\n",
      "For Iteration 3318 the Loss is 0.0249.\n",
      "For Iteration 3319 the Loss is 0.0249.\n",
      "For Iteration 3320 the Loss is 0.0249.\n",
      "For Iteration 3321 the Loss is 0.0249.\n",
      "For Iteration 3322 the Loss is 0.0249.\n",
      "For Iteration 3323 the Loss is 0.0249.\n",
      "For Iteration 3324 the Loss is 0.0249.\n",
      "For Iteration 3325 the Loss is 0.0249.\n",
      "For Iteration 3326 the Loss is 0.0248.\n",
      "For Iteration 3327 the Loss is 0.0248.\n",
      "For Iteration 3328 the Loss is 0.0248.\n",
      "For Iteration 3329 the Loss is 0.0248.\n",
      "For Iteration 3330 the Loss is 0.0248.\n",
      "For Iteration 3331 the Loss is 0.0248.\n",
      "For Iteration 3332 the Loss is 0.0248.\n",
      "For Iteration 3333 the Loss is 0.0248.\n",
      "For Iteration 3334 the Loss is 0.0248.\n",
      "For Iteration 3335 the Loss is 0.0248.\n",
      "For Iteration 3336 the Loss is 0.0248.\n",
      "For Iteration 3337 the Loss is 0.0248.\n",
      "For Iteration 3338 the Loss is 0.0248.\n",
      "For Iteration 3339 the Loss is 0.0248.\n",
      "For Iteration 3340 the Loss is 0.0248.\n",
      "For Iteration 3341 the Loss is 0.0248.\n",
      "For Iteration 3342 the Loss is 0.0247.\n",
      "For Iteration 3343 the Loss is 0.0247.\n",
      "For Iteration 3344 the Loss is 0.0247.\n",
      "For Iteration 3345 the Loss is 0.0247.\n",
      "For Iteration 3346 the Loss is 0.0247.\n",
      "For Iteration 3347 the Loss is 0.0247.\n",
      "For Iteration 3348 the Loss is 0.0247.\n",
      "For Iteration 3349 the Loss is 0.0247.\n",
      "For Iteration 3350 the Loss is 0.0247.\n",
      "For Iteration 3351 the Loss is 0.0247.\n",
      "For Iteration 3352 the Loss is 0.0247.\n",
      "For Iteration 3353 the Loss is 0.0247.\n",
      "For Iteration 3354 the Loss is 0.0247.\n",
      "For Iteration 3355 the Loss is 0.0247.\n",
      "For Iteration 3356 the Loss is 0.0247.\n",
      "For Iteration 3357 the Loss is 0.0246.\n",
      "For Iteration 3358 the Loss is 0.0246.\n",
      "For Iteration 3359 the Loss is 0.0246.\n",
      "For Iteration 3360 the Loss is 0.0246.\n",
      "For Iteration 3361 the Loss is 0.0246.\n",
      "For Iteration 3362 the Loss is 0.0246.\n",
      "For Iteration 3363 the Loss is 0.0246.\n",
      "For Iteration 3364 the Loss is 0.0246.\n",
      "For Iteration 3365 the Loss is 0.0246.\n",
      "For Iteration 3366 the Loss is 0.0246.\n",
      "For Iteration 3367 the Loss is 0.0246.\n",
      "For Iteration 3368 the Loss is 0.0246.\n",
      "For Iteration 3369 the Loss is 0.0246.\n",
      "For Iteration 3370 the Loss is 0.0246.\n",
      "For Iteration 3371 the Loss is 0.0246.\n",
      "For Iteration 3372 the Loss is 0.0246.\n",
      "For Iteration 3373 the Loss is 0.0245.\n",
      "For Iteration 3374 the Loss is 0.0245.\n",
      "For Iteration 3375 the Loss is 0.0245.\n",
      "For Iteration 3376 the Loss is 0.0245.\n",
      "For Iteration 3377 the Loss is 0.0245.\n",
      "For Iteration 3378 the Loss is 0.0245.\n",
      "For Iteration 3379 the Loss is 0.0245.\n",
      "For Iteration 3380 the Loss is 0.0245.\n",
      "For Iteration 3381 the Loss is 0.0245.\n",
      "For Iteration 3382 the Loss is 0.0245.\n",
      "For Iteration 3383 the Loss is 0.0245.\n",
      "For Iteration 3384 the Loss is 0.0245.\n",
      "For Iteration 3385 the Loss is 0.0245.\n",
      "For Iteration 3386 the Loss is 0.0245.\n",
      "For Iteration 3387 the Loss is 0.0245.\n",
      "For Iteration 3388 the Loss is 0.0245.\n",
      "For Iteration 3389 the Loss is 0.0244.\n",
      "For Iteration 3390 the Loss is 0.0244.\n",
      "For Iteration 3391 the Loss is 0.0244.\n",
      "For Iteration 3392 the Loss is 0.0244.\n",
      "For Iteration 3393 the Loss is 0.0244.\n",
      "For Iteration 3394 the Loss is 0.0244.\n",
      "For Iteration 3395 the Loss is 0.0244.\n",
      "For Iteration 3396 the Loss is 0.0244.\n",
      "For Iteration 3397 the Loss is 0.0244.\n",
      "For Iteration 3398 the Loss is 0.0244.\n",
      "For Iteration 3399 the Loss is 0.0244.\n",
      "For Iteration 3400 the Loss is 0.0244.\n",
      "For Iteration 3401 the Loss is 0.0244.\n",
      "For Iteration 3402 the Loss is 0.0244.\n",
      "For Iteration 3403 the Loss is 0.0244.\n",
      "For Iteration 3404 the Loss is 0.0244.\n",
      "For Iteration 3405 the Loss is 0.0243.\n",
      "For Iteration 3406 the Loss is 0.0243.\n",
      "For Iteration 3407 the Loss is 0.0243.\n",
      "For Iteration 3408 the Loss is 0.0243.\n",
      "For Iteration 3409 the Loss is 0.0243.\n",
      "For Iteration 3410 the Loss is 0.0243.\n",
      "For Iteration 3411 the Loss is 0.0243.\n",
      "For Iteration 3412 the Loss is 0.0243.\n",
      "For Iteration 3413 the Loss is 0.0243.\n",
      "For Iteration 3414 the Loss is 0.0243.\n",
      "For Iteration 3415 the Loss is 0.0243.\n",
      "For Iteration 3416 the Loss is 0.0243.\n",
      "For Iteration 3417 the Loss is 0.0243.\n",
      "For Iteration 3418 the Loss is 0.0243.\n",
      "For Iteration 3419 the Loss is 0.0243.\n",
      "For Iteration 3420 the Loss is 0.0243.\n",
      "For Iteration 3421 the Loss is 0.0243.\n",
      "For Iteration 3422 the Loss is 0.0242.\n",
      "For Iteration 3423 the Loss is 0.0242.\n",
      "For Iteration 3424 the Loss is 0.0242.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 3425 the Loss is 0.0242.\n",
      "For Iteration 3426 the Loss is 0.0242.\n",
      "For Iteration 3427 the Loss is 0.0242.\n",
      "For Iteration 3428 the Loss is 0.0242.\n",
      "For Iteration 3429 the Loss is 0.0242.\n",
      "For Iteration 3430 the Loss is 0.0242.\n",
      "For Iteration 3431 the Loss is 0.0242.\n",
      "For Iteration 3432 the Loss is 0.0242.\n",
      "For Iteration 3433 the Loss is 0.0242.\n",
      "For Iteration 3434 the Loss is 0.0242.\n",
      "For Iteration 3435 the Loss is 0.0242.\n",
      "For Iteration 3436 the Loss is 0.0242.\n",
      "For Iteration 3437 the Loss is 0.0242.\n",
      "For Iteration 3438 the Loss is 0.0241.\n",
      "For Iteration 3439 the Loss is 0.0241.\n",
      "For Iteration 3440 the Loss is 0.0241.\n",
      "For Iteration 3441 the Loss is 0.0241.\n",
      "For Iteration 3442 the Loss is 0.0241.\n",
      "For Iteration 3443 the Loss is 0.0241.\n",
      "For Iteration 3444 the Loss is 0.0241.\n",
      "For Iteration 3445 the Loss is 0.0241.\n",
      "For Iteration 3446 the Loss is 0.0241.\n",
      "For Iteration 3447 the Loss is 0.0241.\n",
      "For Iteration 3448 the Loss is 0.0241.\n",
      "For Iteration 3449 the Loss is 0.0241.\n",
      "For Iteration 3450 the Loss is 0.0241.\n",
      "For Iteration 3451 the Loss is 0.0241.\n",
      "For Iteration 3452 the Loss is 0.0241.\n",
      "For Iteration 3453 the Loss is 0.0241.\n",
      "For Iteration 3454 the Loss is 0.024.\n",
      "For Iteration 3455 the Loss is 0.024.\n",
      "For Iteration 3456 the Loss is 0.024.\n",
      "For Iteration 3457 the Loss is 0.024.\n",
      "For Iteration 3458 the Loss is 0.024.\n",
      "For Iteration 3459 the Loss is 0.024.\n",
      "For Iteration 3460 the Loss is 0.024.\n",
      "For Iteration 3461 the Loss is 0.024.\n",
      "For Iteration 3462 the Loss is 0.024.\n",
      "For Iteration 3463 the Loss is 0.024.\n",
      "For Iteration 3464 the Loss is 0.024.\n",
      "For Iteration 3465 the Loss is 0.024.\n",
      "For Iteration 3466 the Loss is 0.024.\n",
      "For Iteration 3467 the Loss is 0.024.\n",
      "For Iteration 3468 the Loss is 0.024.\n",
      "For Iteration 3469 the Loss is 0.024.\n",
      "For Iteration 3470 the Loss is 0.024.\n",
      "For Iteration 3471 the Loss is 0.0239.\n",
      "For Iteration 3472 the Loss is 0.0239.\n",
      "For Iteration 3473 the Loss is 0.0239.\n",
      "For Iteration 3474 the Loss is 0.0239.\n",
      "For Iteration 3475 the Loss is 0.0239.\n",
      "For Iteration 3476 the Loss is 0.0239.\n",
      "For Iteration 3477 the Loss is 0.0239.\n",
      "For Iteration 3478 the Loss is 0.0239.\n",
      "For Iteration 3479 the Loss is 0.0239.\n",
      "For Iteration 3480 the Loss is 0.0239.\n",
      "For Iteration 3481 the Loss is 0.0239.\n",
      "For Iteration 3482 the Loss is 0.0239.\n",
      "For Iteration 3483 the Loss is 0.0239.\n",
      "For Iteration 3484 the Loss is 0.0239.\n",
      "For Iteration 3485 the Loss is 0.0239.\n",
      "For Iteration 3486 the Loss is 0.0239.\n",
      "For Iteration 3487 the Loss is 0.0238.\n",
      "For Iteration 3488 the Loss is 0.0238.\n",
      "For Iteration 3489 the Loss is 0.0238.\n",
      "For Iteration 3490 the Loss is 0.0238.\n",
      "For Iteration 3491 the Loss is 0.0238.\n",
      "For Iteration 3492 the Loss is 0.0238.\n",
      "For Iteration 3493 the Loss is 0.0238.\n",
      "For Iteration 3494 the Loss is 0.0238.\n",
      "For Iteration 3495 the Loss is 0.0238.\n",
      "For Iteration 3496 the Loss is 0.0238.\n",
      "For Iteration 3497 the Loss is 0.0238.\n",
      "For Iteration 3498 the Loss is 0.0238.\n",
      "For Iteration 3499 the Loss is 0.0238.\n",
      "For Iteration 3500 the Loss is 0.0238.\n",
      "For Iteration 3501 the Loss is 0.0238.\n",
      "For Iteration 3502 the Loss is 0.0238.\n",
      "For Iteration 3503 the Loss is 0.0238.\n",
      "For Iteration 3504 the Loss is 0.0237.\n",
      "For Iteration 3505 the Loss is 0.0237.\n",
      "For Iteration 3506 the Loss is 0.0237.\n",
      "For Iteration 3507 the Loss is 0.0237.\n",
      "For Iteration 3508 the Loss is 0.0237.\n",
      "For Iteration 3509 the Loss is 0.0237.\n",
      "For Iteration 3510 the Loss is 0.0237.\n",
      "For Iteration 3511 the Loss is 0.0237.\n",
      "For Iteration 3512 the Loss is 0.0237.\n",
      "For Iteration 3513 the Loss is 0.0237.\n",
      "For Iteration 3514 the Loss is 0.0237.\n",
      "For Iteration 3515 the Loss is 0.0237.\n",
      "For Iteration 3516 the Loss is 0.0237.\n",
      "For Iteration 3517 the Loss is 0.0237.\n",
      "For Iteration 3518 the Loss is 0.0237.\n",
      "For Iteration 3519 the Loss is 0.0237.\n",
      "For Iteration 3520 the Loss is 0.0237.\n",
      "For Iteration 3521 the Loss is 0.0236.\n",
      "For Iteration 3522 the Loss is 0.0236.\n",
      "For Iteration 3523 the Loss is 0.0236.\n",
      "For Iteration 3524 the Loss is 0.0236.\n",
      "For Iteration 3525 the Loss is 0.0236.\n",
      "For Iteration 3526 the Loss is 0.0236.\n",
      "For Iteration 3527 the Loss is 0.0236.\n",
      "For Iteration 3528 the Loss is 0.0236.\n",
      "For Iteration 3529 the Loss is 0.0236.\n",
      "For Iteration 3530 the Loss is 0.0236.\n",
      "For Iteration 3531 the Loss is 0.0236.\n",
      "For Iteration 3532 the Loss is 0.0236.\n",
      "For Iteration 3533 the Loss is 0.0236.\n",
      "For Iteration 3534 the Loss is 0.0236.\n",
      "For Iteration 3535 the Loss is 0.0236.\n",
      "For Iteration 3536 the Loss is 0.0236.\n",
      "For Iteration 3537 the Loss is 0.0236.\n",
      "For Iteration 3538 the Loss is 0.0235.\n",
      "For Iteration 3539 the Loss is 0.0235.\n",
      "For Iteration 3540 the Loss is 0.0235.\n",
      "For Iteration 3541 the Loss is 0.0235.\n",
      "For Iteration 3542 the Loss is 0.0235.\n",
      "For Iteration 3543 the Loss is 0.0235.\n",
      "For Iteration 3544 the Loss is 0.0235.\n",
      "For Iteration 3545 the Loss is 0.0235.\n",
      "For Iteration 3546 the Loss is 0.0235.\n",
      "For Iteration 3547 the Loss is 0.0235.\n",
      "For Iteration 3548 the Loss is 0.0235.\n",
      "For Iteration 3549 the Loss is 0.0235.\n",
      "For Iteration 3550 the Loss is 0.0235.\n",
      "For Iteration 3551 the Loss is 0.0235.\n",
      "For Iteration 3552 the Loss is 0.0235.\n",
      "For Iteration 3553 the Loss is 0.0235.\n",
      "For Iteration 3554 the Loss is 0.0235.\n",
      "For Iteration 3555 the Loss is 0.0234.\n",
      "For Iteration 3556 the Loss is 0.0234.\n",
      "For Iteration 3557 the Loss is 0.0234.\n",
      "For Iteration 3558 the Loss is 0.0234.\n",
      "For Iteration 3559 the Loss is 0.0234.\n",
      "For Iteration 3560 the Loss is 0.0234.\n",
      "For Iteration 3561 the Loss is 0.0234.\n",
      "For Iteration 3562 the Loss is 0.0234.\n",
      "For Iteration 3563 the Loss is 0.0234.\n",
      "For Iteration 3564 the Loss is 0.0234.\n",
      "For Iteration 3565 the Loss is 0.0234.\n",
      "For Iteration 3566 the Loss is 0.0234.\n",
      "For Iteration 3567 the Loss is 0.0234.\n",
      "For Iteration 3568 the Loss is 0.0234.\n",
      "For Iteration 3569 the Loss is 0.0234.\n",
      "For Iteration 3570 the Loss is 0.0234.\n",
      "For Iteration 3571 the Loss is 0.0234.\n",
      "For Iteration 3572 the Loss is 0.0233.\n",
      "For Iteration 3573 the Loss is 0.0233.\n",
      "For Iteration 3574 the Loss is 0.0233.\n",
      "For Iteration 3575 the Loss is 0.0233.\n",
      "For Iteration 3576 the Loss is 0.0233.\n",
      "For Iteration 3577 the Loss is 0.0233.\n",
      "For Iteration 3578 the Loss is 0.0233.\n",
      "For Iteration 3579 the Loss is 0.0233.\n",
      "For Iteration 3580 the Loss is 0.0233.\n",
      "For Iteration 3581 the Loss is 0.0233.\n",
      "For Iteration 3582 the Loss is 0.0233.\n",
      "For Iteration 3583 the Loss is 0.0233.\n",
      "For Iteration 3584 the Loss is 0.0233.\n",
      "For Iteration 3585 the Loss is 0.0233.\n",
      "For Iteration 3586 the Loss is 0.0233.\n",
      "For Iteration 3587 the Loss is 0.0233.\n",
      "For Iteration 3588 the Loss is 0.0233.\n",
      "For Iteration 3589 the Loss is 0.0232.\n",
      "For Iteration 3590 the Loss is 0.0232.\n",
      "For Iteration 3591 the Loss is 0.0232.\n",
      "For Iteration 3592 the Loss is 0.0232.\n",
      "For Iteration 3593 the Loss is 0.0232.\n",
      "For Iteration 3594 the Loss is 0.0232.\n",
      "For Iteration 3595 the Loss is 0.0232.\n",
      "For Iteration 3596 the Loss is 0.0232.\n",
      "For Iteration 3597 the Loss is 0.0232.\n",
      "For Iteration 3598 the Loss is 0.0232.\n",
      "For Iteration 3599 the Loss is 0.0232.\n",
      "For Iteration 3600 the Loss is 0.0232.\n",
      "For Iteration 3601 the Loss is 0.0232.\n",
      "For Iteration 3602 the Loss is 0.0232.\n",
      "For Iteration 3603 the Loss is 0.0232.\n",
      "For Iteration 3604 the Loss is 0.0232.\n",
      "For Iteration 3605 the Loss is 0.0232.\n",
      "For Iteration 3606 the Loss is 0.0231.\n",
      "For Iteration 3607 the Loss is 0.0231.\n",
      "For Iteration 3608 the Loss is 0.0231.\n",
      "For Iteration 3609 the Loss is 0.0231.\n",
      "For Iteration 3610 the Loss is 0.0231.\n",
      "For Iteration 3611 the Loss is 0.0231.\n",
      "For Iteration 3612 the Loss is 0.0231.\n",
      "For Iteration 3613 the Loss is 0.0231.\n",
      "For Iteration 3614 the Loss is 0.0231.\n",
      "For Iteration 3615 the Loss is 0.0231.\n",
      "For Iteration 3616 the Loss is 0.0231.\n",
      "For Iteration 3617 the Loss is 0.0231.\n",
      "For Iteration 3618 the Loss is 0.0231.\n",
      "For Iteration 3619 the Loss is 0.0231.\n",
      "For Iteration 3620 the Loss is 0.0231.\n",
      "For Iteration 3621 the Loss is 0.0231.\n",
      "For Iteration 3622 the Loss is 0.0231.\n",
      "For Iteration 3623 the Loss is 0.0231.\n",
      "For Iteration 3624 the Loss is 0.023.\n",
      "For Iteration 3625 the Loss is 0.023.\n",
      "For Iteration 3626 the Loss is 0.023.\n",
      "For Iteration 3627 the Loss is 0.023.\n",
      "For Iteration 3628 the Loss is 0.023.\n",
      "For Iteration 3629 the Loss is 0.023.\n",
      "For Iteration 3630 the Loss is 0.023.\n",
      "For Iteration 3631 the Loss is 0.023.\n",
      "For Iteration 3632 the Loss is 0.023.\n",
      "For Iteration 3633 the Loss is 0.023.\n",
      "For Iteration 3634 the Loss is 0.023.\n",
      "For Iteration 3635 the Loss is 0.023.\n",
      "For Iteration 3636 the Loss is 0.023.\n",
      "For Iteration 3637 the Loss is 0.023.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 3638 the Loss is 0.023.\n",
      "For Iteration 3639 the Loss is 0.023.\n",
      "For Iteration 3640 the Loss is 0.023.\n",
      "For Iteration 3641 the Loss is 0.023.\n",
      "For Iteration 3642 the Loss is 0.0229.\n",
      "For Iteration 3643 the Loss is 0.0229.\n",
      "For Iteration 3644 the Loss is 0.0229.\n",
      "For Iteration 3645 the Loss is 0.0229.\n",
      "For Iteration 3646 the Loss is 0.0229.\n",
      "For Iteration 3647 the Loss is 0.0229.\n",
      "For Iteration 3648 the Loss is 0.0229.\n",
      "For Iteration 3649 the Loss is 0.0229.\n",
      "For Iteration 3650 the Loss is 0.0229.\n",
      "For Iteration 3651 the Loss is 0.0229.\n",
      "For Iteration 3652 the Loss is 0.0229.\n",
      "For Iteration 3653 the Loss is 0.0229.\n",
      "For Iteration 3654 the Loss is 0.0229.\n",
      "For Iteration 3655 the Loss is 0.0229.\n",
      "For Iteration 3656 the Loss is 0.0229.\n",
      "For Iteration 3657 the Loss is 0.0229.\n",
      "For Iteration 3658 the Loss is 0.0229.\n",
      "For Iteration 3659 the Loss is 0.0228.\n",
      "For Iteration 3660 the Loss is 0.0228.\n",
      "For Iteration 3661 the Loss is 0.0228.\n",
      "For Iteration 3662 the Loss is 0.0228.\n",
      "For Iteration 3663 the Loss is 0.0228.\n",
      "For Iteration 3664 the Loss is 0.0228.\n",
      "For Iteration 3665 the Loss is 0.0228.\n",
      "For Iteration 3666 the Loss is 0.0228.\n",
      "For Iteration 3667 the Loss is 0.0228.\n",
      "For Iteration 3668 the Loss is 0.0228.\n",
      "For Iteration 3669 the Loss is 0.0228.\n",
      "For Iteration 3670 the Loss is 0.0228.\n",
      "For Iteration 3671 the Loss is 0.0228.\n",
      "For Iteration 3672 the Loss is 0.0228.\n",
      "For Iteration 3673 the Loss is 0.0228.\n",
      "For Iteration 3674 the Loss is 0.0228.\n",
      "For Iteration 3675 the Loss is 0.0228.\n",
      "For Iteration 3676 the Loss is 0.0228.\n",
      "For Iteration 3677 the Loss is 0.0227.\n",
      "For Iteration 3678 the Loss is 0.0227.\n",
      "For Iteration 3679 the Loss is 0.0227.\n",
      "For Iteration 3680 the Loss is 0.0227.\n",
      "For Iteration 3681 the Loss is 0.0227.\n",
      "For Iteration 3682 the Loss is 0.0227.\n",
      "For Iteration 3683 the Loss is 0.0227.\n",
      "For Iteration 3684 the Loss is 0.0227.\n",
      "For Iteration 3685 the Loss is 0.0227.\n",
      "For Iteration 3686 the Loss is 0.0227.\n",
      "For Iteration 3687 the Loss is 0.0227.\n",
      "For Iteration 3688 the Loss is 0.0227.\n",
      "For Iteration 3689 the Loss is 0.0227.\n",
      "For Iteration 3690 the Loss is 0.0227.\n",
      "For Iteration 3691 the Loss is 0.0227.\n",
      "For Iteration 3692 the Loss is 0.0227.\n",
      "For Iteration 3693 the Loss is 0.0227.\n",
      "For Iteration 3694 the Loss is 0.0227.\n",
      "For Iteration 3695 the Loss is 0.0226.\n",
      "For Iteration 3696 the Loss is 0.0226.\n",
      "For Iteration 3697 the Loss is 0.0226.\n",
      "For Iteration 3698 the Loss is 0.0226.\n",
      "For Iteration 3699 the Loss is 0.0226.\n",
      "For Iteration 3700 the Loss is 0.0226.\n",
      "For Iteration 3701 the Loss is 0.0226.\n",
      "For Iteration 3702 the Loss is 0.0226.\n",
      "For Iteration 3703 the Loss is 0.0226.\n",
      "For Iteration 3704 the Loss is 0.0226.\n",
      "For Iteration 3705 the Loss is 0.0226.\n",
      "For Iteration 3706 the Loss is 0.0226.\n",
      "For Iteration 3707 the Loss is 0.0226.\n",
      "For Iteration 3708 the Loss is 0.0226.\n",
      "For Iteration 3709 the Loss is 0.0226.\n",
      "For Iteration 3710 the Loss is 0.0226.\n",
      "For Iteration 3711 the Loss is 0.0226.\n",
      "For Iteration 3712 the Loss is 0.0226.\n",
      "For Iteration 3713 the Loss is 0.0225.\n",
      "For Iteration 3714 the Loss is 0.0225.\n",
      "For Iteration 3715 the Loss is 0.0225.\n",
      "For Iteration 3716 the Loss is 0.0225.\n",
      "For Iteration 3717 the Loss is 0.0225.\n",
      "For Iteration 3718 the Loss is 0.0225.\n",
      "For Iteration 3719 the Loss is 0.0225.\n",
      "For Iteration 3720 the Loss is 0.0225.\n",
      "For Iteration 3721 the Loss is 0.0225.\n",
      "For Iteration 3722 the Loss is 0.0225.\n",
      "For Iteration 3723 the Loss is 0.0225.\n",
      "For Iteration 3724 the Loss is 0.0225.\n",
      "For Iteration 3725 the Loss is 0.0225.\n",
      "For Iteration 3726 the Loss is 0.0225.\n",
      "For Iteration 3727 the Loss is 0.0225.\n",
      "For Iteration 3728 the Loss is 0.0225.\n",
      "For Iteration 3729 the Loss is 0.0225.\n",
      "For Iteration 3730 the Loss is 0.0225.\n",
      "For Iteration 3731 the Loss is 0.0224.\n",
      "For Iteration 3732 the Loss is 0.0224.\n",
      "For Iteration 3733 the Loss is 0.0224.\n",
      "For Iteration 3734 the Loss is 0.0224.\n",
      "For Iteration 3735 the Loss is 0.0224.\n",
      "For Iteration 3736 the Loss is 0.0224.\n",
      "For Iteration 3737 the Loss is 0.0224.\n",
      "For Iteration 3738 the Loss is 0.0224.\n",
      "For Iteration 3739 the Loss is 0.0224.\n",
      "For Iteration 3740 the Loss is 0.0224.\n",
      "For Iteration 3741 the Loss is 0.0224.\n",
      "For Iteration 3742 the Loss is 0.0224.\n",
      "For Iteration 3743 the Loss is 0.0224.\n",
      "For Iteration 3744 the Loss is 0.0224.\n",
      "For Iteration 3745 the Loss is 0.0224.\n",
      "For Iteration 3746 the Loss is 0.0224.\n",
      "For Iteration 3747 the Loss is 0.0224.\n",
      "For Iteration 3748 the Loss is 0.0224.\n",
      "For Iteration 3749 the Loss is 0.0224.\n",
      "For Iteration 3750 the Loss is 0.0223.\n",
      "For Iteration 3751 the Loss is 0.0223.\n",
      "For Iteration 3752 the Loss is 0.0223.\n",
      "For Iteration 3753 the Loss is 0.0223.\n",
      "For Iteration 3754 the Loss is 0.0223.\n",
      "For Iteration 3755 the Loss is 0.0223.\n",
      "For Iteration 3756 the Loss is 0.0223.\n",
      "For Iteration 3757 the Loss is 0.0223.\n",
      "For Iteration 3758 the Loss is 0.0223.\n",
      "For Iteration 3759 the Loss is 0.0223.\n",
      "For Iteration 3760 the Loss is 0.0223.\n",
      "For Iteration 3761 the Loss is 0.0223.\n",
      "For Iteration 3762 the Loss is 0.0223.\n",
      "For Iteration 3763 the Loss is 0.0223.\n",
      "For Iteration 3764 the Loss is 0.0223.\n",
      "For Iteration 3765 the Loss is 0.0223.\n",
      "For Iteration 3766 the Loss is 0.0223.\n",
      "For Iteration 3767 the Loss is 0.0223.\n",
      "For Iteration 3768 the Loss is 0.0222.\n",
      "For Iteration 3769 the Loss is 0.0222.\n",
      "For Iteration 3770 the Loss is 0.0222.\n",
      "For Iteration 3771 the Loss is 0.0222.\n",
      "For Iteration 3772 the Loss is 0.0222.\n",
      "For Iteration 3773 the Loss is 0.0222.\n",
      "For Iteration 3774 the Loss is 0.0222.\n",
      "For Iteration 3775 the Loss is 0.0222.\n",
      "For Iteration 3776 the Loss is 0.0222.\n",
      "For Iteration 3777 the Loss is 0.0222.\n",
      "For Iteration 3778 the Loss is 0.0222.\n",
      "For Iteration 3779 the Loss is 0.0222.\n",
      "For Iteration 3780 the Loss is 0.0222.\n",
      "For Iteration 3781 the Loss is 0.0222.\n",
      "For Iteration 3782 the Loss is 0.0222.\n",
      "For Iteration 3783 the Loss is 0.0222.\n",
      "For Iteration 3784 the Loss is 0.0222.\n",
      "For Iteration 3785 the Loss is 0.0222.\n",
      "For Iteration 3786 the Loss is 0.0222.\n",
      "For Iteration 3787 the Loss is 0.0221.\n",
      "For Iteration 3788 the Loss is 0.0221.\n",
      "For Iteration 3789 the Loss is 0.0221.\n",
      "For Iteration 3790 the Loss is 0.0221.\n",
      "For Iteration 3791 the Loss is 0.0221.\n",
      "For Iteration 3792 the Loss is 0.0221.\n",
      "For Iteration 3793 the Loss is 0.0221.\n",
      "For Iteration 3794 the Loss is 0.0221.\n",
      "For Iteration 3795 the Loss is 0.0221.\n",
      "For Iteration 3796 the Loss is 0.0221.\n",
      "For Iteration 3797 the Loss is 0.0221.\n",
      "For Iteration 3798 the Loss is 0.0221.\n",
      "For Iteration 3799 the Loss is 0.0221.\n",
      "For Iteration 3800 the Loss is 0.0221.\n",
      "For Iteration 3801 the Loss is 0.0221.\n",
      "For Iteration 3802 the Loss is 0.0221.\n",
      "For Iteration 3803 the Loss is 0.0221.\n",
      "For Iteration 3804 the Loss is 0.0221.\n",
      "For Iteration 3805 the Loss is 0.0221.\n",
      "For Iteration 3806 the Loss is 0.022.\n",
      "For Iteration 3807 the Loss is 0.022.\n",
      "For Iteration 3808 the Loss is 0.022.\n",
      "For Iteration 3809 the Loss is 0.022.\n",
      "For Iteration 3810 the Loss is 0.022.\n",
      "For Iteration 3811 the Loss is 0.022.\n",
      "For Iteration 3812 the Loss is 0.022.\n",
      "For Iteration 3813 the Loss is 0.022.\n",
      "For Iteration 3814 the Loss is 0.022.\n",
      "For Iteration 3815 the Loss is 0.022.\n",
      "For Iteration 3816 the Loss is 0.022.\n",
      "For Iteration 3817 the Loss is 0.022.\n",
      "For Iteration 3818 the Loss is 0.022.\n",
      "For Iteration 3819 the Loss is 0.022.\n",
      "For Iteration 3820 the Loss is 0.022.\n",
      "For Iteration 3821 the Loss is 0.022.\n",
      "For Iteration 3822 the Loss is 0.022.\n",
      "For Iteration 3823 the Loss is 0.022.\n",
      "For Iteration 3824 the Loss is 0.022.\n",
      "For Iteration 3825 the Loss is 0.0219.\n",
      "For Iteration 3826 the Loss is 0.0219.\n",
      "For Iteration 3827 the Loss is 0.0219.\n",
      "For Iteration 3828 the Loss is 0.0219.\n",
      "For Iteration 3829 the Loss is 0.0219.\n",
      "For Iteration 3830 the Loss is 0.0219.\n",
      "For Iteration 3831 the Loss is 0.0219.\n",
      "For Iteration 3832 the Loss is 0.0219.\n",
      "For Iteration 3833 the Loss is 0.0219.\n",
      "For Iteration 3834 the Loss is 0.0219.\n",
      "For Iteration 3835 the Loss is 0.0219.\n",
      "For Iteration 3836 the Loss is 0.0219.\n",
      "For Iteration 3837 the Loss is 0.0219.\n",
      "For Iteration 3838 the Loss is 0.0219.\n",
      "For Iteration 3839 the Loss is 0.0219.\n",
      "For Iteration 3840 the Loss is 0.0219.\n",
      "For Iteration 3841 the Loss is 0.0219.\n",
      "For Iteration 3842 the Loss is 0.0219.\n",
      "For Iteration 3843 the Loss is 0.0219.\n",
      "For Iteration 3844 the Loss is 0.0218.\n",
      "For Iteration 3845 the Loss is 0.0218.\n",
      "For Iteration 3846 the Loss is 0.0218.\n",
      "For Iteration 3847 the Loss is 0.0218.\n",
      "For Iteration 3848 the Loss is 0.0218.\n",
      "For Iteration 3849 the Loss is 0.0218.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 3850 the Loss is 0.0218.\n",
      "For Iteration 3851 the Loss is 0.0218.\n",
      "For Iteration 3852 the Loss is 0.0218.\n",
      "For Iteration 3853 the Loss is 0.0218.\n",
      "For Iteration 3854 the Loss is 0.0218.\n",
      "For Iteration 3855 the Loss is 0.0218.\n",
      "For Iteration 3856 the Loss is 0.0218.\n",
      "For Iteration 3857 the Loss is 0.0218.\n",
      "For Iteration 3858 the Loss is 0.0218.\n",
      "For Iteration 3859 the Loss is 0.0218.\n",
      "For Iteration 3860 the Loss is 0.0218.\n",
      "For Iteration 3861 the Loss is 0.0218.\n",
      "For Iteration 3862 the Loss is 0.0218.\n",
      "For Iteration 3863 the Loss is 0.0217.\n",
      "For Iteration 3864 the Loss is 0.0217.\n",
      "For Iteration 3865 the Loss is 0.0217.\n",
      "For Iteration 3866 the Loss is 0.0217.\n",
      "For Iteration 3867 the Loss is 0.0217.\n",
      "For Iteration 3868 the Loss is 0.0217.\n",
      "For Iteration 3869 the Loss is 0.0217.\n",
      "For Iteration 3870 the Loss is 0.0217.\n",
      "For Iteration 3871 the Loss is 0.0217.\n",
      "For Iteration 3872 the Loss is 0.0217.\n",
      "For Iteration 3873 the Loss is 0.0217.\n",
      "For Iteration 3874 the Loss is 0.0217.\n",
      "For Iteration 3875 the Loss is 0.0217.\n",
      "For Iteration 3876 the Loss is 0.0217.\n",
      "For Iteration 3877 the Loss is 0.0217.\n",
      "For Iteration 3878 the Loss is 0.0217.\n",
      "For Iteration 3879 the Loss is 0.0217.\n",
      "For Iteration 3880 the Loss is 0.0217.\n",
      "For Iteration 3881 the Loss is 0.0217.\n",
      "For Iteration 3882 the Loss is 0.0216.\n",
      "For Iteration 3883 the Loss is 0.0216.\n",
      "For Iteration 3884 the Loss is 0.0216.\n",
      "For Iteration 3885 the Loss is 0.0216.\n",
      "For Iteration 3886 the Loss is 0.0216.\n",
      "For Iteration 3887 the Loss is 0.0216.\n",
      "For Iteration 3888 the Loss is 0.0216.\n",
      "For Iteration 3889 the Loss is 0.0216.\n",
      "For Iteration 3890 the Loss is 0.0216.\n",
      "For Iteration 3891 the Loss is 0.0216.\n",
      "For Iteration 3892 the Loss is 0.0216.\n",
      "For Iteration 3893 the Loss is 0.0216.\n",
      "For Iteration 3894 the Loss is 0.0216.\n",
      "For Iteration 3895 the Loss is 0.0216.\n",
      "For Iteration 3896 the Loss is 0.0216.\n",
      "For Iteration 3897 the Loss is 0.0216.\n",
      "For Iteration 3898 the Loss is 0.0216.\n",
      "For Iteration 3899 the Loss is 0.0216.\n",
      "For Iteration 3900 the Loss is 0.0216.\n",
      "For Iteration 3901 the Loss is 0.0216.\n",
      "For Iteration 3902 the Loss is 0.0215.\n",
      "For Iteration 3903 the Loss is 0.0215.\n",
      "For Iteration 3904 the Loss is 0.0215.\n",
      "For Iteration 3905 the Loss is 0.0215.\n",
      "For Iteration 3906 the Loss is 0.0215.\n",
      "For Iteration 3907 the Loss is 0.0215.\n",
      "For Iteration 3908 the Loss is 0.0215.\n",
      "For Iteration 3909 the Loss is 0.0215.\n",
      "For Iteration 3910 the Loss is 0.0215.\n",
      "For Iteration 3911 the Loss is 0.0215.\n",
      "For Iteration 3912 the Loss is 0.0215.\n",
      "For Iteration 3913 the Loss is 0.0215.\n",
      "For Iteration 3914 the Loss is 0.0215.\n",
      "For Iteration 3915 the Loss is 0.0215.\n",
      "For Iteration 3916 the Loss is 0.0215.\n",
      "For Iteration 3917 the Loss is 0.0215.\n",
      "For Iteration 3918 the Loss is 0.0215.\n",
      "For Iteration 3919 the Loss is 0.0215.\n",
      "For Iteration 3920 the Loss is 0.0215.\n",
      "For Iteration 3921 the Loss is 0.0214.\n",
      "For Iteration 3922 the Loss is 0.0214.\n",
      "For Iteration 3923 the Loss is 0.0214.\n",
      "For Iteration 3924 the Loss is 0.0214.\n",
      "For Iteration 3925 the Loss is 0.0214.\n",
      "For Iteration 3926 the Loss is 0.0214.\n",
      "For Iteration 3927 the Loss is 0.0214.\n",
      "For Iteration 3928 the Loss is 0.0214.\n",
      "For Iteration 3929 the Loss is 0.0214.\n",
      "For Iteration 3930 the Loss is 0.0214.\n",
      "For Iteration 3931 the Loss is 0.0214.\n",
      "For Iteration 3932 the Loss is 0.0214.\n",
      "For Iteration 3933 the Loss is 0.0214.\n",
      "For Iteration 3934 the Loss is 0.0214.\n",
      "For Iteration 3935 the Loss is 0.0214.\n",
      "For Iteration 3936 the Loss is 0.0214.\n",
      "For Iteration 3937 the Loss is 0.0214.\n",
      "For Iteration 3938 the Loss is 0.0214.\n",
      "For Iteration 3939 the Loss is 0.0214.\n",
      "For Iteration 3940 the Loss is 0.0214.\n",
      "For Iteration 3941 the Loss is 0.0213.\n",
      "For Iteration 3942 the Loss is 0.0213.\n",
      "For Iteration 3943 the Loss is 0.0213.\n",
      "For Iteration 3944 the Loss is 0.0213.\n",
      "For Iteration 3945 the Loss is 0.0213.\n",
      "For Iteration 3946 the Loss is 0.0213.\n",
      "For Iteration 3947 the Loss is 0.0213.\n",
      "For Iteration 3948 the Loss is 0.0213.\n",
      "For Iteration 3949 the Loss is 0.0213.\n",
      "For Iteration 3950 the Loss is 0.0213.\n",
      "For Iteration 3951 the Loss is 0.0213.\n",
      "For Iteration 3952 the Loss is 0.0213.\n",
      "For Iteration 3953 the Loss is 0.0213.\n",
      "For Iteration 3954 the Loss is 0.0213.\n",
      "For Iteration 3955 the Loss is 0.0213.\n",
      "For Iteration 3956 the Loss is 0.0213.\n",
      "For Iteration 3957 the Loss is 0.0213.\n",
      "For Iteration 3958 the Loss is 0.0213.\n",
      "For Iteration 3959 the Loss is 0.0213.\n",
      "For Iteration 3960 the Loss is 0.0213.\n",
      "For Iteration 3961 the Loss is 0.0212.\n",
      "For Iteration 3962 the Loss is 0.0212.\n",
      "For Iteration 3963 the Loss is 0.0212.\n",
      "For Iteration 3964 the Loss is 0.0212.\n",
      "For Iteration 3965 the Loss is 0.0212.\n",
      "For Iteration 3966 the Loss is 0.0212.\n",
      "For Iteration 3967 the Loss is 0.0212.\n",
      "For Iteration 3968 the Loss is 0.0212.\n",
      "For Iteration 3969 the Loss is 0.0212.\n",
      "For Iteration 3970 the Loss is 0.0212.\n",
      "For Iteration 3971 the Loss is 0.0212.\n",
      "For Iteration 3972 the Loss is 0.0212.\n",
      "For Iteration 3973 the Loss is 0.0212.\n",
      "For Iteration 3974 the Loss is 0.0212.\n",
      "For Iteration 3975 the Loss is 0.0212.\n",
      "For Iteration 3976 the Loss is 0.0212.\n",
      "For Iteration 3977 the Loss is 0.0212.\n",
      "For Iteration 3978 the Loss is 0.0212.\n",
      "For Iteration 3979 the Loss is 0.0212.\n",
      "For Iteration 3980 the Loss is 0.0212.\n",
      "For Iteration 3981 the Loss is 0.0211.\n",
      "For Iteration 3982 the Loss is 0.0211.\n",
      "For Iteration 3983 the Loss is 0.0211.\n",
      "For Iteration 3984 the Loss is 0.0211.\n",
      "For Iteration 3985 the Loss is 0.0211.\n",
      "For Iteration 3986 the Loss is 0.0211.\n",
      "For Iteration 3987 the Loss is 0.0211.\n",
      "For Iteration 3988 the Loss is 0.0211.\n",
      "For Iteration 3989 the Loss is 0.0211.\n",
      "For Iteration 3990 the Loss is 0.0211.\n",
      "For Iteration 3991 the Loss is 0.0211.\n",
      "For Iteration 3992 the Loss is 0.0211.\n",
      "For Iteration 3993 the Loss is 0.0211.\n",
      "For Iteration 3994 the Loss is 0.0211.\n",
      "For Iteration 3995 the Loss is 0.0211.\n",
      "For Iteration 3996 the Loss is 0.0211.\n",
      "For Iteration 3997 the Loss is 0.0211.\n",
      "For Iteration 3998 the Loss is 0.0211.\n",
      "For Iteration 3999 the Loss is 0.0211.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "model_Ridgegd=Ridge_GD(lamda=.01,itr=4000, learning_rate=.001)\n",
    "model_Ridgegd.fit(train_x,train_y)\n",
    "\n",
    "# predicting the test_y\n",
    "model_Ridgegd.predict(test_x)\n",
    "\n",
    "# getting the test error\n",
    "model_Ridgegd.loss(test_x,test_y)\n",
    "\n",
    "end_time = datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4153b5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model: 0:04:50.940519\n"
     ]
    }
   ],
   "source": [
    "# time taken in fitting the model \n",
    "\n",
    "print('Time taken to fit the model: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b69d9df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021059718296063495"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the train error\n",
    "model_Ridgegd.loss(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "decf672a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020927402577616338"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the train error\n",
    "model_Ridgegd.loss(test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5572ff7f",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/1557571/how-do-i-get-time-of-a-python-programs-execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "651ee2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.array(denorm(test_y)),label = \"Actual\",c = \"b\")\n",
    "plt.plot(np.abs(denorm(model_Ridgegd.predict(test_x))),label = \"Predicted\",c = \"g\",linewidth=.7)\n",
    "plt.xlabel(\"sample\")\n",
    "plt.ylabel(\"price\")\n",
    "plt.title(\"Actual VS. Predicted (Ridge GD model)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a4a897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the Ridge regression Using Gradient Descent model as the pickle file\n",
    "\n",
    "import pickle\n",
    "with open('Hasan_Hussain_assignment1_bonus_RidgeGD', 'wb') as files:\n",
    "    pickle.dump(model_Ridgegd, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed476f",
   "metadata": {},
   "source": [
    "# Elastic Net Regularization from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca2dfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elastic:\n",
    "    \n",
    "    def __init__(self, alpha):\n",
    "        self.weight = None\n",
    "        self.alpha= alpha\n",
    "\n",
    "    def ols(self,train_x,train_y):\n",
    "        I=np.identity(train_x.shape[1])\n",
    "        fir=(np.dot(train_x.T,train_x) + self.alpha*I)\n",
    "        fir=np.linalg.inv(fir) \n",
    "        sec=np.dot(train_x.T,train_y)\n",
    "        f_weight=np.dot(fir,sec)\n",
    "        self.weight=f_weight\n",
    "        return (self.weight)\n",
    "\n",
    "    def predict(self,test_x):\n",
    "        y_hat=np.dot(test_x,self.weight.T)\n",
    "        return (y_hat)\n",
    "\n",
    "    def ols_los(self,test_x,test_y):\n",
    "        predicted=self.predict(test_x)\n",
    "        mse=.5*np.mean(((test_y-predicted)**2) + ((self.alpha/2)*(np.dot(self.weight.T,self.weight))) + ((self.alpha/2)*(np.sum(np.abs(self.weight)))))\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "73fddfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MSE is 0.0012868250723234856 for lambda value 1e-06\n"
     ]
    }
   ],
   "source": [
    "elastic_loss={}\n",
    "for i in [.000001,.00001,.001,.1,1]:\n",
    "    model_Elastic=Elastic(alpha=i)\n",
    "    model_Elastic.ols(train_x,train_y)\n",
    "    model_Elastic.predict(test_x)\n",
    "    elastic_loss[i]=model_Elastic.ols_los(test_x,test_y)\n",
    "print(f\"Best MSE is {min(elastic_loss.values())} for lambda value {min(elastic_loss.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3046d53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(denorm(test_y)),label = \"Actual\",c = \"b\")\n",
    "plt.plot(denorm(model_Elastic.predict(test_x)),label = \"Predicted\",c = \"g\",linewidth=.7)\n",
    "plt.xlabel(\"sample\")\n",
    "plt.ylabel(\"price\")\n",
    "plt.title(\"Actual VS. Predicted (Elastic model)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f98d7458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the Elastic Net regression model as the pickle file\n",
    "\n",
    "import pickle\n",
    "with open('Hasan_Hussain_assignment1_bonus_RidgeGD', 'wb') as files:\n",
    "    pickle.dump(model_Elastic, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e162d",
   "metadata": {},
   "source": [
    "# Resouces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a9c96",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/implementation-of-elastic-net-regression-from-scratch/?ref=rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6cda8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
