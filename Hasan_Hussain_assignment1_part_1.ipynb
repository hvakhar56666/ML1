{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09723b38",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "d4625e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Chinstrap</td>\n",
       "      <td>Dream</td>\n",
       "      <td>55.8</td>\n",
       "      <td>19.8</td>\n",
       "      <td>207.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Chinstrap</td>\n",
       "      <td>Dream</td>\n",
       "      <td>43.5</td>\n",
       "      <td>18.1</td>\n",
       "      <td>202.0</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>Chinstrap</td>\n",
       "      <td>Dream</td>\n",
       "      <td>49.6</td>\n",
       "      <td>18.2</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Chinstrap</td>\n",
       "      <td>Dream</td>\n",
       "      <td>50.8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Chinstrap</td>\n",
       "      <td>Dream</td>\n",
       "      <td>50.2</td>\n",
       "      <td>18.7</td>\n",
       "      <td>198.0</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0       Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1       Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2       Adelie  Torgersen            40.3           18.0              195.0   \n",
       "3       Adelie  Torgersen             NaN            NaN                NaN   \n",
       "4       Adelie  Torgersen            36.7           19.3              193.0   \n",
       "..         ...        ...             ...            ...                ...   \n",
       "339  Chinstrap      Dream            55.8           19.8              207.0   \n",
       "340  Chinstrap      Dream            43.5           18.1              202.0   \n",
       "341  Chinstrap      Dream            49.6           18.2              193.0   \n",
       "342  Chinstrap      Dream            50.8           19.0              210.0   \n",
       "343  Chinstrap      Dream            50.2           18.7              198.0   \n",
       "\n",
       "     body_mass_g     sex  year  \n",
       "0         3750.0    male  2007  \n",
       "1         3800.0  female  2007  \n",
       "2         3250.0  female  2007  \n",
       "3            NaN     NaN  2007  \n",
       "4         3450.0  female  2007  \n",
       "..           ...     ...   ...  \n",
       "339       4000.0    male  2009  \n",
       "340       3400.0  female  2009  \n",
       "341       3775.0    male  2009  \n",
       "342       4100.0    male  2009  \n",
       "343       3775.0  female  2009  \n",
       "\n",
       "[344 rows x 8 columns]"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(r\"C:\\Users\\hasan\\Downloads\\datasets\\datasets\\penguins.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f5f63",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "792a530d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>342.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>344.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>43.921930</td>\n",
       "      <td>17.151170</td>\n",
       "      <td>200.915205</td>\n",
       "      <td>4201.754386</td>\n",
       "      <td>2008.029070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.459584</td>\n",
       "      <td>1.974793</td>\n",
       "      <td>14.061714</td>\n",
       "      <td>801.954536</td>\n",
       "      <td>0.818356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>32.100000</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39.225000</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>3550.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>44.450000</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>4050.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.500000</td>\n",
       "      <td>18.700000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>4750.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>59.600000</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>6300.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
       "count      342.000000     342.000000         342.000000   342.000000   \n",
       "mean        43.921930      17.151170         200.915205  4201.754386   \n",
       "std          5.459584       1.974793          14.061714   801.954536   \n",
       "min         32.100000      13.100000         172.000000  2700.000000   \n",
       "25%         39.225000      15.600000         190.000000  3550.000000   \n",
       "50%         44.450000      17.300000         197.000000  4050.000000   \n",
       "75%         48.500000      18.700000         213.000000  4750.000000   \n",
       "max         59.600000      21.500000         231.000000  6300.000000   \n",
       "\n",
       "              year  \n",
       "count   344.000000  \n",
       "mean   2008.029070  \n",
       "std       0.818356  \n",
       "min    2007.000000  \n",
       "25%    2007.000000  \n",
       "50%    2008.000000  \n",
       "75%    2009.000000  \n",
       "max    2009.000000  "
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "fae59580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species               0\n",
       "island                0\n",
       "bill_length_mm        2\n",
       "bill_depth_mm         2\n",
       "flipper_length_mm     2\n",
       "body_mass_g           2\n",
       "sex                  11\n",
       "year                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "8baa2dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species              0\n",
       "island               0\n",
       "bill_length_mm       0\n",
       "bill_depth_mm        0\n",
       "flipper_length_mm    0\n",
       "body_mass_g          0\n",
       "sex                  0\n",
       "year                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis=0,inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "6461d8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species               object\n",
       "island                object\n",
       "bill_length_mm       float64\n",
       "bill_depth_mm        float64\n",
       "flipper_length_mm    float64\n",
       "body_mass_g          float64\n",
       "sex                   object\n",
       "year                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ea9a14a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Adelie', 'Gentoo', 'Chinstrap'], dtype=object)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "96553eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Torgersen', 'Biscoe', 'Dream'], dtype=object)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.island.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "223581ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'female'], dtype=object)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sex.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c133a2",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "de301888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['species']==\"Adelie\", 'species'] = 1\n",
    "df.loc[df['species']==\"Gentoo\", 'species'] = 2\n",
    "df.loc[df['species']==\"Chinstrap\", 'species'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "616b4afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[df['island']==\"Torgersen\", 'island'] = 1\n",
    "#df.loc[df['island']==\"Biscoe\", 'island'] = 2\n",
    "#df.loc[df['island']==\"Dream\", 'island'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "ab3ebcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['island']==\"Torgersen\", 'island'] = 1\n",
    "df.loc[df['island']==\"Biscoe\", 'island'] = 0\n",
    "df.loc[df['island']==\"Dream\", 'island'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "c1b3a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['sex']==\"male\", 'sex'] = 1\n",
    "df.loc[df['sex']==\"female\", 'sex'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "bea08efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[1 0]\n",
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "print(df.species.unique())\n",
    "print(df.island.unique())\n",
    "print(df.sex.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "b442669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['island'] = df['island'].astype(\"int\")\n",
    "df['species'] = df['species'].astype(int)\n",
    "df['sex'] = df['sex'].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "97e37dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species                int32\n",
       "island                 int32\n",
       "bill_length_mm       float64\n",
       "bill_depth_mm        float64\n",
       "flipper_length_mm    float64\n",
       "body_mass_g          float64\n",
       "sex                    int32\n",
       "year                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104f7e7",
   "metadata": {},
   "source": [
    "# Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "4cece9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bill_length_mm: Max is 59.6 and Min is 32.1\n",
      "bill_depth_mm: Max is 21.5 and Min is 13.1\n",
      "flipper_length_mm: Max is 231.0 and Min is 172.0\n",
      "body_mass_g: Max is 6300.0 and Min is 2700.0\n"
     ]
    }
   ],
   "source": [
    "non_categorial_col=(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\")\n",
    "for i in non_categorial_col:\n",
    "    maximum=df[i].max()\n",
    "    minimum=df[i].min()\n",
    "    print(f\"{i}: Max is {maximum} and Min is {minimum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "035bdb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in non_categorial_col:\n",
    "    df[i]=(df[i]-df[i].min())/(df[i].max()-df[i].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "f9422f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bill_length_mm: Max is 1.0 and Min is 0.0\n",
      "bill_depth_mm: Max is 1.0 and Min is 0.0\n",
      "flipper_length_mm: Max is 1.0 and Min is 0.0\n",
      "body_mass_g: Max is 1.0 and Min is 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in non_categorial_col:\n",
    "    maximum=df[i].max()\n",
    "    minimum=df[i].min()\n",
    "    print(f\"{i}: Max is {maximum} and Min is {minimum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07181d95",
   "metadata": {},
   "source": [
    "# Step 5,6,7,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "c0b3db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"year\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "0a3eb11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def train_test_split(df):\n",
    "    train_index = np.random.rand(len(df)) < 0.8\n",
    "    train_data = df[train_index]\n",
    "    test_data = df[~train_index]\n",
    "    train_x=train_data.drop(\"island\",axis=1)\n",
    "    test_x=test_data.drop(\"island\",axis=1)\n",
    "    train_y=train_data[\"island\"]\n",
    "    test_y=test_data[\"island\"]\n",
    "    return(train_x,train_y,test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "e3fd439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263, 6)\n",
      "(263,)\n",
      "(70, 6)\n",
      "(70,)\n"
     ]
    }
   ],
   "source": [
    "train_x,train_y,test_x,test_y=train_test_split(df)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff6214",
   "metadata": {},
   "source": [
    "# Step 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "cbfa6702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, itr, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.itr = itr\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss = []\n",
    "        self.we = []\n",
    "        self.bi = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return(1/(1+ np.exp(-z)))\n",
    "\n",
    "    def cost(self,train_y, y_predicted):\n",
    "        return(-(1/train_y.shape[0])*(np.sum(train_y*np.log(y_predicted+ 1e-8)+ (1-train_y) *np.log(1-y_predicted+ 1e-8))))\n",
    "\n",
    "    def gradient_descent(self,x_train,train_y, y_predicted):\n",
    "        delta = y_predicted- train_y\n",
    "        dW=np.dot(x_train.T,delta)/x_train.shape[0]\n",
    "        db =np.sum(y_predicted- train_y)/train_y.shape[0]\n",
    "        return (dW,db)\n",
    "\n",
    "    def fit(self, x_train, train_y):\n",
    "        self.weights=np.ones(x_train.shape[1])\n",
    "        self.bias= 0\n",
    "        for i in range(self.itr):\n",
    "            z = np.dot(x_train,self.weights.T) +self.bias\n",
    "            y_predicted= self.sigmoid(z)\n",
    "            dW,db= self.gradient_descent(x_train,train_y, y_predicted)\n",
    "            self.weights=self.weights-(self.learning_rate*dW)\n",
    "            self.bias=self.bias-(db*self.learning_rate)\n",
    "            cost=self.cost(train_y, y_predicted)\n",
    "            \n",
    "            self.loss.append(cost)\n",
    "            print(f\"For Iteration {i} the Loss is {round(self.loss[i],4)}.\")\n",
    "            self.we.append(self.weights)\n",
    "            self.bi.append(self.bias)\n",
    "\n",
    "    def predict(self, x_train):\n",
    "        z=np.dot(x_train,self.weights.T)+ self.bias\n",
    "        y_predicted=self.sigmoid(z)\n",
    "        tmp=np.ones(x_train.shape[0])\n",
    "        for i in range(x_train.shape[0]):\n",
    "            if y_predicted[i]<.5:\n",
    "                tmp[i]=0\n",
    "            else:\n",
    "                tmp[i]=1\n",
    "            \n",
    "        return(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "935abaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y,y_predicted):\n",
    "    acc=0\n",
    "    n=y_predicted.shape[0]\n",
    "    y_mat = np.array(y)\n",
    "    for i in range(n):\n",
    "        #print(y_mat[i],y_hat[0,i])\n",
    "        if y_mat[i]==y_predicted[i]:\n",
    "            acc+=1\n",
    "        else:\n",
    "            continue\n",
    "    return(round(acc/n,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f9f49",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "c3b798b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 0 the Loss is 4.5526.\n",
      "For Iteration 1 the Loss is 4.5358.\n",
      "For Iteration 2 the Loss is 4.519.\n",
      "For Iteration 3 the Loss is 4.5022.\n",
      "For Iteration 4 the Loss is 4.4855.\n",
      "For Iteration 5 the Loss is 4.4687.\n",
      "For Iteration 6 the Loss is 4.4519.\n",
      "For Iteration 7 the Loss is 4.4352.\n",
      "For Iteration 8 the Loss is 4.4184.\n",
      "For Iteration 9 the Loss is 4.4017.\n",
      "For Iteration 10 the Loss is 4.3849.\n",
      "For Iteration 11 the Loss is 4.3682.\n",
      "For Iteration 12 the Loss is 4.3514.\n",
      "For Iteration 13 the Loss is 4.3347.\n",
      "For Iteration 14 the Loss is 4.318.\n",
      "For Iteration 15 the Loss is 4.3013.\n",
      "For Iteration 16 the Loss is 4.2846.\n",
      "For Iteration 17 the Loss is 4.2679.\n",
      "For Iteration 18 the Loss is 4.2512.\n",
      "For Iteration 19 the Loss is 4.2345.\n",
      "For Iteration 20 the Loss is 4.2178.\n",
      "For Iteration 21 the Loss is 4.2011.\n",
      "For Iteration 22 the Loss is 4.1845.\n",
      "For Iteration 23 the Loss is 4.1678.\n",
      "For Iteration 24 the Loss is 4.1511.\n",
      "For Iteration 25 the Loss is 4.1345.\n",
      "For Iteration 26 the Loss is 4.1179.\n",
      "For Iteration 27 the Loss is 4.1012.\n",
      "For Iteration 28 the Loss is 4.0846.\n",
      "For Iteration 29 the Loss is 4.068.\n",
      "For Iteration 30 the Loss is 4.0514.\n",
      "For Iteration 31 the Loss is 4.0348.\n",
      "For Iteration 32 the Loss is 4.0182.\n",
      "For Iteration 33 the Loss is 4.0016.\n",
      "For Iteration 34 the Loss is 3.985.\n",
      "For Iteration 35 the Loss is 3.9684.\n",
      "For Iteration 36 the Loss is 3.9519.\n",
      "For Iteration 37 the Loss is 3.9353.\n",
      "For Iteration 38 the Loss is 3.9188.\n",
      "For Iteration 39 the Loss is 3.9023.\n",
      "For Iteration 40 the Loss is 3.8857.\n",
      "For Iteration 41 the Loss is 3.8692.\n",
      "For Iteration 42 the Loss is 3.8527.\n",
      "For Iteration 43 the Loss is 3.8362.\n",
      "For Iteration 44 the Loss is 3.8197.\n",
      "For Iteration 45 the Loss is 3.8032.\n",
      "For Iteration 46 the Loss is 3.7868.\n",
      "For Iteration 47 the Loss is 3.7703.\n",
      "For Iteration 48 the Loss is 3.7539.\n",
      "For Iteration 49 the Loss is 3.7374.\n",
      "For Iteration 50 the Loss is 3.721.\n",
      "For Iteration 51 the Loss is 3.7046.\n",
      "For Iteration 52 the Loss is 3.6882.\n",
      "For Iteration 53 the Loss is 3.6718.\n",
      "For Iteration 54 the Loss is 3.6554.\n",
      "For Iteration 55 the Loss is 3.6391.\n",
      "For Iteration 56 the Loss is 3.6227.\n",
      "For Iteration 57 the Loss is 3.6064.\n",
      "For Iteration 58 the Loss is 3.59.\n",
      "For Iteration 59 the Loss is 3.5737.\n",
      "For Iteration 60 the Loss is 3.5574.\n",
      "For Iteration 61 the Loss is 3.5411.\n",
      "For Iteration 62 the Loss is 3.5248.\n",
      "For Iteration 63 the Loss is 3.5086.\n",
      "For Iteration 64 the Loss is 3.4923.\n",
      "For Iteration 65 the Loss is 3.4761.\n",
      "For Iteration 66 the Loss is 3.4598.\n",
      "For Iteration 67 the Loss is 3.4436.\n",
      "For Iteration 68 the Loss is 3.4274.\n",
      "For Iteration 69 the Loss is 3.4112.\n",
      "For Iteration 70 the Loss is 3.3951.\n",
      "For Iteration 71 the Loss is 3.3789.\n",
      "For Iteration 72 the Loss is 3.3628.\n",
      "For Iteration 73 the Loss is 3.3466.\n",
      "For Iteration 74 the Loss is 3.3305.\n",
      "For Iteration 75 the Loss is 3.3144.\n",
      "For Iteration 76 the Loss is 3.2984.\n",
      "For Iteration 77 the Loss is 3.2823.\n",
      "For Iteration 78 the Loss is 3.2663.\n",
      "For Iteration 79 the Loss is 3.2502.\n",
      "For Iteration 80 the Loss is 3.2342.\n",
      "For Iteration 81 the Loss is 3.2182.\n",
      "For Iteration 82 the Loss is 3.2023.\n",
      "For Iteration 83 the Loss is 3.1863.\n",
      "For Iteration 84 the Loss is 3.1704.\n",
      "For Iteration 85 the Loss is 3.1544.\n",
      "For Iteration 86 the Loss is 3.1385.\n",
      "For Iteration 87 the Loss is 3.1227.\n",
      "For Iteration 88 the Loss is 3.1068.\n",
      "For Iteration 89 the Loss is 3.091.\n",
      "For Iteration 90 the Loss is 3.0751.\n",
      "For Iteration 91 the Loss is 3.0593.\n",
      "For Iteration 92 the Loss is 3.0436.\n",
      "For Iteration 93 the Loss is 3.0278.\n",
      "For Iteration 94 the Loss is 3.0121.\n",
      "For Iteration 95 the Loss is 2.9963.\n",
      "For Iteration 96 the Loss is 2.9806.\n",
      "For Iteration 97 the Loss is 2.965.\n",
      "For Iteration 98 the Loss is 2.9493.\n",
      "For Iteration 99 the Loss is 2.9337.\n",
      "For Iteration 100 the Loss is 2.9181.\n",
      "For Iteration 101 the Loss is 2.9025.\n",
      "For Iteration 102 the Loss is 2.887.\n",
      "For Iteration 103 the Loss is 2.8714.\n",
      "For Iteration 104 the Loss is 2.8559.\n",
      "For Iteration 105 the Loss is 2.8404.\n",
      "For Iteration 106 the Loss is 2.825.\n",
      "For Iteration 107 the Loss is 2.8096.\n",
      "For Iteration 108 the Loss is 2.7941.\n",
      "For Iteration 109 the Loss is 2.7788.\n",
      "For Iteration 110 the Loss is 2.7634.\n",
      "For Iteration 111 the Loss is 2.7481.\n",
      "For Iteration 112 the Loss is 2.7328.\n",
      "For Iteration 113 the Loss is 2.7175.\n",
      "For Iteration 114 the Loss is 2.7023.\n",
      "For Iteration 115 the Loss is 2.6871.\n",
      "For Iteration 116 the Loss is 2.6719.\n",
      "For Iteration 117 the Loss is 2.6568.\n",
      "For Iteration 118 the Loss is 2.6416.\n",
      "For Iteration 119 the Loss is 2.6265.\n",
      "For Iteration 120 the Loss is 2.6115.\n",
      "For Iteration 121 the Loss is 2.5965.\n",
      "For Iteration 122 the Loss is 2.5815.\n",
      "For Iteration 123 the Loss is 2.5665.\n",
      "For Iteration 124 the Loss is 2.5516.\n",
      "For Iteration 125 the Loss is 2.5367.\n",
      "For Iteration 126 the Loss is 2.5218.\n",
      "For Iteration 127 the Loss is 2.507.\n",
      "For Iteration 128 the Loss is 2.4922.\n",
      "For Iteration 129 the Loss is 2.4774.\n",
      "For Iteration 130 the Loss is 2.4627.\n",
      "For Iteration 131 the Loss is 2.448.\n",
      "For Iteration 132 the Loss is 2.4334.\n",
      "For Iteration 133 the Loss is 2.4188.\n",
      "For Iteration 134 the Loss is 2.4042.\n",
      "For Iteration 135 the Loss is 2.3897.\n",
      "For Iteration 136 the Loss is 2.3752.\n",
      "For Iteration 137 the Loss is 2.3607.\n",
      "For Iteration 138 the Loss is 2.3463.\n",
      "For Iteration 139 the Loss is 2.3319.\n",
      "For Iteration 140 the Loss is 2.3176.\n",
      "For Iteration 141 the Loss is 2.3033.\n",
      "For Iteration 142 the Loss is 2.289.\n",
      "For Iteration 143 the Loss is 2.2748.\n",
      "For Iteration 144 the Loss is 2.2606.\n",
      "For Iteration 145 the Loss is 2.2465.\n",
      "For Iteration 146 the Loss is 2.2324.\n",
      "For Iteration 147 the Loss is 2.2184.\n",
      "For Iteration 148 the Loss is 2.2044.\n",
      "For Iteration 149 the Loss is 2.1904.\n",
      "For Iteration 150 the Loss is 2.1765.\n",
      "For Iteration 151 the Loss is 2.1627.\n",
      "For Iteration 152 the Loss is 2.1489.\n",
      "For Iteration 153 the Loss is 2.1351.\n",
      "For Iteration 154 the Loss is 2.1214.\n",
      "For Iteration 155 the Loss is 2.1077.\n",
      "For Iteration 156 the Loss is 2.0941.\n",
      "For Iteration 157 the Loss is 2.0805.\n",
      "For Iteration 158 the Loss is 2.067.\n",
      "For Iteration 159 the Loss is 2.0535.\n",
      "For Iteration 160 the Loss is 2.0401.\n",
      "For Iteration 161 the Loss is 2.0267.\n",
      "For Iteration 162 the Loss is 2.0134.\n",
      "For Iteration 163 the Loss is 2.0002.\n",
      "For Iteration 164 the Loss is 1.9869.\n",
      "For Iteration 165 the Loss is 1.9738.\n",
      "For Iteration 166 the Loss is 1.9607.\n",
      "For Iteration 167 the Loss is 1.9476.\n",
      "For Iteration 168 the Loss is 1.9347.\n",
      "For Iteration 169 the Loss is 1.9217.\n",
      "For Iteration 170 the Loss is 1.9088.\n",
      "For Iteration 171 the Loss is 1.896.\n",
      "For Iteration 172 the Loss is 1.8832.\n",
      "For Iteration 173 the Loss is 1.8705.\n",
      "For Iteration 174 the Loss is 1.8579.\n",
      "For Iteration 175 the Loss is 1.8453.\n",
      "For Iteration 176 the Loss is 1.8328.\n",
      "For Iteration 177 the Loss is 1.8203.\n",
      "For Iteration 178 the Loss is 1.8079.\n",
      "For Iteration 179 the Loss is 1.7955.\n",
      "For Iteration 180 the Loss is 1.7832.\n",
      "For Iteration 181 the Loss is 1.771.\n",
      "For Iteration 182 the Loss is 1.7589.\n",
      "For Iteration 183 the Loss is 1.7467.\n",
      "For Iteration 184 the Loss is 1.7347.\n",
      "For Iteration 185 the Loss is 1.7227.\n",
      "For Iteration 186 the Loss is 1.7108.\n",
      "For Iteration 187 the Loss is 1.699.\n",
      "For Iteration 188 the Loss is 1.6872.\n",
      "For Iteration 189 the Loss is 1.6755.\n",
      "For Iteration 190 the Loss is 1.6638.\n",
      "For Iteration 191 the Loss is 1.6522.\n",
      "For Iteration 192 the Loss is 1.6407.\n",
      "For Iteration 193 the Loss is 1.6293.\n",
      "For Iteration 194 the Loss is 1.6179.\n",
      "For Iteration 195 the Loss is 1.6066.\n",
      "For Iteration 196 the Loss is 1.5953.\n",
      "For Iteration 197 the Loss is 1.5841.\n",
      "For Iteration 198 the Loss is 1.573.\n",
      "For Iteration 199 the Loss is 1.562.\n",
      "For Iteration 200 the Loss is 1.551.\n",
      "For Iteration 201 the Loss is 1.5401.\n",
      "For Iteration 202 the Loss is 1.5293.\n",
      "For Iteration 203 the Loss is 1.5185.\n",
      "For Iteration 204 the Loss is 1.5078.\n",
      "For Iteration 205 the Loss is 1.4972.\n",
      "For Iteration 206 the Loss is 1.4866.\n",
      "For Iteration 207 the Loss is 1.4761.\n",
      "For Iteration 208 the Loss is 1.4657.\n",
      "For Iteration 209 the Loss is 1.4554.\n",
      "For Iteration 210 the Loss is 1.4451.\n",
      "For Iteration 211 the Loss is 1.4349.\n",
      "For Iteration 212 the Loss is 1.4248.\n",
      "For Iteration 213 the Loss is 1.4148.\n",
      "For Iteration 214 the Loss is 1.4048.\n",
      "For Iteration 215 the Loss is 1.3949.\n",
      "For Iteration 216 the Loss is 1.3851.\n",
      "For Iteration 217 the Loss is 1.3753.\n",
      "For Iteration 218 the Loss is 1.3656.\n",
      "For Iteration 219 the Loss is 1.356.\n",
      "For Iteration 220 the Loss is 1.3465.\n",
      "For Iteration 221 the Loss is 1.337.\n",
      "For Iteration 222 the Loss is 1.3276.\n",
      "For Iteration 223 the Loss is 1.3183.\n",
      "For Iteration 224 the Loss is 1.3091.\n",
      "For Iteration 225 the Loss is 1.2999.\n",
      "For Iteration 226 the Loss is 1.2908.\n",
      "For Iteration 227 the Loss is 1.2818.\n",
      "For Iteration 228 the Loss is 1.2728.\n",
      "For Iteration 229 the Loss is 1.264.\n",
      "For Iteration 230 the Loss is 1.2552.\n",
      "For Iteration 231 the Loss is 1.2464.\n",
      "For Iteration 232 the Loss is 1.2378.\n",
      "For Iteration 233 the Loss is 1.2292.\n",
      "For Iteration 234 the Loss is 1.2207.\n",
      "For Iteration 235 the Loss is 1.2122.\n",
      "For Iteration 236 the Loss is 1.2039.\n",
      "For Iteration 237 the Loss is 1.1956.\n",
      "For Iteration 238 the Loss is 1.1874.\n",
      "For Iteration 239 the Loss is 1.1792.\n",
      "For Iteration 240 the Loss is 1.1712.\n",
      "For Iteration 241 the Loss is 1.1632.\n",
      "For Iteration 242 the Loss is 1.1552.\n",
      "For Iteration 243 the Loss is 1.1474.\n",
      "For Iteration 244 the Loss is 1.1396.\n",
      "For Iteration 245 the Loss is 1.1319.\n",
      "For Iteration 246 the Loss is 1.1242.\n",
      "For Iteration 247 the Loss is 1.1167.\n",
      "For Iteration 248 the Loss is 1.1092.\n",
      "For Iteration 249 the Loss is 1.1017.\n",
      "For Iteration 250 the Loss is 1.0944.\n",
      "For Iteration 251 the Loss is 1.0871.\n",
      "For Iteration 252 the Loss is 1.0799.\n",
      "For Iteration 253 the Loss is 1.0727.\n",
      "For Iteration 254 the Loss is 1.0656.\n",
      "For Iteration 255 the Loss is 1.0586.\n",
      "For Iteration 256 the Loss is 1.0516.\n",
      "For Iteration 257 the Loss is 1.0448.\n",
      "For Iteration 258 the Loss is 1.038.\n",
      "For Iteration 259 the Loss is 1.0312.\n",
      "For Iteration 260 the Loss is 1.0245.\n",
      "For Iteration 261 the Loss is 1.0179.\n",
      "For Iteration 262 the Loss is 1.0114.\n",
      "For Iteration 263 the Loss is 1.0049.\n",
      "For Iteration 264 the Loss is 0.9985.\n",
      "For Iteration 265 the Loss is 0.9921.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 266 the Loss is 0.9858.\n",
      "For Iteration 267 the Loss is 0.9796.\n",
      "For Iteration 268 the Loss is 0.9734.\n",
      "For Iteration 269 the Loss is 0.9673.\n",
      "For Iteration 270 the Loss is 0.9613.\n",
      "For Iteration 271 the Loss is 0.9553.\n",
      "For Iteration 272 the Loss is 0.9494.\n",
      "For Iteration 273 the Loss is 0.9436.\n",
      "For Iteration 274 the Loss is 0.9378.\n",
      "For Iteration 275 the Loss is 0.932.\n",
      "For Iteration 276 the Loss is 0.9264.\n",
      "For Iteration 277 the Loss is 0.9208.\n",
      "For Iteration 278 the Loss is 0.9152.\n",
      "For Iteration 279 the Loss is 0.9097.\n",
      "For Iteration 280 the Loss is 0.9043.\n",
      "For Iteration 281 the Loss is 0.8989.\n",
      "For Iteration 282 the Loss is 0.8936.\n",
      "For Iteration 283 the Loss is 0.8883.\n",
      "For Iteration 284 the Loss is 0.8831.\n",
      "For Iteration 285 the Loss is 0.878.\n",
      "For Iteration 286 the Loss is 0.8729.\n",
      "For Iteration 287 the Loss is 0.8678.\n",
      "For Iteration 288 the Loss is 0.8628.\n",
      "For Iteration 289 the Loss is 0.8579.\n",
      "For Iteration 290 the Loss is 0.853.\n",
      "For Iteration 291 the Loss is 0.8482.\n",
      "For Iteration 292 the Loss is 0.8434.\n",
      "For Iteration 293 the Loss is 0.8387.\n",
      "For Iteration 294 the Loss is 0.834.\n",
      "For Iteration 295 the Loss is 0.8294.\n",
      "For Iteration 296 the Loss is 0.8248.\n",
      "For Iteration 297 the Loss is 0.8203.\n",
      "For Iteration 298 the Loss is 0.8158.\n",
      "For Iteration 299 the Loss is 0.8114.\n",
      "For Iteration 300 the Loss is 0.807.\n",
      "For Iteration 301 the Loss is 0.8027.\n",
      "For Iteration 302 the Loss is 0.7984.\n",
      "For Iteration 303 the Loss is 0.7941.\n",
      "For Iteration 304 the Loss is 0.79.\n",
      "For Iteration 305 the Loss is 0.7858.\n",
      "For Iteration 306 the Loss is 0.7817.\n",
      "For Iteration 307 the Loss is 0.7777.\n",
      "For Iteration 308 the Loss is 0.7737.\n",
      "For Iteration 309 the Loss is 0.7697.\n",
      "For Iteration 310 the Loss is 0.7658.\n",
      "For Iteration 311 the Loss is 0.7619.\n",
      "For Iteration 312 the Loss is 0.7581.\n",
      "For Iteration 313 the Loss is 0.7543.\n",
      "For Iteration 314 the Loss is 0.7505.\n",
      "For Iteration 315 the Loss is 0.7468.\n",
      "For Iteration 316 the Loss is 0.7431.\n",
      "For Iteration 317 the Loss is 0.7395.\n",
      "For Iteration 318 the Loss is 0.7359.\n",
      "For Iteration 319 the Loss is 0.7324.\n",
      "For Iteration 320 the Loss is 0.7289.\n",
      "For Iteration 321 the Loss is 0.7254.\n",
      "For Iteration 322 the Loss is 0.7219.\n",
      "For Iteration 323 the Loss is 0.7186.\n",
      "For Iteration 324 the Loss is 0.7152.\n",
      "For Iteration 325 the Loss is 0.7119.\n",
      "For Iteration 326 the Loss is 0.7086.\n",
      "For Iteration 327 the Loss is 0.7053.\n",
      "For Iteration 328 the Loss is 0.7021.\n",
      "For Iteration 329 the Loss is 0.699.\n",
      "For Iteration 330 the Loss is 0.6958.\n",
      "For Iteration 331 the Loss is 0.6927.\n",
      "For Iteration 332 the Loss is 0.6896.\n",
      "For Iteration 333 the Loss is 0.6866.\n",
      "For Iteration 334 the Loss is 0.6836.\n",
      "For Iteration 335 the Loss is 0.6806.\n",
      "For Iteration 336 the Loss is 0.6777.\n",
      "For Iteration 337 the Loss is 0.6748.\n",
      "For Iteration 338 the Loss is 0.6719.\n",
      "For Iteration 339 the Loss is 0.669.\n",
      "For Iteration 340 the Loss is 0.6662.\n",
      "For Iteration 341 the Loss is 0.6634.\n",
      "For Iteration 342 the Loss is 0.6607.\n",
      "For Iteration 343 the Loss is 0.658.\n",
      "For Iteration 344 the Loss is 0.6553.\n",
      "For Iteration 345 the Loss is 0.6526.\n",
      "For Iteration 346 the Loss is 0.65.\n",
      "For Iteration 347 the Loss is 0.6474.\n",
      "For Iteration 348 the Loss is 0.6448.\n",
      "For Iteration 349 the Loss is 0.6422.\n",
      "For Iteration 350 the Loss is 0.6397.\n",
      "For Iteration 351 the Loss is 0.6372.\n",
      "For Iteration 352 the Loss is 0.6347.\n",
      "For Iteration 353 the Loss is 0.6323.\n",
      "For Iteration 354 the Loss is 0.6299.\n",
      "For Iteration 355 the Loss is 0.6275.\n",
      "For Iteration 356 the Loss is 0.6251.\n",
      "For Iteration 357 the Loss is 0.6228.\n",
      "For Iteration 358 the Loss is 0.6205.\n",
      "For Iteration 359 the Loss is 0.6182.\n",
      "For Iteration 360 the Loss is 0.6159.\n",
      "For Iteration 361 the Loss is 0.6137.\n",
      "For Iteration 362 the Loss is 0.6115.\n",
      "For Iteration 363 the Loss is 0.6093.\n",
      "For Iteration 364 the Loss is 0.6071.\n",
      "For Iteration 365 the Loss is 0.6049.\n",
      "For Iteration 366 the Loss is 0.6028.\n",
      "For Iteration 367 the Loss is 0.6007.\n",
      "For Iteration 368 the Loss is 0.5986.\n",
      "For Iteration 369 the Loss is 0.5966.\n",
      "For Iteration 370 the Loss is 0.5945.\n",
      "For Iteration 371 the Loss is 0.5925.\n",
      "For Iteration 372 the Loss is 0.5905.\n",
      "For Iteration 373 the Loss is 0.5886.\n",
      "For Iteration 374 the Loss is 0.5866.\n",
      "For Iteration 375 the Loss is 0.5847.\n",
      "For Iteration 376 the Loss is 0.5828.\n",
      "For Iteration 377 the Loss is 0.5809.\n",
      "For Iteration 378 the Loss is 0.579.\n",
      "For Iteration 379 the Loss is 0.5771.\n",
      "For Iteration 380 the Loss is 0.5753.\n",
      "For Iteration 381 the Loss is 0.5735.\n",
      "For Iteration 382 the Loss is 0.5717.\n",
      "For Iteration 383 the Loss is 0.5699.\n",
      "For Iteration 384 the Loss is 0.5682.\n",
      "For Iteration 385 the Loss is 0.5664.\n",
      "For Iteration 386 the Loss is 0.5647.\n",
      "For Iteration 387 the Loss is 0.563.\n",
      "For Iteration 388 the Loss is 0.5613.\n",
      "For Iteration 389 the Loss is 0.5596.\n",
      "For Iteration 390 the Loss is 0.558.\n",
      "For Iteration 391 the Loss is 0.5564.\n",
      "For Iteration 392 the Loss is 0.5547.\n",
      "For Iteration 393 the Loss is 0.5531.\n",
      "For Iteration 394 the Loss is 0.5515.\n",
      "For Iteration 395 the Loss is 0.55.\n",
      "For Iteration 396 the Loss is 0.5484.\n",
      "For Iteration 397 the Loss is 0.5469.\n",
      "For Iteration 398 the Loss is 0.5454.\n",
      "For Iteration 399 the Loss is 0.5439.\n",
      "For Iteration 400 the Loss is 0.5424.\n",
      "For Iteration 401 the Loss is 0.5409.\n",
      "For Iteration 402 the Loss is 0.5394.\n",
      "For Iteration 403 the Loss is 0.538.\n",
      "For Iteration 404 the Loss is 0.5365.\n",
      "For Iteration 405 the Loss is 0.5351.\n",
      "For Iteration 406 the Loss is 0.5337.\n",
      "For Iteration 407 the Loss is 0.5323.\n",
      "For Iteration 408 the Loss is 0.5309.\n",
      "For Iteration 409 the Loss is 0.5296.\n",
      "For Iteration 410 the Loss is 0.5282.\n",
      "For Iteration 411 the Loss is 0.5269.\n",
      "For Iteration 412 the Loss is 0.5256.\n",
      "For Iteration 413 the Loss is 0.5242.\n",
      "For Iteration 414 the Loss is 0.5229.\n",
      "For Iteration 415 the Loss is 0.5217.\n",
      "For Iteration 416 the Loss is 0.5204.\n",
      "For Iteration 417 the Loss is 0.5191.\n",
      "For Iteration 418 the Loss is 0.5179.\n",
      "For Iteration 419 the Loss is 0.5166.\n",
      "For Iteration 420 the Loss is 0.5154.\n",
      "For Iteration 421 the Loss is 0.5142.\n",
      "For Iteration 422 the Loss is 0.513.\n",
      "For Iteration 423 the Loss is 0.5118.\n",
      "For Iteration 424 the Loss is 0.5106.\n",
      "For Iteration 425 the Loss is 0.5095.\n",
      "For Iteration 426 the Loss is 0.5083.\n",
      "For Iteration 427 the Loss is 0.5072.\n",
      "For Iteration 428 the Loss is 0.506.\n",
      "For Iteration 429 the Loss is 0.5049.\n",
      "For Iteration 430 the Loss is 0.5038.\n",
      "For Iteration 431 the Loss is 0.5027.\n",
      "For Iteration 432 the Loss is 0.5016.\n",
      "For Iteration 433 the Loss is 0.5005.\n",
      "For Iteration 434 the Loss is 0.4994.\n",
      "For Iteration 435 the Loss is 0.4984.\n",
      "For Iteration 436 the Loss is 0.4973.\n",
      "For Iteration 437 the Loss is 0.4963.\n",
      "For Iteration 438 the Loss is 0.4952.\n",
      "For Iteration 439 the Loss is 0.4942.\n",
      "For Iteration 440 the Loss is 0.4932.\n",
      "For Iteration 441 the Loss is 0.4922.\n",
      "For Iteration 442 the Loss is 0.4912.\n",
      "For Iteration 443 the Loss is 0.4902.\n",
      "For Iteration 444 the Loss is 0.4892.\n",
      "For Iteration 445 the Loss is 0.4883.\n",
      "For Iteration 446 the Loss is 0.4873.\n",
      "For Iteration 447 the Loss is 0.4864.\n",
      "For Iteration 448 the Loss is 0.4854.\n",
      "For Iteration 449 the Loss is 0.4845.\n",
      "For Iteration 450 the Loss is 0.4836.\n",
      "For Iteration 451 the Loss is 0.4827.\n",
      "For Iteration 452 the Loss is 0.4817.\n",
      "For Iteration 453 the Loss is 0.4808.\n",
      "For Iteration 454 the Loss is 0.48.\n",
      "For Iteration 455 the Loss is 0.4791.\n",
      "For Iteration 456 the Loss is 0.4782.\n",
      "For Iteration 457 the Loss is 0.4773.\n",
      "For Iteration 458 the Loss is 0.4765.\n",
      "For Iteration 459 the Loss is 0.4756.\n",
      "For Iteration 460 the Loss is 0.4748.\n",
      "For Iteration 461 the Loss is 0.4739.\n",
      "For Iteration 462 the Loss is 0.4731.\n",
      "For Iteration 463 the Loss is 0.4723.\n",
      "For Iteration 464 the Loss is 0.4715.\n",
      "For Iteration 465 the Loss is 0.4707.\n",
      "For Iteration 466 the Loss is 0.4699.\n",
      "For Iteration 467 the Loss is 0.4691.\n",
      "For Iteration 468 the Loss is 0.4683.\n",
      "For Iteration 469 the Loss is 0.4675.\n",
      "For Iteration 470 the Loss is 0.4667.\n",
      "For Iteration 471 the Loss is 0.466.\n",
      "For Iteration 472 the Loss is 0.4652.\n",
      "For Iteration 473 the Loss is 0.4645.\n",
      "For Iteration 474 the Loss is 0.4637.\n",
      "For Iteration 475 the Loss is 0.463.\n",
      "For Iteration 476 the Loss is 0.4622.\n",
      "For Iteration 477 the Loss is 0.4615.\n",
      "For Iteration 478 the Loss is 0.4608.\n",
      "For Iteration 479 the Loss is 0.4601.\n",
      "For Iteration 480 the Loss is 0.4594.\n",
      "For Iteration 481 the Loss is 0.4587.\n",
      "For Iteration 482 the Loss is 0.458.\n",
      "For Iteration 483 the Loss is 0.4573.\n",
      "For Iteration 484 the Loss is 0.4566.\n",
      "For Iteration 485 the Loss is 0.4559.\n",
      "For Iteration 486 the Loss is 0.4552.\n",
      "For Iteration 487 the Loss is 0.4546.\n",
      "For Iteration 488 the Loss is 0.4539.\n",
      "For Iteration 489 the Loss is 0.4533.\n",
      "For Iteration 490 the Loss is 0.4526.\n",
      "For Iteration 491 the Loss is 0.452.\n",
      "For Iteration 492 the Loss is 0.4513.\n",
      "For Iteration 493 the Loss is 0.4507.\n",
      "For Iteration 494 the Loss is 0.4501.\n",
      "For Iteration 495 the Loss is 0.4494.\n",
      "For Iteration 496 the Loss is 0.4488.\n",
      "For Iteration 497 the Loss is 0.4482.\n",
      "For Iteration 498 the Loss is 0.4476.\n",
      "For Iteration 499 the Loss is 0.447.\n",
      "For Iteration 500 the Loss is 0.4464.\n",
      "For Iteration 501 the Loss is 0.4458.\n",
      "For Iteration 502 the Loss is 0.4452.\n",
      "For Iteration 503 the Loss is 0.4446.\n",
      "For Iteration 504 the Loss is 0.4441.\n",
      "For Iteration 505 the Loss is 0.4435.\n",
      "For Iteration 506 the Loss is 0.4429.\n",
      "For Iteration 507 the Loss is 0.4423.\n",
      "For Iteration 508 the Loss is 0.4418.\n",
      "For Iteration 509 the Loss is 0.4412.\n",
      "For Iteration 510 the Loss is 0.4407.\n",
      "For Iteration 511 the Loss is 0.4401.\n",
      "For Iteration 512 the Loss is 0.4396.\n",
      "For Iteration 513 the Loss is 0.439.\n",
      "For Iteration 514 the Loss is 0.4385.\n",
      "For Iteration 515 the Loss is 0.438.\n",
      "For Iteration 516 the Loss is 0.4375.\n",
      "For Iteration 517 the Loss is 0.4369.\n",
      "For Iteration 518 the Loss is 0.4364.\n",
      "For Iteration 519 the Loss is 0.4359.\n",
      "For Iteration 520 the Loss is 0.4354.\n",
      "For Iteration 521 the Loss is 0.4349.\n",
      "For Iteration 522 the Loss is 0.4344.\n",
      "For Iteration 523 the Loss is 0.4339.\n",
      "For Iteration 524 the Loss is 0.4334.\n",
      "For Iteration 525 the Loss is 0.4329.\n",
      "For Iteration 526 the Loss is 0.4324.\n",
      "For Iteration 527 the Loss is 0.4319.\n",
      "For Iteration 528 the Loss is 0.4315.\n",
      "For Iteration 529 the Loss is 0.431.\n",
      "For Iteration 530 the Loss is 0.4305.\n",
      "For Iteration 531 the Loss is 0.4301.\n",
      "For Iteration 532 the Loss is 0.4296.\n",
      "For Iteration 533 the Loss is 0.4291.\n",
      "For Iteration 534 the Loss is 0.4287.\n",
      "For Iteration 535 the Loss is 0.4282.\n",
      "For Iteration 536 the Loss is 0.4278.\n",
      "For Iteration 537 the Loss is 0.4273.\n",
      "For Iteration 538 the Loss is 0.4269.\n",
      "For Iteration 539 the Loss is 0.4265.\n",
      "For Iteration 540 the Loss is 0.426.\n",
      "For Iteration 541 the Loss is 0.4256.\n",
      "For Iteration 542 the Loss is 0.4252.\n",
      "For Iteration 543 the Loss is 0.4247.\n",
      "For Iteration 544 the Loss is 0.4243.\n",
      "For Iteration 545 the Loss is 0.4239.\n",
      "For Iteration 546 the Loss is 0.4235.\n",
      "For Iteration 547 the Loss is 0.4231.\n",
      "For Iteration 548 the Loss is 0.4227.\n",
      "For Iteration 549 the Loss is 0.4222.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 550 the Loss is 0.4218.\n",
      "For Iteration 551 the Loss is 0.4214.\n",
      "For Iteration 552 the Loss is 0.421.\n",
      "For Iteration 553 the Loss is 0.4206.\n",
      "For Iteration 554 the Loss is 0.4203.\n",
      "For Iteration 555 the Loss is 0.4199.\n",
      "For Iteration 556 the Loss is 0.4195.\n",
      "For Iteration 557 the Loss is 0.4191.\n",
      "For Iteration 558 the Loss is 0.4187.\n",
      "For Iteration 559 the Loss is 0.4183.\n",
      "For Iteration 560 the Loss is 0.418.\n",
      "For Iteration 561 the Loss is 0.4176.\n",
      "For Iteration 562 the Loss is 0.4172.\n",
      "For Iteration 563 the Loss is 0.4169.\n",
      "For Iteration 564 the Loss is 0.4165.\n",
      "For Iteration 565 the Loss is 0.4161.\n",
      "For Iteration 566 the Loss is 0.4158.\n",
      "For Iteration 567 the Loss is 0.4154.\n",
      "For Iteration 568 the Loss is 0.4151.\n",
      "For Iteration 569 the Loss is 0.4147.\n",
      "For Iteration 570 the Loss is 0.4144.\n",
      "For Iteration 571 the Loss is 0.414.\n",
      "For Iteration 572 the Loss is 0.4137.\n",
      "For Iteration 573 the Loss is 0.4133.\n",
      "For Iteration 574 the Loss is 0.413.\n",
      "For Iteration 575 the Loss is 0.4127.\n",
      "For Iteration 576 the Loss is 0.4123.\n",
      "For Iteration 577 the Loss is 0.412.\n",
      "For Iteration 578 the Loss is 0.4117.\n",
      "For Iteration 579 the Loss is 0.4113.\n",
      "For Iteration 580 the Loss is 0.411.\n",
      "For Iteration 581 the Loss is 0.4107.\n",
      "For Iteration 582 the Loss is 0.4104.\n",
      "For Iteration 583 the Loss is 0.4101.\n",
      "For Iteration 584 the Loss is 0.4097.\n",
      "For Iteration 585 the Loss is 0.4094.\n",
      "For Iteration 586 the Loss is 0.4091.\n",
      "For Iteration 587 the Loss is 0.4088.\n",
      "For Iteration 588 the Loss is 0.4085.\n",
      "For Iteration 589 the Loss is 0.4082.\n",
      "For Iteration 590 the Loss is 0.4079.\n",
      "For Iteration 591 the Loss is 0.4076.\n",
      "For Iteration 592 the Loss is 0.4073.\n",
      "For Iteration 593 the Loss is 0.407.\n",
      "For Iteration 594 the Loss is 0.4067.\n",
      "For Iteration 595 the Loss is 0.4064.\n",
      "For Iteration 596 the Loss is 0.4061.\n",
      "For Iteration 597 the Loss is 0.4058.\n",
      "For Iteration 598 the Loss is 0.4056.\n",
      "For Iteration 599 the Loss is 0.4053.\n",
      "For Iteration 600 the Loss is 0.405.\n",
      "For Iteration 601 the Loss is 0.4047.\n",
      "For Iteration 602 the Loss is 0.4044.\n",
      "For Iteration 603 the Loss is 0.4042.\n",
      "For Iteration 604 the Loss is 0.4039.\n",
      "For Iteration 605 the Loss is 0.4036.\n",
      "For Iteration 606 the Loss is 0.4033.\n",
      "For Iteration 607 the Loss is 0.4031.\n",
      "For Iteration 608 the Loss is 0.4028.\n",
      "For Iteration 609 the Loss is 0.4025.\n",
      "For Iteration 610 the Loss is 0.4023.\n",
      "For Iteration 611 the Loss is 0.402.\n",
      "For Iteration 612 the Loss is 0.4018.\n",
      "For Iteration 613 the Loss is 0.4015.\n",
      "For Iteration 614 the Loss is 0.4012.\n",
      "For Iteration 615 the Loss is 0.401.\n",
      "For Iteration 616 the Loss is 0.4007.\n",
      "For Iteration 617 the Loss is 0.4005.\n",
      "For Iteration 618 the Loss is 0.4002.\n",
      "For Iteration 619 the Loss is 0.4.\n",
      "For Iteration 620 the Loss is 0.3997.\n",
      "For Iteration 621 the Loss is 0.3995.\n",
      "For Iteration 622 the Loss is 0.3992.\n",
      "For Iteration 623 the Loss is 0.399.\n",
      "For Iteration 624 the Loss is 0.3988.\n",
      "For Iteration 625 the Loss is 0.3985.\n",
      "For Iteration 626 the Loss is 0.3983.\n",
      "For Iteration 627 the Loss is 0.3981.\n",
      "For Iteration 628 the Loss is 0.3978.\n",
      "For Iteration 629 the Loss is 0.3976.\n",
      "For Iteration 630 the Loss is 0.3974.\n",
      "For Iteration 631 the Loss is 0.3971.\n",
      "For Iteration 632 the Loss is 0.3969.\n",
      "For Iteration 633 the Loss is 0.3967.\n",
      "For Iteration 634 the Loss is 0.3964.\n",
      "For Iteration 635 the Loss is 0.3962.\n",
      "For Iteration 636 the Loss is 0.396.\n",
      "For Iteration 637 the Loss is 0.3958.\n",
      "For Iteration 638 the Loss is 0.3956.\n",
      "For Iteration 639 the Loss is 0.3953.\n",
      "For Iteration 640 the Loss is 0.3951.\n",
      "For Iteration 641 the Loss is 0.3949.\n",
      "For Iteration 642 the Loss is 0.3947.\n",
      "For Iteration 643 the Loss is 0.3945.\n",
      "For Iteration 644 the Loss is 0.3943.\n",
      "For Iteration 645 the Loss is 0.3941.\n",
      "For Iteration 646 the Loss is 0.3939.\n",
      "For Iteration 647 the Loss is 0.3937.\n",
      "For Iteration 648 the Loss is 0.3934.\n",
      "For Iteration 649 the Loss is 0.3932.\n",
      "For Iteration 650 the Loss is 0.393.\n",
      "For Iteration 651 the Loss is 0.3928.\n",
      "For Iteration 652 the Loss is 0.3926.\n",
      "For Iteration 653 the Loss is 0.3924.\n",
      "For Iteration 654 the Loss is 0.3922.\n",
      "For Iteration 655 the Loss is 0.392.\n",
      "For Iteration 656 the Loss is 0.3918.\n",
      "For Iteration 657 the Loss is 0.3916.\n",
      "For Iteration 658 the Loss is 0.3915.\n",
      "For Iteration 659 the Loss is 0.3913.\n",
      "For Iteration 660 the Loss is 0.3911.\n",
      "For Iteration 661 the Loss is 0.3909.\n",
      "For Iteration 662 the Loss is 0.3907.\n",
      "For Iteration 663 the Loss is 0.3905.\n",
      "For Iteration 664 the Loss is 0.3903.\n",
      "For Iteration 665 the Loss is 0.3901.\n",
      "For Iteration 666 the Loss is 0.39.\n",
      "For Iteration 667 the Loss is 0.3898.\n",
      "For Iteration 668 the Loss is 0.3896.\n",
      "For Iteration 669 the Loss is 0.3894.\n",
      "For Iteration 670 the Loss is 0.3892.\n",
      "For Iteration 671 the Loss is 0.389.\n",
      "For Iteration 672 the Loss is 0.3889.\n",
      "For Iteration 673 the Loss is 0.3887.\n",
      "For Iteration 674 the Loss is 0.3885.\n",
      "For Iteration 675 the Loss is 0.3883.\n",
      "For Iteration 676 the Loss is 0.3882.\n",
      "For Iteration 677 the Loss is 0.388.\n",
      "For Iteration 678 the Loss is 0.3878.\n",
      "For Iteration 679 the Loss is 0.3877.\n",
      "For Iteration 680 the Loss is 0.3875.\n",
      "For Iteration 681 the Loss is 0.3873.\n",
      "For Iteration 682 the Loss is 0.3872.\n",
      "For Iteration 683 the Loss is 0.387.\n",
      "For Iteration 684 the Loss is 0.3868.\n",
      "For Iteration 685 the Loss is 0.3867.\n",
      "For Iteration 686 the Loss is 0.3865.\n",
      "For Iteration 687 the Loss is 0.3863.\n",
      "For Iteration 688 the Loss is 0.3862.\n",
      "For Iteration 689 the Loss is 0.386.\n",
      "For Iteration 690 the Loss is 0.3858.\n",
      "For Iteration 691 the Loss is 0.3857.\n",
      "For Iteration 692 the Loss is 0.3855.\n",
      "For Iteration 693 the Loss is 0.3854.\n",
      "For Iteration 694 the Loss is 0.3852.\n",
      "For Iteration 695 the Loss is 0.3851.\n",
      "For Iteration 696 the Loss is 0.3849.\n",
      "For Iteration 697 the Loss is 0.3848.\n",
      "For Iteration 698 the Loss is 0.3846.\n",
      "For Iteration 699 the Loss is 0.3845.\n",
      "For Iteration 700 the Loss is 0.3843.\n",
      "For Iteration 701 the Loss is 0.3842.\n",
      "For Iteration 702 the Loss is 0.384.\n",
      "For Iteration 703 the Loss is 0.3839.\n",
      "For Iteration 704 the Loss is 0.3837.\n",
      "For Iteration 705 the Loss is 0.3836.\n",
      "For Iteration 706 the Loss is 0.3834.\n",
      "For Iteration 707 the Loss is 0.3833.\n",
      "For Iteration 708 the Loss is 0.3831.\n",
      "For Iteration 709 the Loss is 0.383.\n",
      "For Iteration 710 the Loss is 0.3828.\n",
      "For Iteration 711 the Loss is 0.3827.\n",
      "For Iteration 712 the Loss is 0.3826.\n",
      "For Iteration 713 the Loss is 0.3824.\n",
      "For Iteration 714 the Loss is 0.3823.\n",
      "For Iteration 715 the Loss is 0.3821.\n",
      "For Iteration 716 the Loss is 0.382.\n",
      "For Iteration 717 the Loss is 0.3819.\n",
      "For Iteration 718 the Loss is 0.3817.\n",
      "For Iteration 719 the Loss is 0.3816.\n",
      "For Iteration 720 the Loss is 0.3815.\n",
      "For Iteration 721 the Loss is 0.3813.\n",
      "For Iteration 722 the Loss is 0.3812.\n",
      "For Iteration 723 the Loss is 0.3811.\n",
      "For Iteration 724 the Loss is 0.3809.\n",
      "For Iteration 725 the Loss is 0.3808.\n",
      "For Iteration 726 the Loss is 0.3807.\n",
      "For Iteration 727 the Loss is 0.3805.\n",
      "For Iteration 728 the Loss is 0.3804.\n",
      "For Iteration 729 the Loss is 0.3803.\n",
      "For Iteration 730 the Loss is 0.3802.\n",
      "For Iteration 731 the Loss is 0.38.\n",
      "For Iteration 732 the Loss is 0.3799.\n",
      "For Iteration 733 the Loss is 0.3798.\n",
      "For Iteration 734 the Loss is 0.3797.\n",
      "For Iteration 735 the Loss is 0.3795.\n",
      "For Iteration 736 the Loss is 0.3794.\n",
      "For Iteration 737 the Loss is 0.3793.\n",
      "For Iteration 738 the Loss is 0.3792.\n",
      "For Iteration 739 the Loss is 0.379.\n",
      "For Iteration 740 the Loss is 0.3789.\n",
      "For Iteration 741 the Loss is 0.3788.\n",
      "For Iteration 742 the Loss is 0.3787.\n",
      "For Iteration 743 the Loss is 0.3786.\n",
      "For Iteration 744 the Loss is 0.3784.\n",
      "For Iteration 745 the Loss is 0.3783.\n",
      "For Iteration 746 the Loss is 0.3782.\n",
      "For Iteration 747 the Loss is 0.3781.\n",
      "For Iteration 748 the Loss is 0.378.\n",
      "For Iteration 749 the Loss is 0.3779.\n",
      "For Iteration 750 the Loss is 0.3777.\n",
      "For Iteration 751 the Loss is 0.3776.\n",
      "For Iteration 752 the Loss is 0.3775.\n",
      "For Iteration 753 the Loss is 0.3774.\n",
      "For Iteration 754 the Loss is 0.3773.\n",
      "For Iteration 755 the Loss is 0.3772.\n",
      "For Iteration 756 the Loss is 0.3771.\n",
      "For Iteration 757 the Loss is 0.377.\n",
      "For Iteration 758 the Loss is 0.3769.\n",
      "For Iteration 759 the Loss is 0.3767.\n",
      "For Iteration 760 the Loss is 0.3766.\n",
      "For Iteration 761 the Loss is 0.3765.\n",
      "For Iteration 762 the Loss is 0.3764.\n",
      "For Iteration 763 the Loss is 0.3763.\n",
      "For Iteration 764 the Loss is 0.3762.\n",
      "For Iteration 765 the Loss is 0.3761.\n",
      "For Iteration 766 the Loss is 0.376.\n",
      "For Iteration 767 the Loss is 0.3759.\n",
      "For Iteration 768 the Loss is 0.3758.\n",
      "For Iteration 769 the Loss is 0.3757.\n",
      "For Iteration 770 the Loss is 0.3756.\n",
      "For Iteration 771 the Loss is 0.3755.\n",
      "For Iteration 772 the Loss is 0.3754.\n",
      "For Iteration 773 the Loss is 0.3753.\n",
      "For Iteration 774 the Loss is 0.3752.\n",
      "For Iteration 775 the Loss is 0.3751.\n",
      "For Iteration 776 the Loss is 0.375.\n",
      "For Iteration 777 the Loss is 0.3749.\n",
      "For Iteration 778 the Loss is 0.3748.\n",
      "For Iteration 779 the Loss is 0.3747.\n",
      "For Iteration 780 the Loss is 0.3746.\n",
      "For Iteration 781 the Loss is 0.3745.\n",
      "For Iteration 782 the Loss is 0.3744.\n",
      "For Iteration 783 the Loss is 0.3743.\n",
      "For Iteration 784 the Loss is 0.3742.\n",
      "For Iteration 785 the Loss is 0.3741.\n",
      "For Iteration 786 the Loss is 0.374.\n",
      "For Iteration 787 the Loss is 0.3739.\n",
      "For Iteration 788 the Loss is 0.3738.\n",
      "For Iteration 789 the Loss is 0.3737.\n",
      "For Iteration 790 the Loss is 0.3736.\n",
      "For Iteration 791 the Loss is 0.3735.\n",
      "For Iteration 792 the Loss is 0.3734.\n",
      "For Iteration 793 the Loss is 0.3733.\n",
      "For Iteration 794 the Loss is 0.3733.\n",
      "For Iteration 795 the Loss is 0.3732.\n",
      "For Iteration 796 the Loss is 0.3731.\n",
      "For Iteration 797 the Loss is 0.373.\n",
      "For Iteration 798 the Loss is 0.3729.\n",
      "For Iteration 799 the Loss is 0.3728.\n",
      "For Iteration 800 the Loss is 0.3727.\n",
      "For Iteration 801 the Loss is 0.3726.\n",
      "For Iteration 802 the Loss is 0.3725.\n",
      "For Iteration 803 the Loss is 0.3724.\n",
      "For Iteration 804 the Loss is 0.3724.\n",
      "For Iteration 805 the Loss is 0.3723.\n",
      "For Iteration 806 the Loss is 0.3722.\n",
      "For Iteration 807 the Loss is 0.3721.\n",
      "For Iteration 808 the Loss is 0.372.\n",
      "For Iteration 809 the Loss is 0.3719.\n",
      "For Iteration 810 the Loss is 0.3718.\n",
      "For Iteration 811 the Loss is 0.3718.\n",
      "For Iteration 812 the Loss is 0.3717.\n",
      "For Iteration 813 the Loss is 0.3716.\n",
      "For Iteration 814 the Loss is 0.3715.\n",
      "For Iteration 815 the Loss is 0.3714.\n",
      "For Iteration 816 the Loss is 0.3713.\n",
      "For Iteration 817 the Loss is 0.3713.\n",
      "For Iteration 818 the Loss is 0.3712.\n",
      "For Iteration 819 the Loss is 0.3711.\n",
      "For Iteration 820 the Loss is 0.371.\n",
      "For Iteration 821 the Loss is 0.3709.\n",
      "For Iteration 822 the Loss is 0.3709.\n",
      "For Iteration 823 the Loss is 0.3708.\n",
      "For Iteration 824 the Loss is 0.3707.\n",
      "For Iteration 825 the Loss is 0.3706.\n",
      "For Iteration 826 the Loss is 0.3705.\n",
      "For Iteration 827 the Loss is 0.3705.\n",
      "For Iteration 828 the Loss is 0.3704.\n",
      "For Iteration 829 the Loss is 0.3703.\n",
      "For Iteration 830 the Loss is 0.3702.\n",
      "For Iteration 831 the Loss is 0.3702.\n",
      "For Iteration 832 the Loss is 0.3701.\n",
      "For Iteration 833 the Loss is 0.37.\n",
      "For Iteration 834 the Loss is 0.3699.\n",
      "For Iteration 835 the Loss is 0.3698.\n",
      "For Iteration 836 the Loss is 0.3698.\n",
      "For Iteration 837 the Loss is 0.3697.\n",
      "For Iteration 838 the Loss is 0.3696.\n",
      "For Iteration 839 the Loss is 0.3696.\n",
      "For Iteration 840 the Loss is 0.3695.\n",
      "For Iteration 841 the Loss is 0.3694.\n",
      "For Iteration 842 the Loss is 0.3693.\n",
      "For Iteration 843 the Loss is 0.3693.\n",
      "For Iteration 844 the Loss is 0.3692.\n",
      "For Iteration 845 the Loss is 0.3691.\n",
      "For Iteration 846 the Loss is 0.369.\n",
      "For Iteration 847 the Loss is 0.369.\n",
      "For Iteration 848 the Loss is 0.3689.\n",
      "For Iteration 849 the Loss is 0.3688.\n",
      "For Iteration 850 the Loss is 0.3688.\n",
      "For Iteration 851 the Loss is 0.3687.\n",
      "For Iteration 852 the Loss is 0.3686.\n",
      "For Iteration 853 the Loss is 0.3685.\n",
      "For Iteration 854 the Loss is 0.3685.\n",
      "For Iteration 855 the Loss is 0.3684.\n",
      "For Iteration 856 the Loss is 0.3683.\n",
      "For Iteration 857 the Loss is 0.3683.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 858 the Loss is 0.3682.\n",
      "For Iteration 859 the Loss is 0.3681.\n",
      "For Iteration 860 the Loss is 0.3681.\n",
      "For Iteration 861 the Loss is 0.368.\n",
      "For Iteration 862 the Loss is 0.3679.\n",
      "For Iteration 863 the Loss is 0.3679.\n",
      "For Iteration 864 the Loss is 0.3678.\n",
      "For Iteration 865 the Loss is 0.3677.\n",
      "For Iteration 866 the Loss is 0.3677.\n",
      "For Iteration 867 the Loss is 0.3676.\n",
      "For Iteration 868 the Loss is 0.3675.\n",
      "For Iteration 869 the Loss is 0.3675.\n",
      "For Iteration 870 the Loss is 0.3674.\n",
      "For Iteration 871 the Loss is 0.3673.\n",
      "For Iteration 872 the Loss is 0.3673.\n",
      "For Iteration 873 the Loss is 0.3672.\n",
      "For Iteration 874 the Loss is 0.3671.\n",
      "For Iteration 875 the Loss is 0.3671.\n",
      "For Iteration 876 the Loss is 0.367.\n",
      "For Iteration 877 the Loss is 0.367.\n",
      "For Iteration 878 the Loss is 0.3669.\n",
      "For Iteration 879 the Loss is 0.3668.\n",
      "For Iteration 880 the Loss is 0.3668.\n",
      "For Iteration 881 the Loss is 0.3667.\n",
      "For Iteration 882 the Loss is 0.3666.\n",
      "For Iteration 883 the Loss is 0.3666.\n",
      "For Iteration 884 the Loss is 0.3665.\n",
      "For Iteration 885 the Loss is 0.3665.\n",
      "For Iteration 886 the Loss is 0.3664.\n",
      "For Iteration 887 the Loss is 0.3663.\n",
      "For Iteration 888 the Loss is 0.3663.\n",
      "For Iteration 889 the Loss is 0.3662.\n",
      "For Iteration 890 the Loss is 0.3662.\n",
      "For Iteration 891 the Loss is 0.3661.\n",
      "For Iteration 892 the Loss is 0.366.\n",
      "For Iteration 893 the Loss is 0.366.\n",
      "For Iteration 894 the Loss is 0.3659.\n",
      "For Iteration 895 the Loss is 0.3659.\n",
      "For Iteration 896 the Loss is 0.3658.\n",
      "For Iteration 897 the Loss is 0.3658.\n",
      "For Iteration 898 the Loss is 0.3657.\n",
      "For Iteration 899 the Loss is 0.3656.\n",
      "For Iteration 900 the Loss is 0.3656.\n",
      "For Iteration 901 the Loss is 0.3655.\n",
      "For Iteration 902 the Loss is 0.3655.\n",
      "For Iteration 903 the Loss is 0.3654.\n",
      "For Iteration 904 the Loss is 0.3654.\n",
      "For Iteration 905 the Loss is 0.3653.\n",
      "For Iteration 906 the Loss is 0.3652.\n",
      "For Iteration 907 the Loss is 0.3652.\n",
      "For Iteration 908 the Loss is 0.3651.\n",
      "For Iteration 909 the Loss is 0.3651.\n",
      "For Iteration 910 the Loss is 0.365.\n",
      "For Iteration 911 the Loss is 0.365.\n",
      "For Iteration 912 the Loss is 0.3649.\n",
      "For Iteration 913 the Loss is 0.3649.\n",
      "For Iteration 914 the Loss is 0.3648.\n",
      "For Iteration 915 the Loss is 0.3647.\n",
      "For Iteration 916 the Loss is 0.3647.\n",
      "For Iteration 917 the Loss is 0.3646.\n",
      "For Iteration 918 the Loss is 0.3646.\n",
      "For Iteration 919 the Loss is 0.3645.\n",
      "For Iteration 920 the Loss is 0.3645.\n",
      "For Iteration 921 the Loss is 0.3644.\n",
      "For Iteration 922 the Loss is 0.3644.\n",
      "For Iteration 923 the Loss is 0.3643.\n",
      "For Iteration 924 the Loss is 0.3643.\n",
      "For Iteration 925 the Loss is 0.3642.\n",
      "For Iteration 926 the Loss is 0.3642.\n",
      "For Iteration 927 the Loss is 0.3641.\n",
      "For Iteration 928 the Loss is 0.3641.\n",
      "For Iteration 929 the Loss is 0.364.\n",
      "For Iteration 930 the Loss is 0.364.\n",
      "For Iteration 931 the Loss is 0.3639.\n",
      "For Iteration 932 the Loss is 0.3639.\n",
      "For Iteration 933 the Loss is 0.3638.\n",
      "For Iteration 934 the Loss is 0.3638.\n",
      "For Iteration 935 the Loss is 0.3637.\n",
      "For Iteration 936 the Loss is 0.3637.\n",
      "For Iteration 937 the Loss is 0.3636.\n",
      "For Iteration 938 the Loss is 0.3636.\n",
      "For Iteration 939 the Loss is 0.3635.\n",
      "For Iteration 940 the Loss is 0.3635.\n",
      "For Iteration 941 the Loss is 0.3634.\n",
      "For Iteration 942 the Loss is 0.3634.\n",
      "For Iteration 943 the Loss is 0.3633.\n",
      "For Iteration 944 the Loss is 0.3633.\n",
      "For Iteration 945 the Loss is 0.3632.\n",
      "For Iteration 946 the Loss is 0.3632.\n",
      "For Iteration 947 the Loss is 0.3631.\n",
      "For Iteration 948 the Loss is 0.3631.\n",
      "For Iteration 949 the Loss is 0.363.\n",
      "For Iteration 950 the Loss is 0.363.\n",
      "For Iteration 951 the Loss is 0.3629.\n",
      "For Iteration 952 the Loss is 0.3629.\n",
      "For Iteration 953 the Loss is 0.3628.\n",
      "For Iteration 954 the Loss is 0.3628.\n",
      "For Iteration 955 the Loss is 0.3627.\n",
      "For Iteration 956 the Loss is 0.3627.\n",
      "For Iteration 957 the Loss is 0.3626.\n",
      "For Iteration 958 the Loss is 0.3626.\n",
      "For Iteration 959 the Loss is 0.3626.\n",
      "For Iteration 960 the Loss is 0.3625.\n",
      "For Iteration 961 the Loss is 0.3625.\n",
      "For Iteration 962 the Loss is 0.3624.\n",
      "For Iteration 963 the Loss is 0.3624.\n",
      "For Iteration 964 the Loss is 0.3623.\n",
      "For Iteration 965 the Loss is 0.3623.\n",
      "For Iteration 966 the Loss is 0.3622.\n",
      "For Iteration 967 the Loss is 0.3622.\n",
      "For Iteration 968 the Loss is 0.3621.\n",
      "For Iteration 969 the Loss is 0.3621.\n",
      "For Iteration 970 the Loss is 0.3621.\n",
      "For Iteration 971 the Loss is 0.362.\n",
      "For Iteration 972 the Loss is 0.362.\n",
      "For Iteration 973 the Loss is 0.3619.\n",
      "For Iteration 974 the Loss is 0.3619.\n",
      "For Iteration 975 the Loss is 0.3618.\n",
      "For Iteration 976 the Loss is 0.3618.\n",
      "For Iteration 977 the Loss is 0.3618.\n",
      "For Iteration 978 the Loss is 0.3617.\n",
      "For Iteration 979 the Loss is 0.3617.\n",
      "For Iteration 980 the Loss is 0.3616.\n",
      "For Iteration 981 the Loss is 0.3616.\n",
      "For Iteration 982 the Loss is 0.3615.\n",
      "For Iteration 983 the Loss is 0.3615.\n",
      "For Iteration 984 the Loss is 0.3615.\n",
      "For Iteration 985 the Loss is 0.3614.\n",
      "For Iteration 986 the Loss is 0.3614.\n",
      "For Iteration 987 the Loss is 0.3613.\n",
      "For Iteration 988 the Loss is 0.3613.\n",
      "For Iteration 989 the Loss is 0.3612.\n",
      "For Iteration 990 the Loss is 0.3612.\n",
      "For Iteration 991 the Loss is 0.3612.\n",
      "For Iteration 992 the Loss is 0.3611.\n",
      "For Iteration 993 the Loss is 0.3611.\n",
      "For Iteration 994 the Loss is 0.361.\n",
      "For Iteration 995 the Loss is 0.361.\n",
      "For Iteration 996 the Loss is 0.361.\n",
      "For Iteration 997 the Loss is 0.3609.\n",
      "For Iteration 998 the Loss is 0.3609.\n",
      "For Iteration 999 the Loss is 0.3608.\n",
      "For Iteration 1000 the Loss is 0.3608.\n",
      "For Iteration 1001 the Loss is 0.3608.\n",
      "For Iteration 1002 the Loss is 0.3607.\n",
      "For Iteration 1003 the Loss is 0.3607.\n",
      "For Iteration 1004 the Loss is 0.3606.\n",
      "For Iteration 1005 the Loss is 0.3606.\n",
      "For Iteration 1006 the Loss is 0.3606.\n",
      "For Iteration 1007 the Loss is 0.3605.\n",
      "For Iteration 1008 the Loss is 0.3605.\n",
      "For Iteration 1009 the Loss is 0.3604.\n",
      "For Iteration 1010 the Loss is 0.3604.\n",
      "For Iteration 1011 the Loss is 0.3604.\n",
      "For Iteration 1012 the Loss is 0.3603.\n",
      "For Iteration 1013 the Loss is 0.3603.\n",
      "For Iteration 1014 the Loss is 0.3603.\n",
      "For Iteration 1015 the Loss is 0.3602.\n",
      "For Iteration 1016 the Loss is 0.3602.\n",
      "For Iteration 1017 the Loss is 0.3601.\n",
      "For Iteration 1018 the Loss is 0.3601.\n",
      "For Iteration 1019 the Loss is 0.3601.\n",
      "For Iteration 1020 the Loss is 0.36.\n",
      "For Iteration 1021 the Loss is 0.36.\n",
      "For Iteration 1022 the Loss is 0.36.\n",
      "For Iteration 1023 the Loss is 0.3599.\n",
      "For Iteration 1024 the Loss is 0.3599.\n",
      "For Iteration 1025 the Loss is 0.3598.\n",
      "For Iteration 1026 the Loss is 0.3598.\n",
      "For Iteration 1027 the Loss is 0.3598.\n",
      "For Iteration 1028 the Loss is 0.3597.\n",
      "For Iteration 1029 the Loss is 0.3597.\n",
      "For Iteration 1030 the Loss is 0.3597.\n",
      "For Iteration 1031 the Loss is 0.3596.\n",
      "For Iteration 1032 the Loss is 0.3596.\n",
      "For Iteration 1033 the Loss is 0.3595.\n",
      "For Iteration 1034 the Loss is 0.3595.\n",
      "For Iteration 1035 the Loss is 0.3595.\n",
      "For Iteration 1036 the Loss is 0.3594.\n",
      "For Iteration 1037 the Loss is 0.3594.\n",
      "For Iteration 1038 the Loss is 0.3594.\n",
      "For Iteration 1039 the Loss is 0.3593.\n",
      "For Iteration 1040 the Loss is 0.3593.\n",
      "For Iteration 1041 the Loss is 0.3593.\n",
      "For Iteration 1042 the Loss is 0.3592.\n",
      "For Iteration 1043 the Loss is 0.3592.\n",
      "For Iteration 1044 the Loss is 0.3592.\n",
      "For Iteration 1045 the Loss is 0.3591.\n",
      "For Iteration 1046 the Loss is 0.3591.\n",
      "For Iteration 1047 the Loss is 0.3591.\n",
      "For Iteration 1048 the Loss is 0.359.\n",
      "For Iteration 1049 the Loss is 0.359.\n",
      "For Iteration 1050 the Loss is 0.359.\n",
      "For Iteration 1051 the Loss is 0.3589.\n",
      "For Iteration 1052 the Loss is 0.3589.\n",
      "For Iteration 1053 the Loss is 0.3588.\n",
      "For Iteration 1054 the Loss is 0.3588.\n",
      "For Iteration 1055 the Loss is 0.3588.\n",
      "For Iteration 1056 the Loss is 0.3587.\n",
      "For Iteration 1057 the Loss is 0.3587.\n",
      "For Iteration 1058 the Loss is 0.3587.\n",
      "For Iteration 1059 the Loss is 0.3586.\n",
      "For Iteration 1060 the Loss is 0.3586.\n",
      "For Iteration 1061 the Loss is 0.3586.\n",
      "For Iteration 1062 the Loss is 0.3585.\n",
      "For Iteration 1063 the Loss is 0.3585.\n",
      "For Iteration 1064 the Loss is 0.3585.\n",
      "For Iteration 1065 the Loss is 0.3584.\n",
      "For Iteration 1066 the Loss is 0.3584.\n",
      "For Iteration 1067 the Loss is 0.3584.\n",
      "For Iteration 1068 the Loss is 0.3583.\n",
      "For Iteration 1069 the Loss is 0.3583.\n",
      "For Iteration 1070 the Loss is 0.3583.\n",
      "For Iteration 1071 the Loss is 0.3582.\n",
      "For Iteration 1072 the Loss is 0.3582.\n",
      "For Iteration 1073 the Loss is 0.3582.\n",
      "For Iteration 1074 the Loss is 0.3582.\n",
      "For Iteration 1075 the Loss is 0.3581.\n",
      "For Iteration 1076 the Loss is 0.3581.\n",
      "For Iteration 1077 the Loss is 0.3581.\n",
      "For Iteration 1078 the Loss is 0.358.\n",
      "For Iteration 1079 the Loss is 0.358.\n",
      "For Iteration 1080 the Loss is 0.358.\n",
      "For Iteration 1081 the Loss is 0.3579.\n",
      "For Iteration 1082 the Loss is 0.3579.\n",
      "For Iteration 1083 the Loss is 0.3579.\n",
      "For Iteration 1084 the Loss is 0.3578.\n",
      "For Iteration 1085 the Loss is 0.3578.\n",
      "For Iteration 1086 the Loss is 0.3578.\n",
      "For Iteration 1087 the Loss is 0.3577.\n",
      "For Iteration 1088 the Loss is 0.3577.\n",
      "For Iteration 1089 the Loss is 0.3577.\n",
      "For Iteration 1090 the Loss is 0.3577.\n",
      "For Iteration 1091 the Loss is 0.3576.\n",
      "For Iteration 1092 the Loss is 0.3576.\n",
      "For Iteration 1093 the Loss is 0.3576.\n",
      "For Iteration 1094 the Loss is 0.3575.\n",
      "For Iteration 1095 the Loss is 0.3575.\n",
      "For Iteration 1096 the Loss is 0.3575.\n",
      "For Iteration 1097 the Loss is 0.3574.\n",
      "For Iteration 1098 the Loss is 0.3574.\n",
      "For Iteration 1099 the Loss is 0.3574.\n",
      "For Iteration 1100 the Loss is 0.3573.\n",
      "For Iteration 1101 the Loss is 0.3573.\n",
      "For Iteration 1102 the Loss is 0.3573.\n",
      "For Iteration 1103 the Loss is 0.3573.\n",
      "For Iteration 1104 the Loss is 0.3572.\n",
      "For Iteration 1105 the Loss is 0.3572.\n",
      "For Iteration 1106 the Loss is 0.3572.\n",
      "For Iteration 1107 the Loss is 0.3571.\n",
      "For Iteration 1108 the Loss is 0.3571.\n",
      "For Iteration 1109 the Loss is 0.3571.\n",
      "For Iteration 1110 the Loss is 0.357.\n",
      "For Iteration 1111 the Loss is 0.357.\n",
      "For Iteration 1112 the Loss is 0.357.\n",
      "For Iteration 1113 the Loss is 0.357.\n",
      "For Iteration 1114 the Loss is 0.3569.\n",
      "For Iteration 1115 the Loss is 0.3569.\n",
      "For Iteration 1116 the Loss is 0.3569.\n",
      "For Iteration 1117 the Loss is 0.3568.\n",
      "For Iteration 1118 the Loss is 0.3568.\n",
      "For Iteration 1119 the Loss is 0.3568.\n",
      "For Iteration 1120 the Loss is 0.3568.\n",
      "For Iteration 1121 the Loss is 0.3567.\n",
      "For Iteration 1122 the Loss is 0.3567.\n",
      "For Iteration 1123 the Loss is 0.3567.\n",
      "For Iteration 1124 the Loss is 0.3566.\n",
      "For Iteration 1125 the Loss is 0.3566.\n",
      "For Iteration 1126 the Loss is 0.3566.\n",
      "For Iteration 1127 the Loss is 0.3566.\n",
      "For Iteration 1128 the Loss is 0.3565.\n",
      "For Iteration 1129 the Loss is 0.3565.\n",
      "For Iteration 1130 the Loss is 0.3565.\n",
      "For Iteration 1131 the Loss is 0.3565.\n",
      "For Iteration 1132 the Loss is 0.3564.\n",
      "For Iteration 1133 the Loss is 0.3564.\n",
      "For Iteration 1134 the Loss is 0.3564.\n",
      "For Iteration 1135 the Loss is 0.3563.\n",
      "For Iteration 1136 the Loss is 0.3563.\n",
      "For Iteration 1137 the Loss is 0.3563.\n",
      "For Iteration 1138 the Loss is 0.3563.\n",
      "For Iteration 1139 the Loss is 0.3562.\n",
      "For Iteration 1140 the Loss is 0.3562.\n",
      "For Iteration 1141 the Loss is 0.3562.\n",
      "For Iteration 1142 the Loss is 0.3561.\n",
      "For Iteration 1143 the Loss is 0.3561.\n",
      "For Iteration 1144 the Loss is 0.3561.\n",
      "For Iteration 1145 the Loss is 0.3561.\n",
      "For Iteration 1146 the Loss is 0.356.\n",
      "For Iteration 1147 the Loss is 0.356.\n",
      "For Iteration 1148 the Loss is 0.356.\n",
      "For Iteration 1149 the Loss is 0.356.\n",
      "For Iteration 1150 the Loss is 0.3559.\n",
      "For Iteration 1151 the Loss is 0.3559.\n",
      "For Iteration 1152 the Loss is 0.3559.\n",
      "For Iteration 1153 the Loss is 0.3559.\n",
      "For Iteration 1154 the Loss is 0.3558.\n",
      "For Iteration 1155 the Loss is 0.3558.\n",
      "For Iteration 1156 the Loss is 0.3558.\n",
      "For Iteration 1157 the Loss is 0.3557.\n",
      "For Iteration 1158 the Loss is 0.3557.\n",
      "For Iteration 1159 the Loss is 0.3557.\n",
      "For Iteration 1160 the Loss is 0.3557.\n",
      "For Iteration 1161 the Loss is 0.3556.\n",
      "For Iteration 1162 the Loss is 0.3556.\n",
      "For Iteration 1163 the Loss is 0.3556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1164 the Loss is 0.3556.\n",
      "For Iteration 1165 the Loss is 0.3555.\n",
      "For Iteration 1166 the Loss is 0.3555.\n",
      "For Iteration 1167 the Loss is 0.3555.\n",
      "For Iteration 1168 the Loss is 0.3555.\n",
      "For Iteration 1169 the Loss is 0.3554.\n",
      "For Iteration 1170 the Loss is 0.3554.\n",
      "For Iteration 1171 the Loss is 0.3554.\n",
      "For Iteration 1172 the Loss is 0.3554.\n",
      "For Iteration 1173 the Loss is 0.3553.\n",
      "For Iteration 1174 the Loss is 0.3553.\n",
      "For Iteration 1175 the Loss is 0.3553.\n",
      "For Iteration 1176 the Loss is 0.3553.\n",
      "For Iteration 1177 the Loss is 0.3552.\n",
      "For Iteration 1178 the Loss is 0.3552.\n",
      "For Iteration 1179 the Loss is 0.3552.\n",
      "For Iteration 1180 the Loss is 0.3552.\n",
      "For Iteration 1181 the Loss is 0.3551.\n",
      "For Iteration 1182 the Loss is 0.3551.\n",
      "For Iteration 1183 the Loss is 0.3551.\n",
      "For Iteration 1184 the Loss is 0.3551.\n",
      "For Iteration 1185 the Loss is 0.355.\n",
      "For Iteration 1186 the Loss is 0.355.\n",
      "For Iteration 1187 the Loss is 0.355.\n",
      "For Iteration 1188 the Loss is 0.355.\n",
      "For Iteration 1189 the Loss is 0.3549.\n",
      "For Iteration 1190 the Loss is 0.3549.\n",
      "For Iteration 1191 the Loss is 0.3549.\n",
      "For Iteration 1192 the Loss is 0.3549.\n",
      "For Iteration 1193 the Loss is 0.3548.\n",
      "For Iteration 1194 the Loss is 0.3548.\n",
      "For Iteration 1195 the Loss is 0.3548.\n",
      "For Iteration 1196 the Loss is 0.3548.\n",
      "For Iteration 1197 the Loss is 0.3547.\n",
      "For Iteration 1198 the Loss is 0.3547.\n",
      "For Iteration 1199 the Loss is 0.3547.\n",
      "For Iteration 1200 the Loss is 0.3547.\n",
      "For Iteration 1201 the Loss is 0.3546.\n",
      "For Iteration 1202 the Loss is 0.3546.\n",
      "For Iteration 1203 the Loss is 0.3546.\n",
      "For Iteration 1204 the Loss is 0.3546.\n",
      "For Iteration 1205 the Loss is 0.3546.\n",
      "For Iteration 1206 the Loss is 0.3545.\n",
      "For Iteration 1207 the Loss is 0.3545.\n",
      "For Iteration 1208 the Loss is 0.3545.\n",
      "For Iteration 1209 the Loss is 0.3545.\n",
      "For Iteration 1210 the Loss is 0.3544.\n",
      "For Iteration 1211 the Loss is 0.3544.\n",
      "For Iteration 1212 the Loss is 0.3544.\n",
      "For Iteration 1213 the Loss is 0.3544.\n",
      "For Iteration 1214 the Loss is 0.3543.\n",
      "For Iteration 1215 the Loss is 0.3543.\n",
      "For Iteration 1216 the Loss is 0.3543.\n",
      "For Iteration 1217 the Loss is 0.3543.\n",
      "For Iteration 1218 the Loss is 0.3542.\n",
      "For Iteration 1219 the Loss is 0.3542.\n",
      "For Iteration 1220 the Loss is 0.3542.\n",
      "For Iteration 1221 the Loss is 0.3542.\n",
      "For Iteration 1222 the Loss is 0.3542.\n",
      "For Iteration 1223 the Loss is 0.3541.\n",
      "For Iteration 1224 the Loss is 0.3541.\n",
      "For Iteration 1225 the Loss is 0.3541.\n",
      "For Iteration 1226 the Loss is 0.3541.\n",
      "For Iteration 1227 the Loss is 0.354.\n",
      "For Iteration 1228 the Loss is 0.354.\n",
      "For Iteration 1229 the Loss is 0.354.\n",
      "For Iteration 1230 the Loss is 0.354.\n",
      "For Iteration 1231 the Loss is 0.3539.\n",
      "For Iteration 1232 the Loss is 0.3539.\n",
      "For Iteration 1233 the Loss is 0.3539.\n",
      "For Iteration 1234 the Loss is 0.3539.\n",
      "For Iteration 1235 the Loss is 0.3539.\n",
      "For Iteration 1236 the Loss is 0.3538.\n",
      "For Iteration 1237 the Loss is 0.3538.\n",
      "For Iteration 1238 the Loss is 0.3538.\n",
      "For Iteration 1239 the Loss is 0.3538.\n",
      "For Iteration 1240 the Loss is 0.3537.\n",
      "For Iteration 1241 the Loss is 0.3537.\n",
      "For Iteration 1242 the Loss is 0.3537.\n",
      "For Iteration 1243 the Loss is 0.3537.\n",
      "For Iteration 1244 the Loss is 0.3537.\n",
      "For Iteration 1245 the Loss is 0.3536.\n",
      "For Iteration 1246 the Loss is 0.3536.\n",
      "For Iteration 1247 the Loss is 0.3536.\n",
      "For Iteration 1248 the Loss is 0.3536.\n",
      "For Iteration 1249 the Loss is 0.3535.\n",
      "For Iteration 1250 the Loss is 0.3535.\n",
      "For Iteration 1251 the Loss is 0.3535.\n",
      "For Iteration 1252 the Loss is 0.3535.\n",
      "For Iteration 1253 the Loss is 0.3535.\n",
      "For Iteration 1254 the Loss is 0.3534.\n",
      "For Iteration 1255 the Loss is 0.3534.\n",
      "For Iteration 1256 the Loss is 0.3534.\n",
      "For Iteration 1257 the Loss is 0.3534.\n",
      "For Iteration 1258 the Loss is 0.3533.\n",
      "For Iteration 1259 the Loss is 0.3533.\n",
      "For Iteration 1260 the Loss is 0.3533.\n",
      "For Iteration 1261 the Loss is 0.3533.\n",
      "For Iteration 1262 the Loss is 0.3533.\n",
      "For Iteration 1263 the Loss is 0.3532.\n",
      "For Iteration 1264 the Loss is 0.3532.\n",
      "For Iteration 1265 the Loss is 0.3532.\n",
      "For Iteration 1266 the Loss is 0.3532.\n",
      "For Iteration 1267 the Loss is 0.3532.\n",
      "For Iteration 1268 the Loss is 0.3531.\n",
      "For Iteration 1269 the Loss is 0.3531.\n",
      "For Iteration 1270 the Loss is 0.3531.\n",
      "For Iteration 1271 the Loss is 0.3531.\n",
      "For Iteration 1272 the Loss is 0.353.\n",
      "For Iteration 1273 the Loss is 0.353.\n",
      "For Iteration 1274 the Loss is 0.353.\n",
      "For Iteration 1275 the Loss is 0.353.\n",
      "For Iteration 1276 the Loss is 0.353.\n",
      "For Iteration 1277 the Loss is 0.3529.\n",
      "For Iteration 1278 the Loss is 0.3529.\n",
      "For Iteration 1279 the Loss is 0.3529.\n",
      "For Iteration 1280 the Loss is 0.3529.\n",
      "For Iteration 1281 the Loss is 0.3529.\n",
      "For Iteration 1282 the Loss is 0.3528.\n",
      "For Iteration 1283 the Loss is 0.3528.\n",
      "For Iteration 1284 the Loss is 0.3528.\n",
      "For Iteration 1285 the Loss is 0.3528.\n",
      "For Iteration 1286 the Loss is 0.3528.\n",
      "For Iteration 1287 the Loss is 0.3527.\n",
      "For Iteration 1288 the Loss is 0.3527.\n",
      "For Iteration 1289 the Loss is 0.3527.\n",
      "For Iteration 1290 the Loss is 0.3527.\n",
      "For Iteration 1291 the Loss is 0.3527.\n",
      "For Iteration 1292 the Loss is 0.3526.\n",
      "For Iteration 1293 the Loss is 0.3526.\n",
      "For Iteration 1294 the Loss is 0.3526.\n",
      "For Iteration 1295 the Loss is 0.3526.\n",
      "For Iteration 1296 the Loss is 0.3526.\n",
      "For Iteration 1297 the Loss is 0.3525.\n",
      "For Iteration 1298 the Loss is 0.3525.\n",
      "For Iteration 1299 the Loss is 0.3525.\n",
      "For Iteration 1300 the Loss is 0.3525.\n",
      "For Iteration 1301 the Loss is 0.3525.\n",
      "For Iteration 1302 the Loss is 0.3524.\n",
      "For Iteration 1303 the Loss is 0.3524.\n",
      "For Iteration 1304 the Loss is 0.3524.\n",
      "For Iteration 1305 the Loss is 0.3524.\n",
      "For Iteration 1306 the Loss is 0.3523.\n",
      "For Iteration 1307 the Loss is 0.3523.\n",
      "For Iteration 1308 the Loss is 0.3523.\n",
      "For Iteration 1309 the Loss is 0.3523.\n",
      "For Iteration 1310 the Loss is 0.3523.\n",
      "For Iteration 1311 the Loss is 0.3522.\n",
      "For Iteration 1312 the Loss is 0.3522.\n",
      "For Iteration 1313 the Loss is 0.3522.\n",
      "For Iteration 1314 the Loss is 0.3522.\n",
      "For Iteration 1315 the Loss is 0.3522.\n",
      "For Iteration 1316 the Loss is 0.3522.\n",
      "For Iteration 1317 the Loss is 0.3521.\n",
      "For Iteration 1318 the Loss is 0.3521.\n",
      "For Iteration 1319 the Loss is 0.3521.\n",
      "For Iteration 1320 the Loss is 0.3521.\n",
      "For Iteration 1321 the Loss is 0.3521.\n",
      "For Iteration 1322 the Loss is 0.352.\n",
      "For Iteration 1323 the Loss is 0.352.\n",
      "For Iteration 1324 the Loss is 0.352.\n",
      "For Iteration 1325 the Loss is 0.352.\n",
      "For Iteration 1326 the Loss is 0.352.\n",
      "For Iteration 1327 the Loss is 0.3519.\n",
      "For Iteration 1328 the Loss is 0.3519.\n",
      "For Iteration 1329 the Loss is 0.3519.\n",
      "For Iteration 1330 the Loss is 0.3519.\n",
      "For Iteration 1331 the Loss is 0.3519.\n",
      "For Iteration 1332 the Loss is 0.3518.\n",
      "For Iteration 1333 the Loss is 0.3518.\n",
      "For Iteration 1334 the Loss is 0.3518.\n",
      "For Iteration 1335 the Loss is 0.3518.\n",
      "For Iteration 1336 the Loss is 0.3518.\n",
      "For Iteration 1337 the Loss is 0.3517.\n",
      "For Iteration 1338 the Loss is 0.3517.\n",
      "For Iteration 1339 the Loss is 0.3517.\n",
      "For Iteration 1340 the Loss is 0.3517.\n",
      "For Iteration 1341 the Loss is 0.3517.\n",
      "For Iteration 1342 the Loss is 0.3516.\n",
      "For Iteration 1343 the Loss is 0.3516.\n",
      "For Iteration 1344 the Loss is 0.3516.\n",
      "For Iteration 1345 the Loss is 0.3516.\n",
      "For Iteration 1346 the Loss is 0.3516.\n",
      "For Iteration 1347 the Loss is 0.3515.\n",
      "For Iteration 1348 the Loss is 0.3515.\n",
      "For Iteration 1349 the Loss is 0.3515.\n",
      "For Iteration 1350 the Loss is 0.3515.\n",
      "For Iteration 1351 the Loss is 0.3515.\n",
      "For Iteration 1352 the Loss is 0.3515.\n",
      "For Iteration 1353 the Loss is 0.3514.\n",
      "For Iteration 1354 the Loss is 0.3514.\n",
      "For Iteration 1355 the Loss is 0.3514.\n",
      "For Iteration 1356 the Loss is 0.3514.\n",
      "For Iteration 1357 the Loss is 0.3514.\n",
      "For Iteration 1358 the Loss is 0.3513.\n",
      "For Iteration 1359 the Loss is 0.3513.\n",
      "For Iteration 1360 the Loss is 0.3513.\n",
      "For Iteration 1361 the Loss is 0.3513.\n",
      "For Iteration 1362 the Loss is 0.3513.\n",
      "For Iteration 1363 the Loss is 0.3512.\n",
      "For Iteration 1364 the Loss is 0.3512.\n",
      "For Iteration 1365 the Loss is 0.3512.\n",
      "For Iteration 1366 the Loss is 0.3512.\n",
      "For Iteration 1367 the Loss is 0.3512.\n",
      "For Iteration 1368 the Loss is 0.3512.\n",
      "For Iteration 1369 the Loss is 0.3511.\n",
      "For Iteration 1370 the Loss is 0.3511.\n",
      "For Iteration 1371 the Loss is 0.3511.\n",
      "For Iteration 1372 the Loss is 0.3511.\n",
      "For Iteration 1373 the Loss is 0.3511.\n",
      "For Iteration 1374 the Loss is 0.351.\n",
      "For Iteration 1375 the Loss is 0.351.\n",
      "For Iteration 1376 the Loss is 0.351.\n",
      "For Iteration 1377 the Loss is 0.351.\n",
      "For Iteration 1378 the Loss is 0.351.\n",
      "For Iteration 1379 the Loss is 0.351.\n",
      "For Iteration 1380 the Loss is 0.3509.\n",
      "For Iteration 1381 the Loss is 0.3509.\n",
      "For Iteration 1382 the Loss is 0.3509.\n",
      "For Iteration 1383 the Loss is 0.3509.\n",
      "For Iteration 1384 the Loss is 0.3509.\n",
      "For Iteration 1385 the Loss is 0.3508.\n",
      "For Iteration 1386 the Loss is 0.3508.\n",
      "For Iteration 1387 the Loss is 0.3508.\n",
      "For Iteration 1388 the Loss is 0.3508.\n",
      "For Iteration 1389 the Loss is 0.3508.\n",
      "For Iteration 1390 the Loss is 0.3508.\n",
      "For Iteration 1391 the Loss is 0.3507.\n",
      "For Iteration 1392 the Loss is 0.3507.\n",
      "For Iteration 1393 the Loss is 0.3507.\n",
      "For Iteration 1394 the Loss is 0.3507.\n",
      "For Iteration 1395 the Loss is 0.3507.\n",
      "For Iteration 1396 the Loss is 0.3506.\n",
      "For Iteration 1397 the Loss is 0.3506.\n",
      "For Iteration 1398 the Loss is 0.3506.\n",
      "For Iteration 1399 the Loss is 0.3506.\n",
      "For Iteration 1400 the Loss is 0.3506.\n",
      "For Iteration 1401 the Loss is 0.3506.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1402 the Loss is 0.3505.\n",
      "For Iteration 1403 the Loss is 0.3505.\n",
      "For Iteration 1404 the Loss is 0.3505.\n",
      "For Iteration 1405 the Loss is 0.3505.\n",
      "For Iteration 1406 the Loss is 0.3505.\n",
      "For Iteration 1407 the Loss is 0.3505.\n",
      "For Iteration 1408 the Loss is 0.3504.\n",
      "For Iteration 1409 the Loss is 0.3504.\n",
      "For Iteration 1410 the Loss is 0.3504.\n",
      "For Iteration 1411 the Loss is 0.3504.\n",
      "For Iteration 1412 the Loss is 0.3504.\n",
      "For Iteration 1413 the Loss is 0.3503.\n",
      "For Iteration 1414 the Loss is 0.3503.\n",
      "For Iteration 1415 the Loss is 0.3503.\n",
      "For Iteration 1416 the Loss is 0.3503.\n",
      "For Iteration 1417 the Loss is 0.3503.\n",
      "For Iteration 1418 the Loss is 0.3503.\n",
      "For Iteration 1419 the Loss is 0.3502.\n",
      "For Iteration 1420 the Loss is 0.3502.\n",
      "For Iteration 1421 the Loss is 0.3502.\n",
      "For Iteration 1422 the Loss is 0.3502.\n",
      "For Iteration 1423 the Loss is 0.3502.\n",
      "For Iteration 1424 the Loss is 0.3502.\n",
      "For Iteration 1425 the Loss is 0.3501.\n",
      "For Iteration 1426 the Loss is 0.3501.\n",
      "For Iteration 1427 the Loss is 0.3501.\n",
      "For Iteration 1428 the Loss is 0.3501.\n",
      "For Iteration 1429 the Loss is 0.3501.\n",
      "For Iteration 1430 the Loss is 0.35.\n",
      "For Iteration 1431 the Loss is 0.35.\n",
      "For Iteration 1432 the Loss is 0.35.\n",
      "For Iteration 1433 the Loss is 0.35.\n",
      "For Iteration 1434 the Loss is 0.35.\n",
      "For Iteration 1435 the Loss is 0.35.\n",
      "For Iteration 1436 the Loss is 0.3499.\n",
      "For Iteration 1437 the Loss is 0.3499.\n",
      "For Iteration 1438 the Loss is 0.3499.\n",
      "For Iteration 1439 the Loss is 0.3499.\n",
      "For Iteration 1440 the Loss is 0.3499.\n",
      "For Iteration 1441 the Loss is 0.3499.\n",
      "For Iteration 1442 the Loss is 0.3498.\n",
      "For Iteration 1443 the Loss is 0.3498.\n",
      "For Iteration 1444 the Loss is 0.3498.\n",
      "For Iteration 1445 the Loss is 0.3498.\n",
      "For Iteration 1446 the Loss is 0.3498.\n",
      "For Iteration 1447 the Loss is 0.3498.\n",
      "For Iteration 1448 the Loss is 0.3497.\n",
      "For Iteration 1449 the Loss is 0.3497.\n",
      "For Iteration 1450 the Loss is 0.3497.\n",
      "For Iteration 1451 the Loss is 0.3497.\n",
      "For Iteration 1452 the Loss is 0.3497.\n",
      "For Iteration 1453 the Loss is 0.3497.\n",
      "For Iteration 1454 the Loss is 0.3496.\n",
      "For Iteration 1455 the Loss is 0.3496.\n",
      "For Iteration 1456 the Loss is 0.3496.\n",
      "For Iteration 1457 the Loss is 0.3496.\n",
      "For Iteration 1458 the Loss is 0.3496.\n",
      "For Iteration 1459 the Loss is 0.3496.\n",
      "For Iteration 1460 the Loss is 0.3495.\n",
      "For Iteration 1461 the Loss is 0.3495.\n",
      "For Iteration 1462 the Loss is 0.3495.\n",
      "For Iteration 1463 the Loss is 0.3495.\n",
      "For Iteration 1464 the Loss is 0.3495.\n",
      "For Iteration 1465 the Loss is 0.3495.\n",
      "For Iteration 1466 the Loss is 0.3494.\n",
      "For Iteration 1467 the Loss is 0.3494.\n",
      "For Iteration 1468 the Loss is 0.3494.\n",
      "For Iteration 1469 the Loss is 0.3494.\n",
      "For Iteration 1470 the Loss is 0.3494.\n",
      "For Iteration 1471 the Loss is 0.3494.\n",
      "For Iteration 1472 the Loss is 0.3493.\n",
      "For Iteration 1473 the Loss is 0.3493.\n",
      "For Iteration 1474 the Loss is 0.3493.\n",
      "For Iteration 1475 the Loss is 0.3493.\n",
      "For Iteration 1476 the Loss is 0.3493.\n",
      "For Iteration 1477 the Loss is 0.3493.\n",
      "For Iteration 1478 the Loss is 0.3492.\n",
      "For Iteration 1479 the Loss is 0.3492.\n",
      "For Iteration 1480 the Loss is 0.3492.\n",
      "For Iteration 1481 the Loss is 0.3492.\n",
      "For Iteration 1482 the Loss is 0.3492.\n",
      "For Iteration 1483 the Loss is 0.3492.\n",
      "For Iteration 1484 the Loss is 0.3491.\n",
      "For Iteration 1485 the Loss is 0.3491.\n",
      "For Iteration 1486 the Loss is 0.3491.\n",
      "For Iteration 1487 the Loss is 0.3491.\n",
      "For Iteration 1488 the Loss is 0.3491.\n",
      "For Iteration 1489 the Loss is 0.3491.\n",
      "For Iteration 1490 the Loss is 0.349.\n",
      "For Iteration 1491 the Loss is 0.349.\n",
      "For Iteration 1492 the Loss is 0.349.\n",
      "For Iteration 1493 the Loss is 0.349.\n",
      "For Iteration 1494 the Loss is 0.349.\n",
      "For Iteration 1495 the Loss is 0.349.\n",
      "For Iteration 1496 the Loss is 0.3489.\n",
      "For Iteration 1497 the Loss is 0.3489.\n",
      "For Iteration 1498 the Loss is 0.3489.\n",
      "For Iteration 1499 the Loss is 0.3489.\n",
      "For Iteration 1500 the Loss is 0.3489.\n",
      "For Iteration 1501 the Loss is 0.3489.\n",
      "For Iteration 1502 the Loss is 0.3488.\n",
      "For Iteration 1503 the Loss is 0.3488.\n",
      "For Iteration 1504 the Loss is 0.3488.\n",
      "For Iteration 1505 the Loss is 0.3488.\n",
      "For Iteration 1506 the Loss is 0.3488.\n",
      "For Iteration 1507 the Loss is 0.3488.\n",
      "For Iteration 1508 the Loss is 0.3488.\n",
      "For Iteration 1509 the Loss is 0.3487.\n",
      "For Iteration 1510 the Loss is 0.3487.\n",
      "For Iteration 1511 the Loss is 0.3487.\n",
      "For Iteration 1512 the Loss is 0.3487.\n",
      "For Iteration 1513 the Loss is 0.3487.\n",
      "For Iteration 1514 the Loss is 0.3487.\n",
      "For Iteration 1515 the Loss is 0.3486.\n",
      "For Iteration 1516 the Loss is 0.3486.\n",
      "For Iteration 1517 the Loss is 0.3486.\n",
      "For Iteration 1518 the Loss is 0.3486.\n",
      "For Iteration 1519 the Loss is 0.3486.\n",
      "For Iteration 1520 the Loss is 0.3486.\n",
      "For Iteration 1521 the Loss is 0.3485.\n",
      "For Iteration 1522 the Loss is 0.3485.\n",
      "For Iteration 1523 the Loss is 0.3485.\n",
      "For Iteration 1524 the Loss is 0.3485.\n",
      "For Iteration 1525 the Loss is 0.3485.\n",
      "For Iteration 1526 the Loss is 0.3485.\n",
      "For Iteration 1527 the Loss is 0.3484.\n",
      "For Iteration 1528 the Loss is 0.3484.\n",
      "For Iteration 1529 the Loss is 0.3484.\n",
      "For Iteration 1530 the Loss is 0.3484.\n",
      "For Iteration 1531 the Loss is 0.3484.\n",
      "For Iteration 1532 the Loss is 0.3484.\n",
      "For Iteration 1533 the Loss is 0.3484.\n",
      "For Iteration 1534 the Loss is 0.3483.\n",
      "For Iteration 1535 the Loss is 0.3483.\n",
      "For Iteration 1536 the Loss is 0.3483.\n",
      "For Iteration 1537 the Loss is 0.3483.\n",
      "For Iteration 1538 the Loss is 0.3483.\n",
      "For Iteration 1539 the Loss is 0.3483.\n",
      "For Iteration 1540 the Loss is 0.3482.\n",
      "For Iteration 1541 the Loss is 0.3482.\n",
      "For Iteration 1542 the Loss is 0.3482.\n",
      "For Iteration 1543 the Loss is 0.3482.\n",
      "For Iteration 1544 the Loss is 0.3482.\n",
      "For Iteration 1545 the Loss is 0.3482.\n",
      "For Iteration 1546 the Loss is 0.3481.\n",
      "For Iteration 1547 the Loss is 0.3481.\n",
      "For Iteration 1548 the Loss is 0.3481.\n",
      "For Iteration 1549 the Loss is 0.3481.\n",
      "For Iteration 1550 the Loss is 0.3481.\n",
      "For Iteration 1551 the Loss is 0.3481.\n",
      "For Iteration 1552 the Loss is 0.3481.\n",
      "For Iteration 1553 the Loss is 0.348.\n",
      "For Iteration 1554 the Loss is 0.348.\n",
      "For Iteration 1555 the Loss is 0.348.\n",
      "For Iteration 1556 the Loss is 0.348.\n",
      "For Iteration 1557 the Loss is 0.348.\n",
      "For Iteration 1558 the Loss is 0.348.\n",
      "For Iteration 1559 the Loss is 0.3479.\n",
      "For Iteration 1560 the Loss is 0.3479.\n",
      "For Iteration 1561 the Loss is 0.3479.\n",
      "For Iteration 1562 the Loss is 0.3479.\n",
      "For Iteration 1563 the Loss is 0.3479.\n",
      "For Iteration 1564 the Loss is 0.3479.\n",
      "For Iteration 1565 the Loss is 0.3479.\n",
      "For Iteration 1566 the Loss is 0.3478.\n",
      "For Iteration 1567 the Loss is 0.3478.\n",
      "For Iteration 1568 the Loss is 0.3478.\n",
      "For Iteration 1569 the Loss is 0.3478.\n",
      "For Iteration 1570 the Loss is 0.3478.\n",
      "For Iteration 1571 the Loss is 0.3478.\n",
      "For Iteration 1572 the Loss is 0.3477.\n",
      "For Iteration 1573 the Loss is 0.3477.\n",
      "For Iteration 1574 the Loss is 0.3477.\n",
      "For Iteration 1575 the Loss is 0.3477.\n",
      "For Iteration 1576 the Loss is 0.3477.\n",
      "For Iteration 1577 the Loss is 0.3477.\n",
      "For Iteration 1578 the Loss is 0.3477.\n",
      "For Iteration 1579 the Loss is 0.3476.\n",
      "For Iteration 1580 the Loss is 0.3476.\n",
      "For Iteration 1581 the Loss is 0.3476.\n",
      "For Iteration 1582 the Loss is 0.3476.\n",
      "For Iteration 1583 the Loss is 0.3476.\n",
      "For Iteration 1584 the Loss is 0.3476.\n",
      "For Iteration 1585 the Loss is 0.3476.\n",
      "For Iteration 1586 the Loss is 0.3475.\n",
      "For Iteration 1587 the Loss is 0.3475.\n",
      "For Iteration 1588 the Loss is 0.3475.\n",
      "For Iteration 1589 the Loss is 0.3475.\n",
      "For Iteration 1590 the Loss is 0.3475.\n",
      "For Iteration 1591 the Loss is 0.3475.\n",
      "For Iteration 1592 the Loss is 0.3474.\n",
      "For Iteration 1593 the Loss is 0.3474.\n",
      "For Iteration 1594 the Loss is 0.3474.\n",
      "For Iteration 1595 the Loss is 0.3474.\n",
      "For Iteration 1596 the Loss is 0.3474.\n",
      "For Iteration 1597 the Loss is 0.3474.\n",
      "For Iteration 1598 the Loss is 0.3474.\n",
      "For Iteration 1599 the Loss is 0.3473.\n",
      "For Iteration 1600 the Loss is 0.3473.\n",
      "For Iteration 1601 the Loss is 0.3473.\n",
      "For Iteration 1602 the Loss is 0.3473.\n",
      "For Iteration 1603 the Loss is 0.3473.\n",
      "For Iteration 1604 the Loss is 0.3473.\n",
      "For Iteration 1605 the Loss is 0.3472.\n",
      "For Iteration 1606 the Loss is 0.3472.\n",
      "For Iteration 1607 the Loss is 0.3472.\n",
      "For Iteration 1608 the Loss is 0.3472.\n",
      "For Iteration 1609 the Loss is 0.3472.\n",
      "For Iteration 1610 the Loss is 0.3472.\n",
      "For Iteration 1611 the Loss is 0.3472.\n",
      "For Iteration 1612 the Loss is 0.3471.\n",
      "For Iteration 1613 the Loss is 0.3471.\n",
      "For Iteration 1614 the Loss is 0.3471.\n",
      "For Iteration 1615 the Loss is 0.3471.\n",
      "For Iteration 1616 the Loss is 0.3471.\n",
      "For Iteration 1617 the Loss is 0.3471.\n",
      "For Iteration 1618 the Loss is 0.3471.\n",
      "For Iteration 1619 the Loss is 0.347.\n",
      "For Iteration 1620 the Loss is 0.347.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1621 the Loss is 0.347.\n",
      "For Iteration 1622 the Loss is 0.347.\n",
      "For Iteration 1623 the Loss is 0.347.\n",
      "For Iteration 1624 the Loss is 0.347.\n",
      "For Iteration 1625 the Loss is 0.347.\n",
      "For Iteration 1626 the Loss is 0.3469.\n",
      "For Iteration 1627 the Loss is 0.3469.\n",
      "For Iteration 1628 the Loss is 0.3469.\n",
      "For Iteration 1629 the Loss is 0.3469.\n",
      "For Iteration 1630 the Loss is 0.3469.\n",
      "For Iteration 1631 the Loss is 0.3469.\n",
      "For Iteration 1632 the Loss is 0.3468.\n",
      "For Iteration 1633 the Loss is 0.3468.\n",
      "For Iteration 1634 the Loss is 0.3468.\n",
      "For Iteration 1635 the Loss is 0.3468.\n",
      "For Iteration 1636 the Loss is 0.3468.\n",
      "For Iteration 1637 the Loss is 0.3468.\n",
      "For Iteration 1638 the Loss is 0.3468.\n",
      "For Iteration 1639 the Loss is 0.3467.\n",
      "For Iteration 1640 the Loss is 0.3467.\n",
      "For Iteration 1641 the Loss is 0.3467.\n",
      "For Iteration 1642 the Loss is 0.3467.\n",
      "For Iteration 1643 the Loss is 0.3467.\n",
      "For Iteration 1644 the Loss is 0.3467.\n",
      "For Iteration 1645 the Loss is 0.3467.\n",
      "For Iteration 1646 the Loss is 0.3466.\n",
      "For Iteration 1647 the Loss is 0.3466.\n",
      "For Iteration 1648 the Loss is 0.3466.\n",
      "For Iteration 1649 the Loss is 0.3466.\n",
      "For Iteration 1650 the Loss is 0.3466.\n",
      "For Iteration 1651 the Loss is 0.3466.\n",
      "For Iteration 1652 the Loss is 0.3466.\n",
      "For Iteration 1653 the Loss is 0.3465.\n",
      "For Iteration 1654 the Loss is 0.3465.\n",
      "For Iteration 1655 the Loss is 0.3465.\n",
      "For Iteration 1656 the Loss is 0.3465.\n",
      "For Iteration 1657 the Loss is 0.3465.\n",
      "For Iteration 1658 the Loss is 0.3465.\n",
      "For Iteration 1659 the Loss is 0.3465.\n",
      "For Iteration 1660 the Loss is 0.3464.\n",
      "For Iteration 1661 the Loss is 0.3464.\n",
      "For Iteration 1662 the Loss is 0.3464.\n",
      "For Iteration 1663 the Loss is 0.3464.\n",
      "For Iteration 1664 the Loss is 0.3464.\n",
      "For Iteration 1665 the Loss is 0.3464.\n",
      "For Iteration 1666 the Loss is 0.3464.\n",
      "For Iteration 1667 the Loss is 0.3463.\n",
      "For Iteration 1668 the Loss is 0.3463.\n",
      "For Iteration 1669 the Loss is 0.3463.\n",
      "For Iteration 1670 the Loss is 0.3463.\n",
      "For Iteration 1671 the Loss is 0.3463.\n",
      "For Iteration 1672 the Loss is 0.3463.\n",
      "For Iteration 1673 the Loss is 0.3463.\n",
      "For Iteration 1674 the Loss is 0.3462.\n",
      "For Iteration 1675 the Loss is 0.3462.\n",
      "For Iteration 1676 the Loss is 0.3462.\n",
      "For Iteration 1677 the Loss is 0.3462.\n",
      "For Iteration 1678 the Loss is 0.3462.\n",
      "For Iteration 1679 the Loss is 0.3462.\n",
      "For Iteration 1680 the Loss is 0.3462.\n",
      "For Iteration 1681 the Loss is 0.3461.\n",
      "For Iteration 1682 the Loss is 0.3461.\n",
      "For Iteration 1683 the Loss is 0.3461.\n",
      "For Iteration 1684 the Loss is 0.3461.\n",
      "For Iteration 1685 the Loss is 0.3461.\n",
      "For Iteration 1686 the Loss is 0.3461.\n",
      "For Iteration 1687 the Loss is 0.3461.\n",
      "For Iteration 1688 the Loss is 0.346.\n",
      "For Iteration 1689 the Loss is 0.346.\n",
      "For Iteration 1690 the Loss is 0.346.\n",
      "For Iteration 1691 the Loss is 0.346.\n",
      "For Iteration 1692 the Loss is 0.346.\n",
      "For Iteration 1693 the Loss is 0.346.\n",
      "For Iteration 1694 the Loss is 0.346.\n",
      "For Iteration 1695 the Loss is 0.3459.\n",
      "For Iteration 1696 the Loss is 0.3459.\n",
      "For Iteration 1697 the Loss is 0.3459.\n",
      "For Iteration 1698 the Loss is 0.3459.\n",
      "For Iteration 1699 the Loss is 0.3459.\n",
      "For Iteration 1700 the Loss is 0.3459.\n",
      "For Iteration 1701 the Loss is 0.3459.\n",
      "For Iteration 1702 the Loss is 0.3458.\n",
      "For Iteration 1703 the Loss is 0.3458.\n",
      "For Iteration 1704 the Loss is 0.3458.\n",
      "For Iteration 1705 the Loss is 0.3458.\n",
      "For Iteration 1706 the Loss is 0.3458.\n",
      "For Iteration 1707 the Loss is 0.3458.\n",
      "For Iteration 1708 the Loss is 0.3458.\n",
      "For Iteration 1709 the Loss is 0.3457.\n",
      "For Iteration 1710 the Loss is 0.3457.\n",
      "For Iteration 1711 the Loss is 0.3457.\n",
      "For Iteration 1712 the Loss is 0.3457.\n",
      "For Iteration 1713 the Loss is 0.3457.\n",
      "For Iteration 1714 the Loss is 0.3457.\n",
      "For Iteration 1715 the Loss is 0.3457.\n",
      "For Iteration 1716 the Loss is 0.3456.\n",
      "For Iteration 1717 the Loss is 0.3456.\n",
      "For Iteration 1718 the Loss is 0.3456.\n",
      "For Iteration 1719 the Loss is 0.3456.\n",
      "For Iteration 1720 the Loss is 0.3456.\n",
      "For Iteration 1721 the Loss is 0.3456.\n",
      "For Iteration 1722 the Loss is 0.3456.\n",
      "For Iteration 1723 the Loss is 0.3455.\n",
      "For Iteration 1724 the Loss is 0.3455.\n",
      "For Iteration 1725 the Loss is 0.3455.\n",
      "For Iteration 1726 the Loss is 0.3455.\n",
      "For Iteration 1727 the Loss is 0.3455.\n",
      "For Iteration 1728 the Loss is 0.3455.\n",
      "For Iteration 1729 the Loss is 0.3455.\n",
      "For Iteration 1730 the Loss is 0.3454.\n",
      "For Iteration 1731 the Loss is 0.3454.\n",
      "For Iteration 1732 the Loss is 0.3454.\n",
      "For Iteration 1733 the Loss is 0.3454.\n",
      "For Iteration 1734 the Loss is 0.3454.\n",
      "For Iteration 1735 the Loss is 0.3454.\n",
      "For Iteration 1736 the Loss is 0.3454.\n",
      "For Iteration 1737 the Loss is 0.3453.\n",
      "For Iteration 1738 the Loss is 0.3453.\n",
      "For Iteration 1739 the Loss is 0.3453.\n",
      "For Iteration 1740 the Loss is 0.3453.\n",
      "For Iteration 1741 the Loss is 0.3453.\n",
      "For Iteration 1742 the Loss is 0.3453.\n",
      "For Iteration 1743 the Loss is 0.3453.\n",
      "For Iteration 1744 the Loss is 0.3452.\n",
      "For Iteration 1745 the Loss is 0.3452.\n",
      "For Iteration 1746 the Loss is 0.3452.\n",
      "For Iteration 1747 the Loss is 0.3452.\n",
      "For Iteration 1748 the Loss is 0.3452.\n",
      "For Iteration 1749 the Loss is 0.3452.\n",
      "For Iteration 1750 the Loss is 0.3452.\n",
      "For Iteration 1751 the Loss is 0.3452.\n",
      "For Iteration 1752 the Loss is 0.3451.\n",
      "For Iteration 1753 the Loss is 0.3451.\n",
      "For Iteration 1754 the Loss is 0.3451.\n",
      "For Iteration 1755 the Loss is 0.3451.\n",
      "For Iteration 1756 the Loss is 0.3451.\n",
      "For Iteration 1757 the Loss is 0.3451.\n",
      "For Iteration 1758 the Loss is 0.3451.\n",
      "For Iteration 1759 the Loss is 0.345.\n",
      "For Iteration 1760 the Loss is 0.345.\n",
      "For Iteration 1761 the Loss is 0.345.\n",
      "For Iteration 1762 the Loss is 0.345.\n",
      "For Iteration 1763 the Loss is 0.345.\n",
      "For Iteration 1764 the Loss is 0.345.\n",
      "For Iteration 1765 the Loss is 0.345.\n",
      "For Iteration 1766 the Loss is 0.3449.\n",
      "For Iteration 1767 the Loss is 0.3449.\n",
      "For Iteration 1768 the Loss is 0.3449.\n",
      "For Iteration 1769 the Loss is 0.3449.\n",
      "For Iteration 1770 the Loss is 0.3449.\n",
      "For Iteration 1771 the Loss is 0.3449.\n",
      "For Iteration 1772 the Loss is 0.3449.\n",
      "For Iteration 1773 the Loss is 0.3449.\n",
      "For Iteration 1774 the Loss is 0.3448.\n",
      "For Iteration 1775 the Loss is 0.3448.\n",
      "For Iteration 1776 the Loss is 0.3448.\n",
      "For Iteration 1777 the Loss is 0.3448.\n",
      "For Iteration 1778 the Loss is 0.3448.\n",
      "For Iteration 1779 the Loss is 0.3448.\n",
      "For Iteration 1780 the Loss is 0.3448.\n",
      "For Iteration 1781 the Loss is 0.3447.\n",
      "For Iteration 1782 the Loss is 0.3447.\n",
      "For Iteration 1783 the Loss is 0.3447.\n",
      "For Iteration 1784 the Loss is 0.3447.\n",
      "For Iteration 1785 the Loss is 0.3447.\n",
      "For Iteration 1786 the Loss is 0.3447.\n",
      "For Iteration 1787 the Loss is 0.3447.\n",
      "For Iteration 1788 the Loss is 0.3446.\n",
      "For Iteration 1789 the Loss is 0.3446.\n",
      "For Iteration 1790 the Loss is 0.3446.\n",
      "For Iteration 1791 the Loss is 0.3446.\n",
      "For Iteration 1792 the Loss is 0.3446.\n",
      "For Iteration 1793 the Loss is 0.3446.\n",
      "For Iteration 1794 the Loss is 0.3446.\n",
      "For Iteration 1795 the Loss is 0.3446.\n",
      "For Iteration 1796 the Loss is 0.3445.\n",
      "For Iteration 1797 the Loss is 0.3445.\n",
      "For Iteration 1798 the Loss is 0.3445.\n",
      "For Iteration 1799 the Loss is 0.3445.\n",
      "For Iteration 1800 the Loss is 0.3445.\n",
      "For Iteration 1801 the Loss is 0.3445.\n",
      "For Iteration 1802 the Loss is 0.3445.\n",
      "For Iteration 1803 the Loss is 0.3444.\n",
      "For Iteration 1804 the Loss is 0.3444.\n",
      "For Iteration 1805 the Loss is 0.3444.\n",
      "For Iteration 1806 the Loss is 0.3444.\n",
      "For Iteration 1807 the Loss is 0.3444.\n",
      "For Iteration 1808 the Loss is 0.3444.\n",
      "For Iteration 1809 the Loss is 0.3444.\n",
      "For Iteration 1810 the Loss is 0.3443.\n",
      "For Iteration 1811 the Loss is 0.3443.\n",
      "For Iteration 1812 the Loss is 0.3443.\n",
      "For Iteration 1813 the Loss is 0.3443.\n",
      "For Iteration 1814 the Loss is 0.3443.\n",
      "For Iteration 1815 the Loss is 0.3443.\n",
      "For Iteration 1816 the Loss is 0.3443.\n",
      "For Iteration 1817 the Loss is 0.3443.\n",
      "For Iteration 1818 the Loss is 0.3442.\n",
      "For Iteration 1819 the Loss is 0.3442.\n",
      "For Iteration 1820 the Loss is 0.3442.\n",
      "For Iteration 1821 the Loss is 0.3442.\n",
      "For Iteration 1822 the Loss is 0.3442.\n",
      "For Iteration 1823 the Loss is 0.3442.\n",
      "For Iteration 1824 the Loss is 0.3442.\n",
      "For Iteration 1825 the Loss is 0.3441.\n",
      "For Iteration 1826 the Loss is 0.3441.\n",
      "For Iteration 1827 the Loss is 0.3441.\n",
      "For Iteration 1828 the Loss is 0.3441.\n",
      "For Iteration 1829 the Loss is 0.3441.\n",
      "For Iteration 1830 the Loss is 0.3441.\n",
      "For Iteration 1831 the Loss is 0.3441.\n",
      "For Iteration 1832 the Loss is 0.3441.\n",
      "For Iteration 1833 the Loss is 0.344.\n",
      "For Iteration 1834 the Loss is 0.344.\n",
      "For Iteration 1835 the Loss is 0.344.\n",
      "For Iteration 1836 the Loss is 0.344.\n",
      "For Iteration 1837 the Loss is 0.344.\n",
      "For Iteration 1838 the Loss is 0.344.\n",
      "For Iteration 1839 the Loss is 0.344.\n",
      "For Iteration 1840 the Loss is 0.3439.\n",
      "For Iteration 1841 the Loss is 0.3439.\n",
      "For Iteration 1842 the Loss is 0.3439.\n",
      "For Iteration 1843 the Loss is 0.3439.\n",
      "For Iteration 1844 the Loss is 0.3439.\n",
      "For Iteration 1845 the Loss is 0.3439.\n",
      "For Iteration 1846 the Loss is 0.3439.\n",
      "For Iteration 1847 the Loss is 0.3439.\n",
      "For Iteration 1848 the Loss is 0.3438.\n",
      "For Iteration 1849 the Loss is 0.3438.\n",
      "For Iteration 1850 the Loss is 0.3438.\n",
      "For Iteration 1851 the Loss is 0.3438.\n",
      "For Iteration 1852 the Loss is 0.3438.\n",
      "For Iteration 1853 the Loss is 0.3438.\n",
      "For Iteration 1854 the Loss is 0.3438.\n",
      "For Iteration 1855 the Loss is 0.3437.\n",
      "For Iteration 1856 the Loss is 0.3437.\n",
      "For Iteration 1857 the Loss is 0.3437.\n",
      "For Iteration 1858 the Loss is 0.3437.\n",
      "For Iteration 1859 the Loss is 0.3437.\n",
      "For Iteration 1860 the Loss is 0.3437.\n",
      "For Iteration 1861 the Loss is 0.3437.\n",
      "For Iteration 1862 the Loss is 0.3437.\n",
      "For Iteration 1863 the Loss is 0.3436.\n",
      "For Iteration 1864 the Loss is 0.3436.\n",
      "For Iteration 1865 the Loss is 0.3436.\n",
      "For Iteration 1866 the Loss is 0.3436.\n",
      "For Iteration 1867 the Loss is 0.3436.\n",
      "For Iteration 1868 the Loss is 0.3436.\n",
      "For Iteration 1869 the Loss is 0.3436.\n",
      "For Iteration 1870 the Loss is 0.3436.\n",
      "For Iteration 1871 the Loss is 0.3435.\n",
      "For Iteration 1872 the Loss is 0.3435.\n",
      "For Iteration 1873 the Loss is 0.3435.\n",
      "For Iteration 1874 the Loss is 0.3435.\n",
      "For Iteration 1875 the Loss is 0.3435.\n",
      "For Iteration 1876 the Loss is 0.3435.\n",
      "For Iteration 1877 the Loss is 0.3435.\n",
      "For Iteration 1878 the Loss is 0.3434.\n",
      "For Iteration 1879 the Loss is 0.3434.\n",
      "For Iteration 1880 the Loss is 0.3434.\n",
      "For Iteration 1881 the Loss is 0.3434.\n",
      "For Iteration 1882 the Loss is 0.3434.\n",
      "For Iteration 1883 the Loss is 0.3434.\n",
      "For Iteration 1884 the Loss is 0.3434.\n",
      "For Iteration 1885 the Loss is 0.3434.\n",
      "For Iteration 1886 the Loss is 0.3433.\n",
      "For Iteration 1887 the Loss is 0.3433.\n",
      "For Iteration 1888 the Loss is 0.3433.\n",
      "For Iteration 1889 the Loss is 0.3433.\n",
      "For Iteration 1890 the Loss is 0.3433.\n",
      "For Iteration 1891 the Loss is 0.3433.\n",
      "For Iteration 1892 the Loss is 0.3433.\n",
      "For Iteration 1893 the Loss is 0.3432.\n",
      "For Iteration 1894 the Loss is 0.3432.\n",
      "For Iteration 1895 the Loss is 0.3432.\n",
      "For Iteration 1896 the Loss is 0.3432.\n",
      "For Iteration 1897 the Loss is 0.3432.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1898 the Loss is 0.3432.\n",
      "For Iteration 1899 the Loss is 0.3432.\n",
      "For Iteration 1900 the Loss is 0.3432.\n",
      "For Iteration 1901 the Loss is 0.3431.\n",
      "For Iteration 1902 the Loss is 0.3431.\n",
      "For Iteration 1903 the Loss is 0.3431.\n",
      "For Iteration 1904 the Loss is 0.3431.\n",
      "For Iteration 1905 the Loss is 0.3431.\n",
      "For Iteration 1906 the Loss is 0.3431.\n",
      "For Iteration 1907 the Loss is 0.3431.\n",
      "For Iteration 1908 the Loss is 0.3431.\n",
      "For Iteration 1909 the Loss is 0.343.\n",
      "For Iteration 1910 the Loss is 0.343.\n",
      "For Iteration 1911 the Loss is 0.343.\n",
      "For Iteration 1912 the Loss is 0.343.\n",
      "For Iteration 1913 the Loss is 0.343.\n",
      "For Iteration 1914 the Loss is 0.343.\n",
      "For Iteration 1915 the Loss is 0.343.\n",
      "For Iteration 1916 the Loss is 0.343.\n",
      "For Iteration 1917 the Loss is 0.3429.\n",
      "For Iteration 1918 the Loss is 0.3429.\n",
      "For Iteration 1919 the Loss is 0.3429.\n",
      "For Iteration 1920 the Loss is 0.3429.\n",
      "For Iteration 1921 the Loss is 0.3429.\n",
      "For Iteration 1922 the Loss is 0.3429.\n",
      "For Iteration 1923 the Loss is 0.3429.\n",
      "For Iteration 1924 the Loss is 0.3428.\n",
      "For Iteration 1925 the Loss is 0.3428.\n",
      "For Iteration 1926 the Loss is 0.3428.\n",
      "For Iteration 1927 the Loss is 0.3428.\n",
      "For Iteration 1928 the Loss is 0.3428.\n",
      "For Iteration 1929 the Loss is 0.3428.\n",
      "For Iteration 1930 the Loss is 0.3428.\n",
      "For Iteration 1931 the Loss is 0.3428.\n",
      "For Iteration 1932 the Loss is 0.3427.\n",
      "For Iteration 1933 the Loss is 0.3427.\n",
      "For Iteration 1934 the Loss is 0.3427.\n",
      "For Iteration 1935 the Loss is 0.3427.\n",
      "For Iteration 1936 the Loss is 0.3427.\n",
      "For Iteration 1937 the Loss is 0.3427.\n",
      "For Iteration 1938 the Loss is 0.3427.\n",
      "For Iteration 1939 the Loss is 0.3427.\n",
      "For Iteration 1940 the Loss is 0.3426.\n",
      "For Iteration 1941 the Loss is 0.3426.\n",
      "For Iteration 1942 the Loss is 0.3426.\n",
      "For Iteration 1943 the Loss is 0.3426.\n",
      "For Iteration 1944 the Loss is 0.3426.\n",
      "For Iteration 1945 the Loss is 0.3426.\n",
      "For Iteration 1946 the Loss is 0.3426.\n",
      "For Iteration 1947 the Loss is 0.3426.\n",
      "For Iteration 1948 the Loss is 0.3425.\n",
      "For Iteration 1949 the Loss is 0.3425.\n",
      "For Iteration 1950 the Loss is 0.3425.\n",
      "For Iteration 1951 the Loss is 0.3425.\n",
      "For Iteration 1952 the Loss is 0.3425.\n",
      "For Iteration 1953 the Loss is 0.3425.\n",
      "For Iteration 1954 the Loss is 0.3425.\n",
      "For Iteration 1955 the Loss is 0.3425.\n",
      "For Iteration 1956 the Loss is 0.3424.\n",
      "For Iteration 1957 the Loss is 0.3424.\n",
      "For Iteration 1958 the Loss is 0.3424.\n",
      "For Iteration 1959 the Loss is 0.3424.\n",
      "For Iteration 1960 the Loss is 0.3424.\n",
      "For Iteration 1961 the Loss is 0.3424.\n",
      "For Iteration 1962 the Loss is 0.3424.\n",
      "For Iteration 1963 the Loss is 0.3423.\n",
      "For Iteration 1964 the Loss is 0.3423.\n",
      "For Iteration 1965 the Loss is 0.3423.\n",
      "For Iteration 1966 the Loss is 0.3423.\n",
      "For Iteration 1967 the Loss is 0.3423.\n",
      "For Iteration 1968 the Loss is 0.3423.\n",
      "For Iteration 1969 the Loss is 0.3423.\n",
      "For Iteration 1970 the Loss is 0.3423.\n",
      "For Iteration 1971 the Loss is 0.3422.\n",
      "For Iteration 1972 the Loss is 0.3422.\n",
      "For Iteration 1973 the Loss is 0.3422.\n",
      "For Iteration 1974 the Loss is 0.3422.\n",
      "For Iteration 1975 the Loss is 0.3422.\n",
      "For Iteration 1976 the Loss is 0.3422.\n",
      "For Iteration 1977 the Loss is 0.3422.\n",
      "For Iteration 1978 the Loss is 0.3422.\n",
      "For Iteration 1979 the Loss is 0.3421.\n",
      "For Iteration 1980 the Loss is 0.3421.\n",
      "For Iteration 1981 the Loss is 0.3421.\n",
      "For Iteration 1982 the Loss is 0.3421.\n",
      "For Iteration 1983 the Loss is 0.3421.\n",
      "For Iteration 1984 the Loss is 0.3421.\n",
      "For Iteration 1985 the Loss is 0.3421.\n",
      "For Iteration 1986 the Loss is 0.3421.\n",
      "For Iteration 1987 the Loss is 0.342.\n",
      "For Iteration 1988 the Loss is 0.342.\n",
      "For Iteration 1989 the Loss is 0.342.\n",
      "For Iteration 1990 the Loss is 0.342.\n",
      "For Iteration 1991 the Loss is 0.342.\n",
      "For Iteration 1992 the Loss is 0.342.\n",
      "For Iteration 1993 the Loss is 0.342.\n",
      "For Iteration 1994 the Loss is 0.342.\n",
      "For Iteration 1995 the Loss is 0.3419.\n",
      "For Iteration 1996 the Loss is 0.3419.\n",
      "For Iteration 1997 the Loss is 0.3419.\n",
      "For Iteration 1998 the Loss is 0.3419.\n",
      "For Iteration 1999 the Loss is 0.3419.\n",
      "For Iteration 2000 the Loss is 0.3419.\n",
      "For Iteration 2001 the Loss is 0.3419.\n",
      "For Iteration 2002 the Loss is 0.3419.\n",
      "For Iteration 2003 the Loss is 0.3418.\n",
      "For Iteration 2004 the Loss is 0.3418.\n",
      "For Iteration 2005 the Loss is 0.3418.\n",
      "For Iteration 2006 the Loss is 0.3418.\n",
      "For Iteration 2007 the Loss is 0.3418.\n",
      "For Iteration 2008 the Loss is 0.3418.\n",
      "For Iteration 2009 the Loss is 0.3418.\n",
      "For Iteration 2010 the Loss is 0.3418.\n",
      "For Iteration 2011 the Loss is 0.3417.\n",
      "For Iteration 2012 the Loss is 0.3417.\n",
      "For Iteration 2013 the Loss is 0.3417.\n",
      "For Iteration 2014 the Loss is 0.3417.\n",
      "For Iteration 2015 the Loss is 0.3417.\n",
      "For Iteration 2016 the Loss is 0.3417.\n",
      "For Iteration 2017 the Loss is 0.3417.\n",
      "For Iteration 2018 the Loss is 0.3417.\n",
      "For Iteration 2019 the Loss is 0.3416.\n",
      "For Iteration 2020 the Loss is 0.3416.\n",
      "For Iteration 2021 the Loss is 0.3416.\n",
      "For Iteration 2022 the Loss is 0.3416.\n",
      "For Iteration 2023 the Loss is 0.3416.\n",
      "For Iteration 2024 the Loss is 0.3416.\n",
      "For Iteration 2025 the Loss is 0.3416.\n",
      "For Iteration 2026 the Loss is 0.3416.\n",
      "For Iteration 2027 the Loss is 0.3415.\n",
      "For Iteration 2028 the Loss is 0.3415.\n",
      "For Iteration 2029 the Loss is 0.3415.\n",
      "For Iteration 2030 the Loss is 0.3415.\n",
      "For Iteration 2031 the Loss is 0.3415.\n",
      "For Iteration 2032 the Loss is 0.3415.\n",
      "For Iteration 2033 the Loss is 0.3415.\n",
      "For Iteration 2034 the Loss is 0.3415.\n",
      "For Iteration 2035 the Loss is 0.3414.\n",
      "For Iteration 2036 the Loss is 0.3414.\n",
      "For Iteration 2037 the Loss is 0.3414.\n",
      "For Iteration 2038 the Loss is 0.3414.\n",
      "For Iteration 2039 the Loss is 0.3414.\n",
      "For Iteration 2040 the Loss is 0.3414.\n",
      "For Iteration 2041 the Loss is 0.3414.\n",
      "For Iteration 2042 the Loss is 0.3414.\n",
      "For Iteration 2043 the Loss is 0.3413.\n",
      "For Iteration 2044 the Loss is 0.3413.\n",
      "For Iteration 2045 the Loss is 0.3413.\n",
      "For Iteration 2046 the Loss is 0.3413.\n",
      "For Iteration 2047 the Loss is 0.3413.\n",
      "For Iteration 2048 the Loss is 0.3413.\n",
      "For Iteration 2049 the Loss is 0.3413.\n",
      "For Iteration 2050 the Loss is 0.3413.\n",
      "For Iteration 2051 the Loss is 0.3412.\n",
      "For Iteration 2052 the Loss is 0.3412.\n",
      "For Iteration 2053 the Loss is 0.3412.\n",
      "For Iteration 2054 the Loss is 0.3412.\n",
      "For Iteration 2055 the Loss is 0.3412.\n",
      "For Iteration 2056 the Loss is 0.3412.\n",
      "For Iteration 2057 the Loss is 0.3412.\n",
      "For Iteration 2058 the Loss is 0.3412.\n",
      "For Iteration 2059 the Loss is 0.3411.\n",
      "For Iteration 2060 the Loss is 0.3411.\n",
      "For Iteration 2061 the Loss is 0.3411.\n",
      "For Iteration 2062 the Loss is 0.3411.\n",
      "For Iteration 2063 the Loss is 0.3411.\n",
      "For Iteration 2064 the Loss is 0.3411.\n",
      "For Iteration 2065 the Loss is 0.3411.\n",
      "For Iteration 2066 the Loss is 0.3411.\n",
      "For Iteration 2067 the Loss is 0.341.\n",
      "For Iteration 2068 the Loss is 0.341.\n",
      "For Iteration 2069 the Loss is 0.341.\n",
      "For Iteration 2070 the Loss is 0.341.\n",
      "For Iteration 2071 the Loss is 0.341.\n",
      "For Iteration 2072 the Loss is 0.341.\n",
      "For Iteration 2073 the Loss is 0.341.\n",
      "For Iteration 2074 the Loss is 0.341.\n",
      "For Iteration 2075 the Loss is 0.341.\n",
      "For Iteration 2076 the Loss is 0.3409.\n",
      "For Iteration 2077 the Loss is 0.3409.\n",
      "For Iteration 2078 the Loss is 0.3409.\n",
      "For Iteration 2079 the Loss is 0.3409.\n",
      "For Iteration 2080 the Loss is 0.3409.\n",
      "For Iteration 2081 the Loss is 0.3409.\n",
      "For Iteration 2082 the Loss is 0.3409.\n",
      "For Iteration 2083 the Loss is 0.3409.\n",
      "For Iteration 2084 the Loss is 0.3408.\n",
      "For Iteration 2085 the Loss is 0.3408.\n",
      "For Iteration 2086 the Loss is 0.3408.\n",
      "For Iteration 2087 the Loss is 0.3408.\n",
      "For Iteration 2088 the Loss is 0.3408.\n",
      "For Iteration 2089 the Loss is 0.3408.\n",
      "For Iteration 2090 the Loss is 0.3408.\n",
      "For Iteration 2091 the Loss is 0.3408.\n",
      "For Iteration 2092 the Loss is 0.3407.\n",
      "For Iteration 2093 the Loss is 0.3407.\n",
      "For Iteration 2094 the Loss is 0.3407.\n",
      "For Iteration 2095 the Loss is 0.3407.\n",
      "For Iteration 2096 the Loss is 0.3407.\n",
      "For Iteration 2097 the Loss is 0.3407.\n",
      "For Iteration 2098 the Loss is 0.3407.\n",
      "For Iteration 2099 the Loss is 0.3407.\n",
      "For Iteration 2100 the Loss is 0.3406.\n",
      "For Iteration 2101 the Loss is 0.3406.\n",
      "For Iteration 2102 the Loss is 0.3406.\n",
      "For Iteration 2103 the Loss is 0.3406.\n",
      "For Iteration 2104 the Loss is 0.3406.\n",
      "For Iteration 2105 the Loss is 0.3406.\n",
      "For Iteration 2106 the Loss is 0.3406.\n",
      "For Iteration 2107 the Loss is 0.3406.\n",
      "For Iteration 2108 the Loss is 0.3405.\n",
      "For Iteration 2109 the Loss is 0.3405.\n",
      "For Iteration 2110 the Loss is 0.3405.\n",
      "For Iteration 2111 the Loss is 0.3405.\n",
      "For Iteration 2112 the Loss is 0.3405.\n",
      "For Iteration 2113 the Loss is 0.3405.\n",
      "For Iteration 2114 the Loss is 0.3405.\n",
      "For Iteration 2115 the Loss is 0.3405.\n",
      "For Iteration 2116 the Loss is 0.3405.\n",
      "For Iteration 2117 the Loss is 0.3404.\n",
      "For Iteration 2118 the Loss is 0.3404.\n",
      "For Iteration 2119 the Loss is 0.3404.\n",
      "For Iteration 2120 the Loss is 0.3404.\n",
      "For Iteration 2121 the Loss is 0.3404.\n",
      "For Iteration 2122 the Loss is 0.3404.\n",
      "For Iteration 2123 the Loss is 0.3404.\n",
      "For Iteration 2124 the Loss is 0.3404.\n",
      "For Iteration 2125 the Loss is 0.3403.\n",
      "For Iteration 2126 the Loss is 0.3403.\n",
      "For Iteration 2127 the Loss is 0.3403.\n",
      "For Iteration 2128 the Loss is 0.3403.\n",
      "For Iteration 2129 the Loss is 0.3403.\n",
      "For Iteration 2130 the Loss is 0.3403.\n",
      "For Iteration 2131 the Loss is 0.3403.\n",
      "For Iteration 2132 the Loss is 0.3403.\n",
      "For Iteration 2133 the Loss is 0.3402.\n",
      "For Iteration 2134 the Loss is 0.3402.\n",
      "For Iteration 2135 the Loss is 0.3402.\n",
      "For Iteration 2136 the Loss is 0.3402.\n",
      "For Iteration 2137 the Loss is 0.3402.\n",
      "For Iteration 2138 the Loss is 0.3402.\n",
      "For Iteration 2139 the Loss is 0.3402.\n",
      "For Iteration 2140 the Loss is 0.3402.\n",
      "For Iteration 2141 the Loss is 0.3401.\n",
      "For Iteration 2142 the Loss is 0.3401.\n",
      "For Iteration 2143 the Loss is 0.3401.\n",
      "For Iteration 2144 the Loss is 0.3401.\n",
      "For Iteration 2145 the Loss is 0.3401.\n",
      "For Iteration 2146 the Loss is 0.3401.\n",
      "For Iteration 2147 the Loss is 0.3401.\n",
      "For Iteration 2148 the Loss is 0.3401.\n",
      "For Iteration 2149 the Loss is 0.3401.\n",
      "For Iteration 2150 the Loss is 0.34.\n",
      "For Iteration 2151 the Loss is 0.34.\n",
      "For Iteration 2152 the Loss is 0.34.\n",
      "For Iteration 2153 the Loss is 0.34.\n",
      "For Iteration 2154 the Loss is 0.34.\n",
      "For Iteration 2155 the Loss is 0.34.\n",
      "For Iteration 2156 the Loss is 0.34.\n",
      "For Iteration 2157 the Loss is 0.34.\n",
      "For Iteration 2158 the Loss is 0.3399.\n",
      "For Iteration 2159 the Loss is 0.3399.\n",
      "For Iteration 2160 the Loss is 0.3399.\n",
      "For Iteration 2161 the Loss is 0.3399.\n",
      "For Iteration 2162 the Loss is 0.3399.\n",
      "For Iteration 2163 the Loss is 0.3399.\n",
      "For Iteration 2164 the Loss is 0.3399.\n",
      "For Iteration 2165 the Loss is 0.3399.\n",
      "For Iteration 2166 the Loss is 0.3399.\n",
      "For Iteration 2167 the Loss is 0.3398.\n",
      "For Iteration 2168 the Loss is 0.3398.\n",
      "For Iteration 2169 the Loss is 0.3398.\n",
      "For Iteration 2170 the Loss is 0.3398.\n",
      "For Iteration 2171 the Loss is 0.3398.\n",
      "For Iteration 2172 the Loss is 0.3398.\n",
      "For Iteration 2173 the Loss is 0.3398.\n",
      "For Iteration 2174 the Loss is 0.3398.\n",
      "For Iteration 2175 the Loss is 0.3397.\n",
      "For Iteration 2176 the Loss is 0.3397.\n",
      "For Iteration 2177 the Loss is 0.3397.\n",
      "For Iteration 2178 the Loss is 0.3397.\n",
      "For Iteration 2179 the Loss is 0.3397.\n",
      "For Iteration 2180 the Loss is 0.3397.\n",
      "For Iteration 2181 the Loss is 0.3397.\n",
      "For Iteration 2182 the Loss is 0.3397.\n",
      "For Iteration 2183 the Loss is 0.3396.\n",
      "For Iteration 2184 the Loss is 0.3396.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 2185 the Loss is 0.3396.\n",
      "For Iteration 2186 the Loss is 0.3396.\n",
      "For Iteration 2187 the Loss is 0.3396.\n",
      "For Iteration 2188 the Loss is 0.3396.\n",
      "For Iteration 2189 the Loss is 0.3396.\n",
      "For Iteration 2190 the Loss is 0.3396.\n",
      "For Iteration 2191 the Loss is 0.3396.\n",
      "For Iteration 2192 the Loss is 0.3395.\n",
      "For Iteration 2193 the Loss is 0.3395.\n",
      "For Iteration 2194 the Loss is 0.3395.\n",
      "For Iteration 2195 the Loss is 0.3395.\n",
      "For Iteration 2196 the Loss is 0.3395.\n",
      "For Iteration 2197 the Loss is 0.3395.\n",
      "For Iteration 2198 the Loss is 0.3395.\n",
      "For Iteration 2199 the Loss is 0.3395.\n",
      "For Iteration 2200 the Loss is 0.3394.\n",
      "For Iteration 2201 the Loss is 0.3394.\n",
      "For Iteration 2202 the Loss is 0.3394.\n",
      "For Iteration 2203 the Loss is 0.3394.\n",
      "For Iteration 2204 the Loss is 0.3394.\n",
      "For Iteration 2205 the Loss is 0.3394.\n",
      "For Iteration 2206 the Loss is 0.3394.\n",
      "For Iteration 2207 the Loss is 0.3394.\n",
      "For Iteration 2208 the Loss is 0.3394.\n",
      "For Iteration 2209 the Loss is 0.3393.\n",
      "For Iteration 2210 the Loss is 0.3393.\n",
      "For Iteration 2211 the Loss is 0.3393.\n",
      "For Iteration 2212 the Loss is 0.3393.\n",
      "For Iteration 2213 the Loss is 0.3393.\n",
      "For Iteration 2214 the Loss is 0.3393.\n",
      "For Iteration 2215 the Loss is 0.3393.\n",
      "For Iteration 2216 the Loss is 0.3393.\n",
      "For Iteration 2217 the Loss is 0.3392.\n",
      "For Iteration 2218 the Loss is 0.3392.\n",
      "For Iteration 2219 the Loss is 0.3392.\n",
      "For Iteration 2220 the Loss is 0.3392.\n",
      "For Iteration 2221 the Loss is 0.3392.\n",
      "For Iteration 2222 the Loss is 0.3392.\n",
      "For Iteration 2223 the Loss is 0.3392.\n",
      "For Iteration 2224 the Loss is 0.3392.\n",
      "For Iteration 2225 the Loss is 0.3392.\n",
      "For Iteration 2226 the Loss is 0.3391.\n",
      "For Iteration 2227 the Loss is 0.3391.\n",
      "For Iteration 2228 the Loss is 0.3391.\n",
      "For Iteration 2229 the Loss is 0.3391.\n",
      "For Iteration 2230 the Loss is 0.3391.\n",
      "For Iteration 2231 the Loss is 0.3391.\n",
      "For Iteration 2232 the Loss is 0.3391.\n",
      "For Iteration 2233 the Loss is 0.3391.\n",
      "For Iteration 2234 the Loss is 0.339.\n",
      "For Iteration 2235 the Loss is 0.339.\n",
      "For Iteration 2236 the Loss is 0.339.\n",
      "For Iteration 2237 the Loss is 0.339.\n",
      "For Iteration 2238 the Loss is 0.339.\n",
      "For Iteration 2239 the Loss is 0.339.\n",
      "For Iteration 2240 the Loss is 0.339.\n",
      "For Iteration 2241 the Loss is 0.339.\n",
      "For Iteration 2242 the Loss is 0.339.\n",
      "For Iteration 2243 the Loss is 0.3389.\n",
      "For Iteration 2244 the Loss is 0.3389.\n",
      "For Iteration 2245 the Loss is 0.3389.\n",
      "For Iteration 2246 the Loss is 0.3389.\n",
      "For Iteration 2247 the Loss is 0.3389.\n",
      "For Iteration 2248 the Loss is 0.3389.\n",
      "For Iteration 2249 the Loss is 0.3389.\n",
      "For Iteration 2250 the Loss is 0.3389.\n",
      "For Iteration 2251 the Loss is 0.3388.\n",
      "For Iteration 2252 the Loss is 0.3388.\n",
      "For Iteration 2253 the Loss is 0.3388.\n",
      "For Iteration 2254 the Loss is 0.3388.\n",
      "For Iteration 2255 the Loss is 0.3388.\n",
      "For Iteration 2256 the Loss is 0.3388.\n",
      "For Iteration 2257 the Loss is 0.3388.\n",
      "For Iteration 2258 the Loss is 0.3388.\n",
      "For Iteration 2259 the Loss is 0.3388.\n",
      "For Iteration 2260 the Loss is 0.3387.\n",
      "For Iteration 2261 the Loss is 0.3387.\n",
      "For Iteration 2262 the Loss is 0.3387.\n",
      "For Iteration 2263 the Loss is 0.3387.\n",
      "For Iteration 2264 the Loss is 0.3387.\n",
      "For Iteration 2265 the Loss is 0.3387.\n",
      "For Iteration 2266 the Loss is 0.3387.\n",
      "For Iteration 2267 the Loss is 0.3387.\n",
      "For Iteration 2268 the Loss is 0.3386.\n",
      "For Iteration 2269 the Loss is 0.3386.\n",
      "For Iteration 2270 the Loss is 0.3386.\n",
      "For Iteration 2271 the Loss is 0.3386.\n",
      "For Iteration 2272 the Loss is 0.3386.\n",
      "For Iteration 2273 the Loss is 0.3386.\n",
      "For Iteration 2274 the Loss is 0.3386.\n",
      "For Iteration 2275 the Loss is 0.3386.\n",
      "For Iteration 2276 the Loss is 0.3386.\n",
      "For Iteration 2277 the Loss is 0.3385.\n",
      "For Iteration 2278 the Loss is 0.3385.\n",
      "For Iteration 2279 the Loss is 0.3385.\n",
      "For Iteration 2280 the Loss is 0.3385.\n",
      "For Iteration 2281 the Loss is 0.3385.\n",
      "For Iteration 2282 the Loss is 0.3385.\n",
      "For Iteration 2283 the Loss is 0.3385.\n",
      "For Iteration 2284 the Loss is 0.3385.\n",
      "For Iteration 2285 the Loss is 0.3385.\n",
      "For Iteration 2286 the Loss is 0.3384.\n",
      "For Iteration 2287 the Loss is 0.3384.\n",
      "For Iteration 2288 the Loss is 0.3384.\n",
      "For Iteration 2289 the Loss is 0.3384.\n",
      "For Iteration 2290 the Loss is 0.3384.\n",
      "For Iteration 2291 the Loss is 0.3384.\n",
      "For Iteration 2292 the Loss is 0.3384.\n",
      "For Iteration 2293 the Loss is 0.3384.\n",
      "For Iteration 2294 the Loss is 0.3383.\n",
      "For Iteration 2295 the Loss is 0.3383.\n",
      "For Iteration 2296 the Loss is 0.3383.\n",
      "For Iteration 2297 the Loss is 0.3383.\n",
      "For Iteration 2298 the Loss is 0.3383.\n",
      "For Iteration 2299 the Loss is 0.3383.\n",
      "For Iteration 2300 the Loss is 0.3383.\n",
      "For Iteration 2301 the Loss is 0.3383.\n",
      "For Iteration 2302 the Loss is 0.3383.\n",
      "For Iteration 2303 the Loss is 0.3382.\n",
      "For Iteration 2304 the Loss is 0.3382.\n",
      "For Iteration 2305 the Loss is 0.3382.\n",
      "For Iteration 2306 the Loss is 0.3382.\n",
      "For Iteration 2307 the Loss is 0.3382.\n",
      "For Iteration 2308 the Loss is 0.3382.\n",
      "For Iteration 2309 the Loss is 0.3382.\n",
      "For Iteration 2310 the Loss is 0.3382.\n",
      "For Iteration 2311 the Loss is 0.3382.\n",
      "For Iteration 2312 the Loss is 0.3381.\n",
      "For Iteration 2313 the Loss is 0.3381.\n",
      "For Iteration 2314 the Loss is 0.3381.\n",
      "For Iteration 2315 the Loss is 0.3381.\n",
      "For Iteration 2316 the Loss is 0.3381.\n",
      "For Iteration 2317 the Loss is 0.3381.\n",
      "For Iteration 2318 the Loss is 0.3381.\n",
      "For Iteration 2319 the Loss is 0.3381.\n",
      "For Iteration 2320 the Loss is 0.338.\n",
      "For Iteration 2321 the Loss is 0.338.\n",
      "For Iteration 2322 the Loss is 0.338.\n",
      "For Iteration 2323 the Loss is 0.338.\n",
      "For Iteration 2324 the Loss is 0.338.\n",
      "For Iteration 2325 the Loss is 0.338.\n",
      "For Iteration 2326 the Loss is 0.338.\n",
      "For Iteration 2327 the Loss is 0.338.\n",
      "For Iteration 2328 the Loss is 0.338.\n",
      "For Iteration 2329 the Loss is 0.3379.\n",
      "For Iteration 2330 the Loss is 0.3379.\n",
      "For Iteration 2331 the Loss is 0.3379.\n",
      "For Iteration 2332 the Loss is 0.3379.\n",
      "For Iteration 2333 the Loss is 0.3379.\n",
      "For Iteration 2334 the Loss is 0.3379.\n",
      "For Iteration 2335 the Loss is 0.3379.\n",
      "For Iteration 2336 the Loss is 0.3379.\n",
      "For Iteration 2337 the Loss is 0.3379.\n",
      "For Iteration 2338 the Loss is 0.3378.\n",
      "For Iteration 2339 the Loss is 0.3378.\n",
      "For Iteration 2340 the Loss is 0.3378.\n",
      "For Iteration 2341 the Loss is 0.3378.\n",
      "For Iteration 2342 the Loss is 0.3378.\n",
      "For Iteration 2343 the Loss is 0.3378.\n",
      "For Iteration 2344 the Loss is 0.3378.\n",
      "For Iteration 2345 the Loss is 0.3378.\n",
      "For Iteration 2346 the Loss is 0.3378.\n",
      "For Iteration 2347 the Loss is 0.3377.\n",
      "For Iteration 2348 the Loss is 0.3377.\n",
      "For Iteration 2349 the Loss is 0.3377.\n",
      "For Iteration 2350 the Loss is 0.3377.\n",
      "For Iteration 2351 the Loss is 0.3377.\n",
      "For Iteration 2352 the Loss is 0.3377.\n",
      "For Iteration 2353 the Loss is 0.3377.\n",
      "For Iteration 2354 the Loss is 0.3377.\n",
      "For Iteration 2355 the Loss is 0.3377.\n",
      "For Iteration 2356 the Loss is 0.3376.\n",
      "For Iteration 2357 the Loss is 0.3376.\n",
      "For Iteration 2358 the Loss is 0.3376.\n",
      "For Iteration 2359 the Loss is 0.3376.\n",
      "For Iteration 2360 the Loss is 0.3376.\n",
      "For Iteration 2361 the Loss is 0.3376.\n",
      "For Iteration 2362 the Loss is 0.3376.\n",
      "For Iteration 2363 the Loss is 0.3376.\n",
      "For Iteration 2364 the Loss is 0.3375.\n",
      "For Iteration 2365 the Loss is 0.3375.\n",
      "For Iteration 2366 the Loss is 0.3375.\n",
      "For Iteration 2367 the Loss is 0.3375.\n",
      "For Iteration 2368 the Loss is 0.3375.\n",
      "For Iteration 2369 the Loss is 0.3375.\n",
      "For Iteration 2370 the Loss is 0.3375.\n",
      "For Iteration 2371 the Loss is 0.3375.\n",
      "For Iteration 2372 the Loss is 0.3375.\n",
      "For Iteration 2373 the Loss is 0.3374.\n",
      "For Iteration 2374 the Loss is 0.3374.\n",
      "For Iteration 2375 the Loss is 0.3374.\n",
      "For Iteration 2376 the Loss is 0.3374.\n",
      "For Iteration 2377 the Loss is 0.3374.\n",
      "For Iteration 2378 the Loss is 0.3374.\n",
      "For Iteration 2379 the Loss is 0.3374.\n",
      "For Iteration 2380 the Loss is 0.3374.\n",
      "For Iteration 2381 the Loss is 0.3374.\n",
      "For Iteration 2382 the Loss is 0.3373.\n",
      "For Iteration 2383 the Loss is 0.3373.\n",
      "For Iteration 2384 the Loss is 0.3373.\n",
      "For Iteration 2385 the Loss is 0.3373.\n",
      "For Iteration 2386 the Loss is 0.3373.\n",
      "For Iteration 2387 the Loss is 0.3373.\n",
      "For Iteration 2388 the Loss is 0.3373.\n",
      "For Iteration 2389 the Loss is 0.3373.\n",
      "For Iteration 2390 the Loss is 0.3373.\n",
      "For Iteration 2391 the Loss is 0.3372.\n",
      "For Iteration 2392 the Loss is 0.3372.\n",
      "For Iteration 2393 the Loss is 0.3372.\n",
      "For Iteration 2394 the Loss is 0.3372.\n",
      "For Iteration 2395 the Loss is 0.3372.\n",
      "For Iteration 2396 the Loss is 0.3372.\n",
      "For Iteration 2397 the Loss is 0.3372.\n",
      "For Iteration 2398 the Loss is 0.3372.\n",
      "For Iteration 2399 the Loss is 0.3372.\n",
      "For Iteration 2400 the Loss is 0.3371.\n",
      "For Iteration 2401 the Loss is 0.3371.\n",
      "For Iteration 2402 the Loss is 0.3371.\n",
      "For Iteration 2403 the Loss is 0.3371.\n",
      "For Iteration 2404 the Loss is 0.3371.\n",
      "For Iteration 2405 the Loss is 0.3371.\n",
      "For Iteration 2406 the Loss is 0.3371.\n",
      "For Iteration 2407 the Loss is 0.3371.\n",
      "For Iteration 2408 the Loss is 0.3371.\n",
      "For Iteration 2409 the Loss is 0.337.\n",
      "For Iteration 2410 the Loss is 0.337.\n",
      "For Iteration 2411 the Loss is 0.337.\n",
      "For Iteration 2412 the Loss is 0.337.\n",
      "For Iteration 2413 the Loss is 0.337.\n",
      "For Iteration 2414 the Loss is 0.337.\n",
      "For Iteration 2415 the Loss is 0.337.\n",
      "For Iteration 2416 the Loss is 0.337.\n",
      "For Iteration 2417 the Loss is 0.337.\n",
      "For Iteration 2418 the Loss is 0.3369.\n",
      "For Iteration 2419 the Loss is 0.3369.\n",
      "For Iteration 2420 the Loss is 0.3369.\n",
      "For Iteration 2421 the Loss is 0.3369.\n",
      "For Iteration 2422 the Loss is 0.3369.\n",
      "For Iteration 2423 the Loss is 0.3369.\n",
      "For Iteration 2424 the Loss is 0.3369.\n",
      "For Iteration 2425 the Loss is 0.3369.\n",
      "For Iteration 2426 the Loss is 0.3369.\n",
      "For Iteration 2427 the Loss is 0.3368.\n",
      "For Iteration 2428 the Loss is 0.3368.\n",
      "For Iteration 2429 the Loss is 0.3368.\n",
      "For Iteration 2430 the Loss is 0.3368.\n",
      "For Iteration 2431 the Loss is 0.3368.\n",
      "For Iteration 2432 the Loss is 0.3368.\n",
      "For Iteration 2433 the Loss is 0.3368.\n",
      "For Iteration 2434 the Loss is 0.3368.\n",
      "For Iteration 2435 the Loss is 0.3368.\n",
      "For Iteration 2436 the Loss is 0.3367.\n",
      "For Iteration 2437 the Loss is 0.3367.\n",
      "For Iteration 2438 the Loss is 0.3367.\n",
      "For Iteration 2439 the Loss is 0.3367.\n",
      "For Iteration 2440 the Loss is 0.3367.\n",
      "For Iteration 2441 the Loss is 0.3367.\n",
      "For Iteration 2442 the Loss is 0.3367.\n",
      "For Iteration 2443 the Loss is 0.3367.\n",
      "For Iteration 2444 the Loss is 0.3367.\n",
      "For Iteration 2445 the Loss is 0.3366.\n",
      "For Iteration 2446 the Loss is 0.3366.\n",
      "For Iteration 2447 the Loss is 0.3366.\n",
      "For Iteration 2448 the Loss is 0.3366.\n",
      "For Iteration 2449 the Loss is 0.3366.\n",
      "For Iteration 2450 the Loss is 0.3366.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 2451 the Loss is 0.3366.\n",
      "For Iteration 2452 the Loss is 0.3366.\n",
      "For Iteration 2453 the Loss is 0.3366.\n",
      "For Iteration 2454 the Loss is 0.3365.\n",
      "For Iteration 2455 the Loss is 0.3365.\n",
      "For Iteration 2456 the Loss is 0.3365.\n",
      "For Iteration 2457 the Loss is 0.3365.\n",
      "For Iteration 2458 the Loss is 0.3365.\n",
      "For Iteration 2459 the Loss is 0.3365.\n",
      "For Iteration 2460 the Loss is 0.3365.\n",
      "For Iteration 2461 the Loss is 0.3365.\n",
      "For Iteration 2462 the Loss is 0.3365.\n",
      "For Iteration 2463 the Loss is 0.3364.\n",
      "For Iteration 2464 the Loss is 0.3364.\n",
      "For Iteration 2465 the Loss is 0.3364.\n",
      "For Iteration 2466 the Loss is 0.3364.\n",
      "For Iteration 2467 the Loss is 0.3364.\n",
      "For Iteration 2468 the Loss is 0.3364.\n",
      "For Iteration 2469 the Loss is 0.3364.\n",
      "For Iteration 2470 the Loss is 0.3364.\n",
      "For Iteration 2471 the Loss is 0.3364.\n",
      "For Iteration 2472 the Loss is 0.3363.\n",
      "For Iteration 2473 the Loss is 0.3363.\n",
      "For Iteration 2474 the Loss is 0.3363.\n",
      "For Iteration 2475 the Loss is 0.3363.\n",
      "For Iteration 2476 the Loss is 0.3363.\n",
      "For Iteration 2477 the Loss is 0.3363.\n",
      "For Iteration 2478 the Loss is 0.3363.\n",
      "For Iteration 2479 the Loss is 0.3363.\n",
      "For Iteration 2480 the Loss is 0.3363.\n",
      "For Iteration 2481 the Loss is 0.3362.\n",
      "For Iteration 2482 the Loss is 0.3362.\n",
      "For Iteration 2483 the Loss is 0.3362.\n",
      "For Iteration 2484 the Loss is 0.3362.\n",
      "For Iteration 2485 the Loss is 0.3362.\n",
      "For Iteration 2486 the Loss is 0.3362.\n",
      "For Iteration 2487 the Loss is 0.3362.\n",
      "For Iteration 2488 the Loss is 0.3362.\n",
      "For Iteration 2489 the Loss is 0.3362.\n",
      "For Iteration 2490 the Loss is 0.3361.\n",
      "For Iteration 2491 the Loss is 0.3361.\n",
      "For Iteration 2492 the Loss is 0.3361.\n",
      "For Iteration 2493 the Loss is 0.3361.\n",
      "For Iteration 2494 the Loss is 0.3361.\n",
      "For Iteration 2495 the Loss is 0.3361.\n",
      "For Iteration 2496 the Loss is 0.3361.\n",
      "For Iteration 2497 the Loss is 0.3361.\n",
      "For Iteration 2498 the Loss is 0.3361.\n",
      "For Iteration 2499 the Loss is 0.336.\n",
      "For Iteration 2500 the Loss is 0.336.\n",
      "For Iteration 2501 the Loss is 0.336.\n",
      "For Iteration 2502 the Loss is 0.336.\n",
      "For Iteration 2503 the Loss is 0.336.\n",
      "For Iteration 2504 the Loss is 0.336.\n",
      "For Iteration 2505 the Loss is 0.336.\n",
      "For Iteration 2506 the Loss is 0.336.\n",
      "For Iteration 2507 the Loss is 0.336.\n",
      "For Iteration 2508 the Loss is 0.3359.\n",
      "For Iteration 2509 the Loss is 0.3359.\n",
      "For Iteration 2510 the Loss is 0.3359.\n",
      "For Iteration 2511 the Loss is 0.3359.\n",
      "For Iteration 2512 the Loss is 0.3359.\n",
      "For Iteration 2513 the Loss is 0.3359.\n",
      "For Iteration 2514 the Loss is 0.3359.\n",
      "For Iteration 2515 the Loss is 0.3359.\n",
      "For Iteration 2516 the Loss is 0.3359.\n",
      "For Iteration 2517 the Loss is 0.3359.\n",
      "For Iteration 2518 the Loss is 0.3358.\n",
      "For Iteration 2519 the Loss is 0.3358.\n",
      "For Iteration 2520 the Loss is 0.3358.\n",
      "For Iteration 2521 the Loss is 0.3358.\n",
      "For Iteration 2522 the Loss is 0.3358.\n",
      "For Iteration 2523 the Loss is 0.3358.\n",
      "For Iteration 2524 the Loss is 0.3358.\n",
      "For Iteration 2525 the Loss is 0.3358.\n",
      "For Iteration 2526 the Loss is 0.3358.\n",
      "For Iteration 2527 the Loss is 0.3357.\n",
      "For Iteration 2528 the Loss is 0.3357.\n",
      "For Iteration 2529 the Loss is 0.3357.\n",
      "For Iteration 2530 the Loss is 0.3357.\n",
      "For Iteration 2531 the Loss is 0.3357.\n",
      "For Iteration 2532 the Loss is 0.3357.\n",
      "For Iteration 2533 the Loss is 0.3357.\n",
      "For Iteration 2534 the Loss is 0.3357.\n",
      "For Iteration 2535 the Loss is 0.3357.\n",
      "For Iteration 2536 the Loss is 0.3356.\n",
      "For Iteration 2537 the Loss is 0.3356.\n",
      "For Iteration 2538 the Loss is 0.3356.\n",
      "For Iteration 2539 the Loss is 0.3356.\n",
      "For Iteration 2540 the Loss is 0.3356.\n",
      "For Iteration 2541 the Loss is 0.3356.\n",
      "For Iteration 2542 the Loss is 0.3356.\n",
      "For Iteration 2543 the Loss is 0.3356.\n",
      "For Iteration 2544 the Loss is 0.3356.\n",
      "For Iteration 2545 the Loss is 0.3355.\n",
      "For Iteration 2546 the Loss is 0.3355.\n",
      "For Iteration 2547 the Loss is 0.3355.\n",
      "For Iteration 2548 the Loss is 0.3355.\n",
      "For Iteration 2549 the Loss is 0.3355.\n",
      "For Iteration 2550 the Loss is 0.3355.\n",
      "For Iteration 2551 the Loss is 0.3355.\n",
      "For Iteration 2552 the Loss is 0.3355.\n",
      "For Iteration 2553 the Loss is 0.3355.\n",
      "For Iteration 2554 the Loss is 0.3354.\n",
      "For Iteration 2555 the Loss is 0.3354.\n",
      "For Iteration 2556 the Loss is 0.3354.\n",
      "For Iteration 2557 the Loss is 0.3354.\n",
      "For Iteration 2558 the Loss is 0.3354.\n",
      "For Iteration 2559 the Loss is 0.3354.\n",
      "For Iteration 2560 the Loss is 0.3354.\n",
      "For Iteration 2561 the Loss is 0.3354.\n",
      "For Iteration 2562 the Loss is 0.3354.\n",
      "For Iteration 2563 the Loss is 0.3354.\n",
      "For Iteration 2564 the Loss is 0.3353.\n",
      "For Iteration 2565 the Loss is 0.3353.\n",
      "For Iteration 2566 the Loss is 0.3353.\n",
      "For Iteration 2567 the Loss is 0.3353.\n",
      "For Iteration 2568 the Loss is 0.3353.\n",
      "For Iteration 2569 the Loss is 0.3353.\n",
      "For Iteration 2570 the Loss is 0.3353.\n",
      "For Iteration 2571 the Loss is 0.3353.\n",
      "For Iteration 2572 the Loss is 0.3353.\n",
      "For Iteration 2573 the Loss is 0.3352.\n",
      "For Iteration 2574 the Loss is 0.3352.\n",
      "For Iteration 2575 the Loss is 0.3352.\n",
      "For Iteration 2576 the Loss is 0.3352.\n",
      "For Iteration 2577 the Loss is 0.3352.\n",
      "For Iteration 2578 the Loss is 0.3352.\n",
      "For Iteration 2579 the Loss is 0.3352.\n",
      "For Iteration 2580 the Loss is 0.3352.\n",
      "For Iteration 2581 the Loss is 0.3352.\n",
      "For Iteration 2582 the Loss is 0.3351.\n",
      "For Iteration 2583 the Loss is 0.3351.\n",
      "For Iteration 2584 the Loss is 0.3351.\n",
      "For Iteration 2585 the Loss is 0.3351.\n",
      "For Iteration 2586 the Loss is 0.3351.\n",
      "For Iteration 2587 the Loss is 0.3351.\n",
      "For Iteration 2588 the Loss is 0.3351.\n",
      "For Iteration 2589 the Loss is 0.3351.\n",
      "For Iteration 2590 the Loss is 0.3351.\n",
      "For Iteration 2591 the Loss is 0.3351.\n",
      "For Iteration 2592 the Loss is 0.335.\n",
      "For Iteration 2593 the Loss is 0.335.\n",
      "For Iteration 2594 the Loss is 0.335.\n",
      "For Iteration 2595 the Loss is 0.335.\n",
      "For Iteration 2596 the Loss is 0.335.\n",
      "For Iteration 2597 the Loss is 0.335.\n",
      "For Iteration 2598 the Loss is 0.335.\n",
      "For Iteration 2599 the Loss is 0.335.\n",
      "For Iteration 2600 the Loss is 0.335.\n",
      "For Iteration 2601 the Loss is 0.3349.\n",
      "For Iteration 2602 the Loss is 0.3349.\n",
      "For Iteration 2603 the Loss is 0.3349.\n",
      "For Iteration 2604 the Loss is 0.3349.\n",
      "For Iteration 2605 the Loss is 0.3349.\n",
      "For Iteration 2606 the Loss is 0.3349.\n",
      "For Iteration 2607 the Loss is 0.3349.\n",
      "For Iteration 2608 the Loss is 0.3349.\n",
      "For Iteration 2609 the Loss is 0.3349.\n",
      "For Iteration 2610 the Loss is 0.3348.\n",
      "For Iteration 2611 the Loss is 0.3348.\n",
      "For Iteration 2612 the Loss is 0.3348.\n",
      "For Iteration 2613 the Loss is 0.3348.\n",
      "For Iteration 2614 the Loss is 0.3348.\n",
      "For Iteration 2615 the Loss is 0.3348.\n",
      "For Iteration 2616 the Loss is 0.3348.\n",
      "For Iteration 2617 the Loss is 0.3348.\n",
      "For Iteration 2618 the Loss is 0.3348.\n",
      "For Iteration 2619 the Loss is 0.3348.\n",
      "For Iteration 2620 the Loss is 0.3347.\n",
      "For Iteration 2621 the Loss is 0.3347.\n",
      "For Iteration 2622 the Loss is 0.3347.\n",
      "For Iteration 2623 the Loss is 0.3347.\n",
      "For Iteration 2624 the Loss is 0.3347.\n",
      "For Iteration 2625 the Loss is 0.3347.\n",
      "For Iteration 2626 the Loss is 0.3347.\n",
      "For Iteration 2627 the Loss is 0.3347.\n",
      "For Iteration 2628 the Loss is 0.3347.\n",
      "For Iteration 2629 the Loss is 0.3346.\n",
      "For Iteration 2630 the Loss is 0.3346.\n",
      "For Iteration 2631 the Loss is 0.3346.\n",
      "For Iteration 2632 the Loss is 0.3346.\n",
      "For Iteration 2633 the Loss is 0.3346.\n",
      "For Iteration 2634 the Loss is 0.3346.\n",
      "For Iteration 2635 the Loss is 0.3346.\n",
      "For Iteration 2636 the Loss is 0.3346.\n",
      "For Iteration 2637 the Loss is 0.3346.\n",
      "For Iteration 2638 the Loss is 0.3346.\n",
      "For Iteration 2639 the Loss is 0.3345.\n",
      "For Iteration 2640 the Loss is 0.3345.\n",
      "For Iteration 2641 the Loss is 0.3345.\n",
      "For Iteration 2642 the Loss is 0.3345.\n",
      "For Iteration 2643 the Loss is 0.3345.\n",
      "For Iteration 2644 the Loss is 0.3345.\n",
      "For Iteration 2645 the Loss is 0.3345.\n",
      "For Iteration 2646 the Loss is 0.3345.\n",
      "For Iteration 2647 the Loss is 0.3345.\n",
      "For Iteration 2648 the Loss is 0.3344.\n",
      "For Iteration 2649 the Loss is 0.3344.\n",
      "For Iteration 2650 the Loss is 0.3344.\n",
      "For Iteration 2651 the Loss is 0.3344.\n",
      "For Iteration 2652 the Loss is 0.3344.\n",
      "For Iteration 2653 the Loss is 0.3344.\n",
      "For Iteration 2654 the Loss is 0.3344.\n",
      "For Iteration 2655 the Loss is 0.3344.\n",
      "For Iteration 2656 the Loss is 0.3344.\n",
      "For Iteration 2657 the Loss is 0.3344.\n",
      "For Iteration 2658 the Loss is 0.3343.\n",
      "For Iteration 2659 the Loss is 0.3343.\n",
      "For Iteration 2660 the Loss is 0.3343.\n",
      "For Iteration 2661 the Loss is 0.3343.\n",
      "For Iteration 2662 the Loss is 0.3343.\n",
      "For Iteration 2663 the Loss is 0.3343.\n",
      "For Iteration 2664 the Loss is 0.3343.\n",
      "For Iteration 2665 the Loss is 0.3343.\n",
      "For Iteration 2666 the Loss is 0.3343.\n",
      "For Iteration 2667 the Loss is 0.3342.\n",
      "For Iteration 2668 the Loss is 0.3342.\n",
      "For Iteration 2669 the Loss is 0.3342.\n",
      "For Iteration 2670 the Loss is 0.3342.\n",
      "For Iteration 2671 the Loss is 0.3342.\n",
      "For Iteration 2672 the Loss is 0.3342.\n",
      "For Iteration 2673 the Loss is 0.3342.\n",
      "For Iteration 2674 the Loss is 0.3342.\n",
      "For Iteration 2675 the Loss is 0.3342.\n",
      "For Iteration 2676 the Loss is 0.3342.\n",
      "For Iteration 2677 the Loss is 0.3341.\n",
      "For Iteration 2678 the Loss is 0.3341.\n",
      "For Iteration 2679 the Loss is 0.3341.\n",
      "For Iteration 2680 the Loss is 0.3341.\n",
      "For Iteration 2681 the Loss is 0.3341.\n",
      "For Iteration 2682 the Loss is 0.3341.\n",
      "For Iteration 2683 the Loss is 0.3341.\n",
      "For Iteration 2684 the Loss is 0.3341.\n",
      "For Iteration 2685 the Loss is 0.3341.\n",
      "For Iteration 2686 the Loss is 0.334.\n",
      "For Iteration 2687 the Loss is 0.334.\n",
      "For Iteration 2688 the Loss is 0.334.\n",
      "For Iteration 2689 the Loss is 0.334.\n",
      "For Iteration 2690 the Loss is 0.334.\n",
      "For Iteration 2691 the Loss is 0.334.\n",
      "For Iteration 2692 the Loss is 0.334.\n",
      "For Iteration 2693 the Loss is 0.334.\n",
      "For Iteration 2694 the Loss is 0.334.\n",
      "For Iteration 2695 the Loss is 0.334.\n",
      "For Iteration 2696 the Loss is 0.3339.\n",
      "For Iteration 2697 the Loss is 0.3339.\n",
      "For Iteration 2698 the Loss is 0.3339.\n",
      "For Iteration 2699 the Loss is 0.3339.\n",
      "For Iteration 2700 the Loss is 0.3339.\n",
      "For Iteration 2701 the Loss is 0.3339.\n",
      "For Iteration 2702 the Loss is 0.3339.\n",
      "For Iteration 2703 the Loss is 0.3339.\n",
      "For Iteration 2704 the Loss is 0.3339.\n",
      "For Iteration 2705 the Loss is 0.3339.\n",
      "For Iteration 2706 the Loss is 0.3338.\n",
      "For Iteration 2707 the Loss is 0.3338.\n",
      "For Iteration 2708 the Loss is 0.3338.\n",
      "For Iteration 2709 the Loss is 0.3338.\n",
      "For Iteration 2710 the Loss is 0.3338.\n",
      "For Iteration 2711 the Loss is 0.3338.\n",
      "For Iteration 2712 the Loss is 0.3338.\n",
      "For Iteration 2713 the Loss is 0.3338.\n",
      "For Iteration 2714 the Loss is 0.3338.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 2715 the Loss is 0.3337.\n",
      "For Iteration 2716 the Loss is 0.3337.\n",
      "For Iteration 2717 the Loss is 0.3337.\n",
      "For Iteration 2718 the Loss is 0.3337.\n",
      "For Iteration 2719 the Loss is 0.3337.\n",
      "For Iteration 2720 the Loss is 0.3337.\n",
      "For Iteration 2721 the Loss is 0.3337.\n",
      "For Iteration 2722 the Loss is 0.3337.\n",
      "For Iteration 2723 the Loss is 0.3337.\n",
      "For Iteration 2724 the Loss is 0.3337.\n",
      "For Iteration 2725 the Loss is 0.3336.\n",
      "For Iteration 2726 the Loss is 0.3336.\n",
      "For Iteration 2727 the Loss is 0.3336.\n",
      "For Iteration 2728 the Loss is 0.3336.\n",
      "For Iteration 2729 the Loss is 0.3336.\n",
      "For Iteration 2730 the Loss is 0.3336.\n",
      "For Iteration 2731 the Loss is 0.3336.\n",
      "For Iteration 2732 the Loss is 0.3336.\n",
      "For Iteration 2733 the Loss is 0.3336.\n",
      "For Iteration 2734 the Loss is 0.3335.\n",
      "For Iteration 2735 the Loss is 0.3335.\n",
      "For Iteration 2736 the Loss is 0.3335.\n",
      "For Iteration 2737 the Loss is 0.3335.\n",
      "For Iteration 2738 the Loss is 0.3335.\n",
      "For Iteration 2739 the Loss is 0.3335.\n",
      "For Iteration 2740 the Loss is 0.3335.\n",
      "For Iteration 2741 the Loss is 0.3335.\n",
      "For Iteration 2742 the Loss is 0.3335.\n",
      "For Iteration 2743 the Loss is 0.3335.\n",
      "For Iteration 2744 the Loss is 0.3334.\n",
      "For Iteration 2745 the Loss is 0.3334.\n",
      "For Iteration 2746 the Loss is 0.3334.\n",
      "For Iteration 2747 the Loss is 0.3334.\n",
      "For Iteration 2748 the Loss is 0.3334.\n",
      "For Iteration 2749 the Loss is 0.3334.\n",
      "For Iteration 2750 the Loss is 0.3334.\n",
      "For Iteration 2751 the Loss is 0.3334.\n",
      "For Iteration 2752 the Loss is 0.3334.\n",
      "For Iteration 2753 the Loss is 0.3334.\n",
      "For Iteration 2754 the Loss is 0.3333.\n",
      "For Iteration 2755 the Loss is 0.3333.\n",
      "For Iteration 2756 the Loss is 0.3333.\n",
      "For Iteration 2757 the Loss is 0.3333.\n",
      "For Iteration 2758 the Loss is 0.3333.\n",
      "For Iteration 2759 the Loss is 0.3333.\n",
      "For Iteration 2760 the Loss is 0.3333.\n",
      "For Iteration 2761 the Loss is 0.3333.\n",
      "For Iteration 2762 the Loss is 0.3333.\n",
      "For Iteration 2763 the Loss is 0.3333.\n",
      "For Iteration 2764 the Loss is 0.3332.\n",
      "For Iteration 2765 the Loss is 0.3332.\n",
      "For Iteration 2766 the Loss is 0.3332.\n",
      "For Iteration 2767 the Loss is 0.3332.\n",
      "For Iteration 2768 the Loss is 0.3332.\n",
      "For Iteration 2769 the Loss is 0.3332.\n",
      "For Iteration 2770 the Loss is 0.3332.\n",
      "For Iteration 2771 the Loss is 0.3332.\n",
      "For Iteration 2772 the Loss is 0.3332.\n",
      "For Iteration 2773 the Loss is 0.3331.\n",
      "For Iteration 2774 the Loss is 0.3331.\n",
      "For Iteration 2775 the Loss is 0.3331.\n",
      "For Iteration 2776 the Loss is 0.3331.\n",
      "For Iteration 2777 the Loss is 0.3331.\n",
      "For Iteration 2778 the Loss is 0.3331.\n",
      "For Iteration 2779 the Loss is 0.3331.\n",
      "For Iteration 2780 the Loss is 0.3331.\n",
      "For Iteration 2781 the Loss is 0.3331.\n",
      "For Iteration 2782 the Loss is 0.3331.\n",
      "For Iteration 2783 the Loss is 0.333.\n",
      "For Iteration 2784 the Loss is 0.333.\n",
      "For Iteration 2785 the Loss is 0.333.\n",
      "For Iteration 2786 the Loss is 0.333.\n",
      "For Iteration 2787 the Loss is 0.333.\n",
      "For Iteration 2788 the Loss is 0.333.\n",
      "For Iteration 2789 the Loss is 0.333.\n",
      "For Iteration 2790 the Loss is 0.333.\n",
      "For Iteration 2791 the Loss is 0.333.\n",
      "For Iteration 2792 the Loss is 0.333.\n",
      "For Iteration 2793 the Loss is 0.3329.\n",
      "For Iteration 2794 the Loss is 0.3329.\n",
      "For Iteration 2795 the Loss is 0.3329.\n",
      "For Iteration 2796 the Loss is 0.3329.\n",
      "For Iteration 2797 the Loss is 0.3329.\n",
      "For Iteration 2798 the Loss is 0.3329.\n",
      "For Iteration 2799 the Loss is 0.3329.\n",
      "For Iteration 2800 the Loss is 0.3329.\n",
      "For Iteration 2801 the Loss is 0.3329.\n",
      "For Iteration 2802 the Loss is 0.3329.\n",
      "For Iteration 2803 the Loss is 0.3328.\n",
      "For Iteration 2804 the Loss is 0.3328.\n",
      "For Iteration 2805 the Loss is 0.3328.\n",
      "For Iteration 2806 the Loss is 0.3328.\n",
      "For Iteration 2807 the Loss is 0.3328.\n",
      "For Iteration 2808 the Loss is 0.3328.\n",
      "For Iteration 2809 the Loss is 0.3328.\n",
      "For Iteration 2810 the Loss is 0.3328.\n",
      "For Iteration 2811 the Loss is 0.3328.\n",
      "For Iteration 2812 the Loss is 0.3328.\n",
      "For Iteration 2813 the Loss is 0.3327.\n",
      "For Iteration 2814 the Loss is 0.3327.\n",
      "For Iteration 2815 the Loss is 0.3327.\n",
      "For Iteration 2816 the Loss is 0.3327.\n",
      "For Iteration 2817 the Loss is 0.3327.\n",
      "For Iteration 2818 the Loss is 0.3327.\n",
      "For Iteration 2819 the Loss is 0.3327.\n",
      "For Iteration 2820 the Loss is 0.3327.\n",
      "For Iteration 2821 the Loss is 0.3327.\n",
      "For Iteration 2822 the Loss is 0.3327.\n",
      "For Iteration 2823 the Loss is 0.3326.\n",
      "For Iteration 2824 the Loss is 0.3326.\n",
      "For Iteration 2825 the Loss is 0.3326.\n",
      "For Iteration 2826 the Loss is 0.3326.\n",
      "For Iteration 2827 the Loss is 0.3326.\n",
      "For Iteration 2828 the Loss is 0.3326.\n",
      "For Iteration 2829 the Loss is 0.3326.\n",
      "For Iteration 2830 the Loss is 0.3326.\n",
      "For Iteration 2831 the Loss is 0.3326.\n",
      "For Iteration 2832 the Loss is 0.3325.\n",
      "For Iteration 2833 the Loss is 0.3325.\n",
      "For Iteration 2834 the Loss is 0.3325.\n",
      "For Iteration 2835 the Loss is 0.3325.\n",
      "For Iteration 2836 the Loss is 0.3325.\n",
      "For Iteration 2837 the Loss is 0.3325.\n",
      "For Iteration 2838 the Loss is 0.3325.\n",
      "For Iteration 2839 the Loss is 0.3325.\n",
      "For Iteration 2840 the Loss is 0.3325.\n",
      "For Iteration 2841 the Loss is 0.3325.\n",
      "For Iteration 2842 the Loss is 0.3324.\n",
      "For Iteration 2843 the Loss is 0.3324.\n",
      "For Iteration 2844 the Loss is 0.3324.\n",
      "For Iteration 2845 the Loss is 0.3324.\n",
      "For Iteration 2846 the Loss is 0.3324.\n",
      "For Iteration 2847 the Loss is 0.3324.\n",
      "For Iteration 2848 the Loss is 0.3324.\n",
      "For Iteration 2849 the Loss is 0.3324.\n",
      "For Iteration 2850 the Loss is 0.3324.\n",
      "For Iteration 2851 the Loss is 0.3324.\n",
      "For Iteration 2852 the Loss is 0.3323.\n",
      "For Iteration 2853 the Loss is 0.3323.\n",
      "For Iteration 2854 the Loss is 0.3323.\n",
      "For Iteration 2855 the Loss is 0.3323.\n",
      "For Iteration 2856 the Loss is 0.3323.\n",
      "For Iteration 2857 the Loss is 0.3323.\n",
      "For Iteration 2858 the Loss is 0.3323.\n",
      "For Iteration 2859 the Loss is 0.3323.\n",
      "For Iteration 2860 the Loss is 0.3323.\n",
      "For Iteration 2861 the Loss is 0.3323.\n",
      "For Iteration 2862 the Loss is 0.3322.\n",
      "For Iteration 2863 the Loss is 0.3322.\n",
      "For Iteration 2864 the Loss is 0.3322.\n",
      "For Iteration 2865 the Loss is 0.3322.\n",
      "For Iteration 2866 the Loss is 0.3322.\n",
      "For Iteration 2867 the Loss is 0.3322.\n",
      "For Iteration 2868 the Loss is 0.3322.\n",
      "For Iteration 2869 the Loss is 0.3322.\n",
      "For Iteration 2870 the Loss is 0.3322.\n",
      "For Iteration 2871 the Loss is 0.3322.\n",
      "For Iteration 2872 the Loss is 0.3321.\n",
      "For Iteration 2873 the Loss is 0.3321.\n",
      "For Iteration 2874 the Loss is 0.3321.\n",
      "For Iteration 2875 the Loss is 0.3321.\n",
      "For Iteration 2876 the Loss is 0.3321.\n",
      "For Iteration 2877 the Loss is 0.3321.\n",
      "For Iteration 2878 the Loss is 0.3321.\n",
      "For Iteration 2879 the Loss is 0.3321.\n",
      "For Iteration 2880 the Loss is 0.3321.\n",
      "For Iteration 2881 the Loss is 0.3321.\n",
      "For Iteration 2882 the Loss is 0.332.\n",
      "For Iteration 2883 the Loss is 0.332.\n",
      "For Iteration 2884 the Loss is 0.332.\n",
      "For Iteration 2885 the Loss is 0.332.\n",
      "For Iteration 2886 the Loss is 0.332.\n",
      "For Iteration 2887 the Loss is 0.332.\n",
      "For Iteration 2888 the Loss is 0.332.\n",
      "For Iteration 2889 the Loss is 0.332.\n",
      "For Iteration 2890 the Loss is 0.332.\n",
      "For Iteration 2891 the Loss is 0.332.\n",
      "For Iteration 2892 the Loss is 0.3319.\n",
      "For Iteration 2893 the Loss is 0.3319.\n",
      "For Iteration 2894 the Loss is 0.3319.\n",
      "For Iteration 2895 the Loss is 0.3319.\n",
      "For Iteration 2896 the Loss is 0.3319.\n",
      "For Iteration 2897 the Loss is 0.3319.\n",
      "For Iteration 2898 the Loss is 0.3319.\n",
      "For Iteration 2899 the Loss is 0.3319.\n",
      "For Iteration 2900 the Loss is 0.3319.\n",
      "For Iteration 2901 the Loss is 0.3319.\n",
      "For Iteration 2902 the Loss is 0.3318.\n",
      "For Iteration 2903 the Loss is 0.3318.\n",
      "For Iteration 2904 the Loss is 0.3318.\n",
      "For Iteration 2905 the Loss is 0.3318.\n",
      "For Iteration 2906 the Loss is 0.3318.\n",
      "For Iteration 2907 the Loss is 0.3318.\n",
      "For Iteration 2908 the Loss is 0.3318.\n",
      "For Iteration 2909 the Loss is 0.3318.\n",
      "For Iteration 2910 the Loss is 0.3318.\n",
      "For Iteration 2911 the Loss is 0.3318.\n",
      "For Iteration 2912 the Loss is 0.3318.\n",
      "For Iteration 2913 the Loss is 0.3317.\n",
      "For Iteration 2914 the Loss is 0.3317.\n",
      "For Iteration 2915 the Loss is 0.3317.\n",
      "For Iteration 2916 the Loss is 0.3317.\n",
      "For Iteration 2917 the Loss is 0.3317.\n",
      "For Iteration 2918 the Loss is 0.3317.\n",
      "For Iteration 2919 the Loss is 0.3317.\n",
      "For Iteration 2920 the Loss is 0.3317.\n",
      "For Iteration 2921 the Loss is 0.3317.\n",
      "For Iteration 2922 the Loss is 0.3317.\n",
      "For Iteration 2923 the Loss is 0.3316.\n",
      "For Iteration 2924 the Loss is 0.3316.\n",
      "For Iteration 2925 the Loss is 0.3316.\n",
      "For Iteration 2926 the Loss is 0.3316.\n",
      "For Iteration 2927 the Loss is 0.3316.\n",
      "For Iteration 2928 the Loss is 0.3316.\n",
      "For Iteration 2929 the Loss is 0.3316.\n",
      "For Iteration 2930 the Loss is 0.3316.\n",
      "For Iteration 2931 the Loss is 0.3316.\n",
      "For Iteration 2932 the Loss is 0.3316.\n",
      "For Iteration 2933 the Loss is 0.3315.\n",
      "For Iteration 2934 the Loss is 0.3315.\n",
      "For Iteration 2935 the Loss is 0.3315.\n",
      "For Iteration 2936 the Loss is 0.3315.\n",
      "For Iteration 2937 the Loss is 0.3315.\n",
      "For Iteration 2938 the Loss is 0.3315.\n",
      "For Iteration 2939 the Loss is 0.3315.\n",
      "For Iteration 2940 the Loss is 0.3315.\n",
      "For Iteration 2941 the Loss is 0.3315.\n",
      "For Iteration 2942 the Loss is 0.3315.\n",
      "For Iteration 2943 the Loss is 0.3314.\n",
      "For Iteration 2944 the Loss is 0.3314.\n",
      "For Iteration 2945 the Loss is 0.3314.\n",
      "For Iteration 2946 the Loss is 0.3314.\n",
      "For Iteration 2947 the Loss is 0.3314.\n",
      "For Iteration 2948 the Loss is 0.3314.\n",
      "For Iteration 2949 the Loss is 0.3314.\n",
      "For Iteration 2950 the Loss is 0.3314.\n",
      "For Iteration 2951 the Loss is 0.3314.\n",
      "For Iteration 2952 the Loss is 0.3314.\n",
      "For Iteration 2953 the Loss is 0.3313.\n",
      "For Iteration 2954 the Loss is 0.3313.\n",
      "For Iteration 2955 the Loss is 0.3313.\n",
      "For Iteration 2956 the Loss is 0.3313.\n",
      "For Iteration 2957 the Loss is 0.3313.\n",
      "For Iteration 2958 the Loss is 0.3313.\n",
      "For Iteration 2959 the Loss is 0.3313.\n",
      "For Iteration 2960 the Loss is 0.3313.\n",
      "For Iteration 2961 the Loss is 0.3313.\n",
      "For Iteration 2962 the Loss is 0.3313.\n",
      "For Iteration 2963 the Loss is 0.3312.\n",
      "For Iteration 2964 the Loss is 0.3312.\n",
      "For Iteration 2965 the Loss is 0.3312.\n",
      "For Iteration 2966 the Loss is 0.3312.\n",
      "For Iteration 2967 the Loss is 0.3312.\n",
      "For Iteration 2968 the Loss is 0.3312.\n",
      "For Iteration 2969 the Loss is 0.3312.\n",
      "For Iteration 2970 the Loss is 0.3312.\n",
      "For Iteration 2971 the Loss is 0.3312.\n",
      "For Iteration 2972 the Loss is 0.3312.\n",
      "For Iteration 2973 the Loss is 0.3312.\n",
      "For Iteration 2974 the Loss is 0.3311.\n",
      "For Iteration 2975 the Loss is 0.3311.\n",
      "For Iteration 2976 the Loss is 0.3311.\n",
      "For Iteration 2977 the Loss is 0.3311.\n",
      "For Iteration 2978 the Loss is 0.3311.\n",
      "For Iteration 2979 the Loss is 0.3311.\n",
      "For Iteration 2980 the Loss is 0.3311.\n",
      "For Iteration 2981 the Loss is 0.3311.\n",
      "For Iteration 2982 the Loss is 0.3311.\n",
      "For Iteration 2983 the Loss is 0.3311.\n",
      "For Iteration 2984 the Loss is 0.331.\n",
      "For Iteration 2985 the Loss is 0.331.\n",
      "For Iteration 2986 the Loss is 0.331.\n",
      "For Iteration 2987 the Loss is 0.331.\n",
      "For Iteration 2988 the Loss is 0.331.\n",
      "For Iteration 2989 the Loss is 0.331.\n",
      "For Iteration 2990 the Loss is 0.331.\n",
      "For Iteration 2991 the Loss is 0.331.\n",
      "For Iteration 2992 the Loss is 0.331.\n",
      "For Iteration 2993 the Loss is 0.331.\n",
      "For Iteration 2994 the Loss is 0.3309.\n",
      "For Iteration 2995 the Loss is 0.3309.\n",
      "For Iteration 2996 the Loss is 0.3309.\n",
      "For Iteration 2997 the Loss is 0.3309.\n",
      "For Iteration 2998 the Loss is 0.3309.\n",
      "For Iteration 2999 the Loss is 0.3309.\n",
      "For Iteration 3000 the Loss is 0.3309.\n",
      "For Iteration 3001 the Loss is 0.3309.\n",
      "For Iteration 3002 the Loss is 0.3309.\n",
      "For Iteration 3003 the Loss is 0.3309.\n",
      "For Iteration 3004 the Loss is 0.3309.\n",
      "For Iteration 3005 the Loss is 0.3308.\n",
      "For Iteration 3006 the Loss is 0.3308.\n",
      "For Iteration 3007 the Loss is 0.3308.\n",
      "For Iteration 3008 the Loss is 0.3308.\n",
      "For Iteration 3009 the Loss is 0.3308.\n",
      "For Iteration 3010 the Loss is 0.3308.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 3011 the Loss is 0.3308.\n",
      "For Iteration 3012 the Loss is 0.3308.\n",
      "For Iteration 3013 the Loss is 0.3308.\n",
      "For Iteration 3014 the Loss is 0.3308.\n",
      "For Iteration 3015 the Loss is 0.3307.\n",
      "For Iteration 3016 the Loss is 0.3307.\n",
      "For Iteration 3017 the Loss is 0.3307.\n",
      "For Iteration 3018 the Loss is 0.3307.\n",
      "For Iteration 3019 the Loss is 0.3307.\n",
      "For Iteration 3020 the Loss is 0.3307.\n",
      "For Iteration 3021 the Loss is 0.3307.\n",
      "For Iteration 3022 the Loss is 0.3307.\n",
      "For Iteration 3023 the Loss is 0.3307.\n",
      "For Iteration 3024 the Loss is 0.3307.\n",
      "For Iteration 3025 the Loss is 0.3306.\n",
      "For Iteration 3026 the Loss is 0.3306.\n",
      "For Iteration 3027 the Loss is 0.3306.\n",
      "For Iteration 3028 the Loss is 0.3306.\n",
      "For Iteration 3029 the Loss is 0.3306.\n",
      "For Iteration 3030 the Loss is 0.3306.\n",
      "For Iteration 3031 the Loss is 0.3306.\n",
      "For Iteration 3032 the Loss is 0.3306.\n",
      "For Iteration 3033 the Loss is 0.3306.\n",
      "For Iteration 3034 the Loss is 0.3306.\n",
      "For Iteration 3035 the Loss is 0.3306.\n",
      "For Iteration 3036 the Loss is 0.3305.\n",
      "For Iteration 3037 the Loss is 0.3305.\n",
      "For Iteration 3038 the Loss is 0.3305.\n",
      "For Iteration 3039 the Loss is 0.3305.\n",
      "For Iteration 3040 the Loss is 0.3305.\n",
      "For Iteration 3041 the Loss is 0.3305.\n",
      "For Iteration 3042 the Loss is 0.3305.\n",
      "For Iteration 3043 the Loss is 0.3305.\n",
      "For Iteration 3044 the Loss is 0.3305.\n",
      "For Iteration 3045 the Loss is 0.3305.\n",
      "For Iteration 3046 the Loss is 0.3304.\n",
      "For Iteration 3047 the Loss is 0.3304.\n",
      "For Iteration 3048 the Loss is 0.3304.\n",
      "For Iteration 3049 the Loss is 0.3304.\n",
      "For Iteration 3050 the Loss is 0.3304.\n",
      "For Iteration 3051 the Loss is 0.3304.\n",
      "For Iteration 3052 the Loss is 0.3304.\n",
      "For Iteration 3053 the Loss is 0.3304.\n",
      "For Iteration 3054 the Loss is 0.3304.\n",
      "For Iteration 3055 the Loss is 0.3304.\n",
      "For Iteration 3056 the Loss is 0.3304.\n",
      "For Iteration 3057 the Loss is 0.3303.\n",
      "For Iteration 3058 the Loss is 0.3303.\n",
      "For Iteration 3059 the Loss is 0.3303.\n",
      "For Iteration 3060 the Loss is 0.3303.\n",
      "For Iteration 3061 the Loss is 0.3303.\n",
      "For Iteration 3062 the Loss is 0.3303.\n",
      "For Iteration 3063 the Loss is 0.3303.\n",
      "For Iteration 3064 the Loss is 0.3303.\n",
      "For Iteration 3065 the Loss is 0.3303.\n",
      "For Iteration 3066 the Loss is 0.3303.\n",
      "For Iteration 3067 the Loss is 0.3302.\n",
      "For Iteration 3068 the Loss is 0.3302.\n",
      "For Iteration 3069 the Loss is 0.3302.\n",
      "For Iteration 3070 the Loss is 0.3302.\n",
      "For Iteration 3071 the Loss is 0.3302.\n",
      "For Iteration 3072 the Loss is 0.3302.\n",
      "For Iteration 3073 the Loss is 0.3302.\n",
      "For Iteration 3074 the Loss is 0.3302.\n",
      "For Iteration 3075 the Loss is 0.3302.\n",
      "For Iteration 3076 the Loss is 0.3302.\n",
      "For Iteration 3077 the Loss is 0.3302.\n",
      "For Iteration 3078 the Loss is 0.3301.\n",
      "For Iteration 3079 the Loss is 0.3301.\n",
      "For Iteration 3080 the Loss is 0.3301.\n",
      "For Iteration 3081 the Loss is 0.3301.\n",
      "For Iteration 3082 the Loss is 0.3301.\n",
      "For Iteration 3083 the Loss is 0.3301.\n",
      "For Iteration 3084 the Loss is 0.3301.\n",
      "For Iteration 3085 the Loss is 0.3301.\n",
      "For Iteration 3086 the Loss is 0.3301.\n",
      "For Iteration 3087 the Loss is 0.3301.\n",
      "For Iteration 3088 the Loss is 0.33.\n",
      "For Iteration 3089 the Loss is 0.33.\n",
      "For Iteration 3090 the Loss is 0.33.\n",
      "For Iteration 3091 the Loss is 0.33.\n",
      "For Iteration 3092 the Loss is 0.33.\n",
      "For Iteration 3093 the Loss is 0.33.\n",
      "For Iteration 3094 the Loss is 0.33.\n",
      "For Iteration 3095 the Loss is 0.33.\n",
      "For Iteration 3096 the Loss is 0.33.\n",
      "For Iteration 3097 the Loss is 0.33.\n",
      "For Iteration 3098 the Loss is 0.33.\n",
      "For Iteration 3099 the Loss is 0.3299.\n",
      "For Iteration 3100 the Loss is 0.3299.\n",
      "For Iteration 3101 the Loss is 0.3299.\n",
      "For Iteration 3102 the Loss is 0.3299.\n",
      "For Iteration 3103 the Loss is 0.3299.\n",
      "For Iteration 3104 the Loss is 0.3299.\n",
      "For Iteration 3105 the Loss is 0.3299.\n",
      "For Iteration 3106 the Loss is 0.3299.\n",
      "For Iteration 3107 the Loss is 0.3299.\n",
      "For Iteration 3108 the Loss is 0.3299.\n",
      "For Iteration 3109 the Loss is 0.3298.\n",
      "For Iteration 3110 the Loss is 0.3298.\n",
      "For Iteration 3111 the Loss is 0.3298.\n",
      "For Iteration 3112 the Loss is 0.3298.\n",
      "For Iteration 3113 the Loss is 0.3298.\n",
      "For Iteration 3114 the Loss is 0.3298.\n",
      "For Iteration 3115 the Loss is 0.3298.\n",
      "For Iteration 3116 the Loss is 0.3298.\n",
      "For Iteration 3117 the Loss is 0.3298.\n",
      "For Iteration 3118 the Loss is 0.3298.\n",
      "For Iteration 3119 the Loss is 0.3298.\n",
      "For Iteration 3120 the Loss is 0.3297.\n",
      "For Iteration 3121 the Loss is 0.3297.\n",
      "For Iteration 3122 the Loss is 0.3297.\n",
      "For Iteration 3123 the Loss is 0.3297.\n",
      "For Iteration 3124 the Loss is 0.3297.\n",
      "For Iteration 3125 the Loss is 0.3297.\n",
      "For Iteration 3126 the Loss is 0.3297.\n",
      "For Iteration 3127 the Loss is 0.3297.\n",
      "For Iteration 3128 the Loss is 0.3297.\n",
      "For Iteration 3129 the Loss is 0.3297.\n",
      "For Iteration 3130 the Loss is 0.3296.\n",
      "For Iteration 3131 the Loss is 0.3296.\n",
      "For Iteration 3132 the Loss is 0.3296.\n",
      "For Iteration 3133 the Loss is 0.3296.\n",
      "For Iteration 3134 the Loss is 0.3296.\n",
      "For Iteration 3135 the Loss is 0.3296.\n",
      "For Iteration 3136 the Loss is 0.3296.\n",
      "For Iteration 3137 the Loss is 0.3296.\n",
      "For Iteration 3138 the Loss is 0.3296.\n",
      "For Iteration 3139 the Loss is 0.3296.\n",
      "For Iteration 3140 the Loss is 0.3296.\n",
      "For Iteration 3141 the Loss is 0.3295.\n",
      "For Iteration 3142 the Loss is 0.3295.\n",
      "For Iteration 3143 the Loss is 0.3295.\n",
      "For Iteration 3144 the Loss is 0.3295.\n",
      "For Iteration 3145 the Loss is 0.3295.\n",
      "For Iteration 3146 the Loss is 0.3295.\n",
      "For Iteration 3147 the Loss is 0.3295.\n",
      "For Iteration 3148 the Loss is 0.3295.\n",
      "For Iteration 3149 the Loss is 0.3295.\n",
      "For Iteration 3150 the Loss is 0.3295.\n",
      "For Iteration 3151 the Loss is 0.3295.\n",
      "For Iteration 3152 the Loss is 0.3294.\n",
      "For Iteration 3153 the Loss is 0.3294.\n",
      "For Iteration 3154 the Loss is 0.3294.\n",
      "For Iteration 3155 the Loss is 0.3294.\n",
      "For Iteration 3156 the Loss is 0.3294.\n",
      "For Iteration 3157 the Loss is 0.3294.\n",
      "For Iteration 3158 the Loss is 0.3294.\n",
      "For Iteration 3159 the Loss is 0.3294.\n",
      "For Iteration 3160 the Loss is 0.3294.\n",
      "For Iteration 3161 the Loss is 0.3294.\n",
      "For Iteration 3162 the Loss is 0.3293.\n",
      "For Iteration 3163 the Loss is 0.3293.\n",
      "For Iteration 3164 the Loss is 0.3293.\n",
      "For Iteration 3165 the Loss is 0.3293.\n",
      "For Iteration 3166 the Loss is 0.3293.\n",
      "For Iteration 3167 the Loss is 0.3293.\n",
      "For Iteration 3168 the Loss is 0.3293.\n",
      "For Iteration 3169 the Loss is 0.3293.\n",
      "For Iteration 3170 the Loss is 0.3293.\n",
      "For Iteration 3171 the Loss is 0.3293.\n",
      "For Iteration 3172 the Loss is 0.3293.\n",
      "For Iteration 3173 the Loss is 0.3292.\n",
      "For Iteration 3174 the Loss is 0.3292.\n",
      "For Iteration 3175 the Loss is 0.3292.\n",
      "For Iteration 3176 the Loss is 0.3292.\n",
      "For Iteration 3177 the Loss is 0.3292.\n",
      "For Iteration 3178 the Loss is 0.3292.\n",
      "For Iteration 3179 the Loss is 0.3292.\n",
      "For Iteration 3180 the Loss is 0.3292.\n",
      "For Iteration 3181 the Loss is 0.3292.\n",
      "For Iteration 3182 the Loss is 0.3292.\n",
      "For Iteration 3183 the Loss is 0.3292.\n",
      "For Iteration 3184 the Loss is 0.3291.\n",
      "For Iteration 3185 the Loss is 0.3291.\n",
      "For Iteration 3186 the Loss is 0.3291.\n",
      "For Iteration 3187 the Loss is 0.3291.\n",
      "For Iteration 3188 the Loss is 0.3291.\n",
      "For Iteration 3189 the Loss is 0.3291.\n",
      "For Iteration 3190 the Loss is 0.3291.\n",
      "For Iteration 3191 the Loss is 0.3291.\n",
      "For Iteration 3192 the Loss is 0.3291.\n",
      "For Iteration 3193 the Loss is 0.3291.\n",
      "For Iteration 3194 the Loss is 0.3291.\n",
      "For Iteration 3195 the Loss is 0.329.\n",
      "For Iteration 3196 the Loss is 0.329.\n",
      "For Iteration 3197 the Loss is 0.329.\n",
      "For Iteration 3198 the Loss is 0.329.\n",
      "For Iteration 3199 the Loss is 0.329.\n",
      "For Iteration 3200 the Loss is 0.329.\n",
      "For Iteration 3201 the Loss is 0.329.\n",
      "For Iteration 3202 the Loss is 0.329.\n",
      "For Iteration 3203 the Loss is 0.329.\n",
      "For Iteration 3204 the Loss is 0.329.\n",
      "For Iteration 3205 the Loss is 0.329.\n",
      "For Iteration 3206 the Loss is 0.3289.\n",
      "For Iteration 3207 the Loss is 0.3289.\n",
      "For Iteration 3208 the Loss is 0.3289.\n",
      "For Iteration 3209 the Loss is 0.3289.\n",
      "For Iteration 3210 the Loss is 0.3289.\n",
      "For Iteration 3211 the Loss is 0.3289.\n",
      "For Iteration 3212 the Loss is 0.3289.\n",
      "For Iteration 3213 the Loss is 0.3289.\n",
      "For Iteration 3214 the Loss is 0.3289.\n",
      "For Iteration 3215 the Loss is 0.3289.\n",
      "For Iteration 3216 the Loss is 0.3288.\n",
      "For Iteration 3217 the Loss is 0.3288.\n",
      "For Iteration 3218 the Loss is 0.3288.\n",
      "For Iteration 3219 the Loss is 0.3288.\n",
      "For Iteration 3220 the Loss is 0.3288.\n",
      "For Iteration 3221 the Loss is 0.3288.\n",
      "For Iteration 3222 the Loss is 0.3288.\n",
      "For Iteration 3223 the Loss is 0.3288.\n",
      "For Iteration 3224 the Loss is 0.3288.\n",
      "For Iteration 3225 the Loss is 0.3288.\n",
      "For Iteration 3226 the Loss is 0.3288.\n",
      "For Iteration 3227 the Loss is 0.3287.\n",
      "For Iteration 3228 the Loss is 0.3287.\n",
      "For Iteration 3229 the Loss is 0.3287.\n",
      "For Iteration 3230 the Loss is 0.3287.\n",
      "For Iteration 3231 the Loss is 0.3287.\n",
      "For Iteration 3232 the Loss is 0.3287.\n",
      "For Iteration 3233 the Loss is 0.3287.\n",
      "For Iteration 3234 the Loss is 0.3287.\n",
      "For Iteration 3235 the Loss is 0.3287.\n",
      "For Iteration 3236 the Loss is 0.3287.\n",
      "For Iteration 3237 the Loss is 0.3287.\n",
      "For Iteration 3238 the Loss is 0.3286.\n",
      "For Iteration 3239 the Loss is 0.3286.\n",
      "For Iteration 3240 the Loss is 0.3286.\n",
      "For Iteration 3241 the Loss is 0.3286.\n",
      "For Iteration 3242 the Loss is 0.3286.\n",
      "For Iteration 3243 the Loss is 0.3286.\n",
      "For Iteration 3244 the Loss is 0.3286.\n",
      "For Iteration 3245 the Loss is 0.3286.\n",
      "For Iteration 3246 the Loss is 0.3286.\n",
      "For Iteration 3247 the Loss is 0.3286.\n",
      "For Iteration 3248 the Loss is 0.3286.\n",
      "For Iteration 3249 the Loss is 0.3285.\n",
      "For Iteration 3250 the Loss is 0.3285.\n",
      "For Iteration 3251 the Loss is 0.3285.\n",
      "For Iteration 3252 the Loss is 0.3285.\n",
      "For Iteration 3253 the Loss is 0.3285.\n",
      "For Iteration 3254 the Loss is 0.3285.\n",
      "For Iteration 3255 the Loss is 0.3285.\n",
      "For Iteration 3256 the Loss is 0.3285.\n",
      "For Iteration 3257 the Loss is 0.3285.\n",
      "For Iteration 3258 the Loss is 0.3285.\n",
      "For Iteration 3259 the Loss is 0.3285.\n",
      "For Iteration 3260 the Loss is 0.3284.\n",
      "For Iteration 3261 the Loss is 0.3284.\n",
      "For Iteration 3262 the Loss is 0.3284.\n",
      "For Iteration 3263 the Loss is 0.3284.\n",
      "For Iteration 3264 the Loss is 0.3284.\n",
      "For Iteration 3265 the Loss is 0.3284.\n",
      "For Iteration 3266 the Loss is 0.3284.\n",
      "For Iteration 3267 the Loss is 0.3284.\n",
      "For Iteration 3268 the Loss is 0.3284.\n",
      "For Iteration 3269 the Loss is 0.3284.\n",
      "For Iteration 3270 the Loss is 0.3284.\n",
      "For Iteration 3271 the Loss is 0.3283.\n",
      "For Iteration 3272 the Loss is 0.3283.\n",
      "For Iteration 3273 the Loss is 0.3283.\n",
      "For Iteration 3274 the Loss is 0.3283.\n",
      "For Iteration 3275 the Loss is 0.3283.\n",
      "For Iteration 3276 the Loss is 0.3283.\n",
      "For Iteration 3277 the Loss is 0.3283.\n",
      "For Iteration 3278 the Loss is 0.3283.\n",
      "For Iteration 3279 the Loss is 0.3283.\n",
      "For Iteration 3280 the Loss is 0.3283.\n",
      "For Iteration 3281 the Loss is 0.3283.\n",
      "For Iteration 3282 the Loss is 0.3282.\n",
      "For Iteration 3283 the Loss is 0.3282.\n",
      "For Iteration 3284 the Loss is 0.3282.\n",
      "For Iteration 3285 the Loss is 0.3282.\n",
      "For Iteration 3286 the Loss is 0.3282.\n",
      "For Iteration 3287 the Loss is 0.3282.\n",
      "For Iteration 3288 the Loss is 0.3282.\n",
      "For Iteration 3289 the Loss is 0.3282.\n",
      "For Iteration 3290 the Loss is 0.3282.\n",
      "For Iteration 3291 the Loss is 0.3282.\n",
      "For Iteration 3292 the Loss is 0.3282.\n",
      "For Iteration 3293 the Loss is 0.3281.\n",
      "For Iteration 3294 the Loss is 0.3281.\n",
      "For Iteration 3295 the Loss is 0.3281.\n",
      "For Iteration 3296 the Loss is 0.3281.\n",
      "For Iteration 3297 the Loss is 0.3281.\n",
      "For Iteration 3298 the Loss is 0.3281.\n",
      "For Iteration 3299 the Loss is 0.3281.\n",
      "For Iteration 3300 the Loss is 0.3281.\n",
      "For Iteration 3301 the Loss is 0.3281.\n",
      "For Iteration 3302 the Loss is 0.3281.\n",
      "For Iteration 3303 the Loss is 0.3281.\n",
      "For Iteration 3304 the Loss is 0.328.\n",
      "For Iteration 3305 the Loss is 0.328.\n",
      "For Iteration 3306 the Loss is 0.328.\n",
      "For Iteration 3307 the Loss is 0.328.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 3308 the Loss is 0.328.\n",
      "For Iteration 3309 the Loss is 0.328.\n",
      "For Iteration 3310 the Loss is 0.328.\n",
      "For Iteration 3311 the Loss is 0.328.\n",
      "For Iteration 3312 the Loss is 0.328.\n",
      "For Iteration 3313 the Loss is 0.328.\n",
      "For Iteration 3314 the Loss is 0.328.\n",
      "For Iteration 3315 the Loss is 0.3279.\n",
      "For Iteration 3316 the Loss is 0.3279.\n",
      "For Iteration 3317 the Loss is 0.3279.\n",
      "For Iteration 3318 the Loss is 0.3279.\n",
      "For Iteration 3319 the Loss is 0.3279.\n",
      "For Iteration 3320 the Loss is 0.3279.\n",
      "For Iteration 3321 the Loss is 0.3279.\n",
      "For Iteration 3322 the Loss is 0.3279.\n",
      "For Iteration 3323 the Loss is 0.3279.\n",
      "For Iteration 3324 the Loss is 0.3279.\n",
      "For Iteration 3325 the Loss is 0.3279.\n",
      "For Iteration 3326 the Loss is 0.3278.\n",
      "For Iteration 3327 the Loss is 0.3278.\n",
      "For Iteration 3328 the Loss is 0.3278.\n",
      "For Iteration 3329 the Loss is 0.3278.\n",
      "For Iteration 3330 the Loss is 0.3278.\n",
      "For Iteration 3331 the Loss is 0.3278.\n",
      "For Iteration 3332 the Loss is 0.3278.\n",
      "For Iteration 3333 the Loss is 0.3278.\n",
      "For Iteration 3334 the Loss is 0.3278.\n",
      "For Iteration 3335 the Loss is 0.3278.\n",
      "For Iteration 3336 the Loss is 0.3278.\n",
      "For Iteration 3337 the Loss is 0.3278.\n",
      "For Iteration 3338 the Loss is 0.3277.\n",
      "For Iteration 3339 the Loss is 0.3277.\n",
      "For Iteration 3340 the Loss is 0.3277.\n",
      "For Iteration 3341 the Loss is 0.3277.\n",
      "For Iteration 3342 the Loss is 0.3277.\n",
      "For Iteration 3343 the Loss is 0.3277.\n",
      "For Iteration 3344 the Loss is 0.3277.\n",
      "For Iteration 3345 the Loss is 0.3277.\n",
      "For Iteration 3346 the Loss is 0.3277.\n",
      "For Iteration 3347 the Loss is 0.3277.\n",
      "For Iteration 3348 the Loss is 0.3277.\n",
      "For Iteration 3349 the Loss is 0.3276.\n",
      "For Iteration 3350 the Loss is 0.3276.\n",
      "For Iteration 3351 the Loss is 0.3276.\n",
      "For Iteration 3352 the Loss is 0.3276.\n",
      "For Iteration 3353 the Loss is 0.3276.\n",
      "For Iteration 3354 the Loss is 0.3276.\n",
      "For Iteration 3355 the Loss is 0.3276.\n",
      "For Iteration 3356 the Loss is 0.3276.\n",
      "For Iteration 3357 the Loss is 0.3276.\n",
      "For Iteration 3358 the Loss is 0.3276.\n",
      "For Iteration 3359 the Loss is 0.3276.\n",
      "For Iteration 3360 the Loss is 0.3275.\n",
      "For Iteration 3361 the Loss is 0.3275.\n",
      "For Iteration 3362 the Loss is 0.3275.\n",
      "For Iteration 3363 the Loss is 0.3275.\n",
      "For Iteration 3364 the Loss is 0.3275.\n",
      "For Iteration 3365 the Loss is 0.3275.\n",
      "For Iteration 3366 the Loss is 0.3275.\n",
      "For Iteration 3367 the Loss is 0.3275.\n",
      "For Iteration 3368 the Loss is 0.3275.\n",
      "For Iteration 3369 the Loss is 0.3275.\n",
      "For Iteration 3370 the Loss is 0.3275.\n",
      "For Iteration 3371 the Loss is 0.3274.\n",
      "For Iteration 3372 the Loss is 0.3274.\n",
      "For Iteration 3373 the Loss is 0.3274.\n",
      "For Iteration 3374 the Loss is 0.3274.\n",
      "For Iteration 3375 the Loss is 0.3274.\n",
      "For Iteration 3376 the Loss is 0.3274.\n",
      "For Iteration 3377 the Loss is 0.3274.\n",
      "For Iteration 3378 the Loss is 0.3274.\n",
      "For Iteration 3379 the Loss is 0.3274.\n",
      "For Iteration 3380 the Loss is 0.3274.\n",
      "For Iteration 3381 the Loss is 0.3274.\n",
      "For Iteration 3382 the Loss is 0.3273.\n",
      "For Iteration 3383 the Loss is 0.3273.\n",
      "For Iteration 3384 the Loss is 0.3273.\n",
      "For Iteration 3385 the Loss is 0.3273.\n",
      "For Iteration 3386 the Loss is 0.3273.\n",
      "For Iteration 3387 the Loss is 0.3273.\n",
      "For Iteration 3388 the Loss is 0.3273.\n",
      "For Iteration 3389 the Loss is 0.3273.\n",
      "For Iteration 3390 the Loss is 0.3273.\n",
      "For Iteration 3391 the Loss is 0.3273.\n",
      "For Iteration 3392 the Loss is 0.3273.\n",
      "For Iteration 3393 the Loss is 0.3273.\n",
      "For Iteration 3394 the Loss is 0.3272.\n",
      "For Iteration 3395 the Loss is 0.3272.\n",
      "For Iteration 3396 the Loss is 0.3272.\n",
      "For Iteration 3397 the Loss is 0.3272.\n",
      "For Iteration 3398 the Loss is 0.3272.\n",
      "For Iteration 3399 the Loss is 0.3272.\n",
      "For Iteration 3400 the Loss is 0.3272.\n",
      "For Iteration 3401 the Loss is 0.3272.\n",
      "For Iteration 3402 the Loss is 0.3272.\n",
      "For Iteration 3403 the Loss is 0.3272.\n",
      "For Iteration 3404 the Loss is 0.3272.\n",
      "For Iteration 3405 the Loss is 0.3271.\n",
      "For Iteration 3406 the Loss is 0.3271.\n",
      "For Iteration 3407 the Loss is 0.3271.\n",
      "For Iteration 3408 the Loss is 0.3271.\n",
      "For Iteration 3409 the Loss is 0.3271.\n",
      "For Iteration 3410 the Loss is 0.3271.\n",
      "For Iteration 3411 the Loss is 0.3271.\n",
      "For Iteration 3412 the Loss is 0.3271.\n",
      "For Iteration 3413 the Loss is 0.3271.\n",
      "For Iteration 3414 the Loss is 0.3271.\n",
      "For Iteration 3415 the Loss is 0.3271.\n",
      "For Iteration 3416 the Loss is 0.327.\n",
      "For Iteration 3417 the Loss is 0.327.\n",
      "For Iteration 3418 the Loss is 0.327.\n",
      "For Iteration 3419 the Loss is 0.327.\n",
      "For Iteration 3420 the Loss is 0.327.\n",
      "For Iteration 3421 the Loss is 0.327.\n",
      "For Iteration 3422 the Loss is 0.327.\n",
      "For Iteration 3423 the Loss is 0.327.\n",
      "For Iteration 3424 the Loss is 0.327.\n",
      "For Iteration 3425 the Loss is 0.327.\n",
      "For Iteration 3426 the Loss is 0.327.\n",
      "For Iteration 3427 the Loss is 0.327.\n",
      "For Iteration 3428 the Loss is 0.3269.\n",
      "For Iteration 3429 the Loss is 0.3269.\n",
      "For Iteration 3430 the Loss is 0.3269.\n",
      "For Iteration 3431 the Loss is 0.3269.\n",
      "For Iteration 3432 the Loss is 0.3269.\n",
      "For Iteration 3433 the Loss is 0.3269.\n",
      "For Iteration 3434 the Loss is 0.3269.\n",
      "For Iteration 3435 the Loss is 0.3269.\n",
      "For Iteration 3436 the Loss is 0.3269.\n",
      "For Iteration 3437 the Loss is 0.3269.\n",
      "For Iteration 3438 the Loss is 0.3269.\n",
      "For Iteration 3439 the Loss is 0.3268.\n",
      "For Iteration 3440 the Loss is 0.3268.\n",
      "For Iteration 3441 the Loss is 0.3268.\n",
      "For Iteration 3442 the Loss is 0.3268.\n",
      "For Iteration 3443 the Loss is 0.3268.\n",
      "For Iteration 3444 the Loss is 0.3268.\n",
      "For Iteration 3445 the Loss is 0.3268.\n",
      "For Iteration 3446 the Loss is 0.3268.\n",
      "For Iteration 3447 the Loss is 0.3268.\n",
      "For Iteration 3448 the Loss is 0.3268.\n",
      "For Iteration 3449 the Loss is 0.3268.\n",
      "For Iteration 3450 the Loss is 0.3268.\n",
      "For Iteration 3451 the Loss is 0.3267.\n",
      "For Iteration 3452 the Loss is 0.3267.\n",
      "For Iteration 3453 the Loss is 0.3267.\n",
      "For Iteration 3454 the Loss is 0.3267.\n",
      "For Iteration 3455 the Loss is 0.3267.\n",
      "For Iteration 3456 the Loss is 0.3267.\n",
      "For Iteration 3457 the Loss is 0.3267.\n",
      "For Iteration 3458 the Loss is 0.3267.\n",
      "For Iteration 3459 the Loss is 0.3267.\n",
      "For Iteration 3460 the Loss is 0.3267.\n",
      "For Iteration 3461 the Loss is 0.3267.\n",
      "For Iteration 3462 the Loss is 0.3266.\n",
      "For Iteration 3463 the Loss is 0.3266.\n",
      "For Iteration 3464 the Loss is 0.3266.\n",
      "For Iteration 3465 the Loss is 0.3266.\n",
      "For Iteration 3466 the Loss is 0.3266.\n",
      "For Iteration 3467 the Loss is 0.3266.\n",
      "For Iteration 3468 the Loss is 0.3266.\n",
      "For Iteration 3469 the Loss is 0.3266.\n",
      "For Iteration 3470 the Loss is 0.3266.\n",
      "For Iteration 3471 the Loss is 0.3266.\n",
      "For Iteration 3472 the Loss is 0.3266.\n",
      "For Iteration 3473 the Loss is 0.3266.\n",
      "For Iteration 3474 the Loss is 0.3265.\n",
      "For Iteration 3475 the Loss is 0.3265.\n",
      "For Iteration 3476 the Loss is 0.3265.\n",
      "For Iteration 3477 the Loss is 0.3265.\n",
      "For Iteration 3478 the Loss is 0.3265.\n",
      "For Iteration 3479 the Loss is 0.3265.\n",
      "For Iteration 3480 the Loss is 0.3265.\n",
      "For Iteration 3481 the Loss is 0.3265.\n",
      "For Iteration 3482 the Loss is 0.3265.\n",
      "For Iteration 3483 the Loss is 0.3265.\n",
      "For Iteration 3484 the Loss is 0.3265.\n",
      "For Iteration 3485 the Loss is 0.3264.\n",
      "For Iteration 3486 the Loss is 0.3264.\n",
      "For Iteration 3487 the Loss is 0.3264.\n",
      "For Iteration 3488 the Loss is 0.3264.\n",
      "For Iteration 3489 the Loss is 0.3264.\n",
      "For Iteration 3490 the Loss is 0.3264.\n",
      "For Iteration 3491 the Loss is 0.3264.\n",
      "For Iteration 3492 the Loss is 0.3264.\n",
      "For Iteration 3493 the Loss is 0.3264.\n",
      "For Iteration 3494 the Loss is 0.3264.\n",
      "For Iteration 3495 the Loss is 0.3264.\n",
      "For Iteration 3496 the Loss is 0.3264.\n",
      "For Iteration 3497 the Loss is 0.3263.\n",
      "For Iteration 3498 the Loss is 0.3263.\n",
      "For Iteration 3499 the Loss is 0.3263.\n",
      "For Iteration 3500 the Loss is 0.3263.\n",
      "For Iteration 3501 the Loss is 0.3263.\n",
      "For Iteration 3502 the Loss is 0.3263.\n",
      "For Iteration 3503 the Loss is 0.3263.\n",
      "For Iteration 3504 the Loss is 0.3263.\n",
      "For Iteration 3505 the Loss is 0.3263.\n",
      "For Iteration 3506 the Loss is 0.3263.\n",
      "For Iteration 3507 the Loss is 0.3263.\n",
      "For Iteration 3508 the Loss is 0.3262.\n",
      "For Iteration 3509 the Loss is 0.3262.\n",
      "For Iteration 3510 the Loss is 0.3262.\n",
      "For Iteration 3511 the Loss is 0.3262.\n",
      "For Iteration 3512 the Loss is 0.3262.\n",
      "For Iteration 3513 the Loss is 0.3262.\n",
      "For Iteration 3514 the Loss is 0.3262.\n",
      "For Iteration 3515 the Loss is 0.3262.\n",
      "For Iteration 3516 the Loss is 0.3262.\n",
      "For Iteration 3517 the Loss is 0.3262.\n",
      "For Iteration 3518 the Loss is 0.3262.\n",
      "For Iteration 3519 the Loss is 0.3262.\n",
      "For Iteration 3520 the Loss is 0.3261.\n",
      "For Iteration 3521 the Loss is 0.3261.\n",
      "For Iteration 3522 the Loss is 0.3261.\n",
      "For Iteration 3523 the Loss is 0.3261.\n",
      "For Iteration 3524 the Loss is 0.3261.\n",
      "For Iteration 3525 the Loss is 0.3261.\n",
      "For Iteration 3526 the Loss is 0.3261.\n",
      "For Iteration 3527 the Loss is 0.3261.\n",
      "For Iteration 3528 the Loss is 0.3261.\n",
      "For Iteration 3529 the Loss is 0.3261.\n",
      "For Iteration 3530 the Loss is 0.3261.\n",
      "For Iteration 3531 the Loss is 0.3261.\n",
      "For Iteration 3532 the Loss is 0.326.\n",
      "For Iteration 3533 the Loss is 0.326.\n",
      "For Iteration 3534 the Loss is 0.326.\n",
      "For Iteration 3535 the Loss is 0.326.\n",
      "For Iteration 3536 the Loss is 0.326.\n",
      "For Iteration 3537 the Loss is 0.326.\n",
      "For Iteration 3538 the Loss is 0.326.\n",
      "For Iteration 3539 the Loss is 0.326.\n",
      "For Iteration 3540 the Loss is 0.326.\n",
      "For Iteration 3541 the Loss is 0.326.\n",
      "For Iteration 3542 the Loss is 0.326.\n",
      "For Iteration 3543 the Loss is 0.326.\n",
      "For Iteration 3544 the Loss is 0.3259.\n",
      "For Iteration 3545 the Loss is 0.3259.\n",
      "For Iteration 3546 the Loss is 0.3259.\n",
      "For Iteration 3547 the Loss is 0.3259.\n",
      "For Iteration 3548 the Loss is 0.3259.\n",
      "For Iteration 3549 the Loss is 0.3259.\n",
      "For Iteration 3550 the Loss is 0.3259.\n",
      "For Iteration 3551 the Loss is 0.3259.\n",
      "For Iteration 3552 the Loss is 0.3259.\n",
      "For Iteration 3553 the Loss is 0.3259.\n",
      "For Iteration 3554 the Loss is 0.3259.\n",
      "For Iteration 3555 the Loss is 0.3258.\n",
      "For Iteration 3556 the Loss is 0.3258.\n",
      "For Iteration 3557 the Loss is 0.3258.\n",
      "For Iteration 3558 the Loss is 0.3258.\n",
      "For Iteration 3559 the Loss is 0.3258.\n",
      "For Iteration 3560 the Loss is 0.3258.\n",
      "For Iteration 3561 the Loss is 0.3258.\n",
      "For Iteration 3562 the Loss is 0.3258.\n",
      "For Iteration 3563 the Loss is 0.3258.\n",
      "For Iteration 3564 the Loss is 0.3258.\n",
      "For Iteration 3565 the Loss is 0.3258.\n",
      "For Iteration 3566 the Loss is 0.3258.\n",
      "For Iteration 3567 the Loss is 0.3257.\n",
      "For Iteration 3568 the Loss is 0.3257.\n",
      "For Iteration 3569 the Loss is 0.3257.\n",
      "For Iteration 3570 the Loss is 0.3257.\n",
      "For Iteration 3571 the Loss is 0.3257.\n",
      "For Iteration 3572 the Loss is 0.3257.\n",
      "For Iteration 3573 the Loss is 0.3257.\n",
      "For Iteration 3574 the Loss is 0.3257.\n",
      "For Iteration 3575 the Loss is 0.3257.\n",
      "For Iteration 3576 the Loss is 0.3257.\n",
      "For Iteration 3577 the Loss is 0.3257.\n",
      "For Iteration 3578 the Loss is 0.3257.\n",
      "For Iteration 3579 the Loss is 0.3256.\n",
      "For Iteration 3580 the Loss is 0.3256.\n",
      "For Iteration 3581 the Loss is 0.3256.\n",
      "For Iteration 3582 the Loss is 0.3256.\n",
      "For Iteration 3583 the Loss is 0.3256.\n",
      "For Iteration 3584 the Loss is 0.3256.\n",
      "For Iteration 3585 the Loss is 0.3256.\n",
      "For Iteration 3586 the Loss is 0.3256.\n",
      "For Iteration 3587 the Loss is 0.3256.\n",
      "For Iteration 3588 the Loss is 0.3256.\n",
      "For Iteration 3589 the Loss is 0.3256.\n",
      "For Iteration 3590 the Loss is 0.3256.\n",
      "For Iteration 3591 the Loss is 0.3255.\n",
      "For Iteration 3592 the Loss is 0.3255.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 3593 the Loss is 0.3255.\n",
      "For Iteration 3594 the Loss is 0.3255.\n",
      "For Iteration 3595 the Loss is 0.3255.\n",
      "For Iteration 3596 the Loss is 0.3255.\n",
      "For Iteration 3597 the Loss is 0.3255.\n",
      "For Iteration 3598 the Loss is 0.3255.\n",
      "For Iteration 3599 the Loss is 0.3255.\n",
      "For Iteration 3600 the Loss is 0.3255.\n",
      "For Iteration 3601 the Loss is 0.3255.\n",
      "For Iteration 3602 the Loss is 0.3255.\n",
      "For Iteration 3603 the Loss is 0.3254.\n",
      "For Iteration 3604 the Loss is 0.3254.\n",
      "For Iteration 3605 the Loss is 0.3254.\n",
      "For Iteration 3606 the Loss is 0.3254.\n",
      "For Iteration 3607 the Loss is 0.3254.\n",
      "For Iteration 3608 the Loss is 0.3254.\n",
      "For Iteration 3609 the Loss is 0.3254.\n",
      "For Iteration 3610 the Loss is 0.3254.\n",
      "For Iteration 3611 the Loss is 0.3254.\n",
      "For Iteration 3612 the Loss is 0.3254.\n",
      "For Iteration 3613 the Loss is 0.3254.\n",
      "For Iteration 3614 the Loss is 0.3253.\n",
      "For Iteration 3615 the Loss is 0.3253.\n",
      "For Iteration 3616 the Loss is 0.3253.\n",
      "For Iteration 3617 the Loss is 0.3253.\n",
      "For Iteration 3618 the Loss is 0.3253.\n",
      "For Iteration 3619 the Loss is 0.3253.\n",
      "For Iteration 3620 the Loss is 0.3253.\n",
      "For Iteration 3621 the Loss is 0.3253.\n",
      "For Iteration 3622 the Loss is 0.3253.\n",
      "For Iteration 3623 the Loss is 0.3253.\n",
      "For Iteration 3624 the Loss is 0.3253.\n",
      "For Iteration 3625 the Loss is 0.3253.\n",
      "For Iteration 3626 the Loss is 0.3252.\n",
      "For Iteration 3627 the Loss is 0.3252.\n",
      "For Iteration 3628 the Loss is 0.3252.\n",
      "For Iteration 3629 the Loss is 0.3252.\n",
      "For Iteration 3630 the Loss is 0.3252.\n",
      "For Iteration 3631 the Loss is 0.3252.\n",
      "For Iteration 3632 the Loss is 0.3252.\n",
      "For Iteration 3633 the Loss is 0.3252.\n",
      "For Iteration 3634 the Loss is 0.3252.\n",
      "For Iteration 3635 the Loss is 0.3252.\n",
      "For Iteration 3636 the Loss is 0.3252.\n",
      "For Iteration 3637 the Loss is 0.3252.\n",
      "For Iteration 3638 the Loss is 0.3251.\n",
      "For Iteration 3639 the Loss is 0.3251.\n",
      "For Iteration 3640 the Loss is 0.3251.\n",
      "For Iteration 3641 the Loss is 0.3251.\n",
      "For Iteration 3642 the Loss is 0.3251.\n",
      "For Iteration 3643 the Loss is 0.3251.\n",
      "For Iteration 3644 the Loss is 0.3251.\n",
      "For Iteration 3645 the Loss is 0.3251.\n",
      "For Iteration 3646 the Loss is 0.3251.\n",
      "For Iteration 3647 the Loss is 0.3251.\n",
      "For Iteration 3648 the Loss is 0.3251.\n",
      "For Iteration 3649 the Loss is 0.3251.\n",
      "For Iteration 3650 the Loss is 0.325.\n",
      "For Iteration 3651 the Loss is 0.325.\n",
      "For Iteration 3652 the Loss is 0.325.\n",
      "For Iteration 3653 the Loss is 0.325.\n",
      "For Iteration 3654 the Loss is 0.325.\n",
      "For Iteration 3655 the Loss is 0.325.\n",
      "For Iteration 3656 the Loss is 0.325.\n",
      "For Iteration 3657 the Loss is 0.325.\n",
      "For Iteration 3658 the Loss is 0.325.\n",
      "For Iteration 3659 the Loss is 0.325.\n",
      "For Iteration 3660 the Loss is 0.325.\n",
      "For Iteration 3661 the Loss is 0.325.\n",
      "For Iteration 3662 the Loss is 0.3249.\n",
      "For Iteration 3663 the Loss is 0.3249.\n",
      "For Iteration 3664 the Loss is 0.3249.\n",
      "For Iteration 3665 the Loss is 0.3249.\n",
      "For Iteration 3666 the Loss is 0.3249.\n",
      "For Iteration 3667 the Loss is 0.3249.\n",
      "For Iteration 3668 the Loss is 0.3249.\n",
      "For Iteration 3669 the Loss is 0.3249.\n",
      "For Iteration 3670 the Loss is 0.3249.\n",
      "For Iteration 3671 the Loss is 0.3249.\n",
      "For Iteration 3672 the Loss is 0.3249.\n",
      "For Iteration 3673 the Loss is 0.3249.\n",
      "For Iteration 3674 the Loss is 0.3248.\n",
      "For Iteration 3675 the Loss is 0.3248.\n",
      "For Iteration 3676 the Loss is 0.3248.\n",
      "For Iteration 3677 the Loss is 0.3248.\n",
      "For Iteration 3678 the Loss is 0.3248.\n",
      "For Iteration 3679 the Loss is 0.3248.\n",
      "For Iteration 3680 the Loss is 0.3248.\n",
      "For Iteration 3681 the Loss is 0.3248.\n",
      "For Iteration 3682 the Loss is 0.3248.\n",
      "For Iteration 3683 the Loss is 0.3248.\n",
      "For Iteration 3684 the Loss is 0.3248.\n",
      "For Iteration 3685 the Loss is 0.3248.\n",
      "For Iteration 3686 the Loss is 0.3248.\n",
      "For Iteration 3687 the Loss is 0.3247.\n",
      "For Iteration 3688 the Loss is 0.3247.\n",
      "For Iteration 3689 the Loss is 0.3247.\n",
      "For Iteration 3690 the Loss is 0.3247.\n",
      "For Iteration 3691 the Loss is 0.3247.\n",
      "For Iteration 3692 the Loss is 0.3247.\n",
      "For Iteration 3693 the Loss is 0.3247.\n",
      "For Iteration 3694 the Loss is 0.3247.\n",
      "For Iteration 3695 the Loss is 0.3247.\n",
      "For Iteration 3696 the Loss is 0.3247.\n",
      "For Iteration 3697 the Loss is 0.3247.\n",
      "For Iteration 3698 the Loss is 0.3247.\n",
      "For Iteration 3699 the Loss is 0.3246.\n",
      "For Iteration 3700 the Loss is 0.3246.\n",
      "For Iteration 3701 the Loss is 0.3246.\n",
      "For Iteration 3702 the Loss is 0.3246.\n",
      "For Iteration 3703 the Loss is 0.3246.\n",
      "For Iteration 3704 the Loss is 0.3246.\n",
      "For Iteration 3705 the Loss is 0.3246.\n",
      "For Iteration 3706 the Loss is 0.3246.\n",
      "For Iteration 3707 the Loss is 0.3246.\n",
      "For Iteration 3708 the Loss is 0.3246.\n",
      "For Iteration 3709 the Loss is 0.3246.\n",
      "For Iteration 3710 the Loss is 0.3246.\n",
      "For Iteration 3711 the Loss is 0.3245.\n",
      "For Iteration 3712 the Loss is 0.3245.\n",
      "For Iteration 3713 the Loss is 0.3245.\n",
      "For Iteration 3714 the Loss is 0.3245.\n",
      "For Iteration 3715 the Loss is 0.3245.\n",
      "For Iteration 3716 the Loss is 0.3245.\n",
      "For Iteration 3717 the Loss is 0.3245.\n",
      "For Iteration 3718 the Loss is 0.3245.\n",
      "For Iteration 3719 the Loss is 0.3245.\n",
      "For Iteration 3720 the Loss is 0.3245.\n",
      "For Iteration 3721 the Loss is 0.3245.\n",
      "For Iteration 3722 the Loss is 0.3245.\n",
      "For Iteration 3723 the Loss is 0.3244.\n",
      "For Iteration 3724 the Loss is 0.3244.\n",
      "For Iteration 3725 the Loss is 0.3244.\n",
      "For Iteration 3726 the Loss is 0.3244.\n",
      "For Iteration 3727 the Loss is 0.3244.\n",
      "For Iteration 3728 the Loss is 0.3244.\n",
      "For Iteration 3729 the Loss is 0.3244.\n",
      "For Iteration 3730 the Loss is 0.3244.\n",
      "For Iteration 3731 the Loss is 0.3244.\n",
      "For Iteration 3732 the Loss is 0.3244.\n",
      "For Iteration 3733 the Loss is 0.3244.\n",
      "For Iteration 3734 the Loss is 0.3244.\n",
      "For Iteration 3735 the Loss is 0.3243.\n",
      "For Iteration 3736 the Loss is 0.3243.\n",
      "For Iteration 3737 the Loss is 0.3243.\n",
      "For Iteration 3738 the Loss is 0.3243.\n",
      "For Iteration 3739 the Loss is 0.3243.\n",
      "For Iteration 3740 the Loss is 0.3243.\n",
      "For Iteration 3741 the Loss is 0.3243.\n",
      "For Iteration 3742 the Loss is 0.3243.\n",
      "For Iteration 3743 the Loss is 0.3243.\n",
      "For Iteration 3744 the Loss is 0.3243.\n",
      "For Iteration 3745 the Loss is 0.3243.\n",
      "For Iteration 3746 the Loss is 0.3243.\n",
      "For Iteration 3747 the Loss is 0.3243.\n",
      "For Iteration 3748 the Loss is 0.3242.\n",
      "For Iteration 3749 the Loss is 0.3242.\n",
      "For Iteration 3750 the Loss is 0.3242.\n",
      "For Iteration 3751 the Loss is 0.3242.\n",
      "For Iteration 3752 the Loss is 0.3242.\n",
      "For Iteration 3753 the Loss is 0.3242.\n",
      "For Iteration 3754 the Loss is 0.3242.\n",
      "For Iteration 3755 the Loss is 0.3242.\n",
      "For Iteration 3756 the Loss is 0.3242.\n",
      "For Iteration 3757 the Loss is 0.3242.\n",
      "For Iteration 3758 the Loss is 0.3242.\n",
      "For Iteration 3759 the Loss is 0.3242.\n",
      "For Iteration 3760 the Loss is 0.3241.\n",
      "For Iteration 3761 the Loss is 0.3241.\n",
      "For Iteration 3762 the Loss is 0.3241.\n",
      "For Iteration 3763 the Loss is 0.3241.\n",
      "For Iteration 3764 the Loss is 0.3241.\n",
      "For Iteration 3765 the Loss is 0.3241.\n",
      "For Iteration 3766 the Loss is 0.3241.\n",
      "For Iteration 3767 the Loss is 0.3241.\n",
      "For Iteration 3768 the Loss is 0.3241.\n",
      "For Iteration 3769 the Loss is 0.3241.\n",
      "For Iteration 3770 the Loss is 0.3241.\n",
      "For Iteration 3771 the Loss is 0.3241.\n",
      "For Iteration 3772 the Loss is 0.324.\n",
      "For Iteration 3773 the Loss is 0.324.\n",
      "For Iteration 3774 the Loss is 0.324.\n",
      "For Iteration 3775 the Loss is 0.324.\n",
      "For Iteration 3776 the Loss is 0.324.\n",
      "For Iteration 3777 the Loss is 0.324.\n",
      "For Iteration 3778 the Loss is 0.324.\n",
      "For Iteration 3779 the Loss is 0.324.\n",
      "For Iteration 3780 the Loss is 0.324.\n",
      "For Iteration 3781 the Loss is 0.324.\n",
      "For Iteration 3782 the Loss is 0.324.\n",
      "For Iteration 3783 the Loss is 0.324.\n",
      "For Iteration 3784 the Loss is 0.324.\n",
      "For Iteration 3785 the Loss is 0.3239.\n",
      "For Iteration 3786 the Loss is 0.3239.\n",
      "For Iteration 3787 the Loss is 0.3239.\n",
      "For Iteration 3788 the Loss is 0.3239.\n",
      "For Iteration 3789 the Loss is 0.3239.\n",
      "For Iteration 3790 the Loss is 0.3239.\n",
      "For Iteration 3791 the Loss is 0.3239.\n",
      "For Iteration 3792 the Loss is 0.3239.\n",
      "For Iteration 3793 the Loss is 0.3239.\n",
      "For Iteration 3794 the Loss is 0.3239.\n",
      "For Iteration 3795 the Loss is 0.3239.\n",
      "For Iteration 3796 the Loss is 0.3239.\n",
      "For Iteration 3797 the Loss is 0.3238.\n",
      "For Iteration 3798 the Loss is 0.3238.\n",
      "For Iteration 3799 the Loss is 0.3238.\n",
      "For Iteration 3800 the Loss is 0.3238.\n",
      "For Iteration 3801 the Loss is 0.3238.\n",
      "For Iteration 3802 the Loss is 0.3238.\n",
      "For Iteration 3803 the Loss is 0.3238.\n",
      "For Iteration 3804 the Loss is 0.3238.\n",
      "For Iteration 3805 the Loss is 0.3238.\n",
      "For Iteration 3806 the Loss is 0.3238.\n",
      "For Iteration 3807 the Loss is 0.3238.\n",
      "For Iteration 3808 the Loss is 0.3238.\n",
      "For Iteration 3809 the Loss is 0.3237.\n",
      "For Iteration 3810 the Loss is 0.3237.\n",
      "For Iteration 3811 the Loss is 0.3237.\n",
      "For Iteration 3812 the Loss is 0.3237.\n",
      "For Iteration 3813 the Loss is 0.3237.\n",
      "For Iteration 3814 the Loss is 0.3237.\n",
      "For Iteration 3815 the Loss is 0.3237.\n",
      "For Iteration 3816 the Loss is 0.3237.\n",
      "For Iteration 3817 the Loss is 0.3237.\n",
      "For Iteration 3818 the Loss is 0.3237.\n",
      "For Iteration 3819 the Loss is 0.3237.\n",
      "For Iteration 3820 the Loss is 0.3237.\n",
      "For Iteration 3821 the Loss is 0.3237.\n",
      "For Iteration 3822 the Loss is 0.3236.\n",
      "For Iteration 3823 the Loss is 0.3236.\n",
      "For Iteration 3824 the Loss is 0.3236.\n",
      "For Iteration 3825 the Loss is 0.3236.\n",
      "For Iteration 3826 the Loss is 0.3236.\n",
      "For Iteration 3827 the Loss is 0.3236.\n",
      "For Iteration 3828 the Loss is 0.3236.\n",
      "For Iteration 3829 the Loss is 0.3236.\n",
      "For Iteration 3830 the Loss is 0.3236.\n",
      "For Iteration 3831 the Loss is 0.3236.\n",
      "For Iteration 3832 the Loss is 0.3236.\n",
      "For Iteration 3833 the Loss is 0.3236.\n",
      "For Iteration 3834 the Loss is 0.3235.\n",
      "For Iteration 3835 the Loss is 0.3235.\n",
      "For Iteration 3836 the Loss is 0.3235.\n",
      "For Iteration 3837 the Loss is 0.3235.\n",
      "For Iteration 3838 the Loss is 0.3235.\n",
      "For Iteration 3839 the Loss is 0.3235.\n",
      "For Iteration 3840 the Loss is 0.3235.\n",
      "For Iteration 3841 the Loss is 0.3235.\n",
      "For Iteration 3842 the Loss is 0.3235.\n",
      "For Iteration 3843 the Loss is 0.3235.\n",
      "For Iteration 3844 the Loss is 0.3235.\n",
      "For Iteration 3845 the Loss is 0.3235.\n",
      "For Iteration 3846 the Loss is 0.3235.\n",
      "For Iteration 3847 the Loss is 0.3234.\n",
      "For Iteration 3848 the Loss is 0.3234.\n",
      "For Iteration 3849 the Loss is 0.3234.\n",
      "For Iteration 3850 the Loss is 0.3234.\n",
      "For Iteration 3851 the Loss is 0.3234.\n",
      "For Iteration 3852 the Loss is 0.3234.\n",
      "For Iteration 3853 the Loss is 0.3234.\n",
      "For Iteration 3854 the Loss is 0.3234.\n",
      "For Iteration 3855 the Loss is 0.3234.\n",
      "For Iteration 3856 the Loss is 0.3234.\n",
      "For Iteration 3857 the Loss is 0.3234.\n",
      "For Iteration 3858 the Loss is 0.3234.\n",
      "For Iteration 3859 the Loss is 0.3234.\n",
      "For Iteration 3860 the Loss is 0.3233.\n",
      "For Iteration 3861 the Loss is 0.3233.\n",
      "For Iteration 3862 the Loss is 0.3233.\n",
      "For Iteration 3863 the Loss is 0.3233.\n",
      "For Iteration 3864 the Loss is 0.3233.\n",
      "For Iteration 3865 the Loss is 0.3233.\n",
      "For Iteration 3866 the Loss is 0.3233.\n",
      "For Iteration 3867 the Loss is 0.3233.\n",
      "For Iteration 3868 the Loss is 0.3233.\n",
      "For Iteration 3869 the Loss is 0.3233.\n",
      "For Iteration 3870 the Loss is 0.3233.\n",
      "For Iteration 3871 the Loss is 0.3233.\n",
      "For Iteration 3872 the Loss is 0.3232.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 3873 the Loss is 0.3232.\n",
      "For Iteration 3874 the Loss is 0.3232.\n",
      "For Iteration 3875 the Loss is 0.3232.\n",
      "For Iteration 3876 the Loss is 0.3232.\n",
      "For Iteration 3877 the Loss is 0.3232.\n",
      "For Iteration 3878 the Loss is 0.3232.\n",
      "For Iteration 3879 the Loss is 0.3232.\n",
      "For Iteration 3880 the Loss is 0.3232.\n",
      "For Iteration 3881 the Loss is 0.3232.\n",
      "For Iteration 3882 the Loss is 0.3232.\n",
      "For Iteration 3883 the Loss is 0.3232.\n",
      "For Iteration 3884 the Loss is 0.3232.\n",
      "For Iteration 3885 the Loss is 0.3231.\n",
      "For Iteration 3886 the Loss is 0.3231.\n",
      "For Iteration 3887 the Loss is 0.3231.\n",
      "For Iteration 3888 the Loss is 0.3231.\n",
      "For Iteration 3889 the Loss is 0.3231.\n",
      "For Iteration 3890 the Loss is 0.3231.\n",
      "For Iteration 3891 the Loss is 0.3231.\n",
      "For Iteration 3892 the Loss is 0.3231.\n",
      "For Iteration 3893 the Loss is 0.3231.\n",
      "For Iteration 3894 the Loss is 0.3231.\n",
      "For Iteration 3895 the Loss is 0.3231.\n",
      "For Iteration 3896 the Loss is 0.3231.\n",
      "For Iteration 3897 the Loss is 0.323.\n",
      "For Iteration 3898 the Loss is 0.323.\n",
      "For Iteration 3899 the Loss is 0.323.\n",
      "For Iteration 3900 the Loss is 0.323.\n",
      "For Iteration 3901 the Loss is 0.323.\n",
      "For Iteration 3902 the Loss is 0.323.\n",
      "For Iteration 3903 the Loss is 0.323.\n",
      "For Iteration 3904 the Loss is 0.323.\n",
      "For Iteration 3905 the Loss is 0.323.\n",
      "For Iteration 3906 the Loss is 0.323.\n",
      "For Iteration 3907 the Loss is 0.323.\n",
      "For Iteration 3908 the Loss is 0.323.\n",
      "For Iteration 3909 the Loss is 0.323.\n",
      "For Iteration 3910 the Loss is 0.3229.\n",
      "For Iteration 3911 the Loss is 0.3229.\n",
      "For Iteration 3912 the Loss is 0.3229.\n",
      "For Iteration 3913 the Loss is 0.3229.\n",
      "For Iteration 3914 the Loss is 0.3229.\n",
      "For Iteration 3915 the Loss is 0.3229.\n",
      "For Iteration 3916 the Loss is 0.3229.\n",
      "For Iteration 3917 the Loss is 0.3229.\n",
      "For Iteration 3918 the Loss is 0.3229.\n",
      "For Iteration 3919 the Loss is 0.3229.\n",
      "For Iteration 3920 the Loss is 0.3229.\n",
      "For Iteration 3921 the Loss is 0.3229.\n",
      "For Iteration 3922 the Loss is 0.3229.\n",
      "For Iteration 3923 the Loss is 0.3228.\n",
      "For Iteration 3924 the Loss is 0.3228.\n",
      "For Iteration 3925 the Loss is 0.3228.\n",
      "For Iteration 3926 the Loss is 0.3228.\n",
      "For Iteration 3927 the Loss is 0.3228.\n",
      "For Iteration 3928 the Loss is 0.3228.\n",
      "For Iteration 3929 the Loss is 0.3228.\n",
      "For Iteration 3930 the Loss is 0.3228.\n",
      "For Iteration 3931 the Loss is 0.3228.\n",
      "For Iteration 3932 the Loss is 0.3228.\n",
      "For Iteration 3933 the Loss is 0.3228.\n",
      "For Iteration 3934 the Loss is 0.3228.\n",
      "For Iteration 3935 the Loss is 0.3228.\n",
      "For Iteration 3936 the Loss is 0.3227.\n",
      "For Iteration 3937 the Loss is 0.3227.\n",
      "For Iteration 3938 the Loss is 0.3227.\n",
      "For Iteration 3939 the Loss is 0.3227.\n",
      "For Iteration 3940 the Loss is 0.3227.\n",
      "For Iteration 3941 the Loss is 0.3227.\n",
      "For Iteration 3942 the Loss is 0.3227.\n",
      "For Iteration 3943 the Loss is 0.3227.\n",
      "For Iteration 3944 the Loss is 0.3227.\n",
      "For Iteration 3945 the Loss is 0.3227.\n",
      "For Iteration 3946 the Loss is 0.3227.\n",
      "For Iteration 3947 the Loss is 0.3227.\n",
      "For Iteration 3948 the Loss is 0.3227.\n",
      "For Iteration 3949 the Loss is 0.3226.\n",
      "For Iteration 3950 the Loss is 0.3226.\n",
      "For Iteration 3951 the Loss is 0.3226.\n",
      "For Iteration 3952 the Loss is 0.3226.\n",
      "For Iteration 3953 the Loss is 0.3226.\n",
      "For Iteration 3954 the Loss is 0.3226.\n",
      "For Iteration 3955 the Loss is 0.3226.\n",
      "For Iteration 3956 the Loss is 0.3226.\n",
      "For Iteration 3957 the Loss is 0.3226.\n",
      "For Iteration 3958 the Loss is 0.3226.\n",
      "For Iteration 3959 the Loss is 0.3226.\n",
      "For Iteration 3960 the Loss is 0.3226.\n",
      "For Iteration 3961 the Loss is 0.3226.\n",
      "For Iteration 3962 the Loss is 0.3225.\n",
      "For Iteration 3963 the Loss is 0.3225.\n",
      "For Iteration 3964 the Loss is 0.3225.\n",
      "For Iteration 3965 the Loss is 0.3225.\n",
      "For Iteration 3966 the Loss is 0.3225.\n",
      "For Iteration 3967 the Loss is 0.3225.\n",
      "For Iteration 3968 the Loss is 0.3225.\n",
      "For Iteration 3969 the Loss is 0.3225.\n",
      "For Iteration 3970 the Loss is 0.3225.\n",
      "For Iteration 3971 the Loss is 0.3225.\n",
      "For Iteration 3972 the Loss is 0.3225.\n",
      "For Iteration 3973 the Loss is 0.3225.\n",
      "For Iteration 3974 the Loss is 0.3224.\n",
      "For Iteration 3975 the Loss is 0.3224.\n",
      "For Iteration 3976 the Loss is 0.3224.\n",
      "For Iteration 3977 the Loss is 0.3224.\n",
      "For Iteration 3978 the Loss is 0.3224.\n",
      "For Iteration 3979 the Loss is 0.3224.\n",
      "For Iteration 3980 the Loss is 0.3224.\n",
      "For Iteration 3981 the Loss is 0.3224.\n",
      "For Iteration 3982 the Loss is 0.3224.\n",
      "For Iteration 3983 the Loss is 0.3224.\n",
      "For Iteration 3984 the Loss is 0.3224.\n",
      "For Iteration 3985 the Loss is 0.3224.\n",
      "For Iteration 3986 the Loss is 0.3224.\n",
      "For Iteration 3987 the Loss is 0.3223.\n",
      "For Iteration 3988 the Loss is 0.3223.\n",
      "For Iteration 3989 the Loss is 0.3223.\n",
      "For Iteration 3990 the Loss is 0.3223.\n",
      "For Iteration 3991 the Loss is 0.3223.\n",
      "For Iteration 3992 the Loss is 0.3223.\n",
      "For Iteration 3993 the Loss is 0.3223.\n",
      "For Iteration 3994 the Loss is 0.3223.\n",
      "For Iteration 3995 the Loss is 0.3223.\n",
      "For Iteration 3996 the Loss is 0.3223.\n",
      "For Iteration 3997 the Loss is 0.3223.\n",
      "For Iteration 3998 the Loss is 0.3223.\n",
      "For Iteration 3999 the Loss is 0.3223.\n",
      "For Iteration 4000 the Loss is 0.3222.\n",
      "For Iteration 4001 the Loss is 0.3222.\n",
      "For Iteration 4002 the Loss is 0.3222.\n",
      "For Iteration 4003 the Loss is 0.3222.\n",
      "For Iteration 4004 the Loss is 0.3222.\n",
      "For Iteration 4005 the Loss is 0.3222.\n",
      "For Iteration 4006 the Loss is 0.3222.\n",
      "For Iteration 4007 the Loss is 0.3222.\n",
      "For Iteration 4008 the Loss is 0.3222.\n",
      "For Iteration 4009 the Loss is 0.3222.\n",
      "For Iteration 4010 the Loss is 0.3222.\n",
      "For Iteration 4011 the Loss is 0.3222.\n",
      "For Iteration 4012 the Loss is 0.3222.\n",
      "For Iteration 4013 the Loss is 0.3221.\n",
      "For Iteration 4014 the Loss is 0.3221.\n",
      "For Iteration 4015 the Loss is 0.3221.\n",
      "For Iteration 4016 the Loss is 0.3221.\n",
      "For Iteration 4017 the Loss is 0.3221.\n",
      "For Iteration 4018 the Loss is 0.3221.\n",
      "For Iteration 4019 the Loss is 0.3221.\n",
      "For Iteration 4020 the Loss is 0.3221.\n",
      "For Iteration 4021 the Loss is 0.3221.\n",
      "For Iteration 4022 the Loss is 0.3221.\n",
      "For Iteration 4023 the Loss is 0.3221.\n",
      "For Iteration 4024 the Loss is 0.3221.\n",
      "For Iteration 4025 the Loss is 0.3221.\n",
      "For Iteration 4026 the Loss is 0.3221.\n",
      "For Iteration 4027 the Loss is 0.322.\n",
      "For Iteration 4028 the Loss is 0.322.\n",
      "For Iteration 4029 the Loss is 0.322.\n",
      "For Iteration 4030 the Loss is 0.322.\n",
      "For Iteration 4031 the Loss is 0.322.\n",
      "For Iteration 4032 the Loss is 0.322.\n",
      "For Iteration 4033 the Loss is 0.322.\n",
      "For Iteration 4034 the Loss is 0.322.\n",
      "For Iteration 4035 the Loss is 0.322.\n",
      "For Iteration 4036 the Loss is 0.322.\n",
      "For Iteration 4037 the Loss is 0.322.\n",
      "For Iteration 4038 the Loss is 0.322.\n",
      "For Iteration 4039 the Loss is 0.322.\n",
      "For Iteration 4040 the Loss is 0.3219.\n",
      "For Iteration 4041 the Loss is 0.3219.\n",
      "For Iteration 4042 the Loss is 0.3219.\n",
      "For Iteration 4043 the Loss is 0.3219.\n",
      "For Iteration 4044 the Loss is 0.3219.\n",
      "For Iteration 4045 the Loss is 0.3219.\n",
      "For Iteration 4046 the Loss is 0.3219.\n",
      "For Iteration 4047 the Loss is 0.3219.\n",
      "For Iteration 4048 the Loss is 0.3219.\n",
      "For Iteration 4049 the Loss is 0.3219.\n",
      "For Iteration 4050 the Loss is 0.3219.\n",
      "For Iteration 4051 the Loss is 0.3219.\n",
      "For Iteration 4052 the Loss is 0.3219.\n",
      "For Iteration 4053 the Loss is 0.3218.\n",
      "For Iteration 4054 the Loss is 0.3218.\n",
      "For Iteration 4055 the Loss is 0.3218.\n",
      "For Iteration 4056 the Loss is 0.3218.\n",
      "For Iteration 4057 the Loss is 0.3218.\n",
      "For Iteration 4058 the Loss is 0.3218.\n",
      "For Iteration 4059 the Loss is 0.3218.\n",
      "For Iteration 4060 the Loss is 0.3218.\n",
      "For Iteration 4061 the Loss is 0.3218.\n",
      "For Iteration 4062 the Loss is 0.3218.\n",
      "For Iteration 4063 the Loss is 0.3218.\n",
      "For Iteration 4064 the Loss is 0.3218.\n",
      "For Iteration 4065 the Loss is 0.3218.\n",
      "For Iteration 4066 the Loss is 0.3217.\n",
      "For Iteration 4067 the Loss is 0.3217.\n",
      "For Iteration 4068 the Loss is 0.3217.\n",
      "For Iteration 4069 the Loss is 0.3217.\n",
      "For Iteration 4070 the Loss is 0.3217.\n",
      "For Iteration 4071 the Loss is 0.3217.\n",
      "For Iteration 4072 the Loss is 0.3217.\n",
      "For Iteration 4073 the Loss is 0.3217.\n",
      "For Iteration 4074 the Loss is 0.3217.\n",
      "For Iteration 4075 the Loss is 0.3217.\n",
      "For Iteration 4076 the Loss is 0.3217.\n",
      "For Iteration 4077 the Loss is 0.3217.\n",
      "For Iteration 4078 the Loss is 0.3217.\n",
      "For Iteration 4079 the Loss is 0.3216.\n",
      "For Iteration 4080 the Loss is 0.3216.\n",
      "For Iteration 4081 the Loss is 0.3216.\n",
      "For Iteration 4082 the Loss is 0.3216.\n",
      "For Iteration 4083 the Loss is 0.3216.\n",
      "For Iteration 4084 the Loss is 0.3216.\n",
      "For Iteration 4085 the Loss is 0.3216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 4086 the Loss is 0.3216.\n",
      "For Iteration 4087 the Loss is 0.3216.\n",
      "For Iteration 4088 the Loss is 0.3216.\n",
      "For Iteration 4089 the Loss is 0.3216.\n",
      "For Iteration 4090 the Loss is 0.3216.\n",
      "For Iteration 4091 the Loss is 0.3216.\n",
      "For Iteration 4092 the Loss is 0.3215.\n",
      "For Iteration 4093 the Loss is 0.3215.\n",
      "For Iteration 4094 the Loss is 0.3215.\n",
      "For Iteration 4095 the Loss is 0.3215.\n",
      "For Iteration 4096 the Loss is 0.3215.\n",
      "For Iteration 4097 the Loss is 0.3215.\n",
      "For Iteration 4098 the Loss is 0.3215.\n",
      "For Iteration 4099 the Loss is 0.3215.\n",
      "For Iteration 4100 the Loss is 0.3215.\n",
      "For Iteration 4101 the Loss is 0.3215.\n",
      "For Iteration 4102 the Loss is 0.3215.\n",
      "For Iteration 4103 the Loss is 0.3215.\n",
      "For Iteration 4104 the Loss is 0.3215.\n",
      "For Iteration 4105 the Loss is 0.3215.\n",
      "For Iteration 4106 the Loss is 0.3214.\n",
      "For Iteration 4107 the Loss is 0.3214.\n",
      "For Iteration 4108 the Loss is 0.3214.\n",
      "For Iteration 4109 the Loss is 0.3214.\n",
      "For Iteration 4110 the Loss is 0.3214.\n",
      "For Iteration 4111 the Loss is 0.3214.\n",
      "For Iteration 4112 the Loss is 0.3214.\n",
      "For Iteration 4113 the Loss is 0.3214.\n",
      "For Iteration 4114 the Loss is 0.3214.\n",
      "For Iteration 4115 the Loss is 0.3214.\n",
      "For Iteration 4116 the Loss is 0.3214.\n",
      "For Iteration 4117 the Loss is 0.3214.\n",
      "For Iteration 4118 the Loss is 0.3214.\n",
      "For Iteration 4119 the Loss is 0.3213.\n",
      "For Iteration 4120 the Loss is 0.3213.\n",
      "For Iteration 4121 the Loss is 0.3213.\n",
      "For Iteration 4122 the Loss is 0.3213.\n",
      "For Iteration 4123 the Loss is 0.3213.\n",
      "For Iteration 4124 the Loss is 0.3213.\n",
      "For Iteration 4125 the Loss is 0.3213.\n",
      "For Iteration 4126 the Loss is 0.3213.\n",
      "For Iteration 4127 the Loss is 0.3213.\n",
      "For Iteration 4128 the Loss is 0.3213.\n",
      "For Iteration 4129 the Loss is 0.3213.\n",
      "For Iteration 4130 the Loss is 0.3213.\n",
      "For Iteration 4131 the Loss is 0.3213.\n",
      "For Iteration 4132 the Loss is 0.3213.\n",
      "For Iteration 4133 the Loss is 0.3212.\n",
      "For Iteration 4134 the Loss is 0.3212.\n",
      "For Iteration 4135 the Loss is 0.3212.\n",
      "For Iteration 4136 the Loss is 0.3212.\n",
      "For Iteration 4137 the Loss is 0.3212.\n",
      "For Iteration 4138 the Loss is 0.3212.\n",
      "For Iteration 4139 the Loss is 0.3212.\n",
      "For Iteration 4140 the Loss is 0.3212.\n",
      "For Iteration 4141 the Loss is 0.3212.\n",
      "For Iteration 4142 the Loss is 0.3212.\n",
      "For Iteration 4143 the Loss is 0.3212.\n",
      "For Iteration 4144 the Loss is 0.3212.\n",
      "For Iteration 4145 the Loss is 0.3212.\n",
      "For Iteration 4146 the Loss is 0.3211.\n",
      "For Iteration 4147 the Loss is 0.3211.\n",
      "For Iteration 4148 the Loss is 0.3211.\n",
      "For Iteration 4149 the Loss is 0.3211.\n",
      "For Iteration 4150 the Loss is 0.3211.\n",
      "For Iteration 4151 the Loss is 0.3211.\n",
      "For Iteration 4152 the Loss is 0.3211.\n",
      "For Iteration 4153 the Loss is 0.3211.\n",
      "For Iteration 4154 the Loss is 0.3211.\n",
      "For Iteration 4155 the Loss is 0.3211.\n",
      "For Iteration 4156 the Loss is 0.3211.\n",
      "For Iteration 4157 the Loss is 0.3211.\n",
      "For Iteration 4158 the Loss is 0.3211.\n",
      "For Iteration 4159 the Loss is 0.321.\n",
      "For Iteration 4160 the Loss is 0.321.\n",
      "For Iteration 4161 the Loss is 0.321.\n",
      "For Iteration 4162 the Loss is 0.321.\n",
      "For Iteration 4163 the Loss is 0.321.\n",
      "For Iteration 4164 the Loss is 0.321.\n",
      "For Iteration 4165 the Loss is 0.321.\n",
      "For Iteration 4166 the Loss is 0.321.\n",
      "For Iteration 4167 the Loss is 0.321.\n",
      "For Iteration 4168 the Loss is 0.321.\n",
      "For Iteration 4169 the Loss is 0.321.\n",
      "For Iteration 4170 the Loss is 0.321.\n",
      "For Iteration 4171 the Loss is 0.321.\n",
      "For Iteration 4172 the Loss is 0.321.\n",
      "For Iteration 4173 the Loss is 0.3209.\n",
      "For Iteration 4174 the Loss is 0.3209.\n",
      "For Iteration 4175 the Loss is 0.3209.\n",
      "For Iteration 4176 the Loss is 0.3209.\n",
      "For Iteration 4177 the Loss is 0.3209.\n",
      "For Iteration 4178 the Loss is 0.3209.\n",
      "For Iteration 4179 the Loss is 0.3209.\n",
      "For Iteration 4180 the Loss is 0.3209.\n",
      "For Iteration 4181 the Loss is 0.3209.\n",
      "For Iteration 4182 the Loss is 0.3209.\n",
      "For Iteration 4183 the Loss is 0.3209.\n",
      "For Iteration 4184 the Loss is 0.3209.\n",
      "For Iteration 4185 the Loss is 0.3209.\n",
      "For Iteration 4186 the Loss is 0.3209.\n",
      "For Iteration 4187 the Loss is 0.3208.\n",
      "For Iteration 4188 the Loss is 0.3208.\n",
      "For Iteration 4189 the Loss is 0.3208.\n",
      "For Iteration 4190 the Loss is 0.3208.\n",
      "For Iteration 4191 the Loss is 0.3208.\n",
      "For Iteration 4192 the Loss is 0.3208.\n",
      "For Iteration 4193 the Loss is 0.3208.\n",
      "For Iteration 4194 the Loss is 0.3208.\n",
      "For Iteration 4195 the Loss is 0.3208.\n",
      "For Iteration 4196 the Loss is 0.3208.\n",
      "For Iteration 4197 the Loss is 0.3208.\n",
      "For Iteration 4198 the Loss is 0.3208.\n",
      "For Iteration 4199 the Loss is 0.3208.\n",
      "For Iteration 4200 the Loss is 0.3207.\n",
      "For Iteration 4201 the Loss is 0.3207.\n",
      "For Iteration 4202 the Loss is 0.3207.\n",
      "For Iteration 4203 the Loss is 0.3207.\n",
      "For Iteration 4204 the Loss is 0.3207.\n",
      "For Iteration 4205 the Loss is 0.3207.\n",
      "For Iteration 4206 the Loss is 0.3207.\n",
      "For Iteration 4207 the Loss is 0.3207.\n",
      "For Iteration 4208 the Loss is 0.3207.\n",
      "For Iteration 4209 the Loss is 0.3207.\n",
      "For Iteration 4210 the Loss is 0.3207.\n",
      "For Iteration 4211 the Loss is 0.3207.\n",
      "For Iteration 4212 the Loss is 0.3207.\n",
      "For Iteration 4213 the Loss is 0.3207.\n",
      "For Iteration 4214 the Loss is 0.3206.\n",
      "For Iteration 4215 the Loss is 0.3206.\n",
      "For Iteration 4216 the Loss is 0.3206.\n",
      "For Iteration 4217 the Loss is 0.3206.\n",
      "For Iteration 4218 the Loss is 0.3206.\n",
      "For Iteration 4219 the Loss is 0.3206.\n",
      "For Iteration 4220 the Loss is 0.3206.\n",
      "For Iteration 4221 the Loss is 0.3206.\n",
      "For Iteration 4222 the Loss is 0.3206.\n",
      "For Iteration 4223 the Loss is 0.3206.\n",
      "For Iteration 4224 the Loss is 0.3206.\n",
      "For Iteration 4225 the Loss is 0.3206.\n",
      "For Iteration 4226 the Loss is 0.3206.\n",
      "For Iteration 4227 the Loss is 0.3205.\n",
      "For Iteration 4228 the Loss is 0.3205.\n",
      "For Iteration 4229 the Loss is 0.3205.\n",
      "For Iteration 4230 the Loss is 0.3205.\n",
      "For Iteration 4231 the Loss is 0.3205.\n",
      "For Iteration 4232 the Loss is 0.3205.\n",
      "For Iteration 4233 the Loss is 0.3205.\n",
      "For Iteration 4234 the Loss is 0.3205.\n",
      "For Iteration 4235 the Loss is 0.3205.\n",
      "For Iteration 4236 the Loss is 0.3205.\n",
      "For Iteration 4237 the Loss is 0.3205.\n",
      "For Iteration 4238 the Loss is 0.3205.\n",
      "For Iteration 4239 the Loss is 0.3205.\n",
      "For Iteration 4240 the Loss is 0.3205.\n",
      "For Iteration 4241 the Loss is 0.3204.\n",
      "For Iteration 4242 the Loss is 0.3204.\n",
      "For Iteration 4243 the Loss is 0.3204.\n",
      "For Iteration 4244 the Loss is 0.3204.\n",
      "For Iteration 4245 the Loss is 0.3204.\n",
      "For Iteration 4246 the Loss is 0.3204.\n",
      "For Iteration 4247 the Loss is 0.3204.\n",
      "For Iteration 4248 the Loss is 0.3204.\n",
      "For Iteration 4249 the Loss is 0.3204.\n",
      "For Iteration 4250 the Loss is 0.3204.\n",
      "For Iteration 4251 the Loss is 0.3204.\n",
      "For Iteration 4252 the Loss is 0.3204.\n",
      "For Iteration 4253 the Loss is 0.3204.\n",
      "For Iteration 4254 the Loss is 0.3204.\n",
      "For Iteration 4255 the Loss is 0.3203.\n",
      "For Iteration 4256 the Loss is 0.3203.\n",
      "For Iteration 4257 the Loss is 0.3203.\n",
      "For Iteration 4258 the Loss is 0.3203.\n",
      "For Iteration 4259 the Loss is 0.3203.\n",
      "For Iteration 4260 the Loss is 0.3203.\n",
      "For Iteration 4261 the Loss is 0.3203.\n",
      "For Iteration 4262 the Loss is 0.3203.\n",
      "For Iteration 4263 the Loss is 0.3203.\n",
      "For Iteration 4264 the Loss is 0.3203.\n",
      "For Iteration 4265 the Loss is 0.3203.\n",
      "For Iteration 4266 the Loss is 0.3203.\n",
      "For Iteration 4267 the Loss is 0.3203.\n",
      "For Iteration 4268 the Loss is 0.3203.\n",
      "For Iteration 4269 the Loss is 0.3202.\n",
      "For Iteration 4270 the Loss is 0.3202.\n",
      "For Iteration 4271 the Loss is 0.3202.\n",
      "For Iteration 4272 the Loss is 0.3202.\n",
      "For Iteration 4273 the Loss is 0.3202.\n",
      "For Iteration 4274 the Loss is 0.3202.\n",
      "For Iteration 4275 the Loss is 0.3202.\n",
      "For Iteration 4276 the Loss is 0.3202.\n",
      "For Iteration 4277 the Loss is 0.3202.\n",
      "For Iteration 4278 the Loss is 0.3202.\n",
      "For Iteration 4279 the Loss is 0.3202.\n",
      "For Iteration 4280 the Loss is 0.3202.\n",
      "For Iteration 4281 the Loss is 0.3202.\n",
      "For Iteration 4282 the Loss is 0.3202.\n",
      "For Iteration 4283 the Loss is 0.3201.\n",
      "For Iteration 4284 the Loss is 0.3201.\n",
      "For Iteration 4285 the Loss is 0.3201.\n",
      "For Iteration 4286 the Loss is 0.3201.\n",
      "For Iteration 4287 the Loss is 0.3201.\n",
      "For Iteration 4288 the Loss is 0.3201.\n",
      "For Iteration 4289 the Loss is 0.3201.\n",
      "For Iteration 4290 the Loss is 0.3201.\n",
      "For Iteration 4291 the Loss is 0.3201.\n",
      "For Iteration 4292 the Loss is 0.3201.\n",
      "For Iteration 4293 the Loss is 0.3201.\n",
      "For Iteration 4294 the Loss is 0.3201.\n",
      "For Iteration 4295 the Loss is 0.3201.\n",
      "For Iteration 4296 the Loss is 0.32.\n",
      "For Iteration 4297 the Loss is 0.32.\n",
      "For Iteration 4298 the Loss is 0.32.\n",
      "For Iteration 4299 the Loss is 0.32.\n",
      "For Iteration 4300 the Loss is 0.32.\n",
      "For Iteration 4301 the Loss is 0.32.\n",
      "For Iteration 4302 the Loss is 0.32.\n",
      "For Iteration 4303 the Loss is 0.32.\n",
      "For Iteration 4304 the Loss is 0.32.\n",
      "For Iteration 4305 the Loss is 0.32.\n",
      "For Iteration 4306 the Loss is 0.32.\n",
      "For Iteration 4307 the Loss is 0.32.\n",
      "For Iteration 4308 the Loss is 0.32.\n",
      "For Iteration 4309 the Loss is 0.32.\n",
      "For Iteration 4310 the Loss is 0.3199.\n",
      "For Iteration 4311 the Loss is 0.3199.\n",
      "For Iteration 4312 the Loss is 0.3199.\n",
      "For Iteration 4313 the Loss is 0.3199.\n",
      "For Iteration 4314 the Loss is 0.3199.\n",
      "For Iteration 4315 the Loss is 0.3199.\n",
      "For Iteration 4316 the Loss is 0.3199.\n",
      "For Iteration 4317 the Loss is 0.3199.\n",
      "For Iteration 4318 the Loss is 0.3199.\n",
      "For Iteration 4319 the Loss is 0.3199.\n",
      "For Iteration 4320 the Loss is 0.3199.\n",
      "For Iteration 4321 the Loss is 0.3199.\n",
      "For Iteration 4322 the Loss is 0.3199.\n",
      "For Iteration 4323 the Loss is 0.3199.\n",
      "For Iteration 4324 the Loss is 0.3198.\n",
      "For Iteration 4325 the Loss is 0.3198.\n",
      "For Iteration 4326 the Loss is 0.3198.\n",
      "For Iteration 4327 the Loss is 0.3198.\n",
      "For Iteration 4328 the Loss is 0.3198.\n",
      "For Iteration 4329 the Loss is 0.3198.\n",
      "For Iteration 4330 the Loss is 0.3198.\n",
      "For Iteration 4331 the Loss is 0.3198.\n",
      "For Iteration 4332 the Loss is 0.3198.\n",
      "For Iteration 4333 the Loss is 0.3198.\n",
      "For Iteration 4334 the Loss is 0.3198.\n",
      "For Iteration 4335 the Loss is 0.3198.\n",
      "For Iteration 4336 the Loss is 0.3198.\n",
      "For Iteration 4337 the Loss is 0.3198.\n",
      "For Iteration 4338 the Loss is 0.3197.\n",
      "For Iteration 4339 the Loss is 0.3197.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 4340 the Loss is 0.3197.\n",
      "For Iteration 4341 the Loss is 0.3197.\n",
      "For Iteration 4342 the Loss is 0.3197.\n",
      "For Iteration 4343 the Loss is 0.3197.\n",
      "For Iteration 4344 the Loss is 0.3197.\n",
      "For Iteration 4345 the Loss is 0.3197.\n",
      "For Iteration 4346 the Loss is 0.3197.\n",
      "For Iteration 4347 the Loss is 0.3197.\n",
      "For Iteration 4348 the Loss is 0.3197.\n",
      "For Iteration 4349 the Loss is 0.3197.\n",
      "For Iteration 4350 the Loss is 0.3197.\n",
      "For Iteration 4351 the Loss is 0.3197.\n",
      "For Iteration 4352 the Loss is 0.3197.\n",
      "For Iteration 4353 the Loss is 0.3196.\n",
      "For Iteration 4354 the Loss is 0.3196.\n",
      "For Iteration 4355 the Loss is 0.3196.\n",
      "For Iteration 4356 the Loss is 0.3196.\n",
      "For Iteration 4357 the Loss is 0.3196.\n",
      "For Iteration 4358 the Loss is 0.3196.\n",
      "For Iteration 4359 the Loss is 0.3196.\n",
      "For Iteration 4360 the Loss is 0.3196.\n",
      "For Iteration 4361 the Loss is 0.3196.\n",
      "For Iteration 4362 the Loss is 0.3196.\n",
      "For Iteration 4363 the Loss is 0.3196.\n",
      "For Iteration 4364 the Loss is 0.3196.\n",
      "For Iteration 4365 the Loss is 0.3196.\n",
      "For Iteration 4366 the Loss is 0.3196.\n",
      "For Iteration 4367 the Loss is 0.3195.\n",
      "For Iteration 4368 the Loss is 0.3195.\n",
      "For Iteration 4369 the Loss is 0.3195.\n",
      "For Iteration 4370 the Loss is 0.3195.\n",
      "For Iteration 4371 the Loss is 0.3195.\n",
      "For Iteration 4372 the Loss is 0.3195.\n",
      "For Iteration 4373 the Loss is 0.3195.\n",
      "For Iteration 4374 the Loss is 0.3195.\n",
      "For Iteration 4375 the Loss is 0.3195.\n",
      "For Iteration 4376 the Loss is 0.3195.\n",
      "For Iteration 4377 the Loss is 0.3195.\n",
      "For Iteration 4378 the Loss is 0.3195.\n",
      "For Iteration 4379 the Loss is 0.3195.\n",
      "For Iteration 4380 the Loss is 0.3195.\n",
      "For Iteration 4381 the Loss is 0.3194.\n",
      "For Iteration 4382 the Loss is 0.3194.\n",
      "For Iteration 4383 the Loss is 0.3194.\n",
      "For Iteration 4384 the Loss is 0.3194.\n",
      "For Iteration 4385 the Loss is 0.3194.\n",
      "For Iteration 4386 the Loss is 0.3194.\n",
      "For Iteration 4387 the Loss is 0.3194.\n",
      "For Iteration 4388 the Loss is 0.3194.\n",
      "For Iteration 4389 the Loss is 0.3194.\n",
      "For Iteration 4390 the Loss is 0.3194.\n",
      "For Iteration 4391 the Loss is 0.3194.\n",
      "For Iteration 4392 the Loss is 0.3194.\n",
      "For Iteration 4393 the Loss is 0.3194.\n",
      "For Iteration 4394 the Loss is 0.3194.\n",
      "For Iteration 4395 the Loss is 0.3193.\n",
      "For Iteration 4396 the Loss is 0.3193.\n",
      "For Iteration 4397 the Loss is 0.3193.\n",
      "For Iteration 4398 the Loss is 0.3193.\n",
      "For Iteration 4399 the Loss is 0.3193.\n",
      "For Iteration 4400 the Loss is 0.3193.\n",
      "For Iteration 4401 the Loss is 0.3193.\n",
      "For Iteration 4402 the Loss is 0.3193.\n",
      "For Iteration 4403 the Loss is 0.3193.\n",
      "For Iteration 4404 the Loss is 0.3193.\n",
      "For Iteration 4405 the Loss is 0.3193.\n",
      "For Iteration 4406 the Loss is 0.3193.\n",
      "For Iteration 4407 the Loss is 0.3193.\n",
      "For Iteration 4408 the Loss is 0.3193.\n",
      "For Iteration 4409 the Loss is 0.3192.\n",
      "For Iteration 4410 the Loss is 0.3192.\n",
      "For Iteration 4411 the Loss is 0.3192.\n",
      "For Iteration 4412 the Loss is 0.3192.\n",
      "For Iteration 4413 the Loss is 0.3192.\n",
      "For Iteration 4414 the Loss is 0.3192.\n",
      "For Iteration 4415 the Loss is 0.3192.\n",
      "For Iteration 4416 the Loss is 0.3192.\n",
      "For Iteration 4417 the Loss is 0.3192.\n",
      "For Iteration 4418 the Loss is 0.3192.\n",
      "For Iteration 4419 the Loss is 0.3192.\n",
      "For Iteration 4420 the Loss is 0.3192.\n",
      "For Iteration 4421 the Loss is 0.3192.\n",
      "For Iteration 4422 the Loss is 0.3192.\n",
      "For Iteration 4423 the Loss is 0.3192.\n",
      "For Iteration 4424 the Loss is 0.3191.\n",
      "For Iteration 4425 the Loss is 0.3191.\n",
      "For Iteration 4426 the Loss is 0.3191.\n",
      "For Iteration 4427 the Loss is 0.3191.\n",
      "For Iteration 4428 the Loss is 0.3191.\n",
      "For Iteration 4429 the Loss is 0.3191.\n",
      "For Iteration 4430 the Loss is 0.3191.\n",
      "For Iteration 4431 the Loss is 0.3191.\n",
      "For Iteration 4432 the Loss is 0.3191.\n",
      "For Iteration 4433 the Loss is 0.3191.\n",
      "For Iteration 4434 the Loss is 0.3191.\n",
      "For Iteration 4435 the Loss is 0.3191.\n",
      "For Iteration 4436 the Loss is 0.3191.\n",
      "For Iteration 4437 the Loss is 0.3191.\n",
      "For Iteration 4438 the Loss is 0.319.\n",
      "For Iteration 4439 the Loss is 0.319.\n",
      "For Iteration 4440 the Loss is 0.319.\n",
      "For Iteration 4441 the Loss is 0.319.\n",
      "For Iteration 4442 the Loss is 0.319.\n",
      "For Iteration 4443 the Loss is 0.319.\n",
      "For Iteration 4444 the Loss is 0.319.\n",
      "For Iteration 4445 the Loss is 0.319.\n",
      "For Iteration 4446 the Loss is 0.319.\n",
      "For Iteration 4447 the Loss is 0.319.\n",
      "For Iteration 4448 the Loss is 0.319.\n",
      "For Iteration 4449 the Loss is 0.319.\n",
      "For Iteration 4450 the Loss is 0.319.\n",
      "For Iteration 4451 the Loss is 0.319.\n",
      "For Iteration 4452 the Loss is 0.3189.\n",
      "For Iteration 4453 the Loss is 0.3189.\n",
      "For Iteration 4454 the Loss is 0.3189.\n",
      "For Iteration 4455 the Loss is 0.3189.\n",
      "For Iteration 4456 the Loss is 0.3189.\n",
      "For Iteration 4457 the Loss is 0.3189.\n",
      "For Iteration 4458 the Loss is 0.3189.\n",
      "For Iteration 4459 the Loss is 0.3189.\n",
      "For Iteration 4460 the Loss is 0.3189.\n",
      "For Iteration 4461 the Loss is 0.3189.\n",
      "For Iteration 4462 the Loss is 0.3189.\n",
      "For Iteration 4463 the Loss is 0.3189.\n",
      "For Iteration 4464 the Loss is 0.3189.\n",
      "For Iteration 4465 the Loss is 0.3189.\n",
      "For Iteration 4466 the Loss is 0.3189.\n",
      "For Iteration 4467 the Loss is 0.3188.\n",
      "For Iteration 4468 the Loss is 0.3188.\n",
      "For Iteration 4469 the Loss is 0.3188.\n",
      "For Iteration 4470 the Loss is 0.3188.\n",
      "For Iteration 4471 the Loss is 0.3188.\n",
      "For Iteration 4472 the Loss is 0.3188.\n",
      "For Iteration 4473 the Loss is 0.3188.\n",
      "For Iteration 4474 the Loss is 0.3188.\n",
      "For Iteration 4475 the Loss is 0.3188.\n",
      "For Iteration 4476 the Loss is 0.3188.\n",
      "For Iteration 4477 the Loss is 0.3188.\n",
      "For Iteration 4478 the Loss is 0.3188.\n",
      "For Iteration 4479 the Loss is 0.3188.\n",
      "For Iteration 4480 the Loss is 0.3188.\n",
      "For Iteration 4481 the Loss is 0.3187.\n",
      "For Iteration 4482 the Loss is 0.3187.\n",
      "For Iteration 4483 the Loss is 0.3187.\n",
      "For Iteration 4484 the Loss is 0.3187.\n",
      "For Iteration 4485 the Loss is 0.3187.\n",
      "For Iteration 4486 the Loss is 0.3187.\n",
      "For Iteration 4487 the Loss is 0.3187.\n",
      "For Iteration 4488 the Loss is 0.3187.\n",
      "For Iteration 4489 the Loss is 0.3187.\n",
      "For Iteration 4490 the Loss is 0.3187.\n",
      "For Iteration 4491 the Loss is 0.3187.\n",
      "For Iteration 4492 the Loss is 0.3187.\n",
      "For Iteration 4493 the Loss is 0.3187.\n",
      "For Iteration 4494 the Loss is 0.3187.\n",
      "For Iteration 4495 the Loss is 0.3187.\n",
      "For Iteration 4496 the Loss is 0.3186.\n",
      "For Iteration 4497 the Loss is 0.3186.\n",
      "For Iteration 4498 the Loss is 0.3186.\n",
      "For Iteration 4499 the Loss is 0.3186.\n",
      "For Iteration 4500 the Loss is 0.3186.\n",
      "For Iteration 4501 the Loss is 0.3186.\n",
      "For Iteration 4502 the Loss is 0.3186.\n",
      "For Iteration 4503 the Loss is 0.3186.\n",
      "For Iteration 4504 the Loss is 0.3186.\n",
      "For Iteration 4505 the Loss is 0.3186.\n",
      "For Iteration 4506 the Loss is 0.3186.\n",
      "For Iteration 4507 the Loss is 0.3186.\n",
      "For Iteration 4508 the Loss is 0.3186.\n",
      "For Iteration 4509 the Loss is 0.3186.\n",
      "For Iteration 4510 the Loss is 0.3185.\n",
      "For Iteration 4511 the Loss is 0.3185.\n",
      "For Iteration 4512 the Loss is 0.3185.\n",
      "For Iteration 4513 the Loss is 0.3185.\n",
      "For Iteration 4514 the Loss is 0.3185.\n",
      "For Iteration 4515 the Loss is 0.3185.\n",
      "For Iteration 4516 the Loss is 0.3185.\n",
      "For Iteration 4517 the Loss is 0.3185.\n",
      "For Iteration 4518 the Loss is 0.3185.\n",
      "For Iteration 4519 the Loss is 0.3185.\n",
      "For Iteration 4520 the Loss is 0.3185.\n",
      "For Iteration 4521 the Loss is 0.3185.\n",
      "For Iteration 4522 the Loss is 0.3185.\n",
      "For Iteration 4523 the Loss is 0.3185.\n",
      "For Iteration 4524 the Loss is 0.3185.\n",
      "For Iteration 4525 the Loss is 0.3184.\n",
      "For Iteration 4526 the Loss is 0.3184.\n",
      "For Iteration 4527 the Loss is 0.3184.\n",
      "For Iteration 4528 the Loss is 0.3184.\n",
      "For Iteration 4529 the Loss is 0.3184.\n",
      "For Iteration 4530 the Loss is 0.3184.\n",
      "For Iteration 4531 the Loss is 0.3184.\n",
      "For Iteration 4532 the Loss is 0.3184.\n",
      "For Iteration 4533 the Loss is 0.3184.\n",
      "For Iteration 4534 the Loss is 0.3184.\n",
      "For Iteration 4535 the Loss is 0.3184.\n",
      "For Iteration 4536 the Loss is 0.3184.\n",
      "For Iteration 4537 the Loss is 0.3184.\n",
      "For Iteration 4538 the Loss is 0.3184.\n",
      "For Iteration 4539 the Loss is 0.3184.\n",
      "For Iteration 4540 the Loss is 0.3183.\n",
      "For Iteration 4541 the Loss is 0.3183.\n",
      "For Iteration 4542 the Loss is 0.3183.\n",
      "For Iteration 4543 the Loss is 0.3183.\n",
      "For Iteration 4544 the Loss is 0.3183.\n",
      "For Iteration 4545 the Loss is 0.3183.\n",
      "For Iteration 4546 the Loss is 0.3183.\n",
      "For Iteration 4547 the Loss is 0.3183.\n",
      "For Iteration 4548 the Loss is 0.3183.\n",
      "For Iteration 4549 the Loss is 0.3183.\n",
      "For Iteration 4550 the Loss is 0.3183.\n",
      "For Iteration 4551 the Loss is 0.3183.\n",
      "For Iteration 4552 the Loss is 0.3183.\n",
      "For Iteration 4553 the Loss is 0.3183.\n",
      "For Iteration 4554 the Loss is 0.3182.\n",
      "For Iteration 4555 the Loss is 0.3182.\n",
      "For Iteration 4556 the Loss is 0.3182.\n",
      "For Iteration 4557 the Loss is 0.3182.\n",
      "For Iteration 4558 the Loss is 0.3182.\n",
      "For Iteration 4559 the Loss is 0.3182.\n",
      "For Iteration 4560 the Loss is 0.3182.\n",
      "For Iteration 4561 the Loss is 0.3182.\n",
      "For Iteration 4562 the Loss is 0.3182.\n",
      "For Iteration 4563 the Loss is 0.3182.\n",
      "For Iteration 4564 the Loss is 0.3182.\n",
      "For Iteration 4565 the Loss is 0.3182.\n",
      "For Iteration 4566 the Loss is 0.3182.\n",
      "For Iteration 4567 the Loss is 0.3182.\n",
      "For Iteration 4568 the Loss is 0.3182.\n",
      "For Iteration 4569 the Loss is 0.3181.\n",
      "For Iteration 4570 the Loss is 0.3181.\n",
      "For Iteration 4571 the Loss is 0.3181.\n",
      "For Iteration 4572 the Loss is 0.3181.\n",
      "For Iteration 4573 the Loss is 0.3181.\n",
      "For Iteration 4574 the Loss is 0.3181.\n",
      "For Iteration 4575 the Loss is 0.3181.\n",
      "For Iteration 4576 the Loss is 0.3181.\n",
      "For Iteration 4577 the Loss is 0.3181.\n",
      "For Iteration 4578 the Loss is 0.3181.\n",
      "For Iteration 4579 the Loss is 0.3181.\n",
      "For Iteration 4580 the Loss is 0.3181.\n",
      "For Iteration 4581 the Loss is 0.3181.\n",
      "For Iteration 4582 the Loss is 0.3181.\n",
      "For Iteration 4583 the Loss is 0.3181.\n",
      "For Iteration 4584 the Loss is 0.318.\n",
      "For Iteration 4585 the Loss is 0.318.\n",
      "For Iteration 4586 the Loss is 0.318.\n",
      "For Iteration 4587 the Loss is 0.318.\n",
      "For Iteration 4588 the Loss is 0.318.\n",
      "For Iteration 4589 the Loss is 0.318.\n",
      "For Iteration 4590 the Loss is 0.318.\n",
      "For Iteration 4591 the Loss is 0.318.\n",
      "For Iteration 4592 the Loss is 0.318.\n",
      "For Iteration 4593 the Loss is 0.318.\n",
      "For Iteration 4594 the Loss is 0.318.\n",
      "For Iteration 4595 the Loss is 0.318.\n",
      "For Iteration 4596 the Loss is 0.318.\n",
      "For Iteration 4597 the Loss is 0.318.\n",
      "For Iteration 4598 the Loss is 0.318.\n",
      "For Iteration 4599 the Loss is 0.3179.\n",
      "For Iteration 4600 the Loss is 0.3179.\n",
      "For Iteration 4601 the Loss is 0.3179.\n",
      "For Iteration 4602 the Loss is 0.3179.\n",
      "For Iteration 4603 the Loss is 0.3179.\n",
      "For Iteration 4604 the Loss is 0.3179.\n",
      "For Iteration 4605 the Loss is 0.3179.\n",
      "For Iteration 4606 the Loss is 0.3179.\n",
      "For Iteration 4607 the Loss is 0.3179.\n",
      "For Iteration 4608 the Loss is 0.3179.\n",
      "For Iteration 4609 the Loss is 0.3179.\n",
      "For Iteration 4610 the Loss is 0.3179.\n",
      "For Iteration 4611 the Loss is 0.3179.\n",
      "For Iteration 4612 the Loss is 0.3179.\n",
      "For Iteration 4613 the Loss is 0.3179.\n",
      "For Iteration 4614 the Loss is 0.3178.\n",
      "For Iteration 4615 the Loss is 0.3178.\n",
      "For Iteration 4616 the Loss is 0.3178.\n",
      "For Iteration 4617 the Loss is 0.3178.\n",
      "For Iteration 4618 the Loss is 0.3178.\n",
      "For Iteration 4619 the Loss is 0.3178.\n",
      "For Iteration 4620 the Loss is 0.3178.\n",
      "For Iteration 4621 the Loss is 0.3178.\n",
      "For Iteration 4622 the Loss is 0.3178.\n",
      "For Iteration 4623 the Loss is 0.3178.\n",
      "For Iteration 4624 the Loss is 0.3178.\n",
      "For Iteration 4625 the Loss is 0.3178.\n",
      "For Iteration 4626 the Loss is 0.3178.\n",
      "For Iteration 4627 the Loss is 0.3178.\n",
      "For Iteration 4628 the Loss is 0.3178.\n",
      "For Iteration 4629 the Loss is 0.3177.\n",
      "For Iteration 4630 the Loss is 0.3177.\n",
      "For Iteration 4631 the Loss is 0.3177.\n",
      "For Iteration 4632 the Loss is 0.3177.\n",
      "For Iteration 4633 the Loss is 0.3177.\n",
      "For Iteration 4634 the Loss is 0.3177.\n",
      "For Iteration 4635 the Loss is 0.3177.\n",
      "For Iteration 4636 the Loss is 0.3177.\n",
      "For Iteration 4637 the Loss is 0.3177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 4638 the Loss is 0.3177.\n",
      "For Iteration 4639 the Loss is 0.3177.\n",
      "For Iteration 4640 the Loss is 0.3177.\n",
      "For Iteration 4641 the Loss is 0.3177.\n",
      "For Iteration 4642 the Loss is 0.3177.\n",
      "For Iteration 4643 the Loss is 0.3177.\n",
      "For Iteration 4644 the Loss is 0.3176.\n",
      "For Iteration 4645 the Loss is 0.3176.\n",
      "For Iteration 4646 the Loss is 0.3176.\n",
      "For Iteration 4647 the Loss is 0.3176.\n",
      "For Iteration 4648 the Loss is 0.3176.\n",
      "For Iteration 4649 the Loss is 0.3176.\n",
      "For Iteration 4650 the Loss is 0.3176.\n",
      "For Iteration 4651 the Loss is 0.3176.\n",
      "For Iteration 4652 the Loss is 0.3176.\n",
      "For Iteration 4653 the Loss is 0.3176.\n",
      "For Iteration 4654 the Loss is 0.3176.\n",
      "For Iteration 4655 the Loss is 0.3176.\n",
      "For Iteration 4656 the Loss is 0.3176.\n",
      "For Iteration 4657 the Loss is 0.3176.\n",
      "For Iteration 4658 the Loss is 0.3176.\n",
      "For Iteration 4659 the Loss is 0.3175.\n",
      "For Iteration 4660 the Loss is 0.3175.\n",
      "For Iteration 4661 the Loss is 0.3175.\n",
      "For Iteration 4662 the Loss is 0.3175.\n",
      "For Iteration 4663 the Loss is 0.3175.\n",
      "For Iteration 4664 the Loss is 0.3175.\n",
      "For Iteration 4665 the Loss is 0.3175.\n",
      "For Iteration 4666 the Loss is 0.3175.\n",
      "For Iteration 4667 the Loss is 0.3175.\n",
      "For Iteration 4668 the Loss is 0.3175.\n",
      "For Iteration 4669 the Loss is 0.3175.\n",
      "For Iteration 4670 the Loss is 0.3175.\n",
      "For Iteration 4671 the Loss is 0.3175.\n",
      "For Iteration 4672 the Loss is 0.3175.\n",
      "For Iteration 4673 the Loss is 0.3175.\n",
      "For Iteration 4674 the Loss is 0.3174.\n",
      "For Iteration 4675 the Loss is 0.3174.\n",
      "For Iteration 4676 the Loss is 0.3174.\n",
      "For Iteration 4677 the Loss is 0.3174.\n",
      "For Iteration 4678 the Loss is 0.3174.\n",
      "For Iteration 4679 the Loss is 0.3174.\n",
      "For Iteration 4680 the Loss is 0.3174.\n",
      "For Iteration 4681 the Loss is 0.3174.\n",
      "For Iteration 4682 the Loss is 0.3174.\n",
      "For Iteration 4683 the Loss is 0.3174.\n",
      "For Iteration 4684 the Loss is 0.3174.\n",
      "For Iteration 4685 the Loss is 0.3174.\n",
      "For Iteration 4686 the Loss is 0.3174.\n",
      "For Iteration 4687 the Loss is 0.3174.\n",
      "For Iteration 4688 the Loss is 0.3174.\n",
      "For Iteration 4689 the Loss is 0.3173.\n",
      "For Iteration 4690 the Loss is 0.3173.\n",
      "For Iteration 4691 the Loss is 0.3173.\n",
      "For Iteration 4692 the Loss is 0.3173.\n",
      "For Iteration 4693 the Loss is 0.3173.\n",
      "For Iteration 4694 the Loss is 0.3173.\n",
      "For Iteration 4695 the Loss is 0.3173.\n",
      "For Iteration 4696 the Loss is 0.3173.\n",
      "For Iteration 4697 the Loss is 0.3173.\n",
      "For Iteration 4698 the Loss is 0.3173.\n",
      "For Iteration 4699 the Loss is 0.3173.\n",
      "For Iteration 4700 the Loss is 0.3173.\n",
      "For Iteration 4701 the Loss is 0.3173.\n",
      "For Iteration 4702 the Loss is 0.3173.\n",
      "For Iteration 4703 the Loss is 0.3173.\n",
      "For Iteration 4704 the Loss is 0.3172.\n",
      "For Iteration 4705 the Loss is 0.3172.\n",
      "For Iteration 4706 the Loss is 0.3172.\n",
      "For Iteration 4707 the Loss is 0.3172.\n",
      "For Iteration 4708 the Loss is 0.3172.\n",
      "For Iteration 4709 the Loss is 0.3172.\n",
      "For Iteration 4710 the Loss is 0.3172.\n",
      "For Iteration 4711 the Loss is 0.3172.\n",
      "For Iteration 4712 the Loss is 0.3172.\n",
      "For Iteration 4713 the Loss is 0.3172.\n",
      "For Iteration 4714 the Loss is 0.3172.\n",
      "For Iteration 4715 the Loss is 0.3172.\n",
      "For Iteration 4716 the Loss is 0.3172.\n",
      "For Iteration 4717 the Loss is 0.3172.\n",
      "For Iteration 4718 the Loss is 0.3172.\n",
      "For Iteration 4719 the Loss is 0.3171.\n",
      "For Iteration 4720 the Loss is 0.3171.\n",
      "For Iteration 4721 the Loss is 0.3171.\n",
      "For Iteration 4722 the Loss is 0.3171.\n",
      "For Iteration 4723 the Loss is 0.3171.\n",
      "For Iteration 4724 the Loss is 0.3171.\n",
      "For Iteration 4725 the Loss is 0.3171.\n",
      "For Iteration 4726 the Loss is 0.3171.\n",
      "For Iteration 4727 the Loss is 0.3171.\n",
      "For Iteration 4728 the Loss is 0.3171.\n",
      "For Iteration 4729 the Loss is 0.3171.\n",
      "For Iteration 4730 the Loss is 0.3171.\n",
      "For Iteration 4731 the Loss is 0.3171.\n",
      "For Iteration 4732 the Loss is 0.3171.\n",
      "For Iteration 4733 the Loss is 0.3171.\n",
      "For Iteration 4734 the Loss is 0.3171.\n",
      "For Iteration 4735 the Loss is 0.317.\n",
      "For Iteration 4736 the Loss is 0.317.\n",
      "For Iteration 4737 the Loss is 0.317.\n",
      "For Iteration 4738 the Loss is 0.317.\n",
      "For Iteration 4739 the Loss is 0.317.\n",
      "For Iteration 4740 the Loss is 0.317.\n",
      "For Iteration 4741 the Loss is 0.317.\n",
      "For Iteration 4742 the Loss is 0.317.\n",
      "For Iteration 4743 the Loss is 0.317.\n",
      "For Iteration 4744 the Loss is 0.317.\n",
      "For Iteration 4745 the Loss is 0.317.\n",
      "For Iteration 4746 the Loss is 0.317.\n",
      "For Iteration 4747 the Loss is 0.317.\n",
      "For Iteration 4748 the Loss is 0.317.\n",
      "For Iteration 4749 the Loss is 0.317.\n",
      "For Iteration 4750 the Loss is 0.3169.\n",
      "For Iteration 4751 the Loss is 0.3169.\n",
      "For Iteration 4752 the Loss is 0.3169.\n",
      "For Iteration 4753 the Loss is 0.3169.\n",
      "For Iteration 4754 the Loss is 0.3169.\n",
      "For Iteration 4755 the Loss is 0.3169.\n",
      "For Iteration 4756 the Loss is 0.3169.\n",
      "For Iteration 4757 the Loss is 0.3169.\n",
      "For Iteration 4758 the Loss is 0.3169.\n",
      "For Iteration 4759 the Loss is 0.3169.\n",
      "For Iteration 4760 the Loss is 0.3169.\n",
      "For Iteration 4761 the Loss is 0.3169.\n",
      "For Iteration 4762 the Loss is 0.3169.\n",
      "For Iteration 4763 the Loss is 0.3169.\n",
      "For Iteration 4764 the Loss is 0.3169.\n",
      "For Iteration 4765 the Loss is 0.3169.\n",
      "For Iteration 4766 the Loss is 0.3168.\n",
      "For Iteration 4767 the Loss is 0.3168.\n",
      "For Iteration 4768 the Loss is 0.3168.\n",
      "For Iteration 4769 the Loss is 0.3168.\n",
      "For Iteration 4770 the Loss is 0.3168.\n",
      "For Iteration 4771 the Loss is 0.3168.\n",
      "For Iteration 4772 the Loss is 0.3168.\n",
      "For Iteration 4773 the Loss is 0.3168.\n",
      "For Iteration 4774 the Loss is 0.3168.\n",
      "For Iteration 4775 the Loss is 0.3168.\n",
      "For Iteration 4776 the Loss is 0.3168.\n",
      "For Iteration 4777 the Loss is 0.3168.\n",
      "For Iteration 4778 the Loss is 0.3168.\n",
      "For Iteration 4779 the Loss is 0.3168.\n",
      "For Iteration 4780 the Loss is 0.3168.\n",
      "For Iteration 4781 the Loss is 0.3167.\n",
      "For Iteration 4782 the Loss is 0.3167.\n",
      "For Iteration 4783 the Loss is 0.3167.\n",
      "For Iteration 4784 the Loss is 0.3167.\n",
      "For Iteration 4785 the Loss is 0.3167.\n",
      "For Iteration 4786 the Loss is 0.3167.\n",
      "For Iteration 4787 the Loss is 0.3167.\n",
      "For Iteration 4788 the Loss is 0.3167.\n",
      "For Iteration 4789 the Loss is 0.3167.\n",
      "For Iteration 4790 the Loss is 0.3167.\n",
      "For Iteration 4791 the Loss is 0.3167.\n",
      "For Iteration 4792 the Loss is 0.3167.\n",
      "For Iteration 4793 the Loss is 0.3167.\n",
      "For Iteration 4794 the Loss is 0.3167.\n",
      "For Iteration 4795 the Loss is 0.3167.\n",
      "For Iteration 4796 the Loss is 0.3167.\n",
      "For Iteration 4797 the Loss is 0.3166.\n",
      "For Iteration 4798 the Loss is 0.3166.\n",
      "For Iteration 4799 the Loss is 0.3166.\n",
      "For Iteration 4800 the Loss is 0.3166.\n",
      "For Iteration 4801 the Loss is 0.3166.\n",
      "For Iteration 4802 the Loss is 0.3166.\n",
      "For Iteration 4803 the Loss is 0.3166.\n",
      "For Iteration 4804 the Loss is 0.3166.\n",
      "For Iteration 4805 the Loss is 0.3166.\n",
      "For Iteration 4806 the Loss is 0.3166.\n",
      "For Iteration 4807 the Loss is 0.3166.\n",
      "For Iteration 4808 the Loss is 0.3166.\n",
      "For Iteration 4809 the Loss is 0.3166.\n",
      "For Iteration 4810 the Loss is 0.3166.\n",
      "For Iteration 4811 the Loss is 0.3166.\n",
      "For Iteration 4812 the Loss is 0.3165.\n",
      "For Iteration 4813 the Loss is 0.3165.\n",
      "For Iteration 4814 the Loss is 0.3165.\n",
      "For Iteration 4815 the Loss is 0.3165.\n",
      "For Iteration 4816 the Loss is 0.3165.\n",
      "For Iteration 4817 the Loss is 0.3165.\n",
      "For Iteration 4818 the Loss is 0.3165.\n",
      "For Iteration 4819 the Loss is 0.3165.\n",
      "For Iteration 4820 the Loss is 0.3165.\n",
      "For Iteration 4821 the Loss is 0.3165.\n",
      "For Iteration 4822 the Loss is 0.3165.\n",
      "For Iteration 4823 the Loss is 0.3165.\n",
      "For Iteration 4824 the Loss is 0.3165.\n",
      "For Iteration 4825 the Loss is 0.3165.\n",
      "For Iteration 4826 the Loss is 0.3165.\n",
      "For Iteration 4827 the Loss is 0.3165.\n",
      "For Iteration 4828 the Loss is 0.3164.\n",
      "For Iteration 4829 the Loss is 0.3164.\n",
      "For Iteration 4830 the Loss is 0.3164.\n",
      "For Iteration 4831 the Loss is 0.3164.\n",
      "For Iteration 4832 the Loss is 0.3164.\n",
      "For Iteration 4833 the Loss is 0.3164.\n",
      "For Iteration 4834 the Loss is 0.3164.\n",
      "For Iteration 4835 the Loss is 0.3164.\n",
      "For Iteration 4836 the Loss is 0.3164.\n",
      "For Iteration 4837 the Loss is 0.3164.\n",
      "For Iteration 4838 the Loss is 0.3164.\n",
      "For Iteration 4839 the Loss is 0.3164.\n",
      "For Iteration 4840 the Loss is 0.3164.\n",
      "For Iteration 4841 the Loss is 0.3164.\n",
      "For Iteration 4842 the Loss is 0.3164.\n",
      "For Iteration 4843 the Loss is 0.3164.\n",
      "For Iteration 4844 the Loss is 0.3163.\n",
      "For Iteration 4845 the Loss is 0.3163.\n",
      "For Iteration 4846 the Loss is 0.3163.\n",
      "For Iteration 4847 the Loss is 0.3163.\n",
      "For Iteration 4848 the Loss is 0.3163.\n",
      "For Iteration 4849 the Loss is 0.3163.\n",
      "For Iteration 4850 the Loss is 0.3163.\n",
      "For Iteration 4851 the Loss is 0.3163.\n",
      "For Iteration 4852 the Loss is 0.3163.\n",
      "For Iteration 4853 the Loss is 0.3163.\n",
      "For Iteration 4854 the Loss is 0.3163.\n",
      "For Iteration 4855 the Loss is 0.3163.\n",
      "For Iteration 4856 the Loss is 0.3163.\n",
      "For Iteration 4857 the Loss is 0.3163.\n",
      "For Iteration 4858 the Loss is 0.3163.\n",
      "For Iteration 4859 the Loss is 0.3162.\n",
      "For Iteration 4860 the Loss is 0.3162.\n",
      "For Iteration 4861 the Loss is 0.3162.\n",
      "For Iteration 4862 the Loss is 0.3162.\n",
      "For Iteration 4863 the Loss is 0.3162.\n",
      "For Iteration 4864 the Loss is 0.3162.\n",
      "For Iteration 4865 the Loss is 0.3162.\n",
      "For Iteration 4866 the Loss is 0.3162.\n",
      "For Iteration 4867 the Loss is 0.3162.\n",
      "For Iteration 4868 the Loss is 0.3162.\n",
      "For Iteration 4869 the Loss is 0.3162.\n",
      "For Iteration 4870 the Loss is 0.3162.\n",
      "For Iteration 4871 the Loss is 0.3162.\n",
      "For Iteration 4872 the Loss is 0.3162.\n",
      "For Iteration 4873 the Loss is 0.3162.\n",
      "For Iteration 4874 the Loss is 0.3162.\n",
      "For Iteration 4875 the Loss is 0.3161.\n",
      "For Iteration 4876 the Loss is 0.3161.\n",
      "For Iteration 4877 the Loss is 0.3161.\n",
      "For Iteration 4878 the Loss is 0.3161.\n",
      "For Iteration 4879 the Loss is 0.3161.\n",
      "For Iteration 4880 the Loss is 0.3161.\n",
      "For Iteration 4881 the Loss is 0.3161.\n",
      "For Iteration 4882 the Loss is 0.3161.\n",
      "For Iteration 4883 the Loss is 0.3161.\n",
      "For Iteration 4884 the Loss is 0.3161.\n",
      "For Iteration 4885 the Loss is 0.3161.\n",
      "For Iteration 4886 the Loss is 0.3161.\n",
      "For Iteration 4887 the Loss is 0.3161.\n",
      "For Iteration 4888 the Loss is 0.3161.\n",
      "For Iteration 4889 the Loss is 0.3161.\n",
      "For Iteration 4890 the Loss is 0.3161.\n",
      "For Iteration 4891 the Loss is 0.316.\n",
      "For Iteration 4892 the Loss is 0.316.\n",
      "For Iteration 4893 the Loss is 0.316.\n",
      "For Iteration 4894 the Loss is 0.316.\n",
      "For Iteration 4895 the Loss is 0.316.\n",
      "For Iteration 4896 the Loss is 0.316.\n",
      "For Iteration 4897 the Loss is 0.316.\n",
      "For Iteration 4898 the Loss is 0.316.\n",
      "For Iteration 4899 the Loss is 0.316.\n",
      "For Iteration 4900 the Loss is 0.316.\n",
      "For Iteration 4901 the Loss is 0.316.\n",
      "For Iteration 4902 the Loss is 0.316.\n",
      "For Iteration 4903 the Loss is 0.316.\n",
      "For Iteration 4904 the Loss is 0.316.\n",
      "For Iteration 4905 the Loss is 0.316.\n",
      "For Iteration 4906 the Loss is 0.316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 4907 the Loss is 0.3159.\n",
      "For Iteration 4908 the Loss is 0.3159.\n",
      "For Iteration 4909 the Loss is 0.3159.\n",
      "For Iteration 4910 the Loss is 0.3159.\n",
      "For Iteration 4911 the Loss is 0.3159.\n",
      "For Iteration 4912 the Loss is 0.3159.\n",
      "For Iteration 4913 the Loss is 0.3159.\n",
      "For Iteration 4914 the Loss is 0.3159.\n",
      "For Iteration 4915 the Loss is 0.3159.\n",
      "For Iteration 4916 the Loss is 0.3159.\n",
      "For Iteration 4917 the Loss is 0.3159.\n",
      "For Iteration 4918 the Loss is 0.3159.\n",
      "For Iteration 4919 the Loss is 0.3159.\n",
      "For Iteration 4920 the Loss is 0.3159.\n",
      "For Iteration 4921 the Loss is 0.3159.\n",
      "For Iteration 4922 the Loss is 0.3159.\n",
      "For Iteration 4923 the Loss is 0.3158.\n",
      "For Iteration 4924 the Loss is 0.3158.\n",
      "For Iteration 4925 the Loss is 0.3158.\n",
      "For Iteration 4926 the Loss is 0.3158.\n",
      "For Iteration 4927 the Loss is 0.3158.\n",
      "For Iteration 4928 the Loss is 0.3158.\n",
      "For Iteration 4929 the Loss is 0.3158.\n",
      "For Iteration 4930 the Loss is 0.3158.\n",
      "For Iteration 4931 the Loss is 0.3158.\n",
      "For Iteration 4932 the Loss is 0.3158.\n",
      "For Iteration 4933 the Loss is 0.3158.\n",
      "For Iteration 4934 the Loss is 0.3158.\n",
      "For Iteration 4935 the Loss is 0.3158.\n",
      "For Iteration 4936 the Loss is 0.3158.\n",
      "For Iteration 4937 the Loss is 0.3158.\n",
      "For Iteration 4938 the Loss is 0.3158.\n",
      "For Iteration 4939 the Loss is 0.3157.\n",
      "For Iteration 4940 the Loss is 0.3157.\n",
      "For Iteration 4941 the Loss is 0.3157.\n",
      "For Iteration 4942 the Loss is 0.3157.\n",
      "For Iteration 4943 the Loss is 0.3157.\n",
      "For Iteration 4944 the Loss is 0.3157.\n",
      "For Iteration 4945 the Loss is 0.3157.\n",
      "For Iteration 4946 the Loss is 0.3157.\n",
      "For Iteration 4947 the Loss is 0.3157.\n",
      "For Iteration 4948 the Loss is 0.3157.\n",
      "For Iteration 4949 the Loss is 0.3157.\n",
      "For Iteration 4950 the Loss is 0.3157.\n",
      "For Iteration 4951 the Loss is 0.3157.\n",
      "For Iteration 4952 the Loss is 0.3157.\n",
      "For Iteration 4953 the Loss is 0.3157.\n",
      "For Iteration 4954 the Loss is 0.3157.\n",
      "For Iteration 4955 the Loss is 0.3156.\n",
      "For Iteration 4956 the Loss is 0.3156.\n",
      "For Iteration 4957 the Loss is 0.3156.\n",
      "For Iteration 4958 the Loss is 0.3156.\n",
      "For Iteration 4959 the Loss is 0.3156.\n",
      "For Iteration 4960 the Loss is 0.3156.\n",
      "For Iteration 4961 the Loss is 0.3156.\n",
      "For Iteration 4962 the Loss is 0.3156.\n",
      "For Iteration 4963 the Loss is 0.3156.\n",
      "For Iteration 4964 the Loss is 0.3156.\n",
      "For Iteration 4965 the Loss is 0.3156.\n",
      "For Iteration 4966 the Loss is 0.3156.\n",
      "For Iteration 4967 the Loss is 0.3156.\n",
      "For Iteration 4968 the Loss is 0.3156.\n",
      "For Iteration 4969 the Loss is 0.3156.\n",
      "For Iteration 4970 the Loss is 0.3156.\n",
      "For Iteration 4971 the Loss is 0.3155.\n",
      "For Iteration 4972 the Loss is 0.3155.\n",
      "For Iteration 4973 the Loss is 0.3155.\n",
      "For Iteration 4974 the Loss is 0.3155.\n",
      "For Iteration 4975 the Loss is 0.3155.\n",
      "For Iteration 4976 the Loss is 0.3155.\n",
      "For Iteration 4977 the Loss is 0.3155.\n",
      "For Iteration 4978 the Loss is 0.3155.\n",
      "For Iteration 4979 the Loss is 0.3155.\n",
      "For Iteration 4980 the Loss is 0.3155.\n",
      "For Iteration 4981 the Loss is 0.3155.\n",
      "For Iteration 4982 the Loss is 0.3155.\n",
      "For Iteration 4983 the Loss is 0.3155.\n",
      "For Iteration 4984 the Loss is 0.3155.\n",
      "For Iteration 4985 the Loss is 0.3155.\n",
      "For Iteration 4986 the Loss is 0.3155.\n",
      "For Iteration 4987 the Loss is 0.3154.\n",
      "For Iteration 4988 the Loss is 0.3154.\n",
      "For Iteration 4989 the Loss is 0.3154.\n",
      "For Iteration 4990 the Loss is 0.3154.\n",
      "For Iteration 4991 the Loss is 0.3154.\n",
      "For Iteration 4992 the Loss is 0.3154.\n",
      "For Iteration 4993 the Loss is 0.3154.\n",
      "For Iteration 4994 the Loss is 0.3154.\n",
      "For Iteration 4995 the Loss is 0.3154.\n",
      "For Iteration 4996 the Loss is 0.3154.\n",
      "For Iteration 4997 the Loss is 0.3154.\n",
      "For Iteration 4998 the Loss is 0.3154.\n",
      "For Iteration 4999 the Loss is 0.3154.\n"
     ]
    }
   ],
   "source": [
    "model1=LogisticRegression(learning_rate=.003, itr=5000)\n",
    "model1.fit(train_x,train_y)\n",
    "#t=model1.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "93ae21ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(test_y,model1.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "0f426623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26102bb6a90>]"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model1.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "a2f9e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Hasan_Hussain_assignment1_part_1', 'wb') as files:\n",
    "    pickle.dump(model1, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "bfaedb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Hasan_Hussain_assignment1_part_1' , 'rb') as f:\n",
    "    lr1= pickle.load(f)\n",
    "\n",
    "accuracy(test_y,lr1.predict(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f94c4c",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "feb50587",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y,test_x,test_y=train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "b1e00455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 0 the Loss is 4.4981.\n",
      "For Iteration 1 the Loss is 4.4926.\n",
      "For Iteration 2 the Loss is 4.4871.\n",
      "For Iteration 3 the Loss is 4.4816.\n",
      "For Iteration 4 the Loss is 4.476.\n",
      "For Iteration 5 the Loss is 4.4705.\n",
      "For Iteration 6 the Loss is 4.465.\n",
      "For Iteration 7 the Loss is 4.4595.\n",
      "For Iteration 8 the Loss is 4.454.\n",
      "For Iteration 9 the Loss is 4.4485.\n",
      "For Iteration 10 the Loss is 4.443.\n",
      "For Iteration 11 the Loss is 4.4375.\n",
      "For Iteration 12 the Loss is 4.4319.\n",
      "For Iteration 13 the Loss is 4.4264.\n",
      "For Iteration 14 the Loss is 4.4209.\n",
      "For Iteration 15 the Loss is 4.4154.\n",
      "For Iteration 16 the Loss is 4.4099.\n",
      "For Iteration 17 the Loss is 4.4044.\n",
      "For Iteration 18 the Loss is 4.3989.\n",
      "For Iteration 19 the Loss is 4.3934.\n",
      "For Iteration 20 the Loss is 4.3879.\n",
      "For Iteration 21 the Loss is 4.3824.\n",
      "For Iteration 22 the Loss is 4.3769.\n",
      "For Iteration 23 the Loss is 4.3714.\n",
      "For Iteration 24 the Loss is 4.3659.\n",
      "For Iteration 25 the Loss is 4.3604.\n",
      "For Iteration 26 the Loss is 4.3549.\n",
      "For Iteration 27 the Loss is 4.3494.\n",
      "For Iteration 28 the Loss is 4.3439.\n",
      "For Iteration 29 the Loss is 4.3384.\n",
      "For Iteration 30 the Loss is 4.3329.\n",
      "For Iteration 31 the Loss is 4.3274.\n",
      "For Iteration 32 the Loss is 4.3219.\n",
      "For Iteration 33 the Loss is 4.3164.\n",
      "For Iteration 34 the Loss is 4.3109.\n",
      "For Iteration 35 the Loss is 4.3054.\n",
      "For Iteration 36 the Loss is 4.2999.\n",
      "For Iteration 37 the Loss is 4.2944.\n",
      "For Iteration 38 the Loss is 4.2889.\n",
      "For Iteration 39 the Loss is 4.2834.\n",
      "For Iteration 40 the Loss is 4.2779.\n",
      "For Iteration 41 the Loss is 4.2724.\n",
      "For Iteration 42 the Loss is 4.2669.\n",
      "For Iteration 43 the Loss is 4.2614.\n",
      "For Iteration 44 the Loss is 4.2559.\n",
      "For Iteration 45 the Loss is 4.2505.\n",
      "For Iteration 46 the Loss is 4.245.\n",
      "For Iteration 47 the Loss is 4.2395.\n",
      "For Iteration 48 the Loss is 4.234.\n",
      "For Iteration 49 the Loss is 4.2285.\n",
      "For Iteration 50 the Loss is 4.223.\n",
      "For Iteration 51 the Loss is 4.2175.\n",
      "For Iteration 52 the Loss is 4.2121.\n",
      "For Iteration 53 the Loss is 4.2066.\n",
      "For Iteration 54 the Loss is 4.2011.\n",
      "For Iteration 55 the Loss is 4.1956.\n",
      "For Iteration 56 the Loss is 4.1901.\n",
      "For Iteration 57 the Loss is 4.1846.\n",
      "For Iteration 58 the Loss is 4.1792.\n",
      "For Iteration 59 the Loss is 4.1737.\n",
      "For Iteration 60 the Loss is 4.1682.\n",
      "For Iteration 61 the Loss is 4.1627.\n",
      "For Iteration 62 the Loss is 4.1573.\n",
      "For Iteration 63 the Loss is 4.1518.\n",
      "For Iteration 64 the Loss is 4.1463.\n",
      "For Iteration 65 the Loss is 4.1408.\n",
      "For Iteration 66 the Loss is 4.1354.\n",
      "For Iteration 67 the Loss is 4.1299.\n",
      "For Iteration 68 the Loss is 4.1244.\n",
      "For Iteration 69 the Loss is 4.1189.\n",
      "For Iteration 70 the Loss is 4.1135.\n",
      "For Iteration 71 the Loss is 4.108.\n",
      "For Iteration 72 the Loss is 4.1025.\n",
      "For Iteration 73 the Loss is 4.0971.\n",
      "For Iteration 74 the Loss is 4.0916.\n",
      "For Iteration 75 the Loss is 4.0861.\n",
      "For Iteration 76 the Loss is 4.0807.\n",
      "For Iteration 77 the Loss is 4.0752.\n",
      "For Iteration 78 the Loss is 4.0697.\n",
      "For Iteration 79 the Loss is 4.0643.\n",
      "For Iteration 80 the Loss is 4.0588.\n",
      "For Iteration 81 the Loss is 4.0534.\n",
      "For Iteration 82 the Loss is 4.0479.\n",
      "For Iteration 83 the Loss is 4.0424.\n",
      "For Iteration 84 the Loss is 4.037.\n",
      "For Iteration 85 the Loss is 4.0315.\n",
      "For Iteration 86 the Loss is 4.0261.\n",
      "For Iteration 87 the Loss is 4.0206.\n",
      "For Iteration 88 the Loss is 4.0152.\n",
      "For Iteration 89 the Loss is 4.0097.\n",
      "For Iteration 90 the Loss is 4.0042.\n",
      "For Iteration 91 the Loss is 3.9988.\n",
      "For Iteration 92 the Loss is 3.9933.\n",
      "For Iteration 93 the Loss is 3.9879.\n",
      "For Iteration 94 the Loss is 3.9824.\n",
      "For Iteration 95 the Loss is 3.977.\n",
      "For Iteration 96 the Loss is 3.9716.\n",
      "For Iteration 97 the Loss is 3.9661.\n",
      "For Iteration 98 the Loss is 3.9607.\n",
      "For Iteration 99 the Loss is 3.9552.\n",
      "For Iteration 100 the Loss is 3.9498.\n",
      "For Iteration 101 the Loss is 3.9443.\n",
      "For Iteration 102 the Loss is 3.9389.\n",
      "For Iteration 103 the Loss is 3.9334.\n",
      "For Iteration 104 the Loss is 3.928.\n",
      "For Iteration 105 the Loss is 3.9226.\n",
      "For Iteration 106 the Loss is 3.9171.\n",
      "For Iteration 107 the Loss is 3.9117.\n",
      "For Iteration 108 the Loss is 3.9063.\n",
      "For Iteration 109 the Loss is 3.9008.\n",
      "For Iteration 110 the Loss is 3.8954.\n",
      "For Iteration 111 the Loss is 3.89.\n",
      "For Iteration 112 the Loss is 3.8845.\n",
      "For Iteration 113 the Loss is 3.8791.\n",
      "For Iteration 114 the Loss is 3.8737.\n",
      "For Iteration 115 the Loss is 3.8682.\n",
      "For Iteration 116 the Loss is 3.8628.\n",
      "For Iteration 117 the Loss is 3.8574.\n",
      "For Iteration 118 the Loss is 3.852.\n",
      "For Iteration 119 the Loss is 3.8465.\n",
      "For Iteration 120 the Loss is 3.8411.\n",
      "For Iteration 121 the Loss is 3.8357.\n",
      "For Iteration 122 the Loss is 3.8303.\n",
      "For Iteration 123 the Loss is 3.8248.\n",
      "For Iteration 124 the Loss is 3.8194.\n",
      "For Iteration 125 the Loss is 3.814.\n",
      "For Iteration 126 the Loss is 3.8086.\n",
      "For Iteration 127 the Loss is 3.8032.\n",
      "For Iteration 128 the Loss is 3.7978.\n",
      "For Iteration 129 the Loss is 3.7923.\n",
      "For Iteration 130 the Loss is 3.7869.\n",
      "For Iteration 131 the Loss is 3.7815.\n",
      "For Iteration 132 the Loss is 3.7761.\n",
      "For Iteration 133 the Loss is 3.7707.\n",
      "For Iteration 134 the Loss is 3.7653.\n",
      "For Iteration 135 the Loss is 3.7599.\n",
      "For Iteration 136 the Loss is 3.7545.\n",
      "For Iteration 137 the Loss is 3.7491.\n",
      "For Iteration 138 the Loss is 3.7437.\n",
      "For Iteration 139 the Loss is 3.7383.\n",
      "For Iteration 140 the Loss is 3.7329.\n",
      "For Iteration 141 the Loss is 3.7275.\n",
      "For Iteration 142 the Loss is 3.7221.\n",
      "For Iteration 143 the Loss is 3.7167.\n",
      "For Iteration 144 the Loss is 3.7113.\n",
      "For Iteration 145 the Loss is 3.7059.\n",
      "For Iteration 146 the Loss is 3.7005.\n",
      "For Iteration 147 the Loss is 3.6951.\n",
      "For Iteration 148 the Loss is 3.6897.\n",
      "For Iteration 149 the Loss is 3.6843.\n",
      "For Iteration 150 the Loss is 3.6789.\n",
      "For Iteration 151 the Loss is 3.6735.\n",
      "For Iteration 152 the Loss is 3.6681.\n",
      "For Iteration 153 the Loss is 3.6628.\n",
      "For Iteration 154 the Loss is 3.6574.\n",
      "For Iteration 155 the Loss is 3.652.\n",
      "For Iteration 156 the Loss is 3.6466.\n",
      "For Iteration 157 the Loss is 3.6412.\n",
      "For Iteration 158 the Loss is 3.6358.\n",
      "For Iteration 159 the Loss is 3.6305.\n",
      "For Iteration 160 the Loss is 3.6251.\n",
      "For Iteration 161 the Loss is 3.6197.\n",
      "For Iteration 162 the Loss is 3.6143.\n",
      "For Iteration 163 the Loss is 3.609.\n",
      "For Iteration 164 the Loss is 3.6036.\n",
      "For Iteration 165 the Loss is 3.5982.\n",
      "For Iteration 166 the Loss is 3.5929.\n",
      "For Iteration 167 the Loss is 3.5875.\n",
      "For Iteration 168 the Loss is 3.5821.\n",
      "For Iteration 169 the Loss is 3.5768.\n",
      "For Iteration 170 the Loss is 3.5714.\n",
      "For Iteration 171 the Loss is 3.566.\n",
      "For Iteration 172 the Loss is 3.5607.\n",
      "For Iteration 173 the Loss is 3.5553.\n",
      "For Iteration 174 the Loss is 3.55.\n",
      "For Iteration 175 the Loss is 3.5446.\n",
      "For Iteration 176 the Loss is 3.5392.\n",
      "For Iteration 177 the Loss is 3.5339.\n",
      "For Iteration 178 the Loss is 3.5285.\n",
      "For Iteration 179 the Loss is 3.5232.\n",
      "For Iteration 180 the Loss is 3.5178.\n",
      "For Iteration 181 the Loss is 3.5125.\n",
      "For Iteration 182 the Loss is 3.5071.\n",
      "For Iteration 183 the Loss is 3.5018.\n",
      "For Iteration 184 the Loss is 3.4965.\n",
      "For Iteration 185 the Loss is 3.4911.\n",
      "For Iteration 186 the Loss is 3.4858.\n",
      "For Iteration 187 the Loss is 3.4804.\n",
      "For Iteration 188 the Loss is 3.4751.\n",
      "For Iteration 189 the Loss is 3.4698.\n",
      "For Iteration 190 the Loss is 3.4644.\n",
      "For Iteration 191 the Loss is 3.4591.\n",
      "For Iteration 192 the Loss is 3.4538.\n",
      "For Iteration 193 the Loss is 3.4484.\n",
      "For Iteration 194 the Loss is 3.4431.\n",
      "For Iteration 195 the Loss is 3.4378.\n",
      "For Iteration 196 the Loss is 3.4324.\n",
      "For Iteration 197 the Loss is 3.4271.\n",
      "For Iteration 198 the Loss is 3.4218.\n",
      "For Iteration 199 the Loss is 3.4165.\n",
      "For Iteration 200 the Loss is 3.4112.\n",
      "For Iteration 201 the Loss is 3.4058.\n",
      "For Iteration 202 the Loss is 3.4005.\n",
      "For Iteration 203 the Loss is 3.3952.\n",
      "For Iteration 204 the Loss is 3.3899.\n",
      "For Iteration 205 the Loss is 3.3846.\n",
      "For Iteration 206 the Loss is 3.3793.\n",
      "For Iteration 207 the Loss is 3.374.\n",
      "For Iteration 208 the Loss is 3.3687.\n",
      "For Iteration 209 the Loss is 3.3634.\n",
      "For Iteration 210 the Loss is 3.3581.\n",
      "For Iteration 211 the Loss is 3.3528.\n",
      "For Iteration 212 the Loss is 3.3475.\n",
      "For Iteration 213 the Loss is 3.3422.\n",
      "For Iteration 214 the Loss is 3.3369.\n",
      "For Iteration 215 the Loss is 3.3316.\n",
      "For Iteration 216 the Loss is 3.3263.\n",
      "For Iteration 217 the Loss is 3.321.\n",
      "For Iteration 218 the Loss is 3.3157.\n",
      "For Iteration 219 the Loss is 3.3104.\n",
      "For Iteration 220 the Loss is 3.3051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 221 the Loss is 3.2998.\n",
      "For Iteration 222 the Loss is 3.2946.\n",
      "For Iteration 223 the Loss is 3.2893.\n",
      "For Iteration 224 the Loss is 3.284.\n",
      "For Iteration 225 the Loss is 3.2787.\n",
      "For Iteration 226 the Loss is 3.2735.\n",
      "For Iteration 227 the Loss is 3.2682.\n",
      "For Iteration 228 the Loss is 3.2629.\n",
      "For Iteration 229 the Loss is 3.2576.\n",
      "For Iteration 230 the Loss is 3.2524.\n",
      "For Iteration 231 the Loss is 3.2471.\n",
      "For Iteration 232 the Loss is 3.2418.\n",
      "For Iteration 233 the Loss is 3.2366.\n",
      "For Iteration 234 the Loss is 3.2313.\n",
      "For Iteration 235 the Loss is 3.2261.\n",
      "For Iteration 236 the Loss is 3.2208.\n",
      "For Iteration 237 the Loss is 3.2156.\n",
      "For Iteration 238 the Loss is 3.2103.\n",
      "For Iteration 239 the Loss is 3.205.\n",
      "For Iteration 240 the Loss is 3.1998.\n",
      "For Iteration 241 the Loss is 3.1946.\n",
      "For Iteration 242 the Loss is 3.1893.\n",
      "For Iteration 243 the Loss is 3.1841.\n",
      "For Iteration 244 the Loss is 3.1788.\n",
      "For Iteration 245 the Loss is 3.1736.\n",
      "For Iteration 246 the Loss is 3.1684.\n",
      "For Iteration 247 the Loss is 3.1631.\n",
      "For Iteration 248 the Loss is 3.1579.\n",
      "For Iteration 249 the Loss is 3.1527.\n",
      "For Iteration 250 the Loss is 3.1474.\n",
      "For Iteration 251 the Loss is 3.1422.\n",
      "For Iteration 252 the Loss is 3.137.\n",
      "For Iteration 253 the Loss is 3.1318.\n",
      "For Iteration 254 the Loss is 3.1265.\n",
      "For Iteration 255 the Loss is 3.1213.\n",
      "For Iteration 256 the Loss is 3.1161.\n",
      "For Iteration 257 the Loss is 3.1109.\n",
      "For Iteration 258 the Loss is 3.1057.\n",
      "For Iteration 259 the Loss is 3.1005.\n",
      "For Iteration 260 the Loss is 3.0953.\n",
      "For Iteration 261 the Loss is 3.0901.\n",
      "For Iteration 262 the Loss is 3.0849.\n",
      "For Iteration 263 the Loss is 3.0797.\n",
      "For Iteration 264 the Loss is 3.0745.\n",
      "For Iteration 265 the Loss is 3.0693.\n",
      "For Iteration 266 the Loss is 3.0641.\n",
      "For Iteration 267 the Loss is 3.0589.\n",
      "For Iteration 268 the Loss is 3.0537.\n",
      "For Iteration 269 the Loss is 3.0485.\n",
      "For Iteration 270 the Loss is 3.0433.\n",
      "For Iteration 271 the Loss is 3.0381.\n",
      "For Iteration 272 the Loss is 3.0329.\n",
      "For Iteration 273 the Loss is 3.0278.\n",
      "For Iteration 274 the Loss is 3.0226.\n",
      "For Iteration 275 the Loss is 3.0174.\n",
      "For Iteration 276 the Loss is 3.0123.\n",
      "For Iteration 277 the Loss is 3.0071.\n",
      "For Iteration 278 the Loss is 3.0019.\n",
      "For Iteration 279 the Loss is 2.9968.\n",
      "For Iteration 280 the Loss is 2.9916.\n",
      "For Iteration 281 the Loss is 2.9864.\n",
      "For Iteration 282 the Loss is 2.9813.\n",
      "For Iteration 283 the Loss is 2.9761.\n",
      "For Iteration 284 the Loss is 2.971.\n",
      "For Iteration 285 the Loss is 2.9658.\n",
      "For Iteration 286 the Loss is 2.9607.\n",
      "For Iteration 287 the Loss is 2.9555.\n",
      "For Iteration 288 the Loss is 2.9504.\n",
      "For Iteration 289 the Loss is 2.9452.\n",
      "For Iteration 290 the Loss is 2.9401.\n",
      "For Iteration 291 the Loss is 2.935.\n",
      "For Iteration 292 the Loss is 2.9298.\n",
      "For Iteration 293 the Loss is 2.9247.\n",
      "For Iteration 294 the Loss is 2.9196.\n",
      "For Iteration 295 the Loss is 2.9145.\n",
      "For Iteration 296 the Loss is 2.9093.\n",
      "For Iteration 297 the Loss is 2.9042.\n",
      "For Iteration 298 the Loss is 2.8991.\n",
      "For Iteration 299 the Loss is 2.894.\n",
      "For Iteration 300 the Loss is 2.8889.\n",
      "For Iteration 301 the Loss is 2.8838.\n",
      "For Iteration 302 the Loss is 2.8787.\n",
      "For Iteration 303 the Loss is 2.8736.\n",
      "For Iteration 304 the Loss is 2.8685.\n",
      "For Iteration 305 the Loss is 2.8634.\n",
      "For Iteration 306 the Loss is 2.8583.\n",
      "For Iteration 307 the Loss is 2.8532.\n",
      "For Iteration 308 the Loss is 2.8481.\n",
      "For Iteration 309 the Loss is 2.843.\n",
      "For Iteration 310 the Loss is 2.8379.\n",
      "For Iteration 311 the Loss is 2.8328.\n",
      "For Iteration 312 the Loss is 2.8277.\n",
      "For Iteration 313 the Loss is 2.8227.\n",
      "For Iteration 314 the Loss is 2.8176.\n",
      "For Iteration 315 the Loss is 2.8125.\n",
      "For Iteration 316 the Loss is 2.8075.\n",
      "For Iteration 317 the Loss is 2.8024.\n",
      "For Iteration 318 the Loss is 2.7973.\n",
      "For Iteration 319 the Loss is 2.7923.\n",
      "For Iteration 320 the Loss is 2.7872.\n",
      "For Iteration 321 the Loss is 2.7822.\n",
      "For Iteration 322 the Loss is 2.7771.\n",
      "For Iteration 323 the Loss is 2.7721.\n",
      "For Iteration 324 the Loss is 2.767.\n",
      "For Iteration 325 the Loss is 2.762.\n",
      "For Iteration 326 the Loss is 2.7569.\n",
      "For Iteration 327 the Loss is 2.7519.\n",
      "For Iteration 328 the Loss is 2.7469.\n",
      "For Iteration 329 the Loss is 2.7418.\n",
      "For Iteration 330 the Loss is 2.7368.\n",
      "For Iteration 331 the Loss is 2.7318.\n",
      "For Iteration 332 the Loss is 2.7268.\n",
      "For Iteration 333 the Loss is 2.7217.\n",
      "For Iteration 334 the Loss is 2.7167.\n",
      "For Iteration 335 the Loss is 2.7117.\n",
      "For Iteration 336 the Loss is 2.7067.\n",
      "For Iteration 337 the Loss is 2.7017.\n",
      "For Iteration 338 the Loss is 2.6967.\n",
      "For Iteration 339 the Loss is 2.6917.\n",
      "For Iteration 340 the Loss is 2.6867.\n",
      "For Iteration 341 the Loss is 2.6817.\n",
      "For Iteration 342 the Loss is 2.6767.\n",
      "For Iteration 343 the Loss is 2.6717.\n",
      "For Iteration 344 the Loss is 2.6668.\n",
      "For Iteration 345 the Loss is 2.6618.\n",
      "For Iteration 346 the Loss is 2.6568.\n",
      "For Iteration 347 the Loss is 2.6518.\n",
      "For Iteration 348 the Loss is 2.6469.\n",
      "For Iteration 349 the Loss is 2.6419.\n",
      "For Iteration 350 the Loss is 2.6369.\n",
      "For Iteration 351 the Loss is 2.632.\n",
      "For Iteration 352 the Loss is 2.627.\n",
      "For Iteration 353 the Loss is 2.6221.\n",
      "For Iteration 354 the Loss is 2.6171.\n",
      "For Iteration 355 the Loss is 2.6122.\n",
      "For Iteration 356 the Loss is 2.6072.\n",
      "For Iteration 357 the Loss is 2.6023.\n",
      "For Iteration 358 the Loss is 2.5973.\n",
      "For Iteration 359 the Loss is 2.5924.\n",
      "For Iteration 360 the Loss is 2.5875.\n",
      "For Iteration 361 the Loss is 2.5826.\n",
      "For Iteration 362 the Loss is 2.5776.\n",
      "For Iteration 363 the Loss is 2.5727.\n",
      "For Iteration 364 the Loss is 2.5678.\n",
      "For Iteration 365 the Loss is 2.5629.\n",
      "For Iteration 366 the Loss is 2.558.\n",
      "For Iteration 367 the Loss is 2.5531.\n",
      "For Iteration 368 the Loss is 2.5482.\n",
      "For Iteration 369 the Loss is 2.5433.\n",
      "For Iteration 370 the Loss is 2.5384.\n",
      "For Iteration 371 the Loss is 2.5335.\n",
      "For Iteration 372 the Loss is 2.5286.\n",
      "For Iteration 373 the Loss is 2.5237.\n",
      "For Iteration 374 the Loss is 2.5188.\n",
      "For Iteration 375 the Loss is 2.514.\n",
      "For Iteration 376 the Loss is 2.5091.\n",
      "For Iteration 377 the Loss is 2.5042.\n",
      "For Iteration 378 the Loss is 2.4994.\n",
      "For Iteration 379 the Loss is 2.4945.\n",
      "For Iteration 380 the Loss is 2.4897.\n",
      "For Iteration 381 the Loss is 2.4848.\n",
      "For Iteration 382 the Loss is 2.48.\n",
      "For Iteration 383 the Loss is 2.4751.\n",
      "For Iteration 384 the Loss is 2.4703.\n",
      "For Iteration 385 the Loss is 2.4654.\n",
      "For Iteration 386 the Loss is 2.4606.\n",
      "For Iteration 387 the Loss is 2.4558.\n",
      "For Iteration 388 the Loss is 2.4509.\n",
      "For Iteration 389 the Loss is 2.4461.\n",
      "For Iteration 390 the Loss is 2.4413.\n",
      "For Iteration 391 the Loss is 2.4365.\n",
      "For Iteration 392 the Loss is 2.4317.\n",
      "For Iteration 393 the Loss is 2.4269.\n",
      "For Iteration 394 the Loss is 2.4221.\n",
      "For Iteration 395 the Loss is 2.4173.\n",
      "For Iteration 396 the Loss is 2.4125.\n",
      "For Iteration 397 the Loss is 2.4077.\n",
      "For Iteration 398 the Loss is 2.4029.\n",
      "For Iteration 399 the Loss is 2.3981.\n",
      "For Iteration 400 the Loss is 2.3933.\n",
      "For Iteration 401 the Loss is 2.3886.\n",
      "For Iteration 402 the Loss is 2.3838.\n",
      "For Iteration 403 the Loss is 2.379.\n",
      "For Iteration 404 the Loss is 2.3743.\n",
      "For Iteration 405 the Loss is 2.3695.\n",
      "For Iteration 406 the Loss is 2.3648.\n",
      "For Iteration 407 the Loss is 2.36.\n",
      "For Iteration 408 the Loss is 2.3553.\n",
      "For Iteration 409 the Loss is 2.3505.\n",
      "For Iteration 410 the Loss is 2.3458.\n",
      "For Iteration 411 the Loss is 2.3411.\n",
      "For Iteration 412 the Loss is 2.3363.\n",
      "For Iteration 413 the Loss is 2.3316.\n",
      "For Iteration 414 the Loss is 2.3269.\n",
      "For Iteration 415 the Loss is 2.3222.\n",
      "For Iteration 416 the Loss is 2.3175.\n",
      "For Iteration 417 the Loss is 2.3128.\n",
      "For Iteration 418 the Loss is 2.3081.\n",
      "For Iteration 419 the Loss is 2.3034.\n",
      "For Iteration 420 the Loss is 2.2987.\n",
      "For Iteration 421 the Loss is 2.294.\n",
      "For Iteration 422 the Loss is 2.2893.\n",
      "For Iteration 423 the Loss is 2.2846.\n",
      "For Iteration 424 the Loss is 2.28.\n",
      "For Iteration 425 the Loss is 2.2753.\n",
      "For Iteration 426 the Loss is 2.2706.\n",
      "For Iteration 427 the Loss is 2.266.\n",
      "For Iteration 428 the Loss is 2.2613.\n",
      "For Iteration 429 the Loss is 2.2567.\n",
      "For Iteration 430 the Loss is 2.252.\n",
      "For Iteration 431 the Loss is 2.2474.\n",
      "For Iteration 432 the Loss is 2.2428.\n",
      "For Iteration 433 the Loss is 2.2381.\n",
      "For Iteration 434 the Loss is 2.2335.\n",
      "For Iteration 435 the Loss is 2.2289.\n",
      "For Iteration 436 the Loss is 2.2243.\n",
      "For Iteration 437 the Loss is 2.2196.\n",
      "For Iteration 438 the Loss is 2.215.\n",
      "For Iteration 439 the Loss is 2.2104.\n",
      "For Iteration 440 the Loss is 2.2058.\n",
      "For Iteration 441 the Loss is 2.2012.\n",
      "For Iteration 442 the Loss is 2.1967.\n",
      "For Iteration 443 the Loss is 2.1921.\n",
      "For Iteration 444 the Loss is 2.1875.\n",
      "For Iteration 445 the Loss is 2.1829.\n",
      "For Iteration 446 the Loss is 2.1784.\n",
      "For Iteration 447 the Loss is 2.1738.\n",
      "For Iteration 448 the Loss is 2.1692.\n",
      "For Iteration 449 the Loss is 2.1647.\n",
      "For Iteration 450 the Loss is 2.1601.\n",
      "For Iteration 451 the Loss is 2.1556.\n",
      "For Iteration 452 the Loss is 2.1511.\n",
      "For Iteration 453 the Loss is 2.1465.\n",
      "For Iteration 454 the Loss is 2.142.\n",
      "For Iteration 455 the Loss is 2.1375.\n",
      "For Iteration 456 the Loss is 2.133.\n",
      "For Iteration 457 the Loss is 2.1284.\n",
      "For Iteration 458 the Loss is 2.1239.\n",
      "For Iteration 459 the Loss is 2.1194.\n",
      "For Iteration 460 the Loss is 2.1149.\n",
      "For Iteration 461 the Loss is 2.1104.\n",
      "For Iteration 462 the Loss is 2.106.\n",
      "For Iteration 463 the Loss is 2.1015.\n",
      "For Iteration 464 the Loss is 2.097.\n",
      "For Iteration 465 the Loss is 2.0925.\n",
      "For Iteration 466 the Loss is 2.0881.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 467 the Loss is 2.0836.\n",
      "For Iteration 468 the Loss is 2.0792.\n",
      "For Iteration 469 the Loss is 2.0747.\n",
      "For Iteration 470 the Loss is 2.0703.\n",
      "For Iteration 471 the Loss is 2.0658.\n",
      "For Iteration 472 the Loss is 2.0614.\n",
      "For Iteration 473 the Loss is 2.057.\n",
      "For Iteration 474 the Loss is 2.0525.\n",
      "For Iteration 475 the Loss is 2.0481.\n",
      "For Iteration 476 the Loss is 2.0437.\n",
      "For Iteration 477 the Loss is 2.0393.\n",
      "For Iteration 478 the Loss is 2.0349.\n",
      "For Iteration 479 the Loss is 2.0305.\n",
      "For Iteration 480 the Loss is 2.0261.\n",
      "For Iteration 481 the Loss is 2.0217.\n",
      "For Iteration 482 the Loss is 2.0174.\n",
      "For Iteration 483 the Loss is 2.013.\n",
      "For Iteration 484 the Loss is 2.0086.\n",
      "For Iteration 485 the Loss is 2.0043.\n",
      "For Iteration 486 the Loss is 1.9999.\n",
      "For Iteration 487 the Loss is 1.9956.\n",
      "For Iteration 488 the Loss is 1.9912.\n",
      "For Iteration 489 the Loss is 1.9869.\n",
      "For Iteration 490 the Loss is 1.9826.\n",
      "For Iteration 491 the Loss is 1.9782.\n",
      "For Iteration 492 the Loss is 1.9739.\n",
      "For Iteration 493 the Loss is 1.9696.\n",
      "For Iteration 494 the Loss is 1.9653.\n",
      "For Iteration 495 the Loss is 1.961.\n",
      "For Iteration 496 the Loss is 1.9567.\n",
      "For Iteration 497 the Loss is 1.9524.\n",
      "For Iteration 498 the Loss is 1.9481.\n",
      "For Iteration 499 the Loss is 1.9438.\n",
      "For Iteration 500 the Loss is 1.9396.\n",
      "For Iteration 501 the Loss is 1.9353.\n",
      "For Iteration 502 the Loss is 1.931.\n",
      "For Iteration 503 the Loss is 1.9268.\n",
      "For Iteration 504 the Loss is 1.9225.\n",
      "For Iteration 505 the Loss is 1.9183.\n",
      "For Iteration 506 the Loss is 1.9141.\n",
      "For Iteration 507 the Loss is 1.9098.\n",
      "For Iteration 508 the Loss is 1.9056.\n",
      "For Iteration 509 the Loss is 1.9014.\n",
      "For Iteration 510 the Loss is 1.8972.\n",
      "For Iteration 511 the Loss is 1.893.\n",
      "For Iteration 512 the Loss is 1.8888.\n",
      "For Iteration 513 the Loss is 1.8846.\n",
      "For Iteration 514 the Loss is 1.8804.\n",
      "For Iteration 515 the Loss is 1.8762.\n",
      "For Iteration 516 the Loss is 1.872.\n",
      "For Iteration 517 the Loss is 1.8679.\n",
      "For Iteration 518 the Loss is 1.8637.\n",
      "For Iteration 519 the Loss is 1.8595.\n",
      "For Iteration 520 the Loss is 1.8554.\n",
      "For Iteration 521 the Loss is 1.8513.\n",
      "For Iteration 522 the Loss is 1.8471.\n",
      "For Iteration 523 the Loss is 1.843.\n",
      "For Iteration 524 the Loss is 1.8389.\n",
      "For Iteration 525 the Loss is 1.8347.\n",
      "For Iteration 526 the Loss is 1.8306.\n",
      "For Iteration 527 the Loss is 1.8265.\n",
      "For Iteration 528 the Loss is 1.8224.\n",
      "For Iteration 529 the Loss is 1.8183.\n",
      "For Iteration 530 the Loss is 1.8143.\n",
      "For Iteration 531 the Loss is 1.8102.\n",
      "For Iteration 532 the Loss is 1.8061.\n",
      "For Iteration 533 the Loss is 1.802.\n",
      "For Iteration 534 the Loss is 1.798.\n",
      "For Iteration 535 the Loss is 1.7939.\n",
      "For Iteration 536 the Loss is 1.7899.\n",
      "For Iteration 537 the Loss is 1.7858.\n",
      "For Iteration 538 the Loss is 1.7818.\n",
      "For Iteration 539 the Loss is 1.7778.\n",
      "For Iteration 540 the Loss is 1.7738.\n",
      "For Iteration 541 the Loss is 1.7697.\n",
      "For Iteration 542 the Loss is 1.7657.\n",
      "For Iteration 543 the Loss is 1.7617.\n",
      "For Iteration 544 the Loss is 1.7578.\n",
      "For Iteration 545 the Loss is 1.7538.\n",
      "For Iteration 546 the Loss is 1.7498.\n",
      "For Iteration 547 the Loss is 1.7458.\n",
      "For Iteration 548 the Loss is 1.7418.\n",
      "For Iteration 549 the Loss is 1.7379.\n",
      "For Iteration 550 the Loss is 1.7339.\n",
      "For Iteration 551 the Loss is 1.73.\n",
      "For Iteration 552 the Loss is 1.7261.\n",
      "For Iteration 553 the Loss is 1.7221.\n",
      "For Iteration 554 the Loss is 1.7182.\n",
      "For Iteration 555 the Loss is 1.7143.\n",
      "For Iteration 556 the Loss is 1.7104.\n",
      "For Iteration 557 the Loss is 1.7065.\n",
      "For Iteration 558 the Loss is 1.7026.\n",
      "For Iteration 559 the Loss is 1.6987.\n",
      "For Iteration 560 the Loss is 1.6948.\n",
      "For Iteration 561 the Loss is 1.6909.\n",
      "For Iteration 562 the Loss is 1.6871.\n",
      "For Iteration 563 the Loss is 1.6832.\n",
      "For Iteration 564 the Loss is 1.6793.\n",
      "For Iteration 565 the Loss is 1.6755.\n",
      "For Iteration 566 the Loss is 1.6717.\n",
      "For Iteration 567 the Loss is 1.6678.\n",
      "For Iteration 568 the Loss is 1.664.\n",
      "For Iteration 569 the Loss is 1.6602.\n",
      "For Iteration 570 the Loss is 1.6564.\n",
      "For Iteration 571 the Loss is 1.6526.\n",
      "For Iteration 572 the Loss is 1.6488.\n",
      "For Iteration 573 the Loss is 1.645.\n",
      "For Iteration 574 the Loss is 1.6412.\n",
      "For Iteration 575 the Loss is 1.6374.\n",
      "For Iteration 576 the Loss is 1.6336.\n",
      "For Iteration 577 the Loss is 1.6299.\n",
      "For Iteration 578 the Loss is 1.6261.\n",
      "For Iteration 579 the Loss is 1.6224.\n",
      "For Iteration 580 the Loss is 1.6186.\n",
      "For Iteration 581 the Loss is 1.6149.\n",
      "For Iteration 582 the Loss is 1.6112.\n",
      "For Iteration 583 the Loss is 1.6075.\n",
      "For Iteration 584 the Loss is 1.6038.\n",
      "For Iteration 585 the Loss is 1.6001.\n",
      "For Iteration 586 the Loss is 1.5964.\n",
      "For Iteration 587 the Loss is 1.5927.\n",
      "For Iteration 588 the Loss is 1.589.\n",
      "For Iteration 589 the Loss is 1.5853.\n",
      "For Iteration 590 the Loss is 1.5817.\n",
      "For Iteration 591 the Loss is 1.578.\n",
      "For Iteration 592 the Loss is 1.5743.\n",
      "For Iteration 593 the Loss is 1.5707.\n",
      "For Iteration 594 the Loss is 1.5671.\n",
      "For Iteration 595 the Loss is 1.5634.\n",
      "For Iteration 596 the Loss is 1.5598.\n",
      "For Iteration 597 the Loss is 1.5562.\n",
      "For Iteration 598 the Loss is 1.5526.\n",
      "For Iteration 599 the Loss is 1.549.\n",
      "For Iteration 600 the Loss is 1.5454.\n",
      "For Iteration 601 the Loss is 1.5418.\n",
      "For Iteration 602 the Loss is 1.5382.\n",
      "For Iteration 603 the Loss is 1.5347.\n",
      "For Iteration 604 the Loss is 1.5311.\n",
      "For Iteration 605 the Loss is 1.5276.\n",
      "For Iteration 606 the Loss is 1.524.\n",
      "For Iteration 607 the Loss is 1.5205.\n",
      "For Iteration 608 the Loss is 1.5169.\n",
      "For Iteration 609 the Loss is 1.5134.\n",
      "For Iteration 610 the Loss is 1.5099.\n",
      "For Iteration 611 the Loss is 1.5064.\n",
      "For Iteration 612 the Loss is 1.5029.\n",
      "For Iteration 613 the Loss is 1.4994.\n",
      "For Iteration 614 the Loss is 1.4959.\n",
      "For Iteration 615 the Loss is 1.4924.\n",
      "For Iteration 616 the Loss is 1.489.\n",
      "For Iteration 617 the Loss is 1.4855.\n",
      "For Iteration 618 the Loss is 1.4821.\n",
      "For Iteration 619 the Loss is 1.4786.\n",
      "For Iteration 620 the Loss is 1.4752.\n",
      "For Iteration 621 the Loss is 1.4717.\n",
      "For Iteration 622 the Loss is 1.4683.\n",
      "For Iteration 623 the Loss is 1.4649.\n",
      "For Iteration 624 the Loss is 1.4615.\n",
      "For Iteration 625 the Loss is 1.4581.\n",
      "For Iteration 626 the Loss is 1.4547.\n",
      "For Iteration 627 the Loss is 1.4513.\n",
      "For Iteration 628 the Loss is 1.4479.\n",
      "For Iteration 629 the Loss is 1.4446.\n",
      "For Iteration 630 the Loss is 1.4412.\n",
      "For Iteration 631 the Loss is 1.4379.\n",
      "For Iteration 632 the Loss is 1.4345.\n",
      "For Iteration 633 the Loss is 1.4312.\n",
      "For Iteration 634 the Loss is 1.4278.\n",
      "For Iteration 635 the Loss is 1.4245.\n",
      "For Iteration 636 the Loss is 1.4212.\n",
      "For Iteration 637 the Loss is 1.4179.\n",
      "For Iteration 638 the Loss is 1.4146.\n",
      "For Iteration 639 the Loss is 1.4113.\n",
      "For Iteration 640 the Loss is 1.408.\n",
      "For Iteration 641 the Loss is 1.4048.\n",
      "For Iteration 642 the Loss is 1.4015.\n",
      "For Iteration 643 the Loss is 1.3982.\n",
      "For Iteration 644 the Loss is 1.395.\n",
      "For Iteration 645 the Loss is 1.3917.\n",
      "For Iteration 646 the Loss is 1.3885.\n",
      "For Iteration 647 the Loss is 1.3853.\n",
      "For Iteration 648 the Loss is 1.382.\n",
      "For Iteration 649 the Loss is 1.3788.\n",
      "For Iteration 650 the Loss is 1.3756.\n",
      "For Iteration 651 the Loss is 1.3724.\n",
      "For Iteration 652 the Loss is 1.3692.\n",
      "For Iteration 653 the Loss is 1.3661.\n",
      "For Iteration 654 the Loss is 1.3629.\n",
      "For Iteration 655 the Loss is 1.3597.\n",
      "For Iteration 656 the Loss is 1.3566.\n",
      "For Iteration 657 the Loss is 1.3534.\n",
      "For Iteration 658 the Loss is 1.3503.\n",
      "For Iteration 659 the Loss is 1.3471.\n",
      "For Iteration 660 the Loss is 1.344.\n",
      "For Iteration 661 the Loss is 1.3409.\n",
      "For Iteration 662 the Loss is 1.3378.\n",
      "For Iteration 663 the Loss is 1.3347.\n",
      "For Iteration 664 the Loss is 1.3316.\n",
      "For Iteration 665 the Loss is 1.3285.\n",
      "For Iteration 666 the Loss is 1.3254.\n",
      "For Iteration 667 the Loss is 1.3224.\n",
      "For Iteration 668 the Loss is 1.3193.\n",
      "For Iteration 669 the Loss is 1.3162.\n",
      "For Iteration 670 the Loss is 1.3132.\n",
      "For Iteration 671 the Loss is 1.3102.\n",
      "For Iteration 672 the Loss is 1.3071.\n",
      "For Iteration 673 the Loss is 1.3041.\n",
      "For Iteration 674 the Loss is 1.3011.\n",
      "For Iteration 675 the Loss is 1.2981.\n",
      "For Iteration 676 the Loss is 1.2951.\n",
      "For Iteration 677 the Loss is 1.2921.\n",
      "For Iteration 678 the Loss is 1.2891.\n",
      "For Iteration 679 the Loss is 1.2861.\n",
      "For Iteration 680 the Loss is 1.2832.\n",
      "For Iteration 681 the Loss is 1.2802.\n",
      "For Iteration 682 the Loss is 1.2773.\n",
      "For Iteration 683 the Loss is 1.2743.\n",
      "For Iteration 684 the Loss is 1.2714.\n",
      "For Iteration 685 the Loss is 1.2685.\n",
      "For Iteration 686 the Loss is 1.2655.\n",
      "For Iteration 687 the Loss is 1.2626.\n",
      "For Iteration 688 the Loss is 1.2597.\n",
      "For Iteration 689 the Loss is 1.2568.\n",
      "For Iteration 690 the Loss is 1.254.\n",
      "For Iteration 691 the Loss is 1.2511.\n",
      "For Iteration 692 the Loss is 1.2482.\n",
      "For Iteration 693 the Loss is 1.2453.\n",
      "For Iteration 694 the Loss is 1.2425.\n",
      "For Iteration 695 the Loss is 1.2396.\n",
      "For Iteration 696 the Loss is 1.2368.\n",
      "For Iteration 697 the Loss is 1.234.\n",
      "For Iteration 698 the Loss is 1.2311.\n",
      "For Iteration 699 the Loss is 1.2283.\n",
      "For Iteration 700 the Loss is 1.2255.\n",
      "For Iteration 701 the Loss is 1.2227.\n",
      "For Iteration 702 the Loss is 1.2199.\n",
      "For Iteration 703 the Loss is 1.2171.\n",
      "For Iteration 704 the Loss is 1.2144.\n",
      "For Iteration 705 the Loss is 1.2116.\n",
      "For Iteration 706 the Loss is 1.2088.\n",
      "For Iteration 707 the Loss is 1.2061.\n",
      "For Iteration 708 the Loss is 1.2033.\n",
      "For Iteration 709 the Loss is 1.2006.\n",
      "For Iteration 710 the Loss is 1.1979.\n",
      "For Iteration 711 the Loss is 1.1952.\n",
      "For Iteration 712 the Loss is 1.1924.\n",
      "For Iteration 713 the Loss is 1.1897.\n",
      "For Iteration 714 the Loss is 1.187.\n",
      "For Iteration 715 the Loss is 1.1843.\n",
      "For Iteration 716 the Loss is 1.1817.\n",
      "For Iteration 717 the Loss is 1.179.\n",
      "For Iteration 718 the Loss is 1.1763.\n",
      "For Iteration 719 the Loss is 1.1737.\n",
      "For Iteration 720 the Loss is 1.171.\n",
      "For Iteration 721 the Loss is 1.1684.\n",
      "For Iteration 722 the Loss is 1.1657.\n",
      "For Iteration 723 the Loss is 1.1631.\n",
      "For Iteration 724 the Loss is 1.1605.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 725 the Loss is 1.1579.\n",
      "For Iteration 726 the Loss is 1.1553.\n",
      "For Iteration 727 the Loss is 1.1527.\n",
      "For Iteration 728 the Loss is 1.1501.\n",
      "For Iteration 729 the Loss is 1.1475.\n",
      "For Iteration 730 the Loss is 1.1449.\n",
      "For Iteration 731 the Loss is 1.1424.\n",
      "For Iteration 732 the Loss is 1.1398.\n",
      "For Iteration 733 the Loss is 1.1373.\n",
      "For Iteration 734 the Loss is 1.1347.\n",
      "For Iteration 735 the Loss is 1.1322.\n",
      "For Iteration 736 the Loss is 1.1297.\n",
      "For Iteration 737 the Loss is 1.1271.\n",
      "For Iteration 738 the Loss is 1.1246.\n",
      "For Iteration 739 the Loss is 1.1221.\n",
      "For Iteration 740 the Loss is 1.1196.\n",
      "For Iteration 741 the Loss is 1.1171.\n",
      "For Iteration 742 the Loss is 1.1147.\n",
      "For Iteration 743 the Loss is 1.1122.\n",
      "For Iteration 744 the Loss is 1.1097.\n",
      "For Iteration 745 the Loss is 1.1073.\n",
      "For Iteration 746 the Loss is 1.1048.\n",
      "For Iteration 747 the Loss is 1.1024.\n",
      "For Iteration 748 the Loss is 1.0999.\n",
      "For Iteration 749 the Loss is 1.0975.\n",
      "For Iteration 750 the Loss is 1.0951.\n",
      "For Iteration 751 the Loss is 1.0927.\n",
      "For Iteration 752 the Loss is 1.0903.\n",
      "For Iteration 753 the Loss is 1.0879.\n",
      "For Iteration 754 the Loss is 1.0855.\n",
      "For Iteration 755 the Loss is 1.0831.\n",
      "For Iteration 756 the Loss is 1.0807.\n",
      "For Iteration 757 the Loss is 1.0783.\n",
      "For Iteration 758 the Loss is 1.076.\n",
      "For Iteration 759 the Loss is 1.0736.\n",
      "For Iteration 760 the Loss is 1.0713.\n",
      "For Iteration 761 the Loss is 1.0689.\n",
      "For Iteration 762 the Loss is 1.0666.\n",
      "For Iteration 763 the Loss is 1.0643.\n",
      "For Iteration 764 the Loss is 1.062.\n",
      "For Iteration 765 the Loss is 1.0597.\n",
      "For Iteration 766 the Loss is 1.0574.\n",
      "For Iteration 767 the Loss is 1.0551.\n",
      "For Iteration 768 the Loss is 1.0528.\n",
      "For Iteration 769 the Loss is 1.0505.\n",
      "For Iteration 770 the Loss is 1.0482.\n",
      "For Iteration 771 the Loss is 1.046.\n",
      "For Iteration 772 the Loss is 1.0437.\n",
      "For Iteration 773 the Loss is 1.0415.\n",
      "For Iteration 774 the Loss is 1.0392.\n",
      "For Iteration 775 the Loss is 1.037.\n",
      "For Iteration 776 the Loss is 1.0348.\n",
      "For Iteration 777 the Loss is 1.0325.\n",
      "For Iteration 778 the Loss is 1.0303.\n",
      "For Iteration 779 the Loss is 1.0281.\n",
      "For Iteration 780 the Loss is 1.0259.\n",
      "For Iteration 781 the Loss is 1.0237.\n",
      "For Iteration 782 the Loss is 1.0215.\n",
      "For Iteration 783 the Loss is 1.0194.\n",
      "For Iteration 784 the Loss is 1.0172.\n",
      "For Iteration 785 the Loss is 1.015.\n",
      "For Iteration 786 the Loss is 1.0129.\n",
      "For Iteration 787 the Loss is 1.0107.\n",
      "For Iteration 788 the Loss is 1.0086.\n",
      "For Iteration 789 the Loss is 1.0064.\n",
      "For Iteration 790 the Loss is 1.0043.\n",
      "For Iteration 791 the Loss is 1.0022.\n",
      "For Iteration 792 the Loss is 1.0001.\n",
      "For Iteration 793 the Loss is 0.998.\n",
      "For Iteration 794 the Loss is 0.9959.\n",
      "For Iteration 795 the Loss is 0.9938.\n",
      "For Iteration 796 the Loss is 0.9917.\n",
      "For Iteration 797 the Loss is 0.9896.\n",
      "For Iteration 798 the Loss is 0.9875.\n",
      "For Iteration 799 the Loss is 0.9855.\n",
      "For Iteration 800 the Loss is 0.9834.\n",
      "For Iteration 801 the Loss is 0.9814.\n",
      "For Iteration 802 the Loss is 0.9793.\n",
      "For Iteration 803 the Loss is 0.9773.\n",
      "For Iteration 804 the Loss is 0.9753.\n",
      "For Iteration 805 the Loss is 0.9732.\n",
      "For Iteration 806 the Loss is 0.9712.\n",
      "For Iteration 807 the Loss is 0.9692.\n",
      "For Iteration 808 the Loss is 0.9672.\n",
      "For Iteration 809 the Loss is 0.9652.\n",
      "For Iteration 810 the Loss is 0.9632.\n",
      "For Iteration 811 the Loss is 0.9612.\n",
      "For Iteration 812 the Loss is 0.9593.\n",
      "For Iteration 813 the Loss is 0.9573.\n",
      "For Iteration 814 the Loss is 0.9553.\n",
      "For Iteration 815 the Loss is 0.9534.\n",
      "For Iteration 816 the Loss is 0.9514.\n",
      "For Iteration 817 the Loss is 0.9495.\n",
      "For Iteration 818 the Loss is 0.9475.\n",
      "For Iteration 819 the Loss is 0.9456.\n",
      "For Iteration 820 the Loss is 0.9437.\n",
      "For Iteration 821 the Loss is 0.9418.\n",
      "For Iteration 822 the Loss is 0.9398.\n",
      "For Iteration 823 the Loss is 0.9379.\n",
      "For Iteration 824 the Loss is 0.936.\n",
      "For Iteration 825 the Loss is 0.9342.\n",
      "For Iteration 826 the Loss is 0.9323.\n",
      "For Iteration 827 the Loss is 0.9304.\n",
      "For Iteration 828 the Loss is 0.9285.\n",
      "For Iteration 829 the Loss is 0.9267.\n",
      "For Iteration 830 the Loss is 0.9248.\n",
      "For Iteration 831 the Loss is 0.9229.\n",
      "For Iteration 832 the Loss is 0.9211.\n",
      "For Iteration 833 the Loss is 0.9193.\n",
      "For Iteration 834 the Loss is 0.9174.\n",
      "For Iteration 835 the Loss is 0.9156.\n",
      "For Iteration 836 the Loss is 0.9138.\n",
      "For Iteration 837 the Loss is 0.912.\n",
      "For Iteration 838 the Loss is 0.9102.\n",
      "For Iteration 839 the Loss is 0.9084.\n",
      "For Iteration 840 the Loss is 0.9066.\n",
      "For Iteration 841 the Loss is 0.9048.\n",
      "For Iteration 842 the Loss is 0.903.\n",
      "For Iteration 843 the Loss is 0.9012.\n",
      "For Iteration 844 the Loss is 0.8995.\n",
      "For Iteration 845 the Loss is 0.8977.\n",
      "For Iteration 846 the Loss is 0.8959.\n",
      "For Iteration 847 the Loss is 0.8942.\n",
      "For Iteration 848 the Loss is 0.8924.\n",
      "For Iteration 849 the Loss is 0.8907.\n",
      "For Iteration 850 the Loss is 0.889.\n",
      "For Iteration 851 the Loss is 0.8872.\n",
      "For Iteration 852 the Loss is 0.8855.\n",
      "For Iteration 853 the Loss is 0.8838.\n",
      "For Iteration 854 the Loss is 0.8821.\n",
      "For Iteration 855 the Loss is 0.8804.\n",
      "For Iteration 856 the Loss is 0.8787.\n",
      "For Iteration 857 the Loss is 0.877.\n",
      "For Iteration 858 the Loss is 0.8753.\n",
      "For Iteration 859 the Loss is 0.8736.\n",
      "For Iteration 860 the Loss is 0.872.\n",
      "For Iteration 861 the Loss is 0.8703.\n",
      "For Iteration 862 the Loss is 0.8686.\n",
      "For Iteration 863 the Loss is 0.867.\n",
      "For Iteration 864 the Loss is 0.8653.\n",
      "For Iteration 865 the Loss is 0.8637.\n",
      "For Iteration 866 the Loss is 0.862.\n",
      "For Iteration 867 the Loss is 0.8604.\n",
      "For Iteration 868 the Loss is 0.8588.\n",
      "For Iteration 869 the Loss is 0.8572.\n",
      "For Iteration 870 the Loss is 0.8556.\n",
      "For Iteration 871 the Loss is 0.8539.\n",
      "For Iteration 872 the Loss is 0.8523.\n",
      "For Iteration 873 the Loss is 0.8507.\n",
      "For Iteration 874 the Loss is 0.8491.\n",
      "For Iteration 875 the Loss is 0.8476.\n",
      "For Iteration 876 the Loss is 0.846.\n",
      "For Iteration 877 the Loss is 0.8444.\n",
      "For Iteration 878 the Loss is 0.8428.\n",
      "For Iteration 879 the Loss is 0.8413.\n",
      "For Iteration 880 the Loss is 0.8397.\n",
      "For Iteration 881 the Loss is 0.8382.\n",
      "For Iteration 882 the Loss is 0.8366.\n",
      "For Iteration 883 the Loss is 0.8351.\n",
      "For Iteration 884 the Loss is 0.8335.\n",
      "For Iteration 885 the Loss is 0.832.\n",
      "For Iteration 886 the Loss is 0.8305.\n",
      "For Iteration 887 the Loss is 0.829.\n",
      "For Iteration 888 the Loss is 0.8275.\n",
      "For Iteration 889 the Loss is 0.8259.\n",
      "For Iteration 890 the Loss is 0.8244.\n",
      "For Iteration 891 the Loss is 0.8229.\n",
      "For Iteration 892 the Loss is 0.8214.\n",
      "For Iteration 893 the Loss is 0.82.\n",
      "For Iteration 894 the Loss is 0.8185.\n",
      "For Iteration 895 the Loss is 0.817.\n",
      "For Iteration 896 the Loss is 0.8155.\n",
      "For Iteration 897 the Loss is 0.8141.\n",
      "For Iteration 898 the Loss is 0.8126.\n",
      "For Iteration 899 the Loss is 0.8112.\n",
      "For Iteration 900 the Loss is 0.8097.\n",
      "For Iteration 901 the Loss is 0.8083.\n",
      "For Iteration 902 the Loss is 0.8068.\n",
      "For Iteration 903 the Loss is 0.8054.\n",
      "For Iteration 904 the Loss is 0.804.\n",
      "For Iteration 905 the Loss is 0.8025.\n",
      "For Iteration 906 the Loss is 0.8011.\n",
      "For Iteration 907 the Loss is 0.7997.\n",
      "For Iteration 908 the Loss is 0.7983.\n",
      "For Iteration 909 the Loss is 0.7969.\n",
      "For Iteration 910 the Loss is 0.7955.\n",
      "For Iteration 911 the Loss is 0.7941.\n",
      "For Iteration 912 the Loss is 0.7927.\n",
      "For Iteration 913 the Loss is 0.7913.\n",
      "For Iteration 914 the Loss is 0.7899.\n",
      "For Iteration 915 the Loss is 0.7886.\n",
      "For Iteration 916 the Loss is 0.7872.\n",
      "For Iteration 917 the Loss is 0.7858.\n",
      "For Iteration 918 the Loss is 0.7845.\n",
      "For Iteration 919 the Loss is 0.7831.\n",
      "For Iteration 920 the Loss is 0.7818.\n",
      "For Iteration 921 the Loss is 0.7804.\n",
      "For Iteration 922 the Loss is 0.7791.\n",
      "For Iteration 923 the Loss is 0.7778.\n",
      "For Iteration 924 the Loss is 0.7764.\n",
      "For Iteration 925 the Loss is 0.7751.\n",
      "For Iteration 926 the Loss is 0.7738.\n",
      "For Iteration 927 the Loss is 0.7725.\n",
      "For Iteration 928 the Loss is 0.7712.\n",
      "For Iteration 929 the Loss is 0.7699.\n",
      "For Iteration 930 the Loss is 0.7686.\n",
      "For Iteration 931 the Loss is 0.7673.\n",
      "For Iteration 932 the Loss is 0.766.\n",
      "For Iteration 933 the Loss is 0.7647.\n",
      "For Iteration 934 the Loss is 0.7634.\n",
      "For Iteration 935 the Loss is 0.7621.\n",
      "For Iteration 936 the Loss is 0.7609.\n",
      "For Iteration 937 the Loss is 0.7596.\n",
      "For Iteration 938 the Loss is 0.7583.\n",
      "For Iteration 939 the Loss is 0.7571.\n",
      "For Iteration 940 the Loss is 0.7558.\n",
      "For Iteration 941 the Loss is 0.7546.\n",
      "For Iteration 942 the Loss is 0.7533.\n",
      "For Iteration 943 the Loss is 0.7521.\n",
      "For Iteration 944 the Loss is 0.7508.\n",
      "For Iteration 945 the Loss is 0.7496.\n",
      "For Iteration 946 the Loss is 0.7484.\n",
      "For Iteration 947 the Loss is 0.7472.\n",
      "For Iteration 948 the Loss is 0.7459.\n",
      "For Iteration 949 the Loss is 0.7447.\n",
      "For Iteration 950 the Loss is 0.7435.\n",
      "For Iteration 951 the Loss is 0.7423.\n",
      "For Iteration 952 the Loss is 0.7411.\n",
      "For Iteration 953 the Loss is 0.7399.\n",
      "For Iteration 954 the Loss is 0.7387.\n",
      "For Iteration 955 the Loss is 0.7375.\n",
      "For Iteration 956 the Loss is 0.7364.\n",
      "For Iteration 957 the Loss is 0.7352.\n",
      "For Iteration 958 the Loss is 0.734.\n",
      "For Iteration 959 the Loss is 0.7328.\n",
      "For Iteration 960 the Loss is 0.7317.\n",
      "For Iteration 961 the Loss is 0.7305.\n",
      "For Iteration 962 the Loss is 0.7293.\n",
      "For Iteration 963 the Loss is 0.7282.\n",
      "For Iteration 964 the Loss is 0.727.\n",
      "For Iteration 965 the Loss is 0.7259.\n",
      "For Iteration 966 the Loss is 0.7248.\n",
      "For Iteration 967 the Loss is 0.7236.\n",
      "For Iteration 968 the Loss is 0.7225.\n",
      "For Iteration 969 the Loss is 0.7214.\n",
      "For Iteration 970 the Loss is 0.7202.\n",
      "For Iteration 971 the Loss is 0.7191.\n",
      "For Iteration 972 the Loss is 0.718.\n",
      "For Iteration 973 the Loss is 0.7169.\n",
      "For Iteration 974 the Loss is 0.7158.\n",
      "For Iteration 975 the Loss is 0.7147.\n",
      "For Iteration 976 the Loss is 0.7136.\n",
      "For Iteration 977 the Loss is 0.7125.\n",
      "For Iteration 978 the Loss is 0.7114.\n",
      "For Iteration 979 the Loss is 0.7103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 980 the Loss is 0.7092.\n",
      "For Iteration 981 the Loss is 0.7081.\n",
      "For Iteration 982 the Loss is 0.7071.\n",
      "For Iteration 983 the Loss is 0.706.\n",
      "For Iteration 984 the Loss is 0.7049.\n",
      "For Iteration 985 the Loss is 0.7039.\n",
      "For Iteration 986 the Loss is 0.7028.\n",
      "For Iteration 987 the Loss is 0.7017.\n",
      "For Iteration 988 the Loss is 0.7007.\n",
      "For Iteration 989 the Loss is 0.6996.\n",
      "For Iteration 990 the Loss is 0.6986.\n",
      "For Iteration 991 the Loss is 0.6976.\n",
      "For Iteration 992 the Loss is 0.6965.\n",
      "For Iteration 993 the Loss is 0.6955.\n",
      "For Iteration 994 the Loss is 0.6945.\n",
      "For Iteration 995 the Loss is 0.6934.\n",
      "For Iteration 996 the Loss is 0.6924.\n",
      "For Iteration 997 the Loss is 0.6914.\n",
      "For Iteration 998 the Loss is 0.6904.\n",
      "For Iteration 999 the Loss is 0.6894.\n",
      "For Iteration 1000 the Loss is 0.6884.\n",
      "For Iteration 1001 the Loss is 0.6873.\n",
      "For Iteration 1002 the Loss is 0.6863.\n",
      "For Iteration 1003 the Loss is 0.6853.\n",
      "For Iteration 1004 the Loss is 0.6844.\n",
      "For Iteration 1005 the Loss is 0.6834.\n",
      "For Iteration 1006 the Loss is 0.6824.\n",
      "For Iteration 1007 the Loss is 0.6814.\n",
      "For Iteration 1008 the Loss is 0.6804.\n",
      "For Iteration 1009 the Loss is 0.6794.\n",
      "For Iteration 1010 the Loss is 0.6785.\n",
      "For Iteration 1011 the Loss is 0.6775.\n",
      "For Iteration 1012 the Loss is 0.6765.\n",
      "For Iteration 1013 the Loss is 0.6756.\n",
      "For Iteration 1014 the Loss is 0.6746.\n",
      "For Iteration 1015 the Loss is 0.6737.\n",
      "For Iteration 1016 the Loss is 0.6727.\n",
      "For Iteration 1017 the Loss is 0.6718.\n",
      "For Iteration 1018 the Loss is 0.6708.\n",
      "For Iteration 1019 the Loss is 0.6699.\n",
      "For Iteration 1020 the Loss is 0.6689.\n",
      "For Iteration 1021 the Loss is 0.668.\n",
      "For Iteration 1022 the Loss is 0.6671.\n",
      "For Iteration 1023 the Loss is 0.6662.\n",
      "For Iteration 1024 the Loss is 0.6652.\n",
      "For Iteration 1025 the Loss is 0.6643.\n",
      "For Iteration 1026 the Loss is 0.6634.\n",
      "For Iteration 1027 the Loss is 0.6625.\n",
      "For Iteration 1028 the Loss is 0.6616.\n",
      "For Iteration 1029 the Loss is 0.6607.\n",
      "For Iteration 1030 the Loss is 0.6598.\n",
      "For Iteration 1031 the Loss is 0.6589.\n",
      "For Iteration 1032 the Loss is 0.658.\n",
      "For Iteration 1033 the Loss is 0.6571.\n",
      "For Iteration 1034 the Loss is 0.6562.\n",
      "For Iteration 1035 the Loss is 0.6553.\n",
      "For Iteration 1036 the Loss is 0.6544.\n",
      "For Iteration 1037 the Loss is 0.6535.\n",
      "For Iteration 1038 the Loss is 0.6526.\n",
      "For Iteration 1039 the Loss is 0.6518.\n",
      "For Iteration 1040 the Loss is 0.6509.\n",
      "For Iteration 1041 the Loss is 0.65.\n",
      "For Iteration 1042 the Loss is 0.6492.\n",
      "For Iteration 1043 the Loss is 0.6483.\n",
      "For Iteration 1044 the Loss is 0.6474.\n",
      "For Iteration 1045 the Loss is 0.6466.\n",
      "For Iteration 1046 the Loss is 0.6457.\n",
      "For Iteration 1047 the Loss is 0.6449.\n",
      "For Iteration 1048 the Loss is 0.644.\n",
      "For Iteration 1049 the Loss is 0.6432.\n",
      "For Iteration 1050 the Loss is 0.6423.\n",
      "For Iteration 1051 the Loss is 0.6415.\n",
      "For Iteration 1052 the Loss is 0.6407.\n",
      "For Iteration 1053 the Loss is 0.6398.\n",
      "For Iteration 1054 the Loss is 0.639.\n",
      "For Iteration 1055 the Loss is 0.6382.\n",
      "For Iteration 1056 the Loss is 0.6374.\n",
      "For Iteration 1057 the Loss is 0.6365.\n",
      "For Iteration 1058 the Loss is 0.6357.\n",
      "For Iteration 1059 the Loss is 0.6349.\n",
      "For Iteration 1060 the Loss is 0.6341.\n",
      "For Iteration 1061 the Loss is 0.6333.\n",
      "For Iteration 1062 the Loss is 0.6325.\n",
      "For Iteration 1063 the Loss is 0.6317.\n",
      "For Iteration 1064 the Loss is 0.6309.\n",
      "For Iteration 1065 the Loss is 0.6301.\n",
      "For Iteration 1066 the Loss is 0.6293.\n",
      "For Iteration 1067 the Loss is 0.6285.\n",
      "For Iteration 1068 the Loss is 0.6277.\n",
      "For Iteration 1069 the Loss is 0.6269.\n",
      "For Iteration 1070 the Loss is 0.6261.\n",
      "For Iteration 1071 the Loss is 0.6253.\n",
      "For Iteration 1072 the Loss is 0.6246.\n",
      "For Iteration 1073 the Loss is 0.6238.\n",
      "For Iteration 1074 the Loss is 0.623.\n",
      "For Iteration 1075 the Loss is 0.6222.\n",
      "For Iteration 1076 the Loss is 0.6215.\n",
      "For Iteration 1077 the Loss is 0.6207.\n",
      "For Iteration 1078 the Loss is 0.62.\n",
      "For Iteration 1079 the Loss is 0.6192.\n",
      "For Iteration 1080 the Loss is 0.6184.\n",
      "For Iteration 1081 the Loss is 0.6177.\n",
      "For Iteration 1082 the Loss is 0.6169.\n",
      "For Iteration 1083 the Loss is 0.6162.\n",
      "For Iteration 1084 the Loss is 0.6154.\n",
      "For Iteration 1085 the Loss is 0.6147.\n",
      "For Iteration 1086 the Loss is 0.614.\n",
      "For Iteration 1087 the Loss is 0.6132.\n",
      "For Iteration 1088 the Loss is 0.6125.\n",
      "For Iteration 1089 the Loss is 0.6118.\n",
      "For Iteration 1090 the Loss is 0.611.\n",
      "For Iteration 1091 the Loss is 0.6103.\n",
      "For Iteration 1092 the Loss is 0.6096.\n",
      "For Iteration 1093 the Loss is 0.6089.\n",
      "For Iteration 1094 the Loss is 0.6081.\n",
      "For Iteration 1095 the Loss is 0.6074.\n",
      "For Iteration 1096 the Loss is 0.6067.\n",
      "For Iteration 1097 the Loss is 0.606.\n",
      "For Iteration 1098 the Loss is 0.6053.\n",
      "For Iteration 1099 the Loss is 0.6046.\n",
      "For Iteration 1100 the Loss is 0.6039.\n",
      "For Iteration 1101 the Loss is 0.6032.\n",
      "For Iteration 1102 the Loss is 0.6025.\n",
      "For Iteration 1103 the Loss is 0.6018.\n",
      "For Iteration 1104 the Loss is 0.6011.\n",
      "For Iteration 1105 the Loss is 0.6004.\n",
      "For Iteration 1106 the Loss is 0.5997.\n",
      "For Iteration 1107 the Loss is 0.599.\n",
      "For Iteration 1108 the Loss is 0.5983.\n",
      "For Iteration 1109 the Loss is 0.5976.\n",
      "For Iteration 1110 the Loss is 0.597.\n",
      "For Iteration 1111 the Loss is 0.5963.\n",
      "For Iteration 1112 the Loss is 0.5956.\n",
      "For Iteration 1113 the Loss is 0.5949.\n",
      "For Iteration 1114 the Loss is 0.5943.\n",
      "For Iteration 1115 the Loss is 0.5936.\n",
      "For Iteration 1116 the Loss is 0.5929.\n",
      "For Iteration 1117 the Loss is 0.5923.\n",
      "For Iteration 1118 the Loss is 0.5916.\n",
      "For Iteration 1119 the Loss is 0.5909.\n",
      "For Iteration 1120 the Loss is 0.5903.\n",
      "For Iteration 1121 the Loss is 0.5896.\n",
      "For Iteration 1122 the Loss is 0.589.\n",
      "For Iteration 1123 the Loss is 0.5883.\n",
      "For Iteration 1124 the Loss is 0.5877.\n",
      "For Iteration 1125 the Loss is 0.587.\n",
      "For Iteration 1126 the Loss is 0.5864.\n",
      "For Iteration 1127 the Loss is 0.5857.\n",
      "For Iteration 1128 the Loss is 0.5851.\n",
      "For Iteration 1129 the Loss is 0.5845.\n",
      "For Iteration 1130 the Loss is 0.5838.\n",
      "For Iteration 1131 the Loss is 0.5832.\n",
      "For Iteration 1132 the Loss is 0.5826.\n",
      "For Iteration 1133 the Loss is 0.5819.\n",
      "For Iteration 1134 the Loss is 0.5813.\n",
      "For Iteration 1135 the Loss is 0.5807.\n",
      "For Iteration 1136 the Loss is 0.5801.\n",
      "For Iteration 1137 the Loss is 0.5795.\n",
      "For Iteration 1138 the Loss is 0.5788.\n",
      "For Iteration 1139 the Loss is 0.5782.\n",
      "For Iteration 1140 the Loss is 0.5776.\n",
      "For Iteration 1141 the Loss is 0.577.\n",
      "For Iteration 1142 the Loss is 0.5764.\n",
      "For Iteration 1143 the Loss is 0.5758.\n",
      "For Iteration 1144 the Loss is 0.5752.\n",
      "For Iteration 1145 the Loss is 0.5746.\n",
      "For Iteration 1146 the Loss is 0.574.\n",
      "For Iteration 1147 the Loss is 0.5734.\n",
      "For Iteration 1148 the Loss is 0.5728.\n",
      "For Iteration 1149 the Loss is 0.5722.\n",
      "For Iteration 1150 the Loss is 0.5716.\n",
      "For Iteration 1151 the Loss is 0.571.\n",
      "For Iteration 1152 the Loss is 0.5704.\n",
      "For Iteration 1153 the Loss is 0.5698.\n",
      "For Iteration 1154 the Loss is 0.5692.\n",
      "For Iteration 1155 the Loss is 0.5687.\n",
      "For Iteration 1156 the Loss is 0.5681.\n",
      "For Iteration 1157 the Loss is 0.5675.\n",
      "For Iteration 1158 the Loss is 0.5669.\n",
      "For Iteration 1159 the Loss is 0.5663.\n",
      "For Iteration 1160 the Loss is 0.5658.\n",
      "For Iteration 1161 the Loss is 0.5652.\n",
      "For Iteration 1162 the Loss is 0.5646.\n",
      "For Iteration 1163 the Loss is 0.5641.\n",
      "For Iteration 1164 the Loss is 0.5635.\n",
      "For Iteration 1165 the Loss is 0.5629.\n",
      "For Iteration 1166 the Loss is 0.5624.\n",
      "For Iteration 1167 the Loss is 0.5618.\n",
      "For Iteration 1168 the Loss is 0.5613.\n",
      "For Iteration 1169 the Loss is 0.5607.\n",
      "For Iteration 1170 the Loss is 0.5602.\n",
      "For Iteration 1171 the Loss is 0.5596.\n",
      "For Iteration 1172 the Loss is 0.5591.\n",
      "For Iteration 1173 the Loss is 0.5585.\n",
      "For Iteration 1174 the Loss is 0.558.\n",
      "For Iteration 1175 the Loss is 0.5574.\n",
      "For Iteration 1176 the Loss is 0.5569.\n",
      "For Iteration 1177 the Loss is 0.5563.\n",
      "For Iteration 1178 the Loss is 0.5558.\n",
      "For Iteration 1179 the Loss is 0.5553.\n",
      "For Iteration 1180 the Loss is 0.5547.\n",
      "For Iteration 1181 the Loss is 0.5542.\n",
      "For Iteration 1182 the Loss is 0.5537.\n",
      "For Iteration 1183 the Loss is 0.5531.\n",
      "For Iteration 1184 the Loss is 0.5526.\n",
      "For Iteration 1185 the Loss is 0.5521.\n",
      "For Iteration 1186 the Loss is 0.5516.\n",
      "For Iteration 1187 the Loss is 0.551.\n",
      "For Iteration 1188 the Loss is 0.5505.\n",
      "For Iteration 1189 the Loss is 0.55.\n",
      "For Iteration 1190 the Loss is 0.5495.\n",
      "For Iteration 1191 the Loss is 0.549.\n",
      "For Iteration 1192 the Loss is 0.5484.\n",
      "For Iteration 1193 the Loss is 0.5479.\n",
      "For Iteration 1194 the Loss is 0.5474.\n",
      "For Iteration 1195 the Loss is 0.5469.\n",
      "For Iteration 1196 the Loss is 0.5464.\n",
      "For Iteration 1197 the Loss is 0.5459.\n",
      "For Iteration 1198 the Loss is 0.5454.\n",
      "For Iteration 1199 the Loss is 0.5449.\n",
      "For Iteration 1200 the Loss is 0.5444.\n",
      "For Iteration 1201 the Loss is 0.5439.\n",
      "For Iteration 1202 the Loss is 0.5434.\n",
      "For Iteration 1203 the Loss is 0.5429.\n",
      "For Iteration 1204 the Loss is 0.5424.\n",
      "For Iteration 1205 the Loss is 0.5419.\n",
      "For Iteration 1206 the Loss is 0.5414.\n",
      "For Iteration 1207 the Loss is 0.5409.\n",
      "For Iteration 1208 the Loss is 0.5404.\n",
      "For Iteration 1209 the Loss is 0.54.\n",
      "For Iteration 1210 the Loss is 0.5395.\n",
      "For Iteration 1211 the Loss is 0.539.\n",
      "For Iteration 1212 the Loss is 0.5385.\n",
      "For Iteration 1213 the Loss is 0.538.\n",
      "For Iteration 1214 the Loss is 0.5376.\n",
      "For Iteration 1215 the Loss is 0.5371.\n",
      "For Iteration 1216 the Loss is 0.5366.\n",
      "For Iteration 1217 the Loss is 0.5361.\n",
      "For Iteration 1218 the Loss is 0.5357.\n",
      "For Iteration 1219 the Loss is 0.5352.\n",
      "For Iteration 1220 the Loss is 0.5347.\n",
      "For Iteration 1221 the Loss is 0.5343.\n",
      "For Iteration 1222 the Loss is 0.5338.\n",
      "For Iteration 1223 the Loss is 0.5333.\n",
      "For Iteration 1224 the Loss is 0.5329.\n",
      "For Iteration 1225 the Loss is 0.5324.\n",
      "For Iteration 1226 the Loss is 0.5319.\n",
      "For Iteration 1227 the Loss is 0.5315.\n",
      "For Iteration 1228 the Loss is 0.531.\n",
      "For Iteration 1229 the Loss is 0.5306.\n",
      "For Iteration 1230 the Loss is 0.5301.\n",
      "For Iteration 1231 the Loss is 0.5297.\n",
      "For Iteration 1232 the Loss is 0.5292.\n",
      "For Iteration 1233 the Loss is 0.5288.\n",
      "For Iteration 1234 the Loss is 0.5283.\n",
      "For Iteration 1235 the Loss is 0.5279.\n",
      "For Iteration 1236 the Loss is 0.5274.\n",
      "For Iteration 1237 the Loss is 0.527.\n",
      "For Iteration 1238 the Loss is 0.5266.\n",
      "For Iteration 1239 the Loss is 0.5261.\n",
      "For Iteration 1240 the Loss is 0.5257.\n",
      "For Iteration 1241 the Loss is 0.5252.\n",
      "For Iteration 1242 the Loss is 0.5248.\n",
      "For Iteration 1243 the Loss is 0.5244.\n",
      "For Iteration 1244 the Loss is 0.5239.\n",
      "For Iteration 1245 the Loss is 0.5235.\n",
      "For Iteration 1246 the Loss is 0.5231.\n",
      "For Iteration 1247 the Loss is 0.5226.\n",
      "For Iteration 1248 the Loss is 0.5222.\n",
      "For Iteration 1249 the Loss is 0.5218.\n",
      "For Iteration 1250 the Loss is 0.5214.\n",
      "For Iteration 1251 the Loss is 0.5209.\n",
      "For Iteration 1252 the Loss is 0.5205.\n",
      "For Iteration 1253 the Loss is 0.5201.\n",
      "For Iteration 1254 the Loss is 0.5197.\n",
      "For Iteration 1255 the Loss is 0.5193.\n",
      "For Iteration 1256 the Loss is 0.5188.\n",
      "For Iteration 1257 the Loss is 0.5184.\n",
      "For Iteration 1258 the Loss is 0.518.\n",
      "For Iteration 1259 the Loss is 0.5176.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1260 the Loss is 0.5172.\n",
      "For Iteration 1261 the Loss is 0.5168.\n",
      "For Iteration 1262 the Loss is 0.5164.\n",
      "For Iteration 1263 the Loss is 0.516.\n",
      "For Iteration 1264 the Loss is 0.5156.\n",
      "For Iteration 1265 the Loss is 0.5152.\n",
      "For Iteration 1266 the Loss is 0.5147.\n",
      "For Iteration 1267 the Loss is 0.5143.\n",
      "For Iteration 1268 the Loss is 0.5139.\n",
      "For Iteration 1269 the Loss is 0.5135.\n",
      "For Iteration 1270 the Loss is 0.5131.\n",
      "For Iteration 1271 the Loss is 0.5128.\n",
      "For Iteration 1272 the Loss is 0.5124.\n",
      "For Iteration 1273 the Loss is 0.512.\n",
      "For Iteration 1274 the Loss is 0.5116.\n",
      "For Iteration 1275 the Loss is 0.5112.\n",
      "For Iteration 1276 the Loss is 0.5108.\n",
      "For Iteration 1277 the Loss is 0.5104.\n",
      "For Iteration 1278 the Loss is 0.51.\n",
      "For Iteration 1279 the Loss is 0.5096.\n",
      "For Iteration 1280 the Loss is 0.5092.\n",
      "For Iteration 1281 the Loss is 0.5088.\n",
      "For Iteration 1282 the Loss is 0.5085.\n",
      "For Iteration 1283 the Loss is 0.5081.\n",
      "For Iteration 1284 the Loss is 0.5077.\n",
      "For Iteration 1285 the Loss is 0.5073.\n",
      "For Iteration 1286 the Loss is 0.5069.\n",
      "For Iteration 1287 the Loss is 0.5066.\n",
      "For Iteration 1288 the Loss is 0.5062.\n",
      "For Iteration 1289 the Loss is 0.5058.\n",
      "For Iteration 1290 the Loss is 0.5054.\n",
      "For Iteration 1291 the Loss is 0.5051.\n",
      "For Iteration 1292 the Loss is 0.5047.\n",
      "For Iteration 1293 the Loss is 0.5043.\n",
      "For Iteration 1294 the Loss is 0.504.\n",
      "For Iteration 1295 the Loss is 0.5036.\n",
      "For Iteration 1296 the Loss is 0.5032.\n",
      "For Iteration 1297 the Loss is 0.5029.\n",
      "For Iteration 1298 the Loss is 0.5025.\n",
      "For Iteration 1299 the Loss is 0.5021.\n",
      "For Iteration 1300 the Loss is 0.5018.\n",
      "For Iteration 1301 the Loss is 0.5014.\n",
      "For Iteration 1302 the Loss is 0.501.\n",
      "For Iteration 1303 the Loss is 0.5007.\n",
      "For Iteration 1304 the Loss is 0.5003.\n",
      "For Iteration 1305 the Loss is 0.5.\n",
      "For Iteration 1306 the Loss is 0.4996.\n",
      "For Iteration 1307 the Loss is 0.4993.\n",
      "For Iteration 1308 the Loss is 0.4989.\n",
      "For Iteration 1309 the Loss is 0.4986.\n",
      "For Iteration 1310 the Loss is 0.4982.\n",
      "For Iteration 1311 the Loss is 0.4979.\n",
      "For Iteration 1312 the Loss is 0.4975.\n",
      "For Iteration 1313 the Loss is 0.4972.\n",
      "For Iteration 1314 the Loss is 0.4968.\n",
      "For Iteration 1315 the Loss is 0.4965.\n",
      "For Iteration 1316 the Loss is 0.4961.\n",
      "For Iteration 1317 the Loss is 0.4958.\n",
      "For Iteration 1318 the Loss is 0.4954.\n",
      "For Iteration 1319 the Loss is 0.4951.\n",
      "For Iteration 1320 the Loss is 0.4948.\n",
      "For Iteration 1321 the Loss is 0.4944.\n",
      "For Iteration 1322 the Loss is 0.4941.\n",
      "For Iteration 1323 the Loss is 0.4937.\n",
      "For Iteration 1324 the Loss is 0.4934.\n",
      "For Iteration 1325 the Loss is 0.4931.\n",
      "For Iteration 1326 the Loss is 0.4927.\n",
      "For Iteration 1327 the Loss is 0.4924.\n",
      "For Iteration 1328 the Loss is 0.4921.\n",
      "For Iteration 1329 the Loss is 0.4917.\n",
      "For Iteration 1330 the Loss is 0.4914.\n",
      "For Iteration 1331 the Loss is 0.4911.\n",
      "For Iteration 1332 the Loss is 0.4907.\n",
      "For Iteration 1333 the Loss is 0.4904.\n",
      "For Iteration 1334 the Loss is 0.4901.\n",
      "For Iteration 1335 the Loss is 0.4898.\n",
      "For Iteration 1336 the Loss is 0.4894.\n",
      "For Iteration 1337 the Loss is 0.4891.\n",
      "For Iteration 1338 the Loss is 0.4888.\n",
      "For Iteration 1339 the Loss is 0.4885.\n",
      "For Iteration 1340 the Loss is 0.4882.\n",
      "For Iteration 1341 the Loss is 0.4878.\n",
      "For Iteration 1342 the Loss is 0.4875.\n",
      "For Iteration 1343 the Loss is 0.4872.\n",
      "For Iteration 1344 the Loss is 0.4869.\n",
      "For Iteration 1345 the Loss is 0.4866.\n",
      "For Iteration 1346 the Loss is 0.4863.\n",
      "For Iteration 1347 the Loss is 0.4859.\n",
      "For Iteration 1348 the Loss is 0.4856.\n",
      "For Iteration 1349 the Loss is 0.4853.\n",
      "For Iteration 1350 the Loss is 0.485.\n",
      "For Iteration 1351 the Loss is 0.4847.\n",
      "For Iteration 1352 the Loss is 0.4844.\n",
      "For Iteration 1353 the Loss is 0.4841.\n",
      "For Iteration 1354 the Loss is 0.4838.\n",
      "For Iteration 1355 the Loss is 0.4835.\n",
      "For Iteration 1356 the Loss is 0.4832.\n",
      "For Iteration 1357 the Loss is 0.4829.\n",
      "For Iteration 1358 the Loss is 0.4826.\n",
      "For Iteration 1359 the Loss is 0.4823.\n",
      "For Iteration 1360 the Loss is 0.482.\n",
      "For Iteration 1361 the Loss is 0.4817.\n",
      "For Iteration 1362 the Loss is 0.4814.\n",
      "For Iteration 1363 the Loss is 0.4811.\n",
      "For Iteration 1364 the Loss is 0.4808.\n",
      "For Iteration 1365 the Loss is 0.4805.\n",
      "For Iteration 1366 the Loss is 0.4802.\n",
      "For Iteration 1367 the Loss is 0.4799.\n",
      "For Iteration 1368 the Loss is 0.4796.\n",
      "For Iteration 1369 the Loss is 0.4793.\n",
      "For Iteration 1370 the Loss is 0.479.\n",
      "For Iteration 1371 the Loss is 0.4787.\n",
      "For Iteration 1372 the Loss is 0.4784.\n",
      "For Iteration 1373 the Loss is 0.4781.\n",
      "For Iteration 1374 the Loss is 0.4778.\n",
      "For Iteration 1375 the Loss is 0.4775.\n",
      "For Iteration 1376 the Loss is 0.4772.\n",
      "For Iteration 1377 the Loss is 0.477.\n",
      "For Iteration 1378 the Loss is 0.4767.\n",
      "For Iteration 1379 the Loss is 0.4764.\n",
      "For Iteration 1380 the Loss is 0.4761.\n",
      "For Iteration 1381 the Loss is 0.4758.\n",
      "For Iteration 1382 the Loss is 0.4755.\n",
      "For Iteration 1383 the Loss is 0.4753.\n",
      "For Iteration 1384 the Loss is 0.475.\n",
      "For Iteration 1385 the Loss is 0.4747.\n",
      "For Iteration 1386 the Loss is 0.4744.\n",
      "For Iteration 1387 the Loss is 0.4741.\n",
      "For Iteration 1388 the Loss is 0.4739.\n",
      "For Iteration 1389 the Loss is 0.4736.\n",
      "For Iteration 1390 the Loss is 0.4733.\n",
      "For Iteration 1391 the Loss is 0.473.\n",
      "For Iteration 1392 the Loss is 0.4728.\n",
      "For Iteration 1393 the Loss is 0.4725.\n",
      "For Iteration 1394 the Loss is 0.4722.\n",
      "For Iteration 1395 the Loss is 0.4719.\n",
      "For Iteration 1396 the Loss is 0.4717.\n",
      "For Iteration 1397 the Loss is 0.4714.\n",
      "For Iteration 1398 the Loss is 0.4711.\n",
      "For Iteration 1399 the Loss is 0.4709.\n",
      "For Iteration 1400 the Loss is 0.4706.\n",
      "For Iteration 1401 the Loss is 0.4703.\n",
      "For Iteration 1402 the Loss is 0.4701.\n",
      "For Iteration 1403 the Loss is 0.4698.\n",
      "For Iteration 1404 the Loss is 0.4695.\n",
      "For Iteration 1405 the Loss is 0.4693.\n",
      "For Iteration 1406 the Loss is 0.469.\n",
      "For Iteration 1407 the Loss is 0.4687.\n",
      "For Iteration 1408 the Loss is 0.4685.\n",
      "For Iteration 1409 the Loss is 0.4682.\n",
      "For Iteration 1410 the Loss is 0.468.\n",
      "For Iteration 1411 the Loss is 0.4677.\n",
      "For Iteration 1412 the Loss is 0.4674.\n",
      "For Iteration 1413 the Loss is 0.4672.\n",
      "For Iteration 1414 the Loss is 0.4669.\n",
      "For Iteration 1415 the Loss is 0.4667.\n",
      "For Iteration 1416 the Loss is 0.4664.\n",
      "For Iteration 1417 the Loss is 0.4662.\n",
      "For Iteration 1418 the Loss is 0.4659.\n",
      "For Iteration 1419 the Loss is 0.4657.\n",
      "For Iteration 1420 the Loss is 0.4654.\n",
      "For Iteration 1421 the Loss is 0.4652.\n",
      "For Iteration 1422 the Loss is 0.4649.\n",
      "For Iteration 1423 the Loss is 0.4647.\n",
      "For Iteration 1424 the Loss is 0.4644.\n",
      "For Iteration 1425 the Loss is 0.4642.\n",
      "For Iteration 1426 the Loss is 0.4639.\n",
      "For Iteration 1427 the Loss is 0.4637.\n",
      "For Iteration 1428 the Loss is 0.4634.\n",
      "For Iteration 1429 the Loss is 0.4632.\n",
      "For Iteration 1430 the Loss is 0.4629.\n",
      "For Iteration 1431 the Loss is 0.4627.\n",
      "For Iteration 1432 the Loss is 0.4624.\n",
      "For Iteration 1433 the Loss is 0.4622.\n",
      "For Iteration 1434 the Loss is 0.4619.\n",
      "For Iteration 1435 the Loss is 0.4617.\n",
      "For Iteration 1436 the Loss is 0.4615.\n",
      "For Iteration 1437 the Loss is 0.4612.\n",
      "For Iteration 1438 the Loss is 0.461.\n",
      "For Iteration 1439 the Loss is 0.4607.\n",
      "For Iteration 1440 the Loss is 0.4605.\n",
      "For Iteration 1441 the Loss is 0.4603.\n",
      "For Iteration 1442 the Loss is 0.46.\n",
      "For Iteration 1443 the Loss is 0.4598.\n",
      "For Iteration 1444 the Loss is 0.4596.\n",
      "For Iteration 1445 the Loss is 0.4593.\n",
      "For Iteration 1446 the Loss is 0.4591.\n",
      "For Iteration 1447 the Loss is 0.4589.\n",
      "For Iteration 1448 the Loss is 0.4586.\n",
      "For Iteration 1449 the Loss is 0.4584.\n",
      "For Iteration 1450 the Loss is 0.4582.\n",
      "For Iteration 1451 the Loss is 0.4579.\n",
      "For Iteration 1452 the Loss is 0.4577.\n",
      "For Iteration 1453 the Loss is 0.4575.\n",
      "For Iteration 1454 the Loss is 0.4572.\n",
      "For Iteration 1455 the Loss is 0.457.\n",
      "For Iteration 1456 the Loss is 0.4568.\n",
      "For Iteration 1457 the Loss is 0.4566.\n",
      "For Iteration 1458 the Loss is 0.4563.\n",
      "For Iteration 1459 the Loss is 0.4561.\n",
      "For Iteration 1460 the Loss is 0.4559.\n",
      "For Iteration 1461 the Loss is 0.4557.\n",
      "For Iteration 1462 the Loss is 0.4554.\n",
      "For Iteration 1463 the Loss is 0.4552.\n",
      "For Iteration 1464 the Loss is 0.455.\n",
      "For Iteration 1465 the Loss is 0.4548.\n",
      "For Iteration 1466 the Loss is 0.4545.\n",
      "For Iteration 1467 the Loss is 0.4543.\n",
      "For Iteration 1468 the Loss is 0.4541.\n",
      "For Iteration 1469 the Loss is 0.4539.\n",
      "For Iteration 1470 the Loss is 0.4537.\n",
      "For Iteration 1471 the Loss is 0.4534.\n",
      "For Iteration 1472 the Loss is 0.4532.\n",
      "For Iteration 1473 the Loss is 0.453.\n",
      "For Iteration 1474 the Loss is 0.4528.\n",
      "For Iteration 1475 the Loss is 0.4526.\n",
      "For Iteration 1476 the Loss is 0.4524.\n",
      "For Iteration 1477 the Loss is 0.4521.\n",
      "For Iteration 1478 the Loss is 0.4519.\n",
      "For Iteration 1479 the Loss is 0.4517.\n",
      "For Iteration 1480 the Loss is 0.4515.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1481 the Loss is 0.4513.\n",
      "For Iteration 1482 the Loss is 0.4511.\n",
      "For Iteration 1483 the Loss is 0.4509.\n",
      "For Iteration 1484 the Loss is 0.4507.\n",
      "For Iteration 1485 the Loss is 0.4504.\n",
      "For Iteration 1486 the Loss is 0.4502.\n",
      "For Iteration 1487 the Loss is 0.45.\n",
      "For Iteration 1488 the Loss is 0.4498.\n",
      "For Iteration 1489 the Loss is 0.4496.\n",
      "For Iteration 1490 the Loss is 0.4494.\n",
      "For Iteration 1491 the Loss is 0.4492.\n",
      "For Iteration 1492 the Loss is 0.449.\n",
      "For Iteration 1493 the Loss is 0.4488.\n",
      "For Iteration 1494 the Loss is 0.4486.\n",
      "For Iteration 1495 the Loss is 0.4484.\n",
      "For Iteration 1496 the Loss is 0.4482.\n",
      "For Iteration 1497 the Loss is 0.448.\n",
      "For Iteration 1498 the Loss is 0.4478.\n",
      "For Iteration 1499 the Loss is 0.4476.\n",
      "For Iteration 1500 the Loss is 0.4474.\n",
      "For Iteration 1501 the Loss is 0.4472.\n",
      "For Iteration 1502 the Loss is 0.447.\n",
      "For Iteration 1503 the Loss is 0.4468.\n",
      "For Iteration 1504 the Loss is 0.4466.\n",
      "For Iteration 1505 the Loss is 0.4464.\n",
      "For Iteration 1506 the Loss is 0.4462.\n",
      "For Iteration 1507 the Loss is 0.446.\n",
      "For Iteration 1508 the Loss is 0.4458.\n",
      "For Iteration 1509 the Loss is 0.4456.\n",
      "For Iteration 1510 the Loss is 0.4454.\n",
      "For Iteration 1511 the Loss is 0.4452.\n",
      "For Iteration 1512 the Loss is 0.445.\n",
      "For Iteration 1513 the Loss is 0.4448.\n",
      "For Iteration 1514 the Loss is 0.4446.\n",
      "For Iteration 1515 the Loss is 0.4444.\n",
      "For Iteration 1516 the Loss is 0.4442.\n",
      "For Iteration 1517 the Loss is 0.444.\n",
      "For Iteration 1518 the Loss is 0.4438.\n",
      "For Iteration 1519 the Loss is 0.4436.\n",
      "For Iteration 1520 the Loss is 0.4434.\n",
      "For Iteration 1521 the Loss is 0.4433.\n",
      "For Iteration 1522 the Loss is 0.4431.\n",
      "For Iteration 1523 the Loss is 0.4429.\n",
      "For Iteration 1524 the Loss is 0.4427.\n",
      "For Iteration 1525 the Loss is 0.4425.\n",
      "For Iteration 1526 the Loss is 0.4423.\n",
      "For Iteration 1527 the Loss is 0.4421.\n",
      "For Iteration 1528 the Loss is 0.4419.\n",
      "For Iteration 1529 the Loss is 0.4418.\n",
      "For Iteration 1530 the Loss is 0.4416.\n",
      "For Iteration 1531 the Loss is 0.4414.\n",
      "For Iteration 1532 the Loss is 0.4412.\n",
      "For Iteration 1533 the Loss is 0.441.\n",
      "For Iteration 1534 the Loss is 0.4408.\n",
      "For Iteration 1535 the Loss is 0.4406.\n",
      "For Iteration 1536 the Loss is 0.4405.\n",
      "For Iteration 1537 the Loss is 0.4403.\n",
      "For Iteration 1538 the Loss is 0.4401.\n",
      "For Iteration 1539 the Loss is 0.4399.\n",
      "For Iteration 1540 the Loss is 0.4397.\n",
      "For Iteration 1541 the Loss is 0.4396.\n",
      "For Iteration 1542 the Loss is 0.4394.\n",
      "For Iteration 1543 the Loss is 0.4392.\n",
      "For Iteration 1544 the Loss is 0.439.\n",
      "For Iteration 1545 the Loss is 0.4388.\n",
      "For Iteration 1546 the Loss is 0.4387.\n",
      "For Iteration 1547 the Loss is 0.4385.\n",
      "For Iteration 1548 the Loss is 0.4383.\n",
      "For Iteration 1549 the Loss is 0.4381.\n",
      "For Iteration 1550 the Loss is 0.438.\n",
      "For Iteration 1551 the Loss is 0.4378.\n",
      "For Iteration 1552 the Loss is 0.4376.\n",
      "For Iteration 1553 the Loss is 0.4374.\n",
      "For Iteration 1554 the Loss is 0.4373.\n",
      "For Iteration 1555 the Loss is 0.4371.\n",
      "For Iteration 1556 the Loss is 0.4369.\n",
      "For Iteration 1557 the Loss is 0.4367.\n",
      "For Iteration 1558 the Loss is 0.4366.\n",
      "For Iteration 1559 the Loss is 0.4364.\n",
      "For Iteration 1560 the Loss is 0.4362.\n",
      "For Iteration 1561 the Loss is 0.436.\n",
      "For Iteration 1562 the Loss is 0.4359.\n",
      "For Iteration 1563 the Loss is 0.4357.\n",
      "For Iteration 1564 the Loss is 0.4355.\n",
      "For Iteration 1565 the Loss is 0.4354.\n",
      "For Iteration 1566 the Loss is 0.4352.\n",
      "For Iteration 1567 the Loss is 0.435.\n",
      "For Iteration 1568 the Loss is 0.4349.\n",
      "For Iteration 1569 the Loss is 0.4347.\n",
      "For Iteration 1570 the Loss is 0.4345.\n",
      "For Iteration 1571 the Loss is 0.4344.\n",
      "For Iteration 1572 the Loss is 0.4342.\n",
      "For Iteration 1573 the Loss is 0.434.\n",
      "For Iteration 1574 the Loss is 0.4339.\n",
      "For Iteration 1575 the Loss is 0.4337.\n",
      "For Iteration 1576 the Loss is 0.4335.\n",
      "For Iteration 1577 the Loss is 0.4334.\n",
      "For Iteration 1578 the Loss is 0.4332.\n",
      "For Iteration 1579 the Loss is 0.433.\n",
      "For Iteration 1580 the Loss is 0.4329.\n",
      "For Iteration 1581 the Loss is 0.4327.\n",
      "For Iteration 1582 the Loss is 0.4326.\n",
      "For Iteration 1583 the Loss is 0.4324.\n",
      "For Iteration 1584 the Loss is 0.4322.\n",
      "For Iteration 1585 the Loss is 0.4321.\n",
      "For Iteration 1586 the Loss is 0.4319.\n",
      "For Iteration 1587 the Loss is 0.4318.\n",
      "For Iteration 1588 the Loss is 0.4316.\n",
      "For Iteration 1589 the Loss is 0.4314.\n",
      "For Iteration 1590 the Loss is 0.4313.\n",
      "For Iteration 1591 the Loss is 0.4311.\n",
      "For Iteration 1592 the Loss is 0.431.\n",
      "For Iteration 1593 the Loss is 0.4308.\n",
      "For Iteration 1594 the Loss is 0.4306.\n",
      "For Iteration 1595 the Loss is 0.4305.\n",
      "For Iteration 1596 the Loss is 0.4303.\n",
      "For Iteration 1597 the Loss is 0.4302.\n",
      "For Iteration 1598 the Loss is 0.43.\n",
      "For Iteration 1599 the Loss is 0.4299.\n",
      "For Iteration 1600 the Loss is 0.4297.\n",
      "For Iteration 1601 the Loss is 0.4296.\n",
      "For Iteration 1602 the Loss is 0.4294.\n",
      "For Iteration 1603 the Loss is 0.4293.\n",
      "For Iteration 1604 the Loss is 0.4291.\n",
      "For Iteration 1605 the Loss is 0.4289.\n",
      "For Iteration 1606 the Loss is 0.4288.\n",
      "For Iteration 1607 the Loss is 0.4286.\n",
      "For Iteration 1608 the Loss is 0.4285.\n",
      "For Iteration 1609 the Loss is 0.4283.\n",
      "For Iteration 1610 the Loss is 0.4282.\n",
      "For Iteration 1611 the Loss is 0.428.\n",
      "For Iteration 1612 the Loss is 0.4279.\n",
      "For Iteration 1613 the Loss is 0.4277.\n",
      "For Iteration 1614 the Loss is 0.4276.\n",
      "For Iteration 1615 the Loss is 0.4274.\n",
      "For Iteration 1616 the Loss is 0.4273.\n",
      "For Iteration 1617 the Loss is 0.4271.\n",
      "For Iteration 1618 the Loss is 0.427.\n",
      "For Iteration 1619 the Loss is 0.4269.\n",
      "For Iteration 1620 the Loss is 0.4267.\n",
      "For Iteration 1621 the Loss is 0.4266.\n",
      "For Iteration 1622 the Loss is 0.4264.\n",
      "For Iteration 1623 the Loss is 0.4263.\n",
      "For Iteration 1624 the Loss is 0.4261.\n",
      "For Iteration 1625 the Loss is 0.426.\n",
      "For Iteration 1626 the Loss is 0.4258.\n",
      "For Iteration 1627 the Loss is 0.4257.\n",
      "For Iteration 1628 the Loss is 0.4255.\n",
      "For Iteration 1629 the Loss is 0.4254.\n",
      "For Iteration 1630 the Loss is 0.4253.\n",
      "For Iteration 1631 the Loss is 0.4251.\n",
      "For Iteration 1632 the Loss is 0.425.\n",
      "For Iteration 1633 the Loss is 0.4248.\n",
      "For Iteration 1634 the Loss is 0.4247.\n",
      "For Iteration 1635 the Loss is 0.4245.\n",
      "For Iteration 1636 the Loss is 0.4244.\n",
      "For Iteration 1637 the Loss is 0.4243.\n",
      "For Iteration 1638 the Loss is 0.4241.\n",
      "For Iteration 1639 the Loss is 0.424.\n",
      "For Iteration 1640 the Loss is 0.4238.\n",
      "For Iteration 1641 the Loss is 0.4237.\n",
      "For Iteration 1642 the Loss is 0.4236.\n",
      "For Iteration 1643 the Loss is 0.4234.\n",
      "For Iteration 1644 the Loss is 0.4233.\n",
      "For Iteration 1645 the Loss is 0.4232.\n",
      "For Iteration 1646 the Loss is 0.423.\n",
      "For Iteration 1647 the Loss is 0.4229.\n",
      "For Iteration 1648 the Loss is 0.4227.\n",
      "For Iteration 1649 the Loss is 0.4226.\n",
      "For Iteration 1650 the Loss is 0.4225.\n",
      "For Iteration 1651 the Loss is 0.4223.\n",
      "For Iteration 1652 the Loss is 0.4222.\n",
      "For Iteration 1653 the Loss is 0.4221.\n",
      "For Iteration 1654 the Loss is 0.4219.\n",
      "For Iteration 1655 the Loss is 0.4218.\n",
      "For Iteration 1656 the Loss is 0.4217.\n",
      "For Iteration 1657 the Loss is 0.4215.\n",
      "For Iteration 1658 the Loss is 0.4214.\n",
      "For Iteration 1659 the Loss is 0.4213.\n",
      "For Iteration 1660 the Loss is 0.4211.\n",
      "For Iteration 1661 the Loss is 0.421.\n",
      "For Iteration 1662 the Loss is 0.4209.\n",
      "For Iteration 1663 the Loss is 0.4207.\n",
      "For Iteration 1664 the Loss is 0.4206.\n",
      "For Iteration 1665 the Loss is 0.4205.\n",
      "For Iteration 1666 the Loss is 0.4203.\n",
      "For Iteration 1667 the Loss is 0.4202.\n",
      "For Iteration 1668 the Loss is 0.4201.\n",
      "For Iteration 1669 the Loss is 0.4199.\n",
      "For Iteration 1670 the Loss is 0.4198.\n",
      "For Iteration 1671 the Loss is 0.4197.\n",
      "For Iteration 1672 the Loss is 0.4196.\n",
      "For Iteration 1673 the Loss is 0.4194.\n",
      "For Iteration 1674 the Loss is 0.4193.\n",
      "For Iteration 1675 the Loss is 0.4192.\n",
      "For Iteration 1676 the Loss is 0.419.\n",
      "For Iteration 1677 the Loss is 0.4189.\n",
      "For Iteration 1678 the Loss is 0.4188.\n",
      "For Iteration 1679 the Loss is 0.4187.\n",
      "For Iteration 1680 the Loss is 0.4185.\n",
      "For Iteration 1681 the Loss is 0.4184.\n",
      "For Iteration 1682 the Loss is 0.4183.\n",
      "For Iteration 1683 the Loss is 0.4182.\n",
      "For Iteration 1684 the Loss is 0.418.\n",
      "For Iteration 1685 the Loss is 0.4179.\n",
      "For Iteration 1686 the Loss is 0.4178.\n",
      "For Iteration 1687 the Loss is 0.4177.\n",
      "For Iteration 1688 the Loss is 0.4175.\n",
      "For Iteration 1689 the Loss is 0.4174.\n",
      "For Iteration 1690 the Loss is 0.4173.\n",
      "For Iteration 1691 the Loss is 0.4172.\n",
      "For Iteration 1692 the Loss is 0.4171.\n",
      "For Iteration 1693 the Loss is 0.4169.\n",
      "For Iteration 1694 the Loss is 0.4168.\n",
      "For Iteration 1695 the Loss is 0.4167.\n",
      "For Iteration 1696 the Loss is 0.4166.\n",
      "For Iteration 1697 the Loss is 0.4164.\n",
      "For Iteration 1698 the Loss is 0.4163.\n",
      "For Iteration 1699 the Loss is 0.4162.\n",
      "For Iteration 1700 the Loss is 0.4161.\n",
      "For Iteration 1701 the Loss is 0.416.\n",
      "For Iteration 1702 the Loss is 0.4158.\n",
      "For Iteration 1703 the Loss is 0.4157.\n",
      "For Iteration 1704 the Loss is 0.4156.\n",
      "For Iteration 1705 the Loss is 0.4155.\n",
      "For Iteration 1706 the Loss is 0.4154.\n",
      "For Iteration 1707 the Loss is 0.4152.\n",
      "For Iteration 1708 the Loss is 0.4151.\n",
      "For Iteration 1709 the Loss is 0.415.\n",
      "For Iteration 1710 the Loss is 0.4149.\n",
      "For Iteration 1711 the Loss is 0.4148.\n",
      "For Iteration 1712 the Loss is 0.4147.\n",
      "For Iteration 1713 the Loss is 0.4145.\n",
      "For Iteration 1714 the Loss is 0.4144.\n",
      "For Iteration 1715 the Loss is 0.4143.\n",
      "For Iteration 1716 the Loss is 0.4142.\n",
      "For Iteration 1717 the Loss is 0.4141.\n",
      "For Iteration 1718 the Loss is 0.414.\n",
      "For Iteration 1719 the Loss is 0.4138.\n",
      "For Iteration 1720 the Loss is 0.4137.\n",
      "For Iteration 1721 the Loss is 0.4136.\n",
      "For Iteration 1722 the Loss is 0.4135.\n",
      "For Iteration 1723 the Loss is 0.4134.\n",
      "For Iteration 1724 the Loss is 0.4133.\n",
      "For Iteration 1725 the Loss is 0.4132.\n",
      "For Iteration 1726 the Loss is 0.4131.\n",
      "For Iteration 1727 the Loss is 0.4129.\n",
      "For Iteration 1728 the Loss is 0.4128.\n",
      "For Iteration 1729 the Loss is 0.4127.\n",
      "For Iteration 1730 the Loss is 0.4126.\n",
      "For Iteration 1731 the Loss is 0.4125.\n",
      "For Iteration 1732 the Loss is 0.4124.\n",
      "For Iteration 1733 the Loss is 0.4123.\n",
      "For Iteration 1734 the Loss is 0.4122.\n",
      "For Iteration 1735 the Loss is 0.412.\n",
      "For Iteration 1736 the Loss is 0.4119.\n",
      "For Iteration 1737 the Loss is 0.4118.\n",
      "For Iteration 1738 the Loss is 0.4117.\n",
      "For Iteration 1739 the Loss is 0.4116.\n",
      "For Iteration 1740 the Loss is 0.4115.\n",
      "For Iteration 1741 the Loss is 0.4114.\n",
      "For Iteration 1742 the Loss is 0.4113.\n",
      "For Iteration 1743 the Loss is 0.4112.\n",
      "For Iteration 1744 the Loss is 0.4111.\n",
      "For Iteration 1745 the Loss is 0.411.\n",
      "For Iteration 1746 the Loss is 0.4108.\n",
      "For Iteration 1747 the Loss is 0.4107.\n",
      "For Iteration 1748 the Loss is 0.4106.\n",
      "For Iteration 1749 the Loss is 0.4105.\n",
      "For Iteration 1750 the Loss is 0.4104.\n",
      "For Iteration 1751 the Loss is 0.4103.\n",
      "For Iteration 1752 the Loss is 0.4102.\n",
      "For Iteration 1753 the Loss is 0.4101.\n",
      "For Iteration 1754 the Loss is 0.41.\n",
      "For Iteration 1755 the Loss is 0.4099.\n",
      "For Iteration 1756 the Loss is 0.4098.\n",
      "For Iteration 1757 the Loss is 0.4097.\n",
      "For Iteration 1758 the Loss is 0.4096.\n",
      "For Iteration 1759 the Loss is 0.4095.\n",
      "For Iteration 1760 the Loss is 0.4094.\n",
      "For Iteration 1761 the Loss is 0.4093.\n",
      "For Iteration 1762 the Loss is 0.4092.\n",
      "For Iteration 1763 the Loss is 0.4091.\n",
      "For Iteration 1764 the Loss is 0.409.\n",
      "For Iteration 1765 the Loss is 0.4088.\n",
      "For Iteration 1766 the Loss is 0.4087.\n",
      "For Iteration 1767 the Loss is 0.4086.\n",
      "For Iteration 1768 the Loss is 0.4085.\n",
      "For Iteration 1769 the Loss is 0.4084.\n",
      "For Iteration 1770 the Loss is 0.4083.\n",
      "For Iteration 1771 the Loss is 0.4082.\n",
      "For Iteration 1772 the Loss is 0.4081.\n",
      "For Iteration 1773 the Loss is 0.408.\n",
      "For Iteration 1774 the Loss is 0.4079.\n",
      "For Iteration 1775 the Loss is 0.4078.\n",
      "For Iteration 1776 the Loss is 0.4077.\n",
      "For Iteration 1777 the Loss is 0.4076.\n",
      "For Iteration 1778 the Loss is 0.4075.\n",
      "For Iteration 1779 the Loss is 0.4074.\n",
      "For Iteration 1780 the Loss is 0.4073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1781 the Loss is 0.4072.\n",
      "For Iteration 1782 the Loss is 0.4071.\n",
      "For Iteration 1783 the Loss is 0.407.\n",
      "For Iteration 1784 the Loss is 0.4069.\n",
      "For Iteration 1785 the Loss is 0.4068.\n",
      "For Iteration 1786 the Loss is 0.4067.\n",
      "For Iteration 1787 the Loss is 0.4066.\n",
      "For Iteration 1788 the Loss is 0.4065.\n",
      "For Iteration 1789 the Loss is 0.4064.\n",
      "For Iteration 1790 the Loss is 0.4063.\n",
      "For Iteration 1791 the Loss is 0.4063.\n",
      "For Iteration 1792 the Loss is 0.4062.\n",
      "For Iteration 1793 the Loss is 0.4061.\n",
      "For Iteration 1794 the Loss is 0.406.\n",
      "For Iteration 1795 the Loss is 0.4059.\n",
      "For Iteration 1796 the Loss is 0.4058.\n",
      "For Iteration 1797 the Loss is 0.4057.\n",
      "For Iteration 1798 the Loss is 0.4056.\n",
      "For Iteration 1799 the Loss is 0.4055.\n",
      "For Iteration 1800 the Loss is 0.4054.\n",
      "For Iteration 1801 the Loss is 0.4053.\n",
      "For Iteration 1802 the Loss is 0.4052.\n",
      "For Iteration 1803 the Loss is 0.4051.\n",
      "For Iteration 1804 the Loss is 0.405.\n",
      "For Iteration 1805 the Loss is 0.4049.\n",
      "For Iteration 1806 the Loss is 0.4048.\n",
      "For Iteration 1807 the Loss is 0.4047.\n",
      "For Iteration 1808 the Loss is 0.4046.\n",
      "For Iteration 1809 the Loss is 0.4045.\n",
      "For Iteration 1810 the Loss is 0.4045.\n",
      "For Iteration 1811 the Loss is 0.4044.\n",
      "For Iteration 1812 the Loss is 0.4043.\n",
      "For Iteration 1813 the Loss is 0.4042.\n",
      "For Iteration 1814 the Loss is 0.4041.\n",
      "For Iteration 1815 the Loss is 0.404.\n",
      "For Iteration 1816 the Loss is 0.4039.\n",
      "For Iteration 1817 the Loss is 0.4038.\n",
      "For Iteration 1818 the Loss is 0.4037.\n",
      "For Iteration 1819 the Loss is 0.4036.\n",
      "For Iteration 1820 the Loss is 0.4035.\n",
      "For Iteration 1821 the Loss is 0.4034.\n",
      "For Iteration 1822 the Loss is 0.4034.\n",
      "For Iteration 1823 the Loss is 0.4033.\n",
      "For Iteration 1824 the Loss is 0.4032.\n",
      "For Iteration 1825 the Loss is 0.4031.\n",
      "For Iteration 1826 the Loss is 0.403.\n",
      "For Iteration 1827 the Loss is 0.4029.\n",
      "For Iteration 1828 the Loss is 0.4028.\n",
      "For Iteration 1829 the Loss is 0.4027.\n",
      "For Iteration 1830 the Loss is 0.4026.\n",
      "For Iteration 1831 the Loss is 0.4025.\n",
      "For Iteration 1832 the Loss is 0.4025.\n",
      "For Iteration 1833 the Loss is 0.4024.\n",
      "For Iteration 1834 the Loss is 0.4023.\n",
      "For Iteration 1835 the Loss is 0.4022.\n",
      "For Iteration 1836 the Loss is 0.4021.\n",
      "For Iteration 1837 the Loss is 0.402.\n",
      "For Iteration 1838 the Loss is 0.4019.\n",
      "For Iteration 1839 the Loss is 0.4018.\n",
      "For Iteration 1840 the Loss is 0.4018.\n",
      "For Iteration 1841 the Loss is 0.4017.\n",
      "For Iteration 1842 the Loss is 0.4016.\n",
      "For Iteration 1843 the Loss is 0.4015.\n",
      "For Iteration 1844 the Loss is 0.4014.\n",
      "For Iteration 1845 the Loss is 0.4013.\n",
      "For Iteration 1846 the Loss is 0.4012.\n",
      "For Iteration 1847 the Loss is 0.4012.\n",
      "For Iteration 1848 the Loss is 0.4011.\n",
      "For Iteration 1849 the Loss is 0.401.\n",
      "For Iteration 1850 the Loss is 0.4009.\n",
      "For Iteration 1851 the Loss is 0.4008.\n",
      "For Iteration 1852 the Loss is 0.4007.\n",
      "For Iteration 1853 the Loss is 0.4006.\n",
      "For Iteration 1854 the Loss is 0.4006.\n",
      "For Iteration 1855 the Loss is 0.4005.\n",
      "For Iteration 1856 the Loss is 0.4004.\n",
      "For Iteration 1857 the Loss is 0.4003.\n",
      "For Iteration 1858 the Loss is 0.4002.\n",
      "For Iteration 1859 the Loss is 0.4001.\n",
      "For Iteration 1860 the Loss is 0.4001.\n",
      "For Iteration 1861 the Loss is 0.4.\n",
      "For Iteration 1862 the Loss is 0.3999.\n",
      "For Iteration 1863 the Loss is 0.3998.\n",
      "For Iteration 1864 the Loss is 0.3997.\n",
      "For Iteration 1865 the Loss is 0.3997.\n",
      "For Iteration 1866 the Loss is 0.3996.\n",
      "For Iteration 1867 the Loss is 0.3995.\n",
      "For Iteration 1868 the Loss is 0.3994.\n",
      "For Iteration 1869 the Loss is 0.3993.\n",
      "For Iteration 1870 the Loss is 0.3992.\n",
      "For Iteration 1871 the Loss is 0.3992.\n",
      "For Iteration 1872 the Loss is 0.3991.\n",
      "For Iteration 1873 the Loss is 0.399.\n",
      "For Iteration 1874 the Loss is 0.3989.\n",
      "For Iteration 1875 the Loss is 0.3988.\n",
      "For Iteration 1876 the Loss is 0.3988.\n",
      "For Iteration 1877 the Loss is 0.3987.\n",
      "For Iteration 1878 the Loss is 0.3986.\n",
      "For Iteration 1879 the Loss is 0.3985.\n",
      "For Iteration 1880 the Loss is 0.3984.\n",
      "For Iteration 1881 the Loss is 0.3984.\n",
      "For Iteration 1882 the Loss is 0.3983.\n",
      "For Iteration 1883 the Loss is 0.3982.\n",
      "For Iteration 1884 the Loss is 0.3981.\n",
      "For Iteration 1885 the Loss is 0.398.\n",
      "For Iteration 1886 the Loss is 0.398.\n",
      "For Iteration 1887 the Loss is 0.3979.\n",
      "For Iteration 1888 the Loss is 0.3978.\n",
      "For Iteration 1889 the Loss is 0.3977.\n",
      "For Iteration 1890 the Loss is 0.3977.\n",
      "For Iteration 1891 the Loss is 0.3976.\n",
      "For Iteration 1892 the Loss is 0.3975.\n",
      "For Iteration 1893 the Loss is 0.3974.\n",
      "For Iteration 1894 the Loss is 0.3973.\n",
      "For Iteration 1895 the Loss is 0.3973.\n",
      "For Iteration 1896 the Loss is 0.3972.\n",
      "For Iteration 1897 the Loss is 0.3971.\n",
      "For Iteration 1898 the Loss is 0.397.\n",
      "For Iteration 1899 the Loss is 0.397.\n",
      "For Iteration 1900 the Loss is 0.3969.\n",
      "For Iteration 1901 the Loss is 0.3968.\n",
      "For Iteration 1902 the Loss is 0.3967.\n",
      "For Iteration 1903 the Loss is 0.3967.\n",
      "For Iteration 1904 the Loss is 0.3966.\n",
      "For Iteration 1905 the Loss is 0.3965.\n",
      "For Iteration 1906 the Loss is 0.3964.\n",
      "For Iteration 1907 the Loss is 0.3964.\n",
      "For Iteration 1908 the Loss is 0.3963.\n",
      "For Iteration 1909 the Loss is 0.3962.\n",
      "For Iteration 1910 the Loss is 0.3961.\n",
      "For Iteration 1911 the Loss is 0.3961.\n",
      "For Iteration 1912 the Loss is 0.396.\n",
      "For Iteration 1913 the Loss is 0.3959.\n",
      "For Iteration 1914 the Loss is 0.3958.\n",
      "For Iteration 1915 the Loss is 0.3958.\n",
      "For Iteration 1916 the Loss is 0.3957.\n",
      "For Iteration 1917 the Loss is 0.3956.\n",
      "For Iteration 1918 the Loss is 0.3955.\n",
      "For Iteration 1919 the Loss is 0.3955.\n",
      "For Iteration 1920 the Loss is 0.3954.\n",
      "For Iteration 1921 the Loss is 0.3953.\n",
      "For Iteration 1922 the Loss is 0.3953.\n",
      "For Iteration 1923 the Loss is 0.3952.\n",
      "For Iteration 1924 the Loss is 0.3951.\n",
      "For Iteration 1925 the Loss is 0.395.\n",
      "For Iteration 1926 the Loss is 0.395.\n",
      "For Iteration 1927 the Loss is 0.3949.\n",
      "For Iteration 1928 the Loss is 0.3948.\n",
      "For Iteration 1929 the Loss is 0.3948.\n",
      "For Iteration 1930 the Loss is 0.3947.\n",
      "For Iteration 1931 the Loss is 0.3946.\n",
      "For Iteration 1932 the Loss is 0.3945.\n",
      "For Iteration 1933 the Loss is 0.3945.\n",
      "For Iteration 1934 the Loss is 0.3944.\n",
      "For Iteration 1935 the Loss is 0.3943.\n",
      "For Iteration 1936 the Loss is 0.3943.\n",
      "For Iteration 1937 the Loss is 0.3942.\n",
      "For Iteration 1938 the Loss is 0.3941.\n",
      "For Iteration 1939 the Loss is 0.394.\n",
      "For Iteration 1940 the Loss is 0.394.\n",
      "For Iteration 1941 the Loss is 0.3939.\n",
      "For Iteration 1942 the Loss is 0.3938.\n",
      "For Iteration 1943 the Loss is 0.3938.\n",
      "For Iteration 1944 the Loss is 0.3937.\n",
      "For Iteration 1945 the Loss is 0.3936.\n",
      "For Iteration 1946 the Loss is 0.3936.\n",
      "For Iteration 1947 the Loss is 0.3935.\n",
      "For Iteration 1948 the Loss is 0.3934.\n",
      "For Iteration 1949 the Loss is 0.3934.\n",
      "For Iteration 1950 the Loss is 0.3933.\n",
      "For Iteration 1951 the Loss is 0.3932.\n",
      "For Iteration 1952 the Loss is 0.3931.\n",
      "For Iteration 1953 the Loss is 0.3931.\n",
      "For Iteration 1954 the Loss is 0.393.\n",
      "For Iteration 1955 the Loss is 0.3929.\n",
      "For Iteration 1956 the Loss is 0.3929.\n",
      "For Iteration 1957 the Loss is 0.3928.\n",
      "For Iteration 1958 the Loss is 0.3927.\n",
      "For Iteration 1959 the Loss is 0.3927.\n",
      "For Iteration 1960 the Loss is 0.3926.\n",
      "For Iteration 1961 the Loss is 0.3925.\n",
      "For Iteration 1962 the Loss is 0.3925.\n",
      "For Iteration 1963 the Loss is 0.3924.\n",
      "For Iteration 1964 the Loss is 0.3923.\n",
      "For Iteration 1965 the Loss is 0.3923.\n",
      "For Iteration 1966 the Loss is 0.3922.\n",
      "For Iteration 1967 the Loss is 0.3921.\n",
      "For Iteration 1968 the Loss is 0.3921.\n",
      "For Iteration 1969 the Loss is 0.392.\n",
      "For Iteration 1970 the Loss is 0.3919.\n",
      "For Iteration 1971 the Loss is 0.3919.\n",
      "For Iteration 1972 the Loss is 0.3918.\n",
      "For Iteration 1973 the Loss is 0.3918.\n",
      "For Iteration 1974 the Loss is 0.3917.\n",
      "For Iteration 1975 the Loss is 0.3916.\n",
      "For Iteration 1976 the Loss is 0.3916.\n",
      "For Iteration 1977 the Loss is 0.3915.\n",
      "For Iteration 1978 the Loss is 0.3914.\n",
      "For Iteration 1979 the Loss is 0.3914.\n",
      "For Iteration 1980 the Loss is 0.3913.\n",
      "For Iteration 1981 the Loss is 0.3912.\n",
      "For Iteration 1982 the Loss is 0.3912.\n",
      "For Iteration 1983 the Loss is 0.3911.\n",
      "For Iteration 1984 the Loss is 0.391.\n",
      "For Iteration 1985 the Loss is 0.391.\n",
      "For Iteration 1986 the Loss is 0.3909.\n",
      "For Iteration 1987 the Loss is 0.3909.\n",
      "For Iteration 1988 the Loss is 0.3908.\n",
      "For Iteration 1989 the Loss is 0.3907.\n",
      "For Iteration 1990 the Loss is 0.3907.\n",
      "For Iteration 1991 the Loss is 0.3906.\n",
      "For Iteration 1992 the Loss is 0.3905.\n",
      "For Iteration 1993 the Loss is 0.3905.\n",
      "For Iteration 1994 the Loss is 0.3904.\n",
      "For Iteration 1995 the Loss is 0.3904.\n",
      "For Iteration 1996 the Loss is 0.3903.\n",
      "For Iteration 1997 the Loss is 0.3902.\n",
      "For Iteration 1998 the Loss is 0.3902.\n",
      "For Iteration 1999 the Loss is 0.3901.\n"
     ]
    }
   ],
   "source": [
    "model2=LogisticRegression(learning_rate=.001, itr=2000)\n",
    "model2.fit(train_x,train_y)\n",
    "#model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "c7494304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(test_y,model2.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "a0974d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x261023faf70>]"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model2.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "10c611d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pkl_file2', 'wb') as files:\n",
    "    pickle.dump(model2, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "82aed7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('pkl_file2' , 'rb') as f:\n",
    "    lr2 = pickle.load(f)\n",
    "\n",
    "accuracy(test_y,lr2.predict(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d9668",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "c743b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y,test_x,test_y=train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "571465bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 0 the Loss is 4.5526.\n",
      "For Iteration 1 the Loss is 2.3667.\n",
      "For Iteration 2 the Loss is 0.7898.\n",
      "For Iteration 3 the Loss is 0.4322.\n",
      "For Iteration 4 the Loss is 0.3853.\n",
      "For Iteration 5 the Loss is 0.3701.\n",
      "For Iteration 6 the Loss is 0.363.\n",
      "For Iteration 7 the Loss is 0.3587.\n",
      "For Iteration 8 the Loss is 0.3556.\n",
      "For Iteration 9 the Loss is 0.3531.\n",
      "For Iteration 10 the Loss is 0.3509.\n",
      "For Iteration 11 the Loss is 0.3489.\n",
      "For Iteration 12 the Loss is 0.347.\n",
      "For Iteration 13 the Loss is 0.3451.\n",
      "For Iteration 14 the Loss is 0.3434.\n",
      "For Iteration 15 the Loss is 0.3417.\n",
      "For Iteration 16 the Loss is 0.3401.\n",
      "For Iteration 17 the Loss is 0.3385.\n",
      "For Iteration 18 the Loss is 0.337.\n",
      "For Iteration 19 the Loss is 0.3355.\n",
      "For Iteration 20 the Loss is 0.3341.\n",
      "For Iteration 21 the Loss is 0.3327.\n",
      "For Iteration 22 the Loss is 0.3314.\n",
      "For Iteration 23 the Loss is 0.3301.\n",
      "For Iteration 24 the Loss is 0.3288.\n",
      "For Iteration 25 the Loss is 0.3276.\n",
      "For Iteration 26 the Loss is 0.3264.\n",
      "For Iteration 27 the Loss is 0.3253.\n",
      "For Iteration 28 the Loss is 0.3242.\n",
      "For Iteration 29 the Loss is 0.3231.\n",
      "For Iteration 30 the Loss is 0.322.\n",
      "For Iteration 31 the Loss is 0.321.\n",
      "For Iteration 32 the Loss is 0.3201.\n",
      "For Iteration 33 the Loss is 0.3191.\n",
      "For Iteration 34 the Loss is 0.3182.\n",
      "For Iteration 35 the Loss is 0.3173.\n",
      "For Iteration 36 the Loss is 0.3164.\n",
      "For Iteration 37 the Loss is 0.3156.\n",
      "For Iteration 38 the Loss is 0.3147.\n",
      "For Iteration 39 the Loss is 0.3139.\n",
      "For Iteration 40 the Loss is 0.3132.\n",
      "For Iteration 41 the Loss is 0.3124.\n",
      "For Iteration 42 the Loss is 0.3117.\n",
      "For Iteration 43 the Loss is 0.311.\n",
      "For Iteration 44 the Loss is 0.3103.\n",
      "For Iteration 45 the Loss is 0.3096.\n",
      "For Iteration 46 the Loss is 0.309.\n",
      "For Iteration 47 the Loss is 0.3083.\n",
      "For Iteration 48 the Loss is 0.3077.\n",
      "For Iteration 49 the Loss is 0.3071.\n",
      "For Iteration 50 the Loss is 0.3065.\n",
      "For Iteration 51 the Loss is 0.306.\n",
      "For Iteration 52 the Loss is 0.3054.\n",
      "For Iteration 53 the Loss is 0.3049.\n",
      "For Iteration 54 the Loss is 0.3043.\n",
      "For Iteration 55 the Loss is 0.3038.\n",
      "For Iteration 56 the Loss is 0.3033.\n",
      "For Iteration 57 the Loss is 0.3028.\n",
      "For Iteration 58 the Loss is 0.3023.\n",
      "For Iteration 59 the Loss is 0.3019.\n",
      "For Iteration 60 the Loss is 0.3014.\n",
      "For Iteration 61 the Loss is 0.301.\n",
      "For Iteration 62 the Loss is 0.3006.\n",
      "For Iteration 63 the Loss is 0.3001.\n",
      "For Iteration 64 the Loss is 0.2997.\n",
      "For Iteration 65 the Loss is 0.2993.\n",
      "For Iteration 66 the Loss is 0.2989.\n",
      "For Iteration 67 the Loss is 0.2985.\n",
      "For Iteration 68 the Loss is 0.2982.\n",
      "For Iteration 69 the Loss is 0.2978.\n",
      "For Iteration 70 the Loss is 0.2974.\n",
      "For Iteration 71 the Loss is 0.2971.\n",
      "For Iteration 72 the Loss is 0.2968.\n",
      "For Iteration 73 the Loss is 0.2964.\n",
      "For Iteration 74 the Loss is 0.2961.\n",
      "For Iteration 75 the Loss is 0.2958.\n",
      "For Iteration 76 the Loss is 0.2955.\n",
      "For Iteration 77 the Loss is 0.2951.\n",
      "For Iteration 78 the Loss is 0.2948.\n",
      "For Iteration 79 the Loss is 0.2946.\n",
      "For Iteration 80 the Loss is 0.2943.\n",
      "For Iteration 81 the Loss is 0.294.\n",
      "For Iteration 82 the Loss is 0.2937.\n",
      "For Iteration 83 the Loss is 0.2934.\n",
      "For Iteration 84 the Loss is 0.2932.\n",
      "For Iteration 85 the Loss is 0.2929.\n",
      "For Iteration 86 the Loss is 0.2927.\n",
      "For Iteration 87 the Loss is 0.2924.\n",
      "For Iteration 88 the Loss is 0.2922.\n",
      "For Iteration 89 the Loss is 0.2919.\n",
      "For Iteration 90 the Loss is 0.2917.\n",
      "For Iteration 91 the Loss is 0.2915.\n",
      "For Iteration 92 the Loss is 0.2912.\n",
      "For Iteration 93 the Loss is 0.291.\n",
      "For Iteration 94 the Loss is 0.2908.\n",
      "For Iteration 95 the Loss is 0.2906.\n",
      "For Iteration 96 the Loss is 0.2904.\n",
      "For Iteration 97 the Loss is 0.2901.\n",
      "For Iteration 98 the Loss is 0.2899.\n",
      "For Iteration 99 the Loss is 0.2897.\n",
      "For Iteration 100 the Loss is 0.2895.\n",
      "For Iteration 101 the Loss is 0.2893.\n",
      "For Iteration 102 the Loss is 0.2892.\n",
      "For Iteration 103 the Loss is 0.289.\n",
      "For Iteration 104 the Loss is 0.2888.\n",
      "For Iteration 105 the Loss is 0.2886.\n",
      "For Iteration 106 the Loss is 0.2884.\n",
      "For Iteration 107 the Loss is 0.2883.\n",
      "For Iteration 108 the Loss is 0.2881.\n",
      "For Iteration 109 the Loss is 0.2879.\n",
      "For Iteration 110 the Loss is 0.2877.\n",
      "For Iteration 111 the Loss is 0.2876.\n",
      "For Iteration 112 the Loss is 0.2874.\n",
      "For Iteration 113 the Loss is 0.2873.\n",
      "For Iteration 114 the Loss is 0.2871.\n",
      "For Iteration 115 the Loss is 0.287.\n",
      "For Iteration 116 the Loss is 0.2868.\n",
      "For Iteration 117 the Loss is 0.2866.\n",
      "For Iteration 118 the Loss is 0.2865.\n",
      "For Iteration 119 the Loss is 0.2864.\n",
      "For Iteration 120 the Loss is 0.2862.\n",
      "For Iteration 121 the Loss is 0.2861.\n",
      "For Iteration 122 the Loss is 0.2859.\n",
      "For Iteration 123 the Loss is 0.2858.\n",
      "For Iteration 124 the Loss is 0.2857.\n",
      "For Iteration 125 the Loss is 0.2855.\n",
      "For Iteration 126 the Loss is 0.2854.\n",
      "For Iteration 127 the Loss is 0.2853.\n",
      "For Iteration 128 the Loss is 0.2851.\n",
      "For Iteration 129 the Loss is 0.285.\n",
      "For Iteration 130 the Loss is 0.2849.\n",
      "For Iteration 131 the Loss is 0.2848.\n",
      "For Iteration 132 the Loss is 0.2846.\n",
      "For Iteration 133 the Loss is 0.2845.\n",
      "For Iteration 134 the Loss is 0.2844.\n",
      "For Iteration 135 the Loss is 0.2843.\n",
      "For Iteration 136 the Loss is 0.2842.\n",
      "For Iteration 137 the Loss is 0.2841.\n",
      "For Iteration 138 the Loss is 0.284.\n",
      "For Iteration 139 the Loss is 0.2838.\n",
      "For Iteration 140 the Loss is 0.2837.\n",
      "For Iteration 141 the Loss is 0.2836.\n",
      "For Iteration 142 the Loss is 0.2835.\n",
      "For Iteration 143 the Loss is 0.2834.\n",
      "For Iteration 144 the Loss is 0.2833.\n",
      "For Iteration 145 the Loss is 0.2832.\n",
      "For Iteration 146 the Loss is 0.2831.\n",
      "For Iteration 147 the Loss is 0.283.\n",
      "For Iteration 148 the Loss is 0.2829.\n",
      "For Iteration 149 the Loss is 0.2828.\n",
      "For Iteration 150 the Loss is 0.2827.\n",
      "For Iteration 151 the Loss is 0.2826.\n",
      "For Iteration 152 the Loss is 0.2825.\n",
      "For Iteration 153 the Loss is 0.2824.\n",
      "For Iteration 154 the Loss is 0.2823.\n",
      "For Iteration 155 the Loss is 0.2822.\n",
      "For Iteration 156 the Loss is 0.2822.\n",
      "For Iteration 157 the Loss is 0.2821.\n",
      "For Iteration 158 the Loss is 0.282.\n",
      "For Iteration 159 the Loss is 0.2819.\n",
      "For Iteration 160 the Loss is 0.2818.\n",
      "For Iteration 161 the Loss is 0.2817.\n",
      "For Iteration 162 the Loss is 0.2816.\n",
      "For Iteration 163 the Loss is 0.2815.\n",
      "For Iteration 164 the Loss is 0.2815.\n",
      "For Iteration 165 the Loss is 0.2814.\n",
      "For Iteration 166 the Loss is 0.2813.\n",
      "For Iteration 167 the Loss is 0.2812.\n",
      "For Iteration 168 the Loss is 0.2811.\n",
      "For Iteration 169 the Loss is 0.2811.\n",
      "For Iteration 170 the Loss is 0.281.\n",
      "For Iteration 171 the Loss is 0.2809.\n",
      "For Iteration 172 the Loss is 0.2808.\n",
      "For Iteration 173 the Loss is 0.2808.\n",
      "For Iteration 174 the Loss is 0.2807.\n",
      "For Iteration 175 the Loss is 0.2806.\n",
      "For Iteration 176 the Loss is 0.2805.\n",
      "For Iteration 177 the Loss is 0.2805.\n",
      "For Iteration 178 the Loss is 0.2804.\n",
      "For Iteration 179 the Loss is 0.2803.\n",
      "For Iteration 180 the Loss is 0.2802.\n",
      "For Iteration 181 the Loss is 0.2802.\n",
      "For Iteration 182 the Loss is 0.2801.\n",
      "For Iteration 183 the Loss is 0.28.\n",
      "For Iteration 184 the Loss is 0.28.\n",
      "For Iteration 185 the Loss is 0.2799.\n",
      "For Iteration 186 the Loss is 0.2798.\n",
      "For Iteration 187 the Loss is 0.2798.\n",
      "For Iteration 188 the Loss is 0.2797.\n",
      "For Iteration 189 the Loss is 0.2796.\n",
      "For Iteration 190 the Loss is 0.2796.\n",
      "For Iteration 191 the Loss is 0.2795.\n",
      "For Iteration 192 the Loss is 0.2794.\n",
      "For Iteration 193 the Loss is 0.2794.\n",
      "For Iteration 194 the Loss is 0.2793.\n",
      "For Iteration 195 the Loss is 0.2793.\n",
      "For Iteration 196 the Loss is 0.2792.\n",
      "For Iteration 197 the Loss is 0.2791.\n",
      "For Iteration 198 the Loss is 0.2791.\n",
      "For Iteration 199 the Loss is 0.279.\n",
      "For Iteration 200 the Loss is 0.279.\n",
      "For Iteration 201 the Loss is 0.2789.\n",
      "For Iteration 202 the Loss is 0.2788.\n",
      "For Iteration 203 the Loss is 0.2788.\n",
      "For Iteration 204 the Loss is 0.2787.\n",
      "For Iteration 205 the Loss is 0.2787.\n",
      "For Iteration 206 the Loss is 0.2786.\n",
      "For Iteration 207 the Loss is 0.2786.\n",
      "For Iteration 208 the Loss is 0.2785.\n",
      "For Iteration 209 the Loss is 0.2785.\n",
      "For Iteration 210 the Loss is 0.2784.\n",
      "For Iteration 211 the Loss is 0.2783.\n",
      "For Iteration 212 the Loss is 0.2783.\n",
      "For Iteration 213 the Loss is 0.2782.\n",
      "For Iteration 214 the Loss is 0.2782.\n",
      "For Iteration 215 the Loss is 0.2781.\n",
      "For Iteration 216 the Loss is 0.2781.\n",
      "For Iteration 217 the Loss is 0.278.\n",
      "For Iteration 218 the Loss is 0.278.\n",
      "For Iteration 219 the Loss is 0.2779.\n",
      "For Iteration 220 the Loss is 0.2779.\n",
      "For Iteration 221 the Loss is 0.2778.\n",
      "For Iteration 222 the Loss is 0.2778.\n",
      "For Iteration 223 the Loss is 0.2777.\n",
      "For Iteration 224 the Loss is 0.2777.\n",
      "For Iteration 225 the Loss is 0.2776.\n",
      "For Iteration 226 the Loss is 0.2776.\n",
      "For Iteration 227 the Loss is 0.2775.\n",
      "For Iteration 228 the Loss is 0.2775.\n",
      "For Iteration 229 the Loss is 0.2774.\n",
      "For Iteration 230 the Loss is 0.2774.\n",
      "For Iteration 231 the Loss is 0.2773.\n",
      "For Iteration 232 the Loss is 0.2773.\n",
      "For Iteration 233 the Loss is 0.2773.\n",
      "For Iteration 234 the Loss is 0.2772.\n",
      "For Iteration 235 the Loss is 0.2772.\n",
      "For Iteration 236 the Loss is 0.2771.\n",
      "For Iteration 237 the Loss is 0.2771.\n",
      "For Iteration 238 the Loss is 0.277.\n",
      "For Iteration 239 the Loss is 0.277.\n",
      "For Iteration 240 the Loss is 0.2769.\n",
      "For Iteration 241 the Loss is 0.2769.\n",
      "For Iteration 242 the Loss is 0.2769.\n",
      "For Iteration 243 the Loss is 0.2768.\n",
      "For Iteration 244 the Loss is 0.2768.\n",
      "For Iteration 245 the Loss is 0.2767.\n",
      "For Iteration 246 the Loss is 0.2767.\n",
      "For Iteration 247 the Loss is 0.2766.\n",
      "For Iteration 248 the Loss is 0.2766.\n",
      "For Iteration 249 the Loss is 0.2766.\n",
      "For Iteration 250 the Loss is 0.2765.\n",
      "For Iteration 251 the Loss is 0.2765.\n",
      "For Iteration 252 the Loss is 0.2764.\n",
      "For Iteration 253 the Loss is 0.2764.\n",
      "For Iteration 254 the Loss is 0.2764.\n",
      "For Iteration 255 the Loss is 0.2763.\n",
      "For Iteration 256 the Loss is 0.2763.\n",
      "For Iteration 257 the Loss is 0.2762.\n",
      "For Iteration 258 the Loss is 0.2762.\n",
      "For Iteration 259 the Loss is 0.2762.\n",
      "For Iteration 260 the Loss is 0.2761.\n",
      "For Iteration 261 the Loss is 0.2761.\n",
      "For Iteration 262 the Loss is 0.2761.\n",
      "For Iteration 263 the Loss is 0.276.\n",
      "For Iteration 264 the Loss is 0.276.\n",
      "For Iteration 265 the Loss is 0.2759.\n",
      "For Iteration 266 the Loss is 0.2759.\n",
      "For Iteration 267 the Loss is 0.2759.\n",
      "For Iteration 268 the Loss is 0.2758.\n",
      "For Iteration 269 the Loss is 0.2758.\n",
      "For Iteration 270 the Loss is 0.2758.\n",
      "For Iteration 271 the Loss is 0.2757.\n",
      "For Iteration 272 the Loss is 0.2757.\n",
      "For Iteration 273 the Loss is 0.2757.\n",
      "For Iteration 274 the Loss is 0.2756.\n",
      "For Iteration 275 the Loss is 0.2756.\n",
      "For Iteration 276 the Loss is 0.2756.\n",
      "For Iteration 277 the Loss is 0.2755.\n",
      "For Iteration 278 the Loss is 0.2755.\n",
      "For Iteration 279 the Loss is 0.2755.\n",
      "For Iteration 280 the Loss is 0.2754.\n",
      "For Iteration 281 the Loss is 0.2754.\n",
      "For Iteration 282 the Loss is 0.2754.\n",
      "For Iteration 283 the Loss is 0.2753.\n",
      "For Iteration 284 the Loss is 0.2753.\n",
      "For Iteration 285 the Loss is 0.2753.\n",
      "For Iteration 286 the Loss is 0.2752.\n",
      "For Iteration 287 the Loss is 0.2752.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 288 the Loss is 0.2752.\n",
      "For Iteration 289 the Loss is 0.2751.\n",
      "For Iteration 290 the Loss is 0.2751.\n",
      "For Iteration 291 the Loss is 0.2751.\n",
      "For Iteration 292 the Loss is 0.275.\n",
      "For Iteration 293 the Loss is 0.275.\n",
      "For Iteration 294 the Loss is 0.275.\n",
      "For Iteration 295 the Loss is 0.2749.\n",
      "For Iteration 296 the Loss is 0.2749.\n",
      "For Iteration 297 the Loss is 0.2749.\n",
      "For Iteration 298 the Loss is 0.2748.\n",
      "For Iteration 299 the Loss is 0.2748.\n",
      "For Iteration 300 the Loss is 0.2748.\n",
      "For Iteration 301 the Loss is 0.2748.\n",
      "For Iteration 302 the Loss is 0.2747.\n",
      "For Iteration 303 the Loss is 0.2747.\n",
      "For Iteration 304 the Loss is 0.2747.\n",
      "For Iteration 305 the Loss is 0.2746.\n",
      "For Iteration 306 the Loss is 0.2746.\n",
      "For Iteration 307 the Loss is 0.2746.\n",
      "For Iteration 308 the Loss is 0.2746.\n",
      "For Iteration 309 the Loss is 0.2745.\n",
      "For Iteration 310 the Loss is 0.2745.\n",
      "For Iteration 311 the Loss is 0.2745.\n",
      "For Iteration 312 the Loss is 0.2744.\n",
      "For Iteration 313 the Loss is 0.2744.\n",
      "For Iteration 314 the Loss is 0.2744.\n",
      "For Iteration 315 the Loss is 0.2744.\n",
      "For Iteration 316 the Loss is 0.2743.\n",
      "For Iteration 317 the Loss is 0.2743.\n",
      "For Iteration 318 the Loss is 0.2743.\n",
      "For Iteration 319 the Loss is 0.2742.\n",
      "For Iteration 320 the Loss is 0.2742.\n",
      "For Iteration 321 the Loss is 0.2742.\n",
      "For Iteration 322 the Loss is 0.2742.\n",
      "For Iteration 323 the Loss is 0.2741.\n",
      "For Iteration 324 the Loss is 0.2741.\n",
      "For Iteration 325 the Loss is 0.2741.\n",
      "For Iteration 326 the Loss is 0.2741.\n",
      "For Iteration 327 the Loss is 0.274.\n",
      "For Iteration 328 the Loss is 0.274.\n",
      "For Iteration 329 the Loss is 0.274.\n",
      "For Iteration 330 the Loss is 0.274.\n",
      "For Iteration 331 the Loss is 0.2739.\n",
      "For Iteration 332 the Loss is 0.2739.\n",
      "For Iteration 333 the Loss is 0.2739.\n",
      "For Iteration 334 the Loss is 0.2739.\n",
      "For Iteration 335 the Loss is 0.2738.\n",
      "For Iteration 336 the Loss is 0.2738.\n",
      "For Iteration 337 the Loss is 0.2738.\n",
      "For Iteration 338 the Loss is 0.2738.\n",
      "For Iteration 339 the Loss is 0.2737.\n",
      "For Iteration 340 the Loss is 0.2737.\n",
      "For Iteration 341 the Loss is 0.2737.\n",
      "For Iteration 342 the Loss is 0.2737.\n",
      "For Iteration 343 the Loss is 0.2736.\n",
      "For Iteration 344 the Loss is 0.2736.\n",
      "For Iteration 345 the Loss is 0.2736.\n",
      "For Iteration 346 the Loss is 0.2736.\n",
      "For Iteration 347 the Loss is 0.2735.\n",
      "For Iteration 348 the Loss is 0.2735.\n",
      "For Iteration 349 the Loss is 0.2735.\n",
      "For Iteration 350 the Loss is 0.2735.\n",
      "For Iteration 351 the Loss is 0.2735.\n",
      "For Iteration 352 the Loss is 0.2734.\n",
      "For Iteration 353 the Loss is 0.2734.\n",
      "For Iteration 354 the Loss is 0.2734.\n",
      "For Iteration 355 the Loss is 0.2734.\n",
      "For Iteration 356 the Loss is 0.2733.\n",
      "For Iteration 357 the Loss is 0.2733.\n",
      "For Iteration 358 the Loss is 0.2733.\n",
      "For Iteration 359 the Loss is 0.2733.\n",
      "For Iteration 360 the Loss is 0.2733.\n",
      "For Iteration 361 the Loss is 0.2732.\n",
      "For Iteration 362 the Loss is 0.2732.\n",
      "For Iteration 363 the Loss is 0.2732.\n",
      "For Iteration 364 the Loss is 0.2732.\n",
      "For Iteration 365 the Loss is 0.2731.\n",
      "For Iteration 366 the Loss is 0.2731.\n",
      "For Iteration 367 the Loss is 0.2731.\n",
      "For Iteration 368 the Loss is 0.2731.\n",
      "For Iteration 369 the Loss is 0.2731.\n",
      "For Iteration 370 the Loss is 0.273.\n",
      "For Iteration 371 the Loss is 0.273.\n",
      "For Iteration 372 the Loss is 0.273.\n",
      "For Iteration 373 the Loss is 0.273.\n",
      "For Iteration 374 the Loss is 0.273.\n",
      "For Iteration 375 the Loss is 0.2729.\n",
      "For Iteration 376 the Loss is 0.2729.\n",
      "For Iteration 377 the Loss is 0.2729.\n",
      "For Iteration 378 the Loss is 0.2729.\n",
      "For Iteration 379 the Loss is 0.2729.\n",
      "For Iteration 380 the Loss is 0.2728.\n",
      "For Iteration 381 the Loss is 0.2728.\n",
      "For Iteration 382 the Loss is 0.2728.\n",
      "For Iteration 383 the Loss is 0.2728.\n",
      "For Iteration 384 the Loss is 0.2728.\n",
      "For Iteration 385 the Loss is 0.2727.\n",
      "For Iteration 386 the Loss is 0.2727.\n",
      "For Iteration 387 the Loss is 0.2727.\n",
      "For Iteration 388 the Loss is 0.2727.\n",
      "For Iteration 389 the Loss is 0.2727.\n",
      "For Iteration 390 the Loss is 0.2726.\n",
      "For Iteration 391 the Loss is 0.2726.\n",
      "For Iteration 392 the Loss is 0.2726.\n",
      "For Iteration 393 the Loss is 0.2726.\n",
      "For Iteration 394 the Loss is 0.2726.\n",
      "For Iteration 395 the Loss is 0.2725.\n",
      "For Iteration 396 the Loss is 0.2725.\n",
      "For Iteration 397 the Loss is 0.2725.\n",
      "For Iteration 398 the Loss is 0.2725.\n",
      "For Iteration 399 the Loss is 0.2725.\n",
      "For Iteration 400 the Loss is 0.2725.\n",
      "For Iteration 401 the Loss is 0.2724.\n",
      "For Iteration 402 the Loss is 0.2724.\n",
      "For Iteration 403 the Loss is 0.2724.\n",
      "For Iteration 404 the Loss is 0.2724.\n",
      "For Iteration 405 the Loss is 0.2724.\n",
      "For Iteration 406 the Loss is 0.2723.\n",
      "For Iteration 407 the Loss is 0.2723.\n",
      "For Iteration 408 the Loss is 0.2723.\n",
      "For Iteration 409 the Loss is 0.2723.\n",
      "For Iteration 410 the Loss is 0.2723.\n",
      "For Iteration 411 the Loss is 0.2723.\n",
      "For Iteration 412 the Loss is 0.2722.\n",
      "For Iteration 413 the Loss is 0.2722.\n",
      "For Iteration 414 the Loss is 0.2722.\n",
      "For Iteration 415 the Loss is 0.2722.\n",
      "For Iteration 416 the Loss is 0.2722.\n",
      "For Iteration 417 the Loss is 0.2722.\n",
      "For Iteration 418 the Loss is 0.2721.\n",
      "For Iteration 419 the Loss is 0.2721.\n",
      "For Iteration 420 the Loss is 0.2721.\n",
      "For Iteration 421 the Loss is 0.2721.\n",
      "For Iteration 422 the Loss is 0.2721.\n",
      "For Iteration 423 the Loss is 0.2721.\n",
      "For Iteration 424 the Loss is 0.272.\n",
      "For Iteration 425 the Loss is 0.272.\n",
      "For Iteration 426 the Loss is 0.272.\n",
      "For Iteration 427 the Loss is 0.272.\n",
      "For Iteration 428 the Loss is 0.272.\n",
      "For Iteration 429 the Loss is 0.272.\n",
      "For Iteration 430 the Loss is 0.2719.\n",
      "For Iteration 431 the Loss is 0.2719.\n",
      "For Iteration 432 the Loss is 0.2719.\n",
      "For Iteration 433 the Loss is 0.2719.\n",
      "For Iteration 434 the Loss is 0.2719.\n",
      "For Iteration 435 the Loss is 0.2719.\n",
      "For Iteration 436 the Loss is 0.2718.\n",
      "For Iteration 437 the Loss is 0.2718.\n",
      "For Iteration 438 the Loss is 0.2718.\n",
      "For Iteration 439 the Loss is 0.2718.\n",
      "For Iteration 440 the Loss is 0.2718.\n",
      "For Iteration 441 the Loss is 0.2718.\n",
      "For Iteration 442 the Loss is 0.2718.\n",
      "For Iteration 443 the Loss is 0.2717.\n",
      "For Iteration 444 the Loss is 0.2717.\n",
      "For Iteration 445 the Loss is 0.2717.\n",
      "For Iteration 446 the Loss is 0.2717.\n",
      "For Iteration 447 the Loss is 0.2717.\n",
      "For Iteration 448 the Loss is 0.2717.\n",
      "For Iteration 449 the Loss is 0.2716.\n",
      "For Iteration 450 the Loss is 0.2716.\n",
      "For Iteration 451 the Loss is 0.2716.\n",
      "For Iteration 452 the Loss is 0.2716.\n",
      "For Iteration 453 the Loss is 0.2716.\n",
      "For Iteration 454 the Loss is 0.2716.\n",
      "For Iteration 455 the Loss is 0.2716.\n",
      "For Iteration 456 the Loss is 0.2715.\n",
      "For Iteration 457 the Loss is 0.2715.\n",
      "For Iteration 458 the Loss is 0.2715.\n",
      "For Iteration 459 the Loss is 0.2715.\n",
      "For Iteration 460 the Loss is 0.2715.\n",
      "For Iteration 461 the Loss is 0.2715.\n",
      "For Iteration 462 the Loss is 0.2715.\n",
      "For Iteration 463 the Loss is 0.2714.\n",
      "For Iteration 464 the Loss is 0.2714.\n",
      "For Iteration 465 the Loss is 0.2714.\n",
      "For Iteration 466 the Loss is 0.2714.\n",
      "For Iteration 467 the Loss is 0.2714.\n",
      "For Iteration 468 the Loss is 0.2714.\n",
      "For Iteration 469 the Loss is 0.2714.\n",
      "For Iteration 470 the Loss is 0.2713.\n",
      "For Iteration 471 the Loss is 0.2713.\n",
      "For Iteration 472 the Loss is 0.2713.\n",
      "For Iteration 473 the Loss is 0.2713.\n",
      "For Iteration 474 the Loss is 0.2713.\n",
      "For Iteration 475 the Loss is 0.2713.\n",
      "For Iteration 476 the Loss is 0.2713.\n",
      "For Iteration 477 the Loss is 0.2713.\n",
      "For Iteration 478 the Loss is 0.2712.\n",
      "For Iteration 479 the Loss is 0.2712.\n",
      "For Iteration 480 the Loss is 0.2712.\n",
      "For Iteration 481 the Loss is 0.2712.\n",
      "For Iteration 482 the Loss is 0.2712.\n",
      "For Iteration 483 the Loss is 0.2712.\n",
      "For Iteration 484 the Loss is 0.2712.\n",
      "For Iteration 485 the Loss is 0.2711.\n",
      "For Iteration 486 the Loss is 0.2711.\n",
      "For Iteration 487 the Loss is 0.2711.\n",
      "For Iteration 488 the Loss is 0.2711.\n",
      "For Iteration 489 the Loss is 0.2711.\n",
      "For Iteration 490 the Loss is 0.2711.\n",
      "For Iteration 491 the Loss is 0.2711.\n",
      "For Iteration 492 the Loss is 0.2711.\n",
      "For Iteration 493 the Loss is 0.271.\n",
      "For Iteration 494 the Loss is 0.271.\n",
      "For Iteration 495 the Loss is 0.271.\n",
      "For Iteration 496 the Loss is 0.271.\n",
      "For Iteration 497 the Loss is 0.271.\n",
      "For Iteration 498 the Loss is 0.271.\n",
      "For Iteration 499 the Loss is 0.271.\n",
      "For Iteration 500 the Loss is 0.271.\n",
      "For Iteration 501 the Loss is 0.2709.\n",
      "For Iteration 502 the Loss is 0.2709.\n",
      "For Iteration 503 the Loss is 0.2709.\n",
      "For Iteration 504 the Loss is 0.2709.\n",
      "For Iteration 505 the Loss is 0.2709.\n",
      "For Iteration 506 the Loss is 0.2709.\n",
      "For Iteration 507 the Loss is 0.2709.\n",
      "For Iteration 508 the Loss is 0.2709.\n",
      "For Iteration 509 the Loss is 0.2708.\n",
      "For Iteration 510 the Loss is 0.2708.\n",
      "For Iteration 511 the Loss is 0.2708.\n",
      "For Iteration 512 the Loss is 0.2708.\n",
      "For Iteration 513 the Loss is 0.2708.\n",
      "For Iteration 514 the Loss is 0.2708.\n",
      "For Iteration 515 the Loss is 0.2708.\n",
      "For Iteration 516 the Loss is 0.2708.\n",
      "For Iteration 517 the Loss is 0.2708.\n",
      "For Iteration 518 the Loss is 0.2707.\n",
      "For Iteration 519 the Loss is 0.2707.\n",
      "For Iteration 520 the Loss is 0.2707.\n",
      "For Iteration 521 the Loss is 0.2707.\n",
      "For Iteration 522 the Loss is 0.2707.\n",
      "For Iteration 523 the Loss is 0.2707.\n",
      "For Iteration 524 the Loss is 0.2707.\n",
      "For Iteration 525 the Loss is 0.2707.\n",
      "For Iteration 526 the Loss is 0.2707.\n",
      "For Iteration 527 the Loss is 0.2706.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 528 the Loss is 0.2706.\n",
      "For Iteration 529 the Loss is 0.2706.\n",
      "For Iteration 530 the Loss is 0.2706.\n",
      "For Iteration 531 the Loss is 0.2706.\n",
      "For Iteration 532 the Loss is 0.2706.\n",
      "For Iteration 533 the Loss is 0.2706.\n",
      "For Iteration 534 the Loss is 0.2706.\n",
      "For Iteration 535 the Loss is 0.2706.\n",
      "For Iteration 536 the Loss is 0.2705.\n",
      "For Iteration 537 the Loss is 0.2705.\n",
      "For Iteration 538 the Loss is 0.2705.\n",
      "For Iteration 539 the Loss is 0.2705.\n",
      "For Iteration 540 the Loss is 0.2705.\n",
      "For Iteration 541 the Loss is 0.2705.\n",
      "For Iteration 542 the Loss is 0.2705.\n",
      "For Iteration 543 the Loss is 0.2705.\n",
      "For Iteration 544 the Loss is 0.2705.\n",
      "For Iteration 545 the Loss is 0.2704.\n",
      "For Iteration 546 the Loss is 0.2704.\n",
      "For Iteration 547 the Loss is 0.2704.\n",
      "For Iteration 548 the Loss is 0.2704.\n",
      "For Iteration 549 the Loss is 0.2704.\n",
      "For Iteration 550 the Loss is 0.2704.\n",
      "For Iteration 551 the Loss is 0.2704.\n",
      "For Iteration 552 the Loss is 0.2704.\n",
      "For Iteration 553 the Loss is 0.2704.\n",
      "For Iteration 554 the Loss is 0.2703.\n",
      "For Iteration 555 the Loss is 0.2703.\n",
      "For Iteration 556 the Loss is 0.2703.\n",
      "For Iteration 557 the Loss is 0.2703.\n",
      "For Iteration 558 the Loss is 0.2703.\n",
      "For Iteration 559 the Loss is 0.2703.\n",
      "For Iteration 560 the Loss is 0.2703.\n",
      "For Iteration 561 the Loss is 0.2703.\n",
      "For Iteration 562 the Loss is 0.2703.\n",
      "For Iteration 563 the Loss is 0.2703.\n",
      "For Iteration 564 the Loss is 0.2702.\n",
      "For Iteration 565 the Loss is 0.2702.\n",
      "For Iteration 566 the Loss is 0.2702.\n",
      "For Iteration 567 the Loss is 0.2702.\n",
      "For Iteration 568 the Loss is 0.2702.\n",
      "For Iteration 569 the Loss is 0.2702.\n",
      "For Iteration 570 the Loss is 0.2702.\n",
      "For Iteration 571 the Loss is 0.2702.\n",
      "For Iteration 572 the Loss is 0.2702.\n",
      "For Iteration 573 the Loss is 0.2702.\n",
      "For Iteration 574 the Loss is 0.2701.\n",
      "For Iteration 575 the Loss is 0.2701.\n",
      "For Iteration 576 the Loss is 0.2701.\n",
      "For Iteration 577 the Loss is 0.2701.\n",
      "For Iteration 578 the Loss is 0.2701.\n",
      "For Iteration 579 the Loss is 0.2701.\n",
      "For Iteration 580 the Loss is 0.2701.\n",
      "For Iteration 581 the Loss is 0.2701.\n",
      "For Iteration 582 the Loss is 0.2701.\n",
      "For Iteration 583 the Loss is 0.2701.\n",
      "For Iteration 584 the Loss is 0.2701.\n",
      "For Iteration 585 the Loss is 0.27.\n",
      "For Iteration 586 the Loss is 0.27.\n",
      "For Iteration 587 the Loss is 0.27.\n",
      "For Iteration 588 the Loss is 0.27.\n",
      "For Iteration 589 the Loss is 0.27.\n",
      "For Iteration 590 the Loss is 0.27.\n",
      "For Iteration 591 the Loss is 0.27.\n",
      "For Iteration 592 the Loss is 0.27.\n",
      "For Iteration 593 the Loss is 0.27.\n",
      "For Iteration 594 the Loss is 0.27.\n",
      "For Iteration 595 the Loss is 0.27.\n",
      "For Iteration 596 the Loss is 0.2699.\n",
      "For Iteration 597 the Loss is 0.2699.\n",
      "For Iteration 598 the Loss is 0.2699.\n",
      "For Iteration 599 the Loss is 0.2699.\n",
      "For Iteration 600 the Loss is 0.2699.\n",
      "For Iteration 601 the Loss is 0.2699.\n",
      "For Iteration 602 the Loss is 0.2699.\n",
      "For Iteration 603 the Loss is 0.2699.\n",
      "For Iteration 604 the Loss is 0.2699.\n",
      "For Iteration 605 the Loss is 0.2699.\n",
      "For Iteration 606 the Loss is 0.2699.\n",
      "For Iteration 607 the Loss is 0.2698.\n",
      "For Iteration 608 the Loss is 0.2698.\n",
      "For Iteration 609 the Loss is 0.2698.\n",
      "For Iteration 610 the Loss is 0.2698.\n",
      "For Iteration 611 the Loss is 0.2698.\n",
      "For Iteration 612 the Loss is 0.2698.\n",
      "For Iteration 613 the Loss is 0.2698.\n",
      "For Iteration 614 the Loss is 0.2698.\n",
      "For Iteration 615 the Loss is 0.2698.\n",
      "For Iteration 616 the Loss is 0.2698.\n",
      "For Iteration 617 the Loss is 0.2698.\n",
      "For Iteration 618 the Loss is 0.2697.\n",
      "For Iteration 619 the Loss is 0.2697.\n",
      "For Iteration 620 the Loss is 0.2697.\n",
      "For Iteration 621 the Loss is 0.2697.\n",
      "For Iteration 622 the Loss is 0.2697.\n",
      "For Iteration 623 the Loss is 0.2697.\n",
      "For Iteration 624 the Loss is 0.2697.\n",
      "For Iteration 625 the Loss is 0.2697.\n",
      "For Iteration 626 the Loss is 0.2697.\n",
      "For Iteration 627 the Loss is 0.2697.\n",
      "For Iteration 628 the Loss is 0.2697.\n",
      "For Iteration 629 the Loss is 0.2697.\n",
      "For Iteration 630 the Loss is 0.2696.\n",
      "For Iteration 631 the Loss is 0.2696.\n",
      "For Iteration 632 the Loss is 0.2696.\n",
      "For Iteration 633 the Loss is 0.2696.\n",
      "For Iteration 634 the Loss is 0.2696.\n",
      "For Iteration 635 the Loss is 0.2696.\n",
      "For Iteration 636 the Loss is 0.2696.\n",
      "For Iteration 637 the Loss is 0.2696.\n",
      "For Iteration 638 the Loss is 0.2696.\n",
      "For Iteration 639 the Loss is 0.2696.\n",
      "For Iteration 640 the Loss is 0.2696.\n",
      "For Iteration 641 the Loss is 0.2696.\n",
      "For Iteration 642 the Loss is 0.2696.\n",
      "For Iteration 643 the Loss is 0.2695.\n",
      "For Iteration 644 the Loss is 0.2695.\n",
      "For Iteration 645 the Loss is 0.2695.\n",
      "For Iteration 646 the Loss is 0.2695.\n",
      "For Iteration 647 the Loss is 0.2695.\n",
      "For Iteration 648 the Loss is 0.2695.\n",
      "For Iteration 649 the Loss is 0.2695.\n",
      "For Iteration 650 the Loss is 0.2695.\n",
      "For Iteration 651 the Loss is 0.2695.\n",
      "For Iteration 652 the Loss is 0.2695.\n",
      "For Iteration 653 the Loss is 0.2695.\n",
      "For Iteration 654 the Loss is 0.2695.\n",
      "For Iteration 655 the Loss is 0.2694.\n",
      "For Iteration 656 the Loss is 0.2694.\n",
      "For Iteration 657 the Loss is 0.2694.\n",
      "For Iteration 658 the Loss is 0.2694.\n",
      "For Iteration 659 the Loss is 0.2694.\n",
      "For Iteration 660 the Loss is 0.2694.\n",
      "For Iteration 661 the Loss is 0.2694.\n",
      "For Iteration 662 the Loss is 0.2694.\n",
      "For Iteration 663 the Loss is 0.2694.\n",
      "For Iteration 664 the Loss is 0.2694.\n",
      "For Iteration 665 the Loss is 0.2694.\n",
      "For Iteration 666 the Loss is 0.2694.\n",
      "For Iteration 667 the Loss is 0.2694.\n",
      "For Iteration 668 the Loss is 0.2693.\n",
      "For Iteration 669 the Loss is 0.2693.\n",
      "For Iteration 670 the Loss is 0.2693.\n",
      "For Iteration 671 the Loss is 0.2693.\n",
      "For Iteration 672 the Loss is 0.2693.\n",
      "For Iteration 673 the Loss is 0.2693.\n",
      "For Iteration 674 the Loss is 0.2693.\n",
      "For Iteration 675 the Loss is 0.2693.\n",
      "For Iteration 676 the Loss is 0.2693.\n",
      "For Iteration 677 the Loss is 0.2693.\n",
      "For Iteration 678 the Loss is 0.2693.\n",
      "For Iteration 679 the Loss is 0.2693.\n",
      "For Iteration 680 the Loss is 0.2693.\n",
      "For Iteration 681 the Loss is 0.2693.\n",
      "For Iteration 682 the Loss is 0.2692.\n",
      "For Iteration 683 the Loss is 0.2692.\n",
      "For Iteration 684 the Loss is 0.2692.\n",
      "For Iteration 685 the Loss is 0.2692.\n",
      "For Iteration 686 the Loss is 0.2692.\n",
      "For Iteration 687 the Loss is 0.2692.\n",
      "For Iteration 688 the Loss is 0.2692.\n",
      "For Iteration 689 the Loss is 0.2692.\n",
      "For Iteration 690 the Loss is 0.2692.\n",
      "For Iteration 691 the Loss is 0.2692.\n",
      "For Iteration 692 the Loss is 0.2692.\n",
      "For Iteration 693 the Loss is 0.2692.\n",
      "For Iteration 694 the Loss is 0.2692.\n",
      "For Iteration 695 the Loss is 0.2692.\n",
      "For Iteration 696 the Loss is 0.2691.\n",
      "For Iteration 697 the Loss is 0.2691.\n",
      "For Iteration 698 the Loss is 0.2691.\n",
      "For Iteration 699 the Loss is 0.2691.\n",
      "For Iteration 700 the Loss is 0.2691.\n",
      "For Iteration 701 the Loss is 0.2691.\n",
      "For Iteration 702 the Loss is 0.2691.\n",
      "For Iteration 703 the Loss is 0.2691.\n",
      "For Iteration 704 the Loss is 0.2691.\n",
      "For Iteration 705 the Loss is 0.2691.\n",
      "For Iteration 706 the Loss is 0.2691.\n",
      "For Iteration 707 the Loss is 0.2691.\n",
      "For Iteration 708 the Loss is 0.2691.\n",
      "For Iteration 709 the Loss is 0.2691.\n",
      "For Iteration 710 the Loss is 0.269.\n",
      "For Iteration 711 the Loss is 0.269.\n",
      "For Iteration 712 the Loss is 0.269.\n",
      "For Iteration 713 the Loss is 0.269.\n",
      "For Iteration 714 the Loss is 0.269.\n",
      "For Iteration 715 the Loss is 0.269.\n",
      "For Iteration 716 the Loss is 0.269.\n",
      "For Iteration 717 the Loss is 0.269.\n",
      "For Iteration 718 the Loss is 0.269.\n",
      "For Iteration 719 the Loss is 0.269.\n",
      "For Iteration 720 the Loss is 0.269.\n",
      "For Iteration 721 the Loss is 0.269.\n",
      "For Iteration 722 the Loss is 0.269.\n",
      "For Iteration 723 the Loss is 0.269.\n",
      "For Iteration 724 the Loss is 0.269.\n",
      "For Iteration 725 the Loss is 0.2689.\n",
      "For Iteration 726 the Loss is 0.2689.\n",
      "For Iteration 727 the Loss is 0.2689.\n",
      "For Iteration 728 the Loss is 0.2689.\n",
      "For Iteration 729 the Loss is 0.2689.\n",
      "For Iteration 730 the Loss is 0.2689.\n",
      "For Iteration 731 the Loss is 0.2689.\n",
      "For Iteration 732 the Loss is 0.2689.\n",
      "For Iteration 733 the Loss is 0.2689.\n",
      "For Iteration 734 the Loss is 0.2689.\n",
      "For Iteration 735 the Loss is 0.2689.\n",
      "For Iteration 736 the Loss is 0.2689.\n",
      "For Iteration 737 the Loss is 0.2689.\n",
      "For Iteration 738 the Loss is 0.2689.\n",
      "For Iteration 739 the Loss is 0.2689.\n",
      "For Iteration 740 the Loss is 0.2689.\n",
      "For Iteration 741 the Loss is 0.2688.\n",
      "For Iteration 742 the Loss is 0.2688.\n",
      "For Iteration 743 the Loss is 0.2688.\n",
      "For Iteration 744 the Loss is 0.2688.\n",
      "For Iteration 745 the Loss is 0.2688.\n",
      "For Iteration 746 the Loss is 0.2688.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 747 the Loss is 0.2688.\n",
      "For Iteration 748 the Loss is 0.2688.\n",
      "For Iteration 749 the Loss is 0.2688.\n",
      "For Iteration 750 the Loss is 0.2688.\n",
      "For Iteration 751 the Loss is 0.2688.\n",
      "For Iteration 752 the Loss is 0.2688.\n",
      "For Iteration 753 the Loss is 0.2688.\n",
      "For Iteration 754 the Loss is 0.2688.\n",
      "For Iteration 755 the Loss is 0.2688.\n",
      "For Iteration 756 the Loss is 0.2688.\n",
      "For Iteration 757 the Loss is 0.2687.\n",
      "For Iteration 758 the Loss is 0.2687.\n",
      "For Iteration 759 the Loss is 0.2687.\n",
      "For Iteration 760 the Loss is 0.2687.\n",
      "For Iteration 761 the Loss is 0.2687.\n",
      "For Iteration 762 the Loss is 0.2687.\n",
      "For Iteration 763 the Loss is 0.2687.\n",
      "For Iteration 764 the Loss is 0.2687.\n",
      "For Iteration 765 the Loss is 0.2687.\n",
      "For Iteration 766 the Loss is 0.2687.\n",
      "For Iteration 767 the Loss is 0.2687.\n",
      "For Iteration 768 the Loss is 0.2687.\n",
      "For Iteration 769 the Loss is 0.2687.\n",
      "For Iteration 770 the Loss is 0.2687.\n",
      "For Iteration 771 the Loss is 0.2687.\n",
      "For Iteration 772 the Loss is 0.2687.\n",
      "For Iteration 773 the Loss is 0.2687.\n",
      "For Iteration 774 the Loss is 0.2686.\n",
      "For Iteration 775 the Loss is 0.2686.\n",
      "For Iteration 776 the Loss is 0.2686.\n",
      "For Iteration 777 the Loss is 0.2686.\n",
      "For Iteration 778 the Loss is 0.2686.\n",
      "For Iteration 779 the Loss is 0.2686.\n",
      "For Iteration 780 the Loss is 0.2686.\n",
      "For Iteration 781 the Loss is 0.2686.\n",
      "For Iteration 782 the Loss is 0.2686.\n",
      "For Iteration 783 the Loss is 0.2686.\n",
      "For Iteration 784 the Loss is 0.2686.\n",
      "For Iteration 785 the Loss is 0.2686.\n",
      "For Iteration 786 the Loss is 0.2686.\n",
      "For Iteration 787 the Loss is 0.2686.\n",
      "For Iteration 788 the Loss is 0.2686.\n",
      "For Iteration 789 the Loss is 0.2686.\n",
      "For Iteration 790 the Loss is 0.2686.\n",
      "For Iteration 791 the Loss is 0.2685.\n",
      "For Iteration 792 the Loss is 0.2685.\n",
      "For Iteration 793 the Loss is 0.2685.\n",
      "For Iteration 794 the Loss is 0.2685.\n",
      "For Iteration 795 the Loss is 0.2685.\n",
      "For Iteration 796 the Loss is 0.2685.\n",
      "For Iteration 797 the Loss is 0.2685.\n",
      "For Iteration 798 the Loss is 0.2685.\n",
      "For Iteration 799 the Loss is 0.2685.\n",
      "For Iteration 800 the Loss is 0.2685.\n",
      "For Iteration 801 the Loss is 0.2685.\n",
      "For Iteration 802 the Loss is 0.2685.\n",
      "For Iteration 803 the Loss is 0.2685.\n",
      "For Iteration 804 the Loss is 0.2685.\n",
      "For Iteration 805 the Loss is 0.2685.\n",
      "For Iteration 806 the Loss is 0.2685.\n",
      "For Iteration 807 the Loss is 0.2685.\n",
      "For Iteration 808 the Loss is 0.2684.\n",
      "For Iteration 809 the Loss is 0.2684.\n",
      "For Iteration 810 the Loss is 0.2684.\n",
      "For Iteration 811 the Loss is 0.2684.\n",
      "For Iteration 812 the Loss is 0.2684.\n",
      "For Iteration 813 the Loss is 0.2684.\n",
      "For Iteration 814 the Loss is 0.2684.\n",
      "For Iteration 815 the Loss is 0.2684.\n",
      "For Iteration 816 the Loss is 0.2684.\n",
      "For Iteration 817 the Loss is 0.2684.\n",
      "For Iteration 818 the Loss is 0.2684.\n",
      "For Iteration 819 the Loss is 0.2684.\n",
      "For Iteration 820 the Loss is 0.2684.\n",
      "For Iteration 821 the Loss is 0.2684.\n",
      "For Iteration 822 the Loss is 0.2684.\n",
      "For Iteration 823 the Loss is 0.2684.\n",
      "For Iteration 824 the Loss is 0.2684.\n",
      "For Iteration 825 the Loss is 0.2684.\n",
      "For Iteration 826 the Loss is 0.2684.\n",
      "For Iteration 827 the Loss is 0.2683.\n",
      "For Iteration 828 the Loss is 0.2683.\n",
      "For Iteration 829 the Loss is 0.2683.\n",
      "For Iteration 830 the Loss is 0.2683.\n",
      "For Iteration 831 the Loss is 0.2683.\n",
      "For Iteration 832 the Loss is 0.2683.\n",
      "For Iteration 833 the Loss is 0.2683.\n",
      "For Iteration 834 the Loss is 0.2683.\n",
      "For Iteration 835 the Loss is 0.2683.\n",
      "For Iteration 836 the Loss is 0.2683.\n",
      "For Iteration 837 the Loss is 0.2683.\n",
      "For Iteration 838 the Loss is 0.2683.\n",
      "For Iteration 839 the Loss is 0.2683.\n",
      "For Iteration 840 the Loss is 0.2683.\n",
      "For Iteration 841 the Loss is 0.2683.\n",
      "For Iteration 842 the Loss is 0.2683.\n",
      "For Iteration 843 the Loss is 0.2683.\n",
      "For Iteration 844 the Loss is 0.2683.\n",
      "For Iteration 845 the Loss is 0.2683.\n",
      "For Iteration 846 the Loss is 0.2682.\n",
      "For Iteration 847 the Loss is 0.2682.\n",
      "For Iteration 848 the Loss is 0.2682.\n",
      "For Iteration 849 the Loss is 0.2682.\n",
      "For Iteration 850 the Loss is 0.2682.\n",
      "For Iteration 851 the Loss is 0.2682.\n",
      "For Iteration 852 the Loss is 0.2682.\n",
      "For Iteration 853 the Loss is 0.2682.\n",
      "For Iteration 854 the Loss is 0.2682.\n",
      "For Iteration 855 the Loss is 0.2682.\n",
      "For Iteration 856 the Loss is 0.2682.\n",
      "For Iteration 857 the Loss is 0.2682.\n",
      "For Iteration 858 the Loss is 0.2682.\n",
      "For Iteration 859 the Loss is 0.2682.\n",
      "For Iteration 860 the Loss is 0.2682.\n",
      "For Iteration 861 the Loss is 0.2682.\n",
      "For Iteration 862 the Loss is 0.2682.\n",
      "For Iteration 863 the Loss is 0.2682.\n",
      "For Iteration 864 the Loss is 0.2682.\n",
      "For Iteration 865 the Loss is 0.2681.\n",
      "For Iteration 866 the Loss is 0.2681.\n",
      "For Iteration 867 the Loss is 0.2681.\n",
      "For Iteration 868 the Loss is 0.2681.\n",
      "For Iteration 869 the Loss is 0.2681.\n",
      "For Iteration 870 the Loss is 0.2681.\n",
      "For Iteration 871 the Loss is 0.2681.\n",
      "For Iteration 872 the Loss is 0.2681.\n",
      "For Iteration 873 the Loss is 0.2681.\n",
      "For Iteration 874 the Loss is 0.2681.\n",
      "For Iteration 875 the Loss is 0.2681.\n",
      "For Iteration 876 the Loss is 0.2681.\n",
      "For Iteration 877 the Loss is 0.2681.\n",
      "For Iteration 878 the Loss is 0.2681.\n",
      "For Iteration 879 the Loss is 0.2681.\n",
      "For Iteration 880 the Loss is 0.2681.\n",
      "For Iteration 881 the Loss is 0.2681.\n",
      "For Iteration 882 the Loss is 0.2681.\n",
      "For Iteration 883 the Loss is 0.2681.\n",
      "For Iteration 884 the Loss is 0.2681.\n",
      "For Iteration 885 the Loss is 0.2681.\n",
      "For Iteration 886 the Loss is 0.268.\n",
      "For Iteration 887 the Loss is 0.268.\n",
      "For Iteration 888 the Loss is 0.268.\n",
      "For Iteration 889 the Loss is 0.268.\n",
      "For Iteration 890 the Loss is 0.268.\n",
      "For Iteration 891 the Loss is 0.268.\n",
      "For Iteration 892 the Loss is 0.268.\n",
      "For Iteration 893 the Loss is 0.268.\n",
      "For Iteration 894 the Loss is 0.268.\n",
      "For Iteration 895 the Loss is 0.268.\n",
      "For Iteration 896 the Loss is 0.268.\n",
      "For Iteration 897 the Loss is 0.268.\n",
      "For Iteration 898 the Loss is 0.268.\n",
      "For Iteration 899 the Loss is 0.268.\n",
      "For Iteration 900 the Loss is 0.268.\n",
      "For Iteration 901 the Loss is 0.268.\n",
      "For Iteration 902 the Loss is 0.268.\n",
      "For Iteration 903 the Loss is 0.268.\n",
      "For Iteration 904 the Loss is 0.268.\n",
      "For Iteration 905 the Loss is 0.268.\n",
      "For Iteration 906 the Loss is 0.268.\n",
      "For Iteration 907 the Loss is 0.2679.\n",
      "For Iteration 908 the Loss is 0.2679.\n",
      "For Iteration 909 the Loss is 0.2679.\n",
      "For Iteration 910 the Loss is 0.2679.\n",
      "For Iteration 911 the Loss is 0.2679.\n",
      "For Iteration 912 the Loss is 0.2679.\n",
      "For Iteration 913 the Loss is 0.2679.\n",
      "For Iteration 914 the Loss is 0.2679.\n",
      "For Iteration 915 the Loss is 0.2679.\n",
      "For Iteration 916 the Loss is 0.2679.\n",
      "For Iteration 917 the Loss is 0.2679.\n",
      "For Iteration 918 the Loss is 0.2679.\n",
      "For Iteration 919 the Loss is 0.2679.\n",
      "For Iteration 920 the Loss is 0.2679.\n",
      "For Iteration 921 the Loss is 0.2679.\n",
      "For Iteration 922 the Loss is 0.2679.\n",
      "For Iteration 923 the Loss is 0.2679.\n",
      "For Iteration 924 the Loss is 0.2679.\n",
      "For Iteration 925 the Loss is 0.2679.\n",
      "For Iteration 926 the Loss is 0.2679.\n",
      "For Iteration 927 the Loss is 0.2679.\n",
      "For Iteration 928 the Loss is 0.2679.\n",
      "For Iteration 929 the Loss is 0.2678.\n",
      "For Iteration 930 the Loss is 0.2678.\n",
      "For Iteration 931 the Loss is 0.2678.\n",
      "For Iteration 932 the Loss is 0.2678.\n",
      "For Iteration 933 the Loss is 0.2678.\n",
      "For Iteration 934 the Loss is 0.2678.\n",
      "For Iteration 935 the Loss is 0.2678.\n",
      "For Iteration 936 the Loss is 0.2678.\n",
      "For Iteration 937 the Loss is 0.2678.\n",
      "For Iteration 938 the Loss is 0.2678.\n",
      "For Iteration 939 the Loss is 0.2678.\n",
      "For Iteration 940 the Loss is 0.2678.\n",
      "For Iteration 941 the Loss is 0.2678.\n",
      "For Iteration 942 the Loss is 0.2678.\n",
      "For Iteration 943 the Loss is 0.2678.\n",
      "For Iteration 944 the Loss is 0.2678.\n",
      "For Iteration 945 the Loss is 0.2678.\n",
      "For Iteration 946 the Loss is 0.2678.\n",
      "For Iteration 947 the Loss is 0.2678.\n",
      "For Iteration 948 the Loss is 0.2678.\n",
      "For Iteration 949 the Loss is 0.2678.\n",
      "For Iteration 950 the Loss is 0.2678.\n",
      "For Iteration 951 the Loss is 0.2677.\n",
      "For Iteration 952 the Loss is 0.2677.\n",
      "For Iteration 953 the Loss is 0.2677.\n",
      "For Iteration 954 the Loss is 0.2677.\n",
      "For Iteration 955 the Loss is 0.2677.\n",
      "For Iteration 956 the Loss is 0.2677.\n",
      "For Iteration 957 the Loss is 0.2677.\n",
      "For Iteration 958 the Loss is 0.2677.\n",
      "For Iteration 959 the Loss is 0.2677.\n",
      "For Iteration 960 the Loss is 0.2677.\n",
      "For Iteration 961 the Loss is 0.2677.\n",
      "For Iteration 962 the Loss is 0.2677.\n",
      "For Iteration 963 the Loss is 0.2677.\n",
      "For Iteration 964 the Loss is 0.2677.\n",
      "For Iteration 965 the Loss is 0.2677.\n",
      "For Iteration 966 the Loss is 0.2677.\n",
      "For Iteration 967 the Loss is 0.2677.\n",
      "For Iteration 968 the Loss is 0.2677.\n",
      "For Iteration 969 the Loss is 0.2677.\n",
      "For Iteration 970 the Loss is 0.2677.\n",
      "For Iteration 971 the Loss is 0.2677.\n",
      "For Iteration 972 the Loss is 0.2677.\n",
      "For Iteration 973 the Loss is 0.2677.\n",
      "For Iteration 974 the Loss is 0.2676.\n",
      "For Iteration 975 the Loss is 0.2676.\n",
      "For Iteration 976 the Loss is 0.2676.\n",
      "For Iteration 977 the Loss is 0.2676.\n",
      "For Iteration 978 the Loss is 0.2676.\n",
      "For Iteration 979 the Loss is 0.2676.\n",
      "For Iteration 980 the Loss is 0.2676.\n",
      "For Iteration 981 the Loss is 0.2676.\n",
      "For Iteration 982 the Loss is 0.2676.\n",
      "For Iteration 983 the Loss is 0.2676.\n",
      "For Iteration 984 the Loss is 0.2676.\n",
      "For Iteration 985 the Loss is 0.2676.\n",
      "For Iteration 986 the Loss is 0.2676.\n",
      "For Iteration 987 the Loss is 0.2676.\n",
      "For Iteration 988 the Loss is 0.2676.\n",
      "For Iteration 989 the Loss is 0.2676.\n",
      "For Iteration 990 the Loss is 0.2676.\n",
      "For Iteration 991 the Loss is 0.2676.\n",
      "For Iteration 992 the Loss is 0.2676.\n",
      "For Iteration 993 the Loss is 0.2676.\n",
      "For Iteration 994 the Loss is 0.2676.\n",
      "For Iteration 995 the Loss is 0.2676.\n",
      "For Iteration 996 the Loss is 0.2676.\n",
      "For Iteration 997 the Loss is 0.2676.\n",
      "For Iteration 998 the Loss is 0.2675.\n",
      "For Iteration 999 the Loss is 0.2675.\n",
      "For Iteration 1000 the Loss is 0.2675.\n",
      "For Iteration 1001 the Loss is 0.2675.\n",
      "For Iteration 1002 the Loss is 0.2675.\n",
      "For Iteration 1003 the Loss is 0.2675.\n",
      "For Iteration 1004 the Loss is 0.2675.\n",
      "For Iteration 1005 the Loss is 0.2675.\n",
      "For Iteration 1006 the Loss is 0.2675.\n",
      "For Iteration 1007 the Loss is 0.2675.\n",
      "For Iteration 1008 the Loss is 0.2675.\n",
      "For Iteration 1009 the Loss is 0.2675.\n",
      "For Iteration 1010 the Loss is 0.2675.\n",
      "For Iteration 1011 the Loss is 0.2675.\n",
      "For Iteration 1012 the Loss is 0.2675.\n",
      "For Iteration 1013 the Loss is 0.2675.\n",
      "For Iteration 1014 the Loss is 0.2675.\n",
      "For Iteration 1015 the Loss is 0.2675.\n",
      "For Iteration 1016 the Loss is 0.2675.\n",
      "For Iteration 1017 the Loss is 0.2675.\n",
      "For Iteration 1018 the Loss is 0.2675.\n",
      "For Iteration 1019 the Loss is 0.2675.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1020 the Loss is 0.2675.\n",
      "For Iteration 1021 the Loss is 0.2675.\n",
      "For Iteration 1022 the Loss is 0.2675.\n",
      "For Iteration 1023 the Loss is 0.2674.\n",
      "For Iteration 1024 the Loss is 0.2674.\n",
      "For Iteration 1025 the Loss is 0.2674.\n",
      "For Iteration 1026 the Loss is 0.2674.\n",
      "For Iteration 1027 the Loss is 0.2674.\n",
      "For Iteration 1028 the Loss is 0.2674.\n",
      "For Iteration 1029 the Loss is 0.2674.\n",
      "For Iteration 1030 the Loss is 0.2674.\n",
      "For Iteration 1031 the Loss is 0.2674.\n",
      "For Iteration 1032 the Loss is 0.2674.\n",
      "For Iteration 1033 the Loss is 0.2674.\n",
      "For Iteration 1034 the Loss is 0.2674.\n",
      "For Iteration 1035 the Loss is 0.2674.\n",
      "For Iteration 1036 the Loss is 0.2674.\n",
      "For Iteration 1037 the Loss is 0.2674.\n",
      "For Iteration 1038 the Loss is 0.2674.\n",
      "For Iteration 1039 the Loss is 0.2674.\n",
      "For Iteration 1040 the Loss is 0.2674.\n",
      "For Iteration 1041 the Loss is 0.2674.\n",
      "For Iteration 1042 the Loss is 0.2674.\n",
      "For Iteration 1043 the Loss is 0.2674.\n",
      "For Iteration 1044 the Loss is 0.2674.\n",
      "For Iteration 1045 the Loss is 0.2674.\n",
      "For Iteration 1046 the Loss is 0.2674.\n",
      "For Iteration 1047 the Loss is 0.2674.\n",
      "For Iteration 1048 the Loss is 0.2674.\n",
      "For Iteration 1049 the Loss is 0.2673.\n",
      "For Iteration 1050 the Loss is 0.2673.\n",
      "For Iteration 1051 the Loss is 0.2673.\n",
      "For Iteration 1052 the Loss is 0.2673.\n",
      "For Iteration 1053 the Loss is 0.2673.\n",
      "For Iteration 1054 the Loss is 0.2673.\n",
      "For Iteration 1055 the Loss is 0.2673.\n",
      "For Iteration 1056 the Loss is 0.2673.\n",
      "For Iteration 1057 the Loss is 0.2673.\n",
      "For Iteration 1058 the Loss is 0.2673.\n",
      "For Iteration 1059 the Loss is 0.2673.\n",
      "For Iteration 1060 the Loss is 0.2673.\n",
      "For Iteration 1061 the Loss is 0.2673.\n",
      "For Iteration 1062 the Loss is 0.2673.\n",
      "For Iteration 1063 the Loss is 0.2673.\n",
      "For Iteration 1064 the Loss is 0.2673.\n",
      "For Iteration 1065 the Loss is 0.2673.\n",
      "For Iteration 1066 the Loss is 0.2673.\n",
      "For Iteration 1067 the Loss is 0.2673.\n",
      "For Iteration 1068 the Loss is 0.2673.\n",
      "For Iteration 1069 the Loss is 0.2673.\n",
      "For Iteration 1070 the Loss is 0.2673.\n",
      "For Iteration 1071 the Loss is 0.2673.\n",
      "For Iteration 1072 the Loss is 0.2673.\n",
      "For Iteration 1073 the Loss is 0.2673.\n",
      "For Iteration 1074 the Loss is 0.2673.\n",
      "For Iteration 1075 the Loss is 0.2672.\n",
      "For Iteration 1076 the Loss is 0.2672.\n",
      "For Iteration 1077 the Loss is 0.2672.\n",
      "For Iteration 1078 the Loss is 0.2672.\n",
      "For Iteration 1079 the Loss is 0.2672.\n",
      "For Iteration 1080 the Loss is 0.2672.\n",
      "For Iteration 1081 the Loss is 0.2672.\n",
      "For Iteration 1082 the Loss is 0.2672.\n",
      "For Iteration 1083 the Loss is 0.2672.\n",
      "For Iteration 1084 the Loss is 0.2672.\n",
      "For Iteration 1085 the Loss is 0.2672.\n",
      "For Iteration 1086 the Loss is 0.2672.\n",
      "For Iteration 1087 the Loss is 0.2672.\n",
      "For Iteration 1088 the Loss is 0.2672.\n",
      "For Iteration 1089 the Loss is 0.2672.\n",
      "For Iteration 1090 the Loss is 0.2672.\n",
      "For Iteration 1091 the Loss is 0.2672.\n",
      "For Iteration 1092 the Loss is 0.2672.\n",
      "For Iteration 1093 the Loss is 0.2672.\n",
      "For Iteration 1094 the Loss is 0.2672.\n",
      "For Iteration 1095 the Loss is 0.2672.\n",
      "For Iteration 1096 the Loss is 0.2672.\n",
      "For Iteration 1097 the Loss is 0.2672.\n",
      "For Iteration 1098 the Loss is 0.2672.\n",
      "For Iteration 1099 the Loss is 0.2672.\n",
      "For Iteration 1100 the Loss is 0.2672.\n",
      "For Iteration 1101 the Loss is 0.2672.\n",
      "For Iteration 1102 the Loss is 0.2672.\n",
      "For Iteration 1103 the Loss is 0.2671.\n",
      "For Iteration 1104 the Loss is 0.2671.\n",
      "For Iteration 1105 the Loss is 0.2671.\n",
      "For Iteration 1106 the Loss is 0.2671.\n",
      "For Iteration 1107 the Loss is 0.2671.\n",
      "For Iteration 1108 the Loss is 0.2671.\n",
      "For Iteration 1109 the Loss is 0.2671.\n",
      "For Iteration 1110 the Loss is 0.2671.\n",
      "For Iteration 1111 the Loss is 0.2671.\n",
      "For Iteration 1112 the Loss is 0.2671.\n",
      "For Iteration 1113 the Loss is 0.2671.\n",
      "For Iteration 1114 the Loss is 0.2671.\n",
      "For Iteration 1115 the Loss is 0.2671.\n",
      "For Iteration 1116 the Loss is 0.2671.\n",
      "For Iteration 1117 the Loss is 0.2671.\n",
      "For Iteration 1118 the Loss is 0.2671.\n",
      "For Iteration 1119 the Loss is 0.2671.\n",
      "For Iteration 1120 the Loss is 0.2671.\n",
      "For Iteration 1121 the Loss is 0.2671.\n",
      "For Iteration 1122 the Loss is 0.2671.\n",
      "For Iteration 1123 the Loss is 0.2671.\n",
      "For Iteration 1124 the Loss is 0.2671.\n",
      "For Iteration 1125 the Loss is 0.2671.\n",
      "For Iteration 1126 the Loss is 0.2671.\n",
      "For Iteration 1127 the Loss is 0.2671.\n",
      "For Iteration 1128 the Loss is 0.2671.\n",
      "For Iteration 1129 the Loss is 0.2671.\n",
      "For Iteration 1130 the Loss is 0.2671.\n",
      "For Iteration 1131 the Loss is 0.267.\n",
      "For Iteration 1132 the Loss is 0.267.\n",
      "For Iteration 1133 the Loss is 0.267.\n",
      "For Iteration 1134 the Loss is 0.267.\n",
      "For Iteration 1135 the Loss is 0.267.\n",
      "For Iteration 1136 the Loss is 0.267.\n",
      "For Iteration 1137 the Loss is 0.267.\n",
      "For Iteration 1138 the Loss is 0.267.\n",
      "For Iteration 1139 the Loss is 0.267.\n",
      "For Iteration 1140 the Loss is 0.267.\n",
      "For Iteration 1141 the Loss is 0.267.\n",
      "For Iteration 1142 the Loss is 0.267.\n",
      "For Iteration 1143 the Loss is 0.267.\n",
      "For Iteration 1144 the Loss is 0.267.\n",
      "For Iteration 1145 the Loss is 0.267.\n",
      "For Iteration 1146 the Loss is 0.267.\n",
      "For Iteration 1147 the Loss is 0.267.\n",
      "For Iteration 1148 the Loss is 0.267.\n",
      "For Iteration 1149 the Loss is 0.267.\n",
      "For Iteration 1150 the Loss is 0.267.\n",
      "For Iteration 1151 the Loss is 0.267.\n",
      "For Iteration 1152 the Loss is 0.267.\n",
      "For Iteration 1153 the Loss is 0.267.\n",
      "For Iteration 1154 the Loss is 0.267.\n",
      "For Iteration 1155 the Loss is 0.267.\n",
      "For Iteration 1156 the Loss is 0.267.\n",
      "For Iteration 1157 the Loss is 0.267.\n",
      "For Iteration 1158 the Loss is 0.267.\n",
      "For Iteration 1159 the Loss is 0.267.\n",
      "For Iteration 1160 the Loss is 0.2669.\n",
      "For Iteration 1161 the Loss is 0.2669.\n",
      "For Iteration 1162 the Loss is 0.2669.\n",
      "For Iteration 1163 the Loss is 0.2669.\n",
      "For Iteration 1164 the Loss is 0.2669.\n",
      "For Iteration 1165 the Loss is 0.2669.\n",
      "For Iteration 1166 the Loss is 0.2669.\n",
      "For Iteration 1167 the Loss is 0.2669.\n",
      "For Iteration 1168 the Loss is 0.2669.\n",
      "For Iteration 1169 the Loss is 0.2669.\n",
      "For Iteration 1170 the Loss is 0.2669.\n",
      "For Iteration 1171 the Loss is 0.2669.\n",
      "For Iteration 1172 the Loss is 0.2669.\n",
      "For Iteration 1173 the Loss is 0.2669.\n",
      "For Iteration 1174 the Loss is 0.2669.\n",
      "For Iteration 1175 the Loss is 0.2669.\n",
      "For Iteration 1176 the Loss is 0.2669.\n",
      "For Iteration 1177 the Loss is 0.2669.\n",
      "For Iteration 1178 the Loss is 0.2669.\n",
      "For Iteration 1179 the Loss is 0.2669.\n",
      "For Iteration 1180 the Loss is 0.2669.\n",
      "For Iteration 1181 the Loss is 0.2669.\n",
      "For Iteration 1182 the Loss is 0.2669.\n",
      "For Iteration 1183 the Loss is 0.2669.\n",
      "For Iteration 1184 the Loss is 0.2669.\n",
      "For Iteration 1185 the Loss is 0.2669.\n",
      "For Iteration 1186 the Loss is 0.2669.\n",
      "For Iteration 1187 the Loss is 0.2669.\n",
      "For Iteration 1188 the Loss is 0.2669.\n",
      "For Iteration 1189 the Loss is 0.2669.\n",
      "For Iteration 1190 the Loss is 0.2669.\n",
      "For Iteration 1191 the Loss is 0.2668.\n",
      "For Iteration 1192 the Loss is 0.2668.\n",
      "For Iteration 1193 the Loss is 0.2668.\n",
      "For Iteration 1194 the Loss is 0.2668.\n",
      "For Iteration 1195 the Loss is 0.2668.\n",
      "For Iteration 1196 the Loss is 0.2668.\n",
      "For Iteration 1197 the Loss is 0.2668.\n",
      "For Iteration 1198 the Loss is 0.2668.\n",
      "For Iteration 1199 the Loss is 0.2668.\n",
      "For Iteration 1200 the Loss is 0.2668.\n",
      "For Iteration 1201 the Loss is 0.2668.\n",
      "For Iteration 1202 the Loss is 0.2668.\n",
      "For Iteration 1203 the Loss is 0.2668.\n",
      "For Iteration 1204 the Loss is 0.2668.\n",
      "For Iteration 1205 the Loss is 0.2668.\n",
      "For Iteration 1206 the Loss is 0.2668.\n",
      "For Iteration 1207 the Loss is 0.2668.\n",
      "For Iteration 1208 the Loss is 0.2668.\n",
      "For Iteration 1209 the Loss is 0.2668.\n",
      "For Iteration 1210 the Loss is 0.2668.\n",
      "For Iteration 1211 the Loss is 0.2668.\n",
      "For Iteration 1212 the Loss is 0.2668.\n",
      "For Iteration 1213 the Loss is 0.2668.\n",
      "For Iteration 1214 the Loss is 0.2668.\n",
      "For Iteration 1215 the Loss is 0.2668.\n",
      "For Iteration 1216 the Loss is 0.2668.\n",
      "For Iteration 1217 the Loss is 0.2668.\n",
      "For Iteration 1218 the Loss is 0.2668.\n",
      "For Iteration 1219 the Loss is 0.2668.\n",
      "For Iteration 1220 the Loss is 0.2668.\n",
      "For Iteration 1221 the Loss is 0.2668.\n",
      "For Iteration 1222 the Loss is 0.2667.\n",
      "For Iteration 1223 the Loss is 0.2667.\n",
      "For Iteration 1224 the Loss is 0.2667.\n",
      "For Iteration 1225 the Loss is 0.2667.\n",
      "For Iteration 1226 the Loss is 0.2667.\n",
      "For Iteration 1227 the Loss is 0.2667.\n",
      "For Iteration 1228 the Loss is 0.2667.\n",
      "For Iteration 1229 the Loss is 0.2667.\n",
      "For Iteration 1230 the Loss is 0.2667.\n",
      "For Iteration 1231 the Loss is 0.2667.\n",
      "For Iteration 1232 the Loss is 0.2667.\n",
      "For Iteration 1233 the Loss is 0.2667.\n",
      "For Iteration 1234 the Loss is 0.2667.\n",
      "For Iteration 1235 the Loss is 0.2667.\n",
      "For Iteration 1236 the Loss is 0.2667.\n",
      "For Iteration 1237 the Loss is 0.2667.\n",
      "For Iteration 1238 the Loss is 0.2667.\n",
      "For Iteration 1239 the Loss is 0.2667.\n",
      "For Iteration 1240 the Loss is 0.2667.\n",
      "For Iteration 1241 the Loss is 0.2667.\n",
      "For Iteration 1242 the Loss is 0.2667.\n",
      "For Iteration 1243 the Loss is 0.2667.\n",
      "For Iteration 1244 the Loss is 0.2667.\n",
      "For Iteration 1245 the Loss is 0.2667.\n",
      "For Iteration 1246 the Loss is 0.2667.\n",
      "For Iteration 1247 the Loss is 0.2667.\n",
      "For Iteration 1248 the Loss is 0.2667.\n",
      "For Iteration 1249 the Loss is 0.2667.\n",
      "For Iteration 1250 the Loss is 0.2667.\n",
      "For Iteration 1251 the Loss is 0.2667.\n",
      "For Iteration 1252 the Loss is 0.2667.\n",
      "For Iteration 1253 the Loss is 0.2667.\n",
      "For Iteration 1254 the Loss is 0.2666.\n",
      "For Iteration 1255 the Loss is 0.2666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1256 the Loss is 0.2666.\n",
      "For Iteration 1257 the Loss is 0.2666.\n",
      "For Iteration 1258 the Loss is 0.2666.\n",
      "For Iteration 1259 the Loss is 0.2666.\n",
      "For Iteration 1260 the Loss is 0.2666.\n",
      "For Iteration 1261 the Loss is 0.2666.\n",
      "For Iteration 1262 the Loss is 0.2666.\n",
      "For Iteration 1263 the Loss is 0.2666.\n",
      "For Iteration 1264 the Loss is 0.2666.\n",
      "For Iteration 1265 the Loss is 0.2666.\n",
      "For Iteration 1266 the Loss is 0.2666.\n",
      "For Iteration 1267 the Loss is 0.2666.\n",
      "For Iteration 1268 the Loss is 0.2666.\n",
      "For Iteration 1269 the Loss is 0.2666.\n",
      "For Iteration 1270 the Loss is 0.2666.\n",
      "For Iteration 1271 the Loss is 0.2666.\n",
      "For Iteration 1272 the Loss is 0.2666.\n",
      "For Iteration 1273 the Loss is 0.2666.\n",
      "For Iteration 1274 the Loss is 0.2666.\n",
      "For Iteration 1275 the Loss is 0.2666.\n",
      "For Iteration 1276 the Loss is 0.2666.\n",
      "For Iteration 1277 the Loss is 0.2666.\n",
      "For Iteration 1278 the Loss is 0.2666.\n",
      "For Iteration 1279 the Loss is 0.2666.\n",
      "For Iteration 1280 the Loss is 0.2666.\n",
      "For Iteration 1281 the Loss is 0.2666.\n",
      "For Iteration 1282 the Loss is 0.2666.\n",
      "For Iteration 1283 the Loss is 0.2666.\n",
      "For Iteration 1284 the Loss is 0.2666.\n",
      "For Iteration 1285 the Loss is 0.2666.\n",
      "For Iteration 1286 the Loss is 0.2666.\n",
      "For Iteration 1287 the Loss is 0.2665.\n",
      "For Iteration 1288 the Loss is 0.2665.\n",
      "For Iteration 1289 the Loss is 0.2665.\n",
      "For Iteration 1290 the Loss is 0.2665.\n",
      "For Iteration 1291 the Loss is 0.2665.\n",
      "For Iteration 1292 the Loss is 0.2665.\n",
      "For Iteration 1293 the Loss is 0.2665.\n",
      "For Iteration 1294 the Loss is 0.2665.\n",
      "For Iteration 1295 the Loss is 0.2665.\n",
      "For Iteration 1296 the Loss is 0.2665.\n",
      "For Iteration 1297 the Loss is 0.2665.\n",
      "For Iteration 1298 the Loss is 0.2665.\n",
      "For Iteration 1299 the Loss is 0.2665.\n",
      "For Iteration 1300 the Loss is 0.2665.\n",
      "For Iteration 1301 the Loss is 0.2665.\n",
      "For Iteration 1302 the Loss is 0.2665.\n",
      "For Iteration 1303 the Loss is 0.2665.\n",
      "For Iteration 1304 the Loss is 0.2665.\n",
      "For Iteration 1305 the Loss is 0.2665.\n",
      "For Iteration 1306 the Loss is 0.2665.\n",
      "For Iteration 1307 the Loss is 0.2665.\n",
      "For Iteration 1308 the Loss is 0.2665.\n",
      "For Iteration 1309 the Loss is 0.2665.\n",
      "For Iteration 1310 the Loss is 0.2665.\n",
      "For Iteration 1311 the Loss is 0.2665.\n",
      "For Iteration 1312 the Loss is 0.2665.\n",
      "For Iteration 1313 the Loss is 0.2665.\n",
      "For Iteration 1314 the Loss is 0.2665.\n",
      "For Iteration 1315 the Loss is 0.2665.\n",
      "For Iteration 1316 the Loss is 0.2665.\n",
      "For Iteration 1317 the Loss is 0.2665.\n",
      "For Iteration 1318 the Loss is 0.2665.\n",
      "For Iteration 1319 the Loss is 0.2665.\n",
      "For Iteration 1320 the Loss is 0.2665.\n",
      "For Iteration 1321 the Loss is 0.2664.\n",
      "For Iteration 1322 the Loss is 0.2664.\n",
      "For Iteration 1323 the Loss is 0.2664.\n",
      "For Iteration 1324 the Loss is 0.2664.\n",
      "For Iteration 1325 the Loss is 0.2664.\n",
      "For Iteration 1326 the Loss is 0.2664.\n",
      "For Iteration 1327 the Loss is 0.2664.\n",
      "For Iteration 1328 the Loss is 0.2664.\n",
      "For Iteration 1329 the Loss is 0.2664.\n",
      "For Iteration 1330 the Loss is 0.2664.\n",
      "For Iteration 1331 the Loss is 0.2664.\n",
      "For Iteration 1332 the Loss is 0.2664.\n",
      "For Iteration 1333 the Loss is 0.2664.\n",
      "For Iteration 1334 the Loss is 0.2664.\n",
      "For Iteration 1335 the Loss is 0.2664.\n",
      "For Iteration 1336 the Loss is 0.2664.\n",
      "For Iteration 1337 the Loss is 0.2664.\n",
      "For Iteration 1338 the Loss is 0.2664.\n",
      "For Iteration 1339 the Loss is 0.2664.\n",
      "For Iteration 1340 the Loss is 0.2664.\n",
      "For Iteration 1341 the Loss is 0.2664.\n",
      "For Iteration 1342 the Loss is 0.2664.\n",
      "For Iteration 1343 the Loss is 0.2664.\n",
      "For Iteration 1344 the Loss is 0.2664.\n",
      "For Iteration 1345 the Loss is 0.2664.\n",
      "For Iteration 1346 the Loss is 0.2664.\n",
      "For Iteration 1347 the Loss is 0.2664.\n",
      "For Iteration 1348 the Loss is 0.2664.\n",
      "For Iteration 1349 the Loss is 0.2664.\n",
      "For Iteration 1350 the Loss is 0.2664.\n",
      "For Iteration 1351 the Loss is 0.2664.\n",
      "For Iteration 1352 the Loss is 0.2664.\n",
      "For Iteration 1353 the Loss is 0.2664.\n",
      "For Iteration 1354 the Loss is 0.2664.\n",
      "For Iteration 1355 the Loss is 0.2664.\n",
      "For Iteration 1356 the Loss is 0.2663.\n",
      "For Iteration 1357 the Loss is 0.2663.\n",
      "For Iteration 1358 the Loss is 0.2663.\n",
      "For Iteration 1359 the Loss is 0.2663.\n",
      "For Iteration 1360 the Loss is 0.2663.\n",
      "For Iteration 1361 the Loss is 0.2663.\n",
      "For Iteration 1362 the Loss is 0.2663.\n",
      "For Iteration 1363 the Loss is 0.2663.\n",
      "For Iteration 1364 the Loss is 0.2663.\n",
      "For Iteration 1365 the Loss is 0.2663.\n",
      "For Iteration 1366 the Loss is 0.2663.\n",
      "For Iteration 1367 the Loss is 0.2663.\n",
      "For Iteration 1368 the Loss is 0.2663.\n",
      "For Iteration 1369 the Loss is 0.2663.\n",
      "For Iteration 1370 the Loss is 0.2663.\n",
      "For Iteration 1371 the Loss is 0.2663.\n",
      "For Iteration 1372 the Loss is 0.2663.\n",
      "For Iteration 1373 the Loss is 0.2663.\n",
      "For Iteration 1374 the Loss is 0.2663.\n",
      "For Iteration 1375 the Loss is 0.2663.\n",
      "For Iteration 1376 the Loss is 0.2663.\n",
      "For Iteration 1377 the Loss is 0.2663.\n",
      "For Iteration 1378 the Loss is 0.2663.\n",
      "For Iteration 1379 the Loss is 0.2663.\n",
      "For Iteration 1380 the Loss is 0.2663.\n",
      "For Iteration 1381 the Loss is 0.2663.\n",
      "For Iteration 1382 the Loss is 0.2663.\n",
      "For Iteration 1383 the Loss is 0.2663.\n",
      "For Iteration 1384 the Loss is 0.2663.\n",
      "For Iteration 1385 the Loss is 0.2663.\n",
      "For Iteration 1386 the Loss is 0.2663.\n",
      "For Iteration 1387 the Loss is 0.2663.\n",
      "For Iteration 1388 the Loss is 0.2663.\n",
      "For Iteration 1389 the Loss is 0.2663.\n",
      "For Iteration 1390 the Loss is 0.2663.\n",
      "For Iteration 1391 the Loss is 0.2663.\n",
      "For Iteration 1392 the Loss is 0.2662.\n",
      "For Iteration 1393 the Loss is 0.2662.\n",
      "For Iteration 1394 the Loss is 0.2662.\n",
      "For Iteration 1395 the Loss is 0.2662.\n",
      "For Iteration 1396 the Loss is 0.2662.\n",
      "For Iteration 1397 the Loss is 0.2662.\n",
      "For Iteration 1398 the Loss is 0.2662.\n",
      "For Iteration 1399 the Loss is 0.2662.\n",
      "For Iteration 1400 the Loss is 0.2662.\n",
      "For Iteration 1401 the Loss is 0.2662.\n",
      "For Iteration 1402 the Loss is 0.2662.\n",
      "For Iteration 1403 the Loss is 0.2662.\n",
      "For Iteration 1404 the Loss is 0.2662.\n",
      "For Iteration 1405 the Loss is 0.2662.\n",
      "For Iteration 1406 the Loss is 0.2662.\n",
      "For Iteration 1407 the Loss is 0.2662.\n",
      "For Iteration 1408 the Loss is 0.2662.\n",
      "For Iteration 1409 the Loss is 0.2662.\n",
      "For Iteration 1410 the Loss is 0.2662.\n",
      "For Iteration 1411 the Loss is 0.2662.\n",
      "For Iteration 1412 the Loss is 0.2662.\n",
      "For Iteration 1413 the Loss is 0.2662.\n",
      "For Iteration 1414 the Loss is 0.2662.\n",
      "For Iteration 1415 the Loss is 0.2662.\n",
      "For Iteration 1416 the Loss is 0.2662.\n",
      "For Iteration 1417 the Loss is 0.2662.\n",
      "For Iteration 1418 the Loss is 0.2662.\n",
      "For Iteration 1419 the Loss is 0.2662.\n",
      "For Iteration 1420 the Loss is 0.2662.\n",
      "For Iteration 1421 the Loss is 0.2662.\n",
      "For Iteration 1422 the Loss is 0.2662.\n",
      "For Iteration 1423 the Loss is 0.2662.\n",
      "For Iteration 1424 the Loss is 0.2662.\n",
      "For Iteration 1425 the Loss is 0.2662.\n",
      "For Iteration 1426 the Loss is 0.2662.\n",
      "For Iteration 1427 the Loss is 0.2662.\n",
      "For Iteration 1428 the Loss is 0.2662.\n",
      "For Iteration 1429 the Loss is 0.2662.\n",
      "For Iteration 1430 the Loss is 0.2661.\n",
      "For Iteration 1431 the Loss is 0.2661.\n",
      "For Iteration 1432 the Loss is 0.2661.\n",
      "For Iteration 1433 the Loss is 0.2661.\n",
      "For Iteration 1434 the Loss is 0.2661.\n",
      "For Iteration 1435 the Loss is 0.2661.\n",
      "For Iteration 1436 the Loss is 0.2661.\n",
      "For Iteration 1437 the Loss is 0.2661.\n",
      "For Iteration 1438 the Loss is 0.2661.\n",
      "For Iteration 1439 the Loss is 0.2661.\n",
      "For Iteration 1440 the Loss is 0.2661.\n",
      "For Iteration 1441 the Loss is 0.2661.\n",
      "For Iteration 1442 the Loss is 0.2661.\n",
      "For Iteration 1443 the Loss is 0.2661.\n",
      "For Iteration 1444 the Loss is 0.2661.\n",
      "For Iteration 1445 the Loss is 0.2661.\n",
      "For Iteration 1446 the Loss is 0.2661.\n",
      "For Iteration 1447 the Loss is 0.2661.\n",
      "For Iteration 1448 the Loss is 0.2661.\n",
      "For Iteration 1449 the Loss is 0.2661.\n",
      "For Iteration 1450 the Loss is 0.2661.\n",
      "For Iteration 1451 the Loss is 0.2661.\n",
      "For Iteration 1452 the Loss is 0.2661.\n",
      "For Iteration 1453 the Loss is 0.2661.\n",
      "For Iteration 1454 the Loss is 0.2661.\n",
      "For Iteration 1455 the Loss is 0.2661.\n",
      "For Iteration 1456 the Loss is 0.2661.\n",
      "For Iteration 1457 the Loss is 0.2661.\n",
      "For Iteration 1458 the Loss is 0.2661.\n",
      "For Iteration 1459 the Loss is 0.2661.\n",
      "For Iteration 1460 the Loss is 0.2661.\n",
      "For Iteration 1461 the Loss is 0.2661.\n",
      "For Iteration 1462 the Loss is 0.2661.\n",
      "For Iteration 1463 the Loss is 0.2661.\n",
      "For Iteration 1464 the Loss is 0.2661.\n",
      "For Iteration 1465 the Loss is 0.2661.\n",
      "For Iteration 1466 the Loss is 0.2661.\n",
      "For Iteration 1467 the Loss is 0.2661.\n",
      "For Iteration 1468 the Loss is 0.266.\n",
      "For Iteration 1469 the Loss is 0.266.\n",
      "For Iteration 1470 the Loss is 0.266.\n",
      "For Iteration 1471 the Loss is 0.266.\n",
      "For Iteration 1472 the Loss is 0.266.\n",
      "For Iteration 1473 the Loss is 0.266.\n",
      "For Iteration 1474 the Loss is 0.266.\n",
      "For Iteration 1475 the Loss is 0.266.\n",
      "For Iteration 1476 the Loss is 0.266.\n",
      "For Iteration 1477 the Loss is 0.266.\n",
      "For Iteration 1478 the Loss is 0.266.\n",
      "For Iteration 1479 the Loss is 0.266.\n",
      "For Iteration 1480 the Loss is 0.266.\n",
      "For Iteration 1481 the Loss is 0.266.\n",
      "For Iteration 1482 the Loss is 0.266.\n",
      "For Iteration 1483 the Loss is 0.266.\n",
      "For Iteration 1484 the Loss is 0.266.\n",
      "For Iteration 1485 the Loss is 0.266.\n",
      "For Iteration 1486 the Loss is 0.266.\n",
      "For Iteration 1487 the Loss is 0.266.\n",
      "For Iteration 1488 the Loss is 0.266.\n",
      "For Iteration 1489 the Loss is 0.266.\n",
      "For Iteration 1490 the Loss is 0.266.\n",
      "For Iteration 1491 the Loss is 0.266.\n",
      "For Iteration 1492 the Loss is 0.266.\n",
      "For Iteration 1493 the Loss is 0.266.\n",
      "For Iteration 1494 the Loss is 0.266.\n",
      "For Iteration 1495 the Loss is 0.266.\n",
      "For Iteration 1496 the Loss is 0.266.\n",
      "For Iteration 1497 the Loss is 0.266.\n",
      "For Iteration 1498 the Loss is 0.266.\n",
      "For Iteration 1499 the Loss is 0.266.\n",
      "For Iteration 1500 the Loss is 0.266.\n",
      "For Iteration 1501 the Loss is 0.266.\n",
      "For Iteration 1502 the Loss is 0.266.\n",
      "For Iteration 1503 the Loss is 0.266.\n",
      "For Iteration 1504 the Loss is 0.266.\n",
      "For Iteration 1505 the Loss is 0.266.\n",
      "For Iteration 1506 the Loss is 0.266.\n",
      "For Iteration 1507 the Loss is 0.266.\n",
      "For Iteration 1508 the Loss is 0.2659.\n",
      "For Iteration 1509 the Loss is 0.2659.\n",
      "For Iteration 1510 the Loss is 0.2659.\n",
      "For Iteration 1511 the Loss is 0.2659.\n",
      "For Iteration 1512 the Loss is 0.2659.\n",
      "For Iteration 1513 the Loss is 0.2659.\n",
      "For Iteration 1514 the Loss is 0.2659.\n",
      "For Iteration 1515 the Loss is 0.2659.\n",
      "For Iteration 1516 the Loss is 0.2659.\n",
      "For Iteration 1517 the Loss is 0.2659.\n",
      "For Iteration 1518 the Loss is 0.2659.\n",
      "For Iteration 1519 the Loss is 0.2659.\n",
      "For Iteration 1520 the Loss is 0.2659.\n",
      "For Iteration 1521 the Loss is 0.2659.\n",
      "For Iteration 1522 the Loss is 0.2659.\n",
      "For Iteration 1523 the Loss is 0.2659.\n",
      "For Iteration 1524 the Loss is 0.2659.\n",
      "For Iteration 1525 the Loss is 0.2659.\n",
      "For Iteration 1526 the Loss is 0.2659.\n",
      "For Iteration 1527 the Loss is 0.2659.\n",
      "For Iteration 1528 the Loss is 0.2659.\n",
      "For Iteration 1529 the Loss is 0.2659.\n",
      "For Iteration 1530 the Loss is 0.2659.\n",
      "For Iteration 1531 the Loss is 0.2659.\n",
      "For Iteration 1532 the Loss is 0.2659.\n",
      "For Iteration 1533 the Loss is 0.2659.\n",
      "For Iteration 1534 the Loss is 0.2659.\n",
      "For Iteration 1535 the Loss is 0.2659.\n",
      "For Iteration 1536 the Loss is 0.2659.\n",
      "For Iteration 1537 the Loss is 0.2659.\n",
      "For Iteration 1538 the Loss is 0.2659.\n",
      "For Iteration 1539 the Loss is 0.2659.\n",
      "For Iteration 1540 the Loss is 0.2659.\n",
      "For Iteration 1541 the Loss is 0.2659.\n",
      "For Iteration 1542 the Loss is 0.2659.\n",
      "For Iteration 1543 the Loss is 0.2659.\n",
      "For Iteration 1544 the Loss is 0.2659.\n",
      "For Iteration 1545 the Loss is 0.2659.\n",
      "For Iteration 1546 the Loss is 0.2659.\n",
      "For Iteration 1547 the Loss is 0.2659.\n",
      "For Iteration 1548 the Loss is 0.2658.\n",
      "For Iteration 1549 the Loss is 0.2658.\n",
      "For Iteration 1550 the Loss is 0.2658.\n",
      "For Iteration 1551 the Loss is 0.2658.\n",
      "For Iteration 1552 the Loss is 0.2658.\n",
      "For Iteration 1553 the Loss is 0.2658.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1554 the Loss is 0.2658.\n",
      "For Iteration 1555 the Loss is 0.2658.\n",
      "For Iteration 1556 the Loss is 0.2658.\n",
      "For Iteration 1557 the Loss is 0.2658.\n",
      "For Iteration 1558 the Loss is 0.2658.\n",
      "For Iteration 1559 the Loss is 0.2658.\n",
      "For Iteration 1560 the Loss is 0.2658.\n",
      "For Iteration 1561 the Loss is 0.2658.\n",
      "For Iteration 1562 the Loss is 0.2658.\n",
      "For Iteration 1563 the Loss is 0.2658.\n",
      "For Iteration 1564 the Loss is 0.2658.\n",
      "For Iteration 1565 the Loss is 0.2658.\n",
      "For Iteration 1566 the Loss is 0.2658.\n",
      "For Iteration 1567 the Loss is 0.2658.\n",
      "For Iteration 1568 the Loss is 0.2658.\n",
      "For Iteration 1569 the Loss is 0.2658.\n",
      "For Iteration 1570 the Loss is 0.2658.\n",
      "For Iteration 1571 the Loss is 0.2658.\n",
      "For Iteration 1572 the Loss is 0.2658.\n",
      "For Iteration 1573 the Loss is 0.2658.\n",
      "For Iteration 1574 the Loss is 0.2658.\n",
      "For Iteration 1575 the Loss is 0.2658.\n",
      "For Iteration 1576 the Loss is 0.2658.\n",
      "For Iteration 1577 the Loss is 0.2658.\n",
      "For Iteration 1578 the Loss is 0.2658.\n",
      "For Iteration 1579 the Loss is 0.2658.\n",
      "For Iteration 1580 the Loss is 0.2658.\n",
      "For Iteration 1581 the Loss is 0.2658.\n",
      "For Iteration 1582 the Loss is 0.2658.\n",
      "For Iteration 1583 the Loss is 0.2658.\n",
      "For Iteration 1584 the Loss is 0.2658.\n",
      "For Iteration 1585 the Loss is 0.2658.\n",
      "For Iteration 1586 the Loss is 0.2658.\n",
      "For Iteration 1587 the Loss is 0.2658.\n",
      "For Iteration 1588 the Loss is 0.2658.\n",
      "For Iteration 1589 the Loss is 0.2658.\n",
      "For Iteration 1590 the Loss is 0.2657.\n",
      "For Iteration 1591 the Loss is 0.2657.\n",
      "For Iteration 1592 the Loss is 0.2657.\n",
      "For Iteration 1593 the Loss is 0.2657.\n",
      "For Iteration 1594 the Loss is 0.2657.\n",
      "For Iteration 1595 the Loss is 0.2657.\n",
      "For Iteration 1596 the Loss is 0.2657.\n",
      "For Iteration 1597 the Loss is 0.2657.\n",
      "For Iteration 1598 the Loss is 0.2657.\n",
      "For Iteration 1599 the Loss is 0.2657.\n",
      "For Iteration 1600 the Loss is 0.2657.\n",
      "For Iteration 1601 the Loss is 0.2657.\n",
      "For Iteration 1602 the Loss is 0.2657.\n",
      "For Iteration 1603 the Loss is 0.2657.\n",
      "For Iteration 1604 the Loss is 0.2657.\n",
      "For Iteration 1605 the Loss is 0.2657.\n",
      "For Iteration 1606 the Loss is 0.2657.\n",
      "For Iteration 1607 the Loss is 0.2657.\n",
      "For Iteration 1608 the Loss is 0.2657.\n",
      "For Iteration 1609 the Loss is 0.2657.\n",
      "For Iteration 1610 the Loss is 0.2657.\n",
      "For Iteration 1611 the Loss is 0.2657.\n",
      "For Iteration 1612 the Loss is 0.2657.\n",
      "For Iteration 1613 the Loss is 0.2657.\n",
      "For Iteration 1614 the Loss is 0.2657.\n",
      "For Iteration 1615 the Loss is 0.2657.\n",
      "For Iteration 1616 the Loss is 0.2657.\n",
      "For Iteration 1617 the Loss is 0.2657.\n",
      "For Iteration 1618 the Loss is 0.2657.\n",
      "For Iteration 1619 the Loss is 0.2657.\n",
      "For Iteration 1620 the Loss is 0.2657.\n",
      "For Iteration 1621 the Loss is 0.2657.\n",
      "For Iteration 1622 the Loss is 0.2657.\n",
      "For Iteration 1623 the Loss is 0.2657.\n",
      "For Iteration 1624 the Loss is 0.2657.\n",
      "For Iteration 1625 the Loss is 0.2657.\n",
      "For Iteration 1626 the Loss is 0.2657.\n",
      "For Iteration 1627 the Loss is 0.2657.\n",
      "For Iteration 1628 the Loss is 0.2657.\n",
      "For Iteration 1629 the Loss is 0.2657.\n",
      "For Iteration 1630 the Loss is 0.2657.\n",
      "For Iteration 1631 the Loss is 0.2657.\n",
      "For Iteration 1632 the Loss is 0.2657.\n",
      "For Iteration 1633 the Loss is 0.2656.\n",
      "For Iteration 1634 the Loss is 0.2656.\n",
      "For Iteration 1635 the Loss is 0.2656.\n",
      "For Iteration 1636 the Loss is 0.2656.\n",
      "For Iteration 1637 the Loss is 0.2656.\n",
      "For Iteration 1638 the Loss is 0.2656.\n",
      "For Iteration 1639 the Loss is 0.2656.\n",
      "For Iteration 1640 the Loss is 0.2656.\n",
      "For Iteration 1641 the Loss is 0.2656.\n",
      "For Iteration 1642 the Loss is 0.2656.\n",
      "For Iteration 1643 the Loss is 0.2656.\n",
      "For Iteration 1644 the Loss is 0.2656.\n",
      "For Iteration 1645 the Loss is 0.2656.\n",
      "For Iteration 1646 the Loss is 0.2656.\n",
      "For Iteration 1647 the Loss is 0.2656.\n",
      "For Iteration 1648 the Loss is 0.2656.\n",
      "For Iteration 1649 the Loss is 0.2656.\n",
      "For Iteration 1650 the Loss is 0.2656.\n",
      "For Iteration 1651 the Loss is 0.2656.\n",
      "For Iteration 1652 the Loss is 0.2656.\n",
      "For Iteration 1653 the Loss is 0.2656.\n",
      "For Iteration 1654 the Loss is 0.2656.\n",
      "For Iteration 1655 the Loss is 0.2656.\n",
      "For Iteration 1656 the Loss is 0.2656.\n",
      "For Iteration 1657 the Loss is 0.2656.\n",
      "For Iteration 1658 the Loss is 0.2656.\n",
      "For Iteration 1659 the Loss is 0.2656.\n",
      "For Iteration 1660 the Loss is 0.2656.\n",
      "For Iteration 1661 the Loss is 0.2656.\n",
      "For Iteration 1662 the Loss is 0.2656.\n",
      "For Iteration 1663 the Loss is 0.2656.\n",
      "For Iteration 1664 the Loss is 0.2656.\n",
      "For Iteration 1665 the Loss is 0.2656.\n",
      "For Iteration 1666 the Loss is 0.2656.\n",
      "For Iteration 1667 the Loss is 0.2656.\n",
      "For Iteration 1668 the Loss is 0.2656.\n",
      "For Iteration 1669 the Loss is 0.2656.\n",
      "For Iteration 1670 the Loss is 0.2656.\n",
      "For Iteration 1671 the Loss is 0.2656.\n",
      "For Iteration 1672 the Loss is 0.2656.\n",
      "For Iteration 1673 the Loss is 0.2656.\n",
      "For Iteration 1674 the Loss is 0.2656.\n",
      "For Iteration 1675 the Loss is 0.2656.\n",
      "For Iteration 1676 the Loss is 0.2656.\n",
      "For Iteration 1677 the Loss is 0.2656.\n",
      "For Iteration 1678 the Loss is 0.2655.\n",
      "For Iteration 1679 the Loss is 0.2655.\n",
      "For Iteration 1680 the Loss is 0.2655.\n",
      "For Iteration 1681 the Loss is 0.2655.\n",
      "For Iteration 1682 the Loss is 0.2655.\n",
      "For Iteration 1683 the Loss is 0.2655.\n",
      "For Iteration 1684 the Loss is 0.2655.\n",
      "For Iteration 1685 the Loss is 0.2655.\n",
      "For Iteration 1686 the Loss is 0.2655.\n",
      "For Iteration 1687 the Loss is 0.2655.\n",
      "For Iteration 1688 the Loss is 0.2655.\n",
      "For Iteration 1689 the Loss is 0.2655.\n",
      "For Iteration 1690 the Loss is 0.2655.\n",
      "For Iteration 1691 the Loss is 0.2655.\n",
      "For Iteration 1692 the Loss is 0.2655.\n",
      "For Iteration 1693 the Loss is 0.2655.\n",
      "For Iteration 1694 the Loss is 0.2655.\n",
      "For Iteration 1695 the Loss is 0.2655.\n",
      "For Iteration 1696 the Loss is 0.2655.\n",
      "For Iteration 1697 the Loss is 0.2655.\n",
      "For Iteration 1698 the Loss is 0.2655.\n",
      "For Iteration 1699 the Loss is 0.2655.\n",
      "For Iteration 1700 the Loss is 0.2655.\n",
      "For Iteration 1701 the Loss is 0.2655.\n",
      "For Iteration 1702 the Loss is 0.2655.\n",
      "For Iteration 1703 the Loss is 0.2655.\n",
      "For Iteration 1704 the Loss is 0.2655.\n",
      "For Iteration 1705 the Loss is 0.2655.\n",
      "For Iteration 1706 the Loss is 0.2655.\n",
      "For Iteration 1707 the Loss is 0.2655.\n",
      "For Iteration 1708 the Loss is 0.2655.\n",
      "For Iteration 1709 the Loss is 0.2655.\n",
      "For Iteration 1710 the Loss is 0.2655.\n",
      "For Iteration 1711 the Loss is 0.2655.\n",
      "For Iteration 1712 the Loss is 0.2655.\n",
      "For Iteration 1713 the Loss is 0.2655.\n",
      "For Iteration 1714 the Loss is 0.2655.\n",
      "For Iteration 1715 the Loss is 0.2655.\n",
      "For Iteration 1716 the Loss is 0.2655.\n",
      "For Iteration 1717 the Loss is 0.2655.\n",
      "For Iteration 1718 the Loss is 0.2655.\n",
      "For Iteration 1719 the Loss is 0.2655.\n",
      "For Iteration 1720 the Loss is 0.2655.\n",
      "For Iteration 1721 the Loss is 0.2655.\n",
      "For Iteration 1722 the Loss is 0.2655.\n",
      "For Iteration 1723 the Loss is 0.2654.\n",
      "For Iteration 1724 the Loss is 0.2654.\n",
      "For Iteration 1725 the Loss is 0.2654.\n",
      "For Iteration 1726 the Loss is 0.2654.\n",
      "For Iteration 1727 the Loss is 0.2654.\n",
      "For Iteration 1728 the Loss is 0.2654.\n",
      "For Iteration 1729 the Loss is 0.2654.\n",
      "For Iteration 1730 the Loss is 0.2654.\n",
      "For Iteration 1731 the Loss is 0.2654.\n",
      "For Iteration 1732 the Loss is 0.2654.\n",
      "For Iteration 1733 the Loss is 0.2654.\n",
      "For Iteration 1734 the Loss is 0.2654.\n",
      "For Iteration 1735 the Loss is 0.2654.\n",
      "For Iteration 1736 the Loss is 0.2654.\n",
      "For Iteration 1737 the Loss is 0.2654.\n",
      "For Iteration 1738 the Loss is 0.2654.\n",
      "For Iteration 1739 the Loss is 0.2654.\n",
      "For Iteration 1740 the Loss is 0.2654.\n",
      "For Iteration 1741 the Loss is 0.2654.\n",
      "For Iteration 1742 the Loss is 0.2654.\n",
      "For Iteration 1743 the Loss is 0.2654.\n",
      "For Iteration 1744 the Loss is 0.2654.\n",
      "For Iteration 1745 the Loss is 0.2654.\n",
      "For Iteration 1746 the Loss is 0.2654.\n",
      "For Iteration 1747 the Loss is 0.2654.\n",
      "For Iteration 1748 the Loss is 0.2654.\n",
      "For Iteration 1749 the Loss is 0.2654.\n",
      "For Iteration 1750 the Loss is 0.2654.\n",
      "For Iteration 1751 the Loss is 0.2654.\n",
      "For Iteration 1752 the Loss is 0.2654.\n",
      "For Iteration 1753 the Loss is 0.2654.\n",
      "For Iteration 1754 the Loss is 0.2654.\n",
      "For Iteration 1755 the Loss is 0.2654.\n",
      "For Iteration 1756 the Loss is 0.2654.\n",
      "For Iteration 1757 the Loss is 0.2654.\n",
      "For Iteration 1758 the Loss is 0.2654.\n",
      "For Iteration 1759 the Loss is 0.2654.\n",
      "For Iteration 1760 the Loss is 0.2654.\n",
      "For Iteration 1761 the Loss is 0.2654.\n",
      "For Iteration 1762 the Loss is 0.2654.\n",
      "For Iteration 1763 the Loss is 0.2654.\n",
      "For Iteration 1764 the Loss is 0.2654.\n",
      "For Iteration 1765 the Loss is 0.2654.\n",
      "For Iteration 1766 the Loss is 0.2654.\n",
      "For Iteration 1767 the Loss is 0.2654.\n",
      "For Iteration 1768 the Loss is 0.2654.\n",
      "For Iteration 1769 the Loss is 0.2654.\n",
      "For Iteration 1770 the Loss is 0.2653.\n",
      "For Iteration 1771 the Loss is 0.2653.\n",
      "For Iteration 1772 the Loss is 0.2653.\n",
      "For Iteration 1773 the Loss is 0.2653.\n",
      "For Iteration 1774 the Loss is 0.2653.\n",
      "For Iteration 1775 the Loss is 0.2653.\n",
      "For Iteration 1776 the Loss is 0.2653.\n",
      "For Iteration 1777 the Loss is 0.2653.\n",
      "For Iteration 1778 the Loss is 0.2653.\n",
      "For Iteration 1779 the Loss is 0.2653.\n",
      "For Iteration 1780 the Loss is 0.2653.\n",
      "For Iteration 1781 the Loss is 0.2653.\n",
      "For Iteration 1782 the Loss is 0.2653.\n",
      "For Iteration 1783 the Loss is 0.2653.\n",
      "For Iteration 1784 the Loss is 0.2653.\n",
      "For Iteration 1785 the Loss is 0.2653.\n",
      "For Iteration 1786 the Loss is 0.2653.\n",
      "For Iteration 1787 the Loss is 0.2653.\n",
      "For Iteration 1788 the Loss is 0.2653.\n",
      "For Iteration 1789 the Loss is 0.2653.\n",
      "For Iteration 1790 the Loss is 0.2653.\n",
      "For Iteration 1791 the Loss is 0.2653.\n",
      "For Iteration 1792 the Loss is 0.2653.\n",
      "For Iteration 1793 the Loss is 0.2653.\n",
      "For Iteration 1794 the Loss is 0.2653.\n",
      "For Iteration 1795 the Loss is 0.2653.\n",
      "For Iteration 1796 the Loss is 0.2653.\n",
      "For Iteration 1797 the Loss is 0.2653.\n",
      "For Iteration 1798 the Loss is 0.2653.\n",
      "For Iteration 1799 the Loss is 0.2653.\n",
      "For Iteration 1800 the Loss is 0.2653.\n",
      "For Iteration 1801 the Loss is 0.2653.\n",
      "For Iteration 1802 the Loss is 0.2653.\n",
      "For Iteration 1803 the Loss is 0.2653.\n",
      "For Iteration 1804 the Loss is 0.2653.\n",
      "For Iteration 1805 the Loss is 0.2653.\n",
      "For Iteration 1806 the Loss is 0.2653.\n",
      "For Iteration 1807 the Loss is 0.2653.\n",
      "For Iteration 1808 the Loss is 0.2653.\n",
      "For Iteration 1809 the Loss is 0.2653.\n",
      "For Iteration 1810 the Loss is 0.2653.\n",
      "For Iteration 1811 the Loss is 0.2653.\n",
      "For Iteration 1812 the Loss is 0.2653.\n",
      "For Iteration 1813 the Loss is 0.2653.\n",
      "For Iteration 1814 the Loss is 0.2653.\n",
      "For Iteration 1815 the Loss is 0.2653.\n",
      "For Iteration 1816 the Loss is 0.2653.\n",
      "For Iteration 1817 the Loss is 0.2653.\n",
      "For Iteration 1818 the Loss is 0.2653.\n",
      "For Iteration 1819 the Loss is 0.2652.\n",
      "For Iteration 1820 the Loss is 0.2652.\n",
      "For Iteration 1821 the Loss is 0.2652.\n",
      "For Iteration 1822 the Loss is 0.2652.\n",
      "For Iteration 1823 the Loss is 0.2652.\n",
      "For Iteration 1824 the Loss is 0.2652.\n",
      "For Iteration 1825 the Loss is 0.2652.\n",
      "For Iteration 1826 the Loss is 0.2652.\n",
      "For Iteration 1827 the Loss is 0.2652.\n",
      "For Iteration 1828 the Loss is 0.2652.\n",
      "For Iteration 1829 the Loss is 0.2652.\n",
      "For Iteration 1830 the Loss is 0.2652.\n",
      "For Iteration 1831 the Loss is 0.2652.\n",
      "For Iteration 1832 the Loss is 0.2652.\n",
      "For Iteration 1833 the Loss is 0.2652.\n",
      "For Iteration 1834 the Loss is 0.2652.\n",
      "For Iteration 1835 the Loss is 0.2652.\n",
      "For Iteration 1836 the Loss is 0.2652.\n",
      "For Iteration 1837 the Loss is 0.2652.\n",
      "For Iteration 1838 the Loss is 0.2652.\n",
      "For Iteration 1839 the Loss is 0.2652.\n",
      "For Iteration 1840 the Loss is 0.2652.\n",
      "For Iteration 1841 the Loss is 0.2652.\n",
      "For Iteration 1842 the Loss is 0.2652.\n",
      "For Iteration 1843 the Loss is 0.2652.\n",
      "For Iteration 1844 the Loss is 0.2652.\n",
      "For Iteration 1845 the Loss is 0.2652.\n",
      "For Iteration 1846 the Loss is 0.2652.\n",
      "For Iteration 1847 the Loss is 0.2652.\n",
      "For Iteration 1848 the Loss is 0.2652.\n",
      "For Iteration 1849 the Loss is 0.2652.\n",
      "For Iteration 1850 the Loss is 0.2652.\n",
      "For Iteration 1851 the Loss is 0.2652.\n",
      "For Iteration 1852 the Loss is 0.2652.\n",
      "For Iteration 1853 the Loss is 0.2652.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 1854 the Loss is 0.2652.\n",
      "For Iteration 1855 the Loss is 0.2652.\n",
      "For Iteration 1856 the Loss is 0.2652.\n",
      "For Iteration 1857 the Loss is 0.2652.\n",
      "For Iteration 1858 the Loss is 0.2652.\n",
      "For Iteration 1859 the Loss is 0.2652.\n",
      "For Iteration 1860 the Loss is 0.2652.\n",
      "For Iteration 1861 the Loss is 0.2652.\n",
      "For Iteration 1862 the Loss is 0.2652.\n",
      "For Iteration 1863 the Loss is 0.2652.\n",
      "For Iteration 1864 the Loss is 0.2652.\n",
      "For Iteration 1865 the Loss is 0.2652.\n",
      "For Iteration 1866 the Loss is 0.2652.\n",
      "For Iteration 1867 the Loss is 0.2652.\n",
      "For Iteration 1868 the Loss is 0.2651.\n",
      "For Iteration 1869 the Loss is 0.2651.\n",
      "For Iteration 1870 the Loss is 0.2651.\n",
      "For Iteration 1871 the Loss is 0.2651.\n",
      "For Iteration 1872 the Loss is 0.2651.\n",
      "For Iteration 1873 the Loss is 0.2651.\n",
      "For Iteration 1874 the Loss is 0.2651.\n",
      "For Iteration 1875 the Loss is 0.2651.\n",
      "For Iteration 1876 the Loss is 0.2651.\n",
      "For Iteration 1877 the Loss is 0.2651.\n",
      "For Iteration 1878 the Loss is 0.2651.\n",
      "For Iteration 1879 the Loss is 0.2651.\n",
      "For Iteration 1880 the Loss is 0.2651.\n",
      "For Iteration 1881 the Loss is 0.2651.\n",
      "For Iteration 1882 the Loss is 0.2651.\n",
      "For Iteration 1883 the Loss is 0.2651.\n",
      "For Iteration 1884 the Loss is 0.2651.\n",
      "For Iteration 1885 the Loss is 0.2651.\n",
      "For Iteration 1886 the Loss is 0.2651.\n",
      "For Iteration 1887 the Loss is 0.2651.\n",
      "For Iteration 1888 the Loss is 0.2651.\n",
      "For Iteration 1889 the Loss is 0.2651.\n",
      "For Iteration 1890 the Loss is 0.2651.\n",
      "For Iteration 1891 the Loss is 0.2651.\n",
      "For Iteration 1892 the Loss is 0.2651.\n",
      "For Iteration 1893 the Loss is 0.2651.\n",
      "For Iteration 1894 the Loss is 0.2651.\n",
      "For Iteration 1895 the Loss is 0.2651.\n",
      "For Iteration 1896 the Loss is 0.2651.\n",
      "For Iteration 1897 the Loss is 0.2651.\n",
      "For Iteration 1898 the Loss is 0.2651.\n",
      "For Iteration 1899 the Loss is 0.2651.\n",
      "For Iteration 1900 the Loss is 0.2651.\n",
      "For Iteration 1901 the Loss is 0.2651.\n",
      "For Iteration 1902 the Loss is 0.2651.\n",
      "For Iteration 1903 the Loss is 0.2651.\n",
      "For Iteration 1904 the Loss is 0.2651.\n",
      "For Iteration 1905 the Loss is 0.2651.\n",
      "For Iteration 1906 the Loss is 0.2651.\n",
      "For Iteration 1907 the Loss is 0.2651.\n",
      "For Iteration 1908 the Loss is 0.2651.\n",
      "For Iteration 1909 the Loss is 0.2651.\n",
      "For Iteration 1910 the Loss is 0.2651.\n",
      "For Iteration 1911 the Loss is 0.2651.\n",
      "For Iteration 1912 the Loss is 0.2651.\n",
      "For Iteration 1913 the Loss is 0.2651.\n",
      "For Iteration 1914 the Loss is 0.2651.\n",
      "For Iteration 1915 the Loss is 0.2651.\n",
      "For Iteration 1916 the Loss is 0.2651.\n",
      "For Iteration 1917 the Loss is 0.2651.\n",
      "For Iteration 1918 the Loss is 0.2651.\n",
      "For Iteration 1919 the Loss is 0.265.\n",
      "For Iteration 1920 the Loss is 0.265.\n",
      "For Iteration 1921 the Loss is 0.265.\n",
      "For Iteration 1922 the Loss is 0.265.\n",
      "For Iteration 1923 the Loss is 0.265.\n",
      "For Iteration 1924 the Loss is 0.265.\n",
      "For Iteration 1925 the Loss is 0.265.\n",
      "For Iteration 1926 the Loss is 0.265.\n",
      "For Iteration 1927 the Loss is 0.265.\n",
      "For Iteration 1928 the Loss is 0.265.\n",
      "For Iteration 1929 the Loss is 0.265.\n",
      "For Iteration 1930 the Loss is 0.265.\n",
      "For Iteration 1931 the Loss is 0.265.\n",
      "For Iteration 1932 the Loss is 0.265.\n",
      "For Iteration 1933 the Loss is 0.265.\n",
      "For Iteration 1934 the Loss is 0.265.\n",
      "For Iteration 1935 the Loss is 0.265.\n",
      "For Iteration 1936 the Loss is 0.265.\n",
      "For Iteration 1937 the Loss is 0.265.\n",
      "For Iteration 1938 the Loss is 0.265.\n",
      "For Iteration 1939 the Loss is 0.265.\n",
      "For Iteration 1940 the Loss is 0.265.\n",
      "For Iteration 1941 the Loss is 0.265.\n",
      "For Iteration 1942 the Loss is 0.265.\n",
      "For Iteration 1943 the Loss is 0.265.\n",
      "For Iteration 1944 the Loss is 0.265.\n",
      "For Iteration 1945 the Loss is 0.265.\n",
      "For Iteration 1946 the Loss is 0.265.\n",
      "For Iteration 1947 the Loss is 0.265.\n",
      "For Iteration 1948 the Loss is 0.265.\n",
      "For Iteration 1949 the Loss is 0.265.\n",
      "For Iteration 1950 the Loss is 0.265.\n",
      "For Iteration 1951 the Loss is 0.265.\n",
      "For Iteration 1952 the Loss is 0.265.\n",
      "For Iteration 1953 the Loss is 0.265.\n",
      "For Iteration 1954 the Loss is 0.265.\n",
      "For Iteration 1955 the Loss is 0.265.\n",
      "For Iteration 1956 the Loss is 0.265.\n",
      "For Iteration 1957 the Loss is 0.265.\n",
      "For Iteration 1958 the Loss is 0.265.\n",
      "For Iteration 1959 the Loss is 0.265.\n",
      "For Iteration 1960 the Loss is 0.265.\n",
      "For Iteration 1961 the Loss is 0.265.\n",
      "For Iteration 1962 the Loss is 0.265.\n",
      "For Iteration 1963 the Loss is 0.265.\n",
      "For Iteration 1964 the Loss is 0.265.\n",
      "For Iteration 1965 the Loss is 0.265.\n",
      "For Iteration 1966 the Loss is 0.265.\n",
      "For Iteration 1967 the Loss is 0.265.\n",
      "For Iteration 1968 the Loss is 0.265.\n",
      "For Iteration 1969 the Loss is 0.265.\n",
      "For Iteration 1970 the Loss is 0.265.\n",
      "For Iteration 1971 the Loss is 0.265.\n",
      "For Iteration 1972 the Loss is 0.2649.\n",
      "For Iteration 1973 the Loss is 0.2649.\n",
      "For Iteration 1974 the Loss is 0.2649.\n",
      "For Iteration 1975 the Loss is 0.2649.\n",
      "For Iteration 1976 the Loss is 0.2649.\n",
      "For Iteration 1977 the Loss is 0.2649.\n",
      "For Iteration 1978 the Loss is 0.2649.\n",
      "For Iteration 1979 the Loss is 0.2649.\n",
      "For Iteration 1980 the Loss is 0.2649.\n",
      "For Iteration 1981 the Loss is 0.2649.\n",
      "For Iteration 1982 the Loss is 0.2649.\n",
      "For Iteration 1983 the Loss is 0.2649.\n",
      "For Iteration 1984 the Loss is 0.2649.\n",
      "For Iteration 1985 the Loss is 0.2649.\n",
      "For Iteration 1986 the Loss is 0.2649.\n",
      "For Iteration 1987 the Loss is 0.2649.\n",
      "For Iteration 1988 the Loss is 0.2649.\n",
      "For Iteration 1989 the Loss is 0.2649.\n",
      "For Iteration 1990 the Loss is 0.2649.\n",
      "For Iteration 1991 the Loss is 0.2649.\n",
      "For Iteration 1992 the Loss is 0.2649.\n",
      "For Iteration 1993 the Loss is 0.2649.\n",
      "For Iteration 1994 the Loss is 0.2649.\n",
      "For Iteration 1995 the Loss is 0.2649.\n",
      "For Iteration 1996 the Loss is 0.2649.\n",
      "For Iteration 1997 the Loss is 0.2649.\n",
      "For Iteration 1998 the Loss is 0.2649.\n",
      "For Iteration 1999 the Loss is 0.2649.\n",
      "For Iteration 2000 the Loss is 0.2649.\n",
      "For Iteration 2001 the Loss is 0.2649.\n",
      "For Iteration 2002 the Loss is 0.2649.\n",
      "For Iteration 2003 the Loss is 0.2649.\n",
      "For Iteration 2004 the Loss is 0.2649.\n",
      "For Iteration 2005 the Loss is 0.2649.\n",
      "For Iteration 2006 the Loss is 0.2649.\n",
      "For Iteration 2007 the Loss is 0.2649.\n",
      "For Iteration 2008 the Loss is 0.2649.\n",
      "For Iteration 2009 the Loss is 0.2649.\n",
      "For Iteration 2010 the Loss is 0.2649.\n",
      "For Iteration 2011 the Loss is 0.2649.\n",
      "For Iteration 2012 the Loss is 0.2649.\n",
      "For Iteration 2013 the Loss is 0.2649.\n",
      "For Iteration 2014 the Loss is 0.2649.\n",
      "For Iteration 2015 the Loss is 0.2649.\n",
      "For Iteration 2016 the Loss is 0.2649.\n",
      "For Iteration 2017 the Loss is 0.2649.\n",
      "For Iteration 2018 the Loss is 0.2649.\n",
      "For Iteration 2019 the Loss is 0.2649.\n",
      "For Iteration 2020 the Loss is 0.2649.\n",
      "For Iteration 2021 the Loss is 0.2649.\n",
      "For Iteration 2022 the Loss is 0.2649.\n",
      "For Iteration 2023 the Loss is 0.2649.\n",
      "For Iteration 2024 the Loss is 0.2649.\n",
      "For Iteration 2025 the Loss is 0.2649.\n",
      "For Iteration 2026 the Loss is 0.2648.\n",
      "For Iteration 2027 the Loss is 0.2648.\n",
      "For Iteration 2028 the Loss is 0.2648.\n",
      "For Iteration 2029 the Loss is 0.2648.\n",
      "For Iteration 2030 the Loss is 0.2648.\n",
      "For Iteration 2031 the Loss is 0.2648.\n",
      "For Iteration 2032 the Loss is 0.2648.\n",
      "For Iteration 2033 the Loss is 0.2648.\n",
      "For Iteration 2034 the Loss is 0.2648.\n",
      "For Iteration 2035 the Loss is 0.2648.\n",
      "For Iteration 2036 the Loss is 0.2648.\n",
      "For Iteration 2037 the Loss is 0.2648.\n",
      "For Iteration 2038 the Loss is 0.2648.\n",
      "For Iteration 2039 the Loss is 0.2648.\n",
      "For Iteration 2040 the Loss is 0.2648.\n",
      "For Iteration 2041 the Loss is 0.2648.\n",
      "For Iteration 2042 the Loss is 0.2648.\n",
      "For Iteration 2043 the Loss is 0.2648.\n",
      "For Iteration 2044 the Loss is 0.2648.\n",
      "For Iteration 2045 the Loss is 0.2648.\n",
      "For Iteration 2046 the Loss is 0.2648.\n",
      "For Iteration 2047 the Loss is 0.2648.\n",
      "For Iteration 2048 the Loss is 0.2648.\n",
      "For Iteration 2049 the Loss is 0.2648.\n",
      "For Iteration 2050 the Loss is 0.2648.\n",
      "For Iteration 2051 the Loss is 0.2648.\n",
      "For Iteration 2052 the Loss is 0.2648.\n",
      "For Iteration 2053 the Loss is 0.2648.\n",
      "For Iteration 2054 the Loss is 0.2648.\n",
      "For Iteration 2055 the Loss is 0.2648.\n",
      "For Iteration 2056 the Loss is 0.2648.\n",
      "For Iteration 2057 the Loss is 0.2648.\n",
      "For Iteration 2058 the Loss is 0.2648.\n",
      "For Iteration 2059 the Loss is 0.2648.\n",
      "For Iteration 2060 the Loss is 0.2648.\n",
      "For Iteration 2061 the Loss is 0.2648.\n",
      "For Iteration 2062 the Loss is 0.2648.\n",
      "For Iteration 2063 the Loss is 0.2648.\n",
      "For Iteration 2064 the Loss is 0.2648.\n",
      "For Iteration 2065 the Loss is 0.2648.\n",
      "For Iteration 2066 the Loss is 0.2648.\n",
      "For Iteration 2067 the Loss is 0.2648.\n",
      "For Iteration 2068 the Loss is 0.2648.\n",
      "For Iteration 2069 the Loss is 0.2648.\n",
      "For Iteration 2070 the Loss is 0.2648.\n",
      "For Iteration 2071 the Loss is 0.2648.\n",
      "For Iteration 2072 the Loss is 0.2648.\n",
      "For Iteration 2073 the Loss is 0.2648.\n",
      "For Iteration 2074 the Loss is 0.2648.\n",
      "For Iteration 2075 the Loss is 0.2648.\n",
      "For Iteration 2076 the Loss is 0.2648.\n",
      "For Iteration 2077 the Loss is 0.2648.\n",
      "For Iteration 2078 the Loss is 0.2648.\n",
      "For Iteration 2079 the Loss is 0.2648.\n",
      "For Iteration 2080 the Loss is 0.2648.\n",
      "For Iteration 2081 the Loss is 0.2647.\n",
      "For Iteration 2082 the Loss is 0.2647.\n",
      "For Iteration 2083 the Loss is 0.2647.\n",
      "For Iteration 2084 the Loss is 0.2647.\n",
      "For Iteration 2085 the Loss is 0.2647.\n",
      "For Iteration 2086 the Loss is 0.2647.\n",
      "For Iteration 2087 the Loss is 0.2647.\n",
      "For Iteration 2088 the Loss is 0.2647.\n",
      "For Iteration 2089 the Loss is 0.2647.\n",
      "For Iteration 2090 the Loss is 0.2647.\n",
      "For Iteration 2091 the Loss is 0.2647.\n",
      "For Iteration 2092 the Loss is 0.2647.\n",
      "For Iteration 2093 the Loss is 0.2647.\n",
      "For Iteration 2094 the Loss is 0.2647.\n",
      "For Iteration 2095 the Loss is 0.2647.\n",
      "For Iteration 2096 the Loss is 0.2647.\n",
      "For Iteration 2097 the Loss is 0.2647.\n",
      "For Iteration 2098 the Loss is 0.2647.\n",
      "For Iteration 2099 the Loss is 0.2647.\n",
      "For Iteration 2100 the Loss is 0.2647.\n",
      "For Iteration 2101 the Loss is 0.2647.\n",
      "For Iteration 2102 the Loss is 0.2647.\n",
      "For Iteration 2103 the Loss is 0.2647.\n",
      "For Iteration 2104 the Loss is 0.2647.\n",
      "For Iteration 2105 the Loss is 0.2647.\n",
      "For Iteration 2106 the Loss is 0.2647.\n",
      "For Iteration 2107 the Loss is 0.2647.\n",
      "For Iteration 2108 the Loss is 0.2647.\n",
      "For Iteration 2109 the Loss is 0.2647.\n",
      "For Iteration 2110 the Loss is 0.2647.\n",
      "For Iteration 2111 the Loss is 0.2647.\n",
      "For Iteration 2112 the Loss is 0.2647.\n",
      "For Iteration 2113 the Loss is 0.2647.\n",
      "For Iteration 2114 the Loss is 0.2647.\n",
      "For Iteration 2115 the Loss is 0.2647.\n",
      "For Iteration 2116 the Loss is 0.2647.\n",
      "For Iteration 2117 the Loss is 0.2647.\n",
      "For Iteration 2118 the Loss is 0.2647.\n",
      "For Iteration 2119 the Loss is 0.2647.\n",
      "For Iteration 2120 the Loss is 0.2647.\n",
      "For Iteration 2121 the Loss is 0.2647.\n",
      "For Iteration 2122 the Loss is 0.2647.\n",
      "For Iteration 2123 the Loss is 0.2647.\n",
      "For Iteration 2124 the Loss is 0.2647.\n",
      "For Iteration 2125 the Loss is 0.2647.\n",
      "For Iteration 2126 the Loss is 0.2647.\n",
      "For Iteration 2127 the Loss is 0.2647.\n",
      "For Iteration 2128 the Loss is 0.2647.\n",
      "For Iteration 2129 the Loss is 0.2647.\n",
      "For Iteration 2130 the Loss is 0.2647.\n",
      "For Iteration 2131 the Loss is 0.2647.\n",
      "For Iteration 2132 the Loss is 0.2647.\n",
      "For Iteration 2133 the Loss is 0.2647.\n",
      "For Iteration 2134 the Loss is 0.2647.\n",
      "For Iteration 2135 the Loss is 0.2647.\n",
      "For Iteration 2136 the Loss is 0.2647.\n",
      "For Iteration 2137 the Loss is 0.2647.\n",
      "For Iteration 2138 the Loss is 0.2646.\n",
      "For Iteration 2139 the Loss is 0.2646.\n",
      "For Iteration 2140 the Loss is 0.2646.\n",
      "For Iteration 2141 the Loss is 0.2646.\n",
      "For Iteration 2142 the Loss is 0.2646.\n",
      "For Iteration 2143 the Loss is 0.2646.\n",
      "For Iteration 2144 the Loss is 0.2646.\n",
      "For Iteration 2145 the Loss is 0.2646.\n",
      "For Iteration 2146 the Loss is 0.2646.\n",
      "For Iteration 2147 the Loss is 0.2646.\n",
      "For Iteration 2148 the Loss is 0.2646.\n",
      "For Iteration 2149 the Loss is 0.2646.\n",
      "For Iteration 2150 the Loss is 0.2646.\n",
      "For Iteration 2151 the Loss is 0.2646.\n",
      "For Iteration 2152 the Loss is 0.2646.\n",
      "For Iteration 2153 the Loss is 0.2646.\n",
      "For Iteration 2154 the Loss is 0.2646.\n",
      "For Iteration 2155 the Loss is 0.2646.\n",
      "For Iteration 2156 the Loss is 0.2646.\n",
      "For Iteration 2157 the Loss is 0.2646.\n",
      "For Iteration 2158 the Loss is 0.2646.\n",
      "For Iteration 2159 the Loss is 0.2646.\n",
      "For Iteration 2160 the Loss is 0.2646.\n",
      "For Iteration 2161 the Loss is 0.2646.\n",
      "For Iteration 2162 the Loss is 0.2646.\n",
      "For Iteration 2163 the Loss is 0.2646.\n",
      "For Iteration 2164 the Loss is 0.2646.\n",
      "For Iteration 2165 the Loss is 0.2646.\n",
      "For Iteration 2166 the Loss is 0.2646.\n",
      "For Iteration 2167 the Loss is 0.2646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 2168 the Loss is 0.2646.\n",
      "For Iteration 2169 the Loss is 0.2646.\n",
      "For Iteration 2170 the Loss is 0.2646.\n",
      "For Iteration 2171 the Loss is 0.2646.\n",
      "For Iteration 2172 the Loss is 0.2646.\n",
      "For Iteration 2173 the Loss is 0.2646.\n",
      "For Iteration 2174 the Loss is 0.2646.\n",
      "For Iteration 2175 the Loss is 0.2646.\n",
      "For Iteration 2176 the Loss is 0.2646.\n",
      "For Iteration 2177 the Loss is 0.2646.\n",
      "For Iteration 2178 the Loss is 0.2646.\n",
      "For Iteration 2179 the Loss is 0.2646.\n",
      "For Iteration 2180 the Loss is 0.2646.\n",
      "For Iteration 2181 the Loss is 0.2646.\n",
      "For Iteration 2182 the Loss is 0.2646.\n",
      "For Iteration 2183 the Loss is 0.2646.\n",
      "For Iteration 2184 the Loss is 0.2646.\n",
      "For Iteration 2185 the Loss is 0.2646.\n",
      "For Iteration 2186 the Loss is 0.2646.\n",
      "For Iteration 2187 the Loss is 0.2646.\n",
      "For Iteration 2188 the Loss is 0.2646.\n",
      "For Iteration 2189 the Loss is 0.2646.\n",
      "For Iteration 2190 the Loss is 0.2646.\n",
      "For Iteration 2191 the Loss is 0.2646.\n",
      "For Iteration 2192 the Loss is 0.2646.\n",
      "For Iteration 2193 the Loss is 0.2646.\n",
      "For Iteration 2194 the Loss is 0.2646.\n",
      "For Iteration 2195 the Loss is 0.2646.\n",
      "For Iteration 2196 the Loss is 0.2646.\n",
      "For Iteration 2197 the Loss is 0.2645.\n",
      "For Iteration 2198 the Loss is 0.2645.\n",
      "For Iteration 2199 the Loss is 0.2645.\n",
      "For Iteration 2200 the Loss is 0.2645.\n",
      "For Iteration 2201 the Loss is 0.2645.\n",
      "For Iteration 2202 the Loss is 0.2645.\n",
      "For Iteration 2203 the Loss is 0.2645.\n",
      "For Iteration 2204 the Loss is 0.2645.\n",
      "For Iteration 2205 the Loss is 0.2645.\n",
      "For Iteration 2206 the Loss is 0.2645.\n",
      "For Iteration 2207 the Loss is 0.2645.\n",
      "For Iteration 2208 the Loss is 0.2645.\n",
      "For Iteration 2209 the Loss is 0.2645.\n",
      "For Iteration 2210 the Loss is 0.2645.\n",
      "For Iteration 2211 the Loss is 0.2645.\n",
      "For Iteration 2212 the Loss is 0.2645.\n",
      "For Iteration 2213 the Loss is 0.2645.\n",
      "For Iteration 2214 the Loss is 0.2645.\n",
      "For Iteration 2215 the Loss is 0.2645.\n",
      "For Iteration 2216 the Loss is 0.2645.\n",
      "For Iteration 2217 the Loss is 0.2645.\n",
      "For Iteration 2218 the Loss is 0.2645.\n",
      "For Iteration 2219 the Loss is 0.2645.\n",
      "For Iteration 2220 the Loss is 0.2645.\n",
      "For Iteration 2221 the Loss is 0.2645.\n",
      "For Iteration 2222 the Loss is 0.2645.\n",
      "For Iteration 2223 the Loss is 0.2645.\n",
      "For Iteration 2224 the Loss is 0.2645.\n",
      "For Iteration 2225 the Loss is 0.2645.\n",
      "For Iteration 2226 the Loss is 0.2645.\n",
      "For Iteration 2227 the Loss is 0.2645.\n",
      "For Iteration 2228 the Loss is 0.2645.\n",
      "For Iteration 2229 the Loss is 0.2645.\n",
      "For Iteration 2230 the Loss is 0.2645.\n",
      "For Iteration 2231 the Loss is 0.2645.\n",
      "For Iteration 2232 the Loss is 0.2645.\n",
      "For Iteration 2233 the Loss is 0.2645.\n",
      "For Iteration 2234 the Loss is 0.2645.\n",
      "For Iteration 2235 the Loss is 0.2645.\n",
      "For Iteration 2236 the Loss is 0.2645.\n",
      "For Iteration 2237 the Loss is 0.2645.\n",
      "For Iteration 2238 the Loss is 0.2645.\n",
      "For Iteration 2239 the Loss is 0.2645.\n",
      "For Iteration 2240 the Loss is 0.2645.\n",
      "For Iteration 2241 the Loss is 0.2645.\n",
      "For Iteration 2242 the Loss is 0.2645.\n",
      "For Iteration 2243 the Loss is 0.2645.\n",
      "For Iteration 2244 the Loss is 0.2645.\n",
      "For Iteration 2245 the Loss is 0.2645.\n",
      "For Iteration 2246 the Loss is 0.2645.\n",
      "For Iteration 2247 the Loss is 0.2645.\n",
      "For Iteration 2248 the Loss is 0.2645.\n",
      "For Iteration 2249 the Loss is 0.2645.\n",
      "For Iteration 2250 the Loss is 0.2645.\n",
      "For Iteration 2251 the Loss is 0.2645.\n",
      "For Iteration 2252 the Loss is 0.2645.\n",
      "For Iteration 2253 the Loss is 0.2645.\n",
      "For Iteration 2254 the Loss is 0.2645.\n",
      "For Iteration 2255 the Loss is 0.2645.\n",
      "For Iteration 2256 the Loss is 0.2645.\n",
      "For Iteration 2257 the Loss is 0.2645.\n",
      "For Iteration 2258 the Loss is 0.2644.\n",
      "For Iteration 2259 the Loss is 0.2644.\n",
      "For Iteration 2260 the Loss is 0.2644.\n",
      "For Iteration 2261 the Loss is 0.2644.\n",
      "For Iteration 2262 the Loss is 0.2644.\n",
      "For Iteration 2263 the Loss is 0.2644.\n",
      "For Iteration 2264 the Loss is 0.2644.\n",
      "For Iteration 2265 the Loss is 0.2644.\n",
      "For Iteration 2266 the Loss is 0.2644.\n",
      "For Iteration 2267 the Loss is 0.2644.\n",
      "For Iteration 2268 the Loss is 0.2644.\n",
      "For Iteration 2269 the Loss is 0.2644.\n",
      "For Iteration 2270 the Loss is 0.2644.\n",
      "For Iteration 2271 the Loss is 0.2644.\n",
      "For Iteration 2272 the Loss is 0.2644.\n",
      "For Iteration 2273 the Loss is 0.2644.\n",
      "For Iteration 2274 the Loss is 0.2644.\n",
      "For Iteration 2275 the Loss is 0.2644.\n",
      "For Iteration 2276 the Loss is 0.2644.\n",
      "For Iteration 2277 the Loss is 0.2644.\n",
      "For Iteration 2278 the Loss is 0.2644.\n",
      "For Iteration 2279 the Loss is 0.2644.\n",
      "For Iteration 2280 the Loss is 0.2644.\n",
      "For Iteration 2281 the Loss is 0.2644.\n",
      "For Iteration 2282 the Loss is 0.2644.\n",
      "For Iteration 2283 the Loss is 0.2644.\n",
      "For Iteration 2284 the Loss is 0.2644.\n",
      "For Iteration 2285 the Loss is 0.2644.\n",
      "For Iteration 2286 the Loss is 0.2644.\n",
      "For Iteration 2287 the Loss is 0.2644.\n",
      "For Iteration 2288 the Loss is 0.2644.\n",
      "For Iteration 2289 the Loss is 0.2644.\n",
      "For Iteration 2290 the Loss is 0.2644.\n",
      "For Iteration 2291 the Loss is 0.2644.\n",
      "For Iteration 2292 the Loss is 0.2644.\n",
      "For Iteration 2293 the Loss is 0.2644.\n",
      "For Iteration 2294 the Loss is 0.2644.\n",
      "For Iteration 2295 the Loss is 0.2644.\n",
      "For Iteration 2296 the Loss is 0.2644.\n",
      "For Iteration 2297 the Loss is 0.2644.\n",
      "For Iteration 2298 the Loss is 0.2644.\n",
      "For Iteration 2299 the Loss is 0.2644.\n",
      "For Iteration 2300 the Loss is 0.2644.\n",
      "For Iteration 2301 the Loss is 0.2644.\n",
      "For Iteration 2302 the Loss is 0.2644.\n",
      "For Iteration 2303 the Loss is 0.2644.\n",
      "For Iteration 2304 the Loss is 0.2644.\n",
      "For Iteration 2305 the Loss is 0.2644.\n",
      "For Iteration 2306 the Loss is 0.2644.\n",
      "For Iteration 2307 the Loss is 0.2644.\n",
      "For Iteration 2308 the Loss is 0.2644.\n",
      "For Iteration 2309 the Loss is 0.2644.\n",
      "For Iteration 2310 the Loss is 0.2644.\n",
      "For Iteration 2311 the Loss is 0.2644.\n",
      "For Iteration 2312 the Loss is 0.2644.\n",
      "For Iteration 2313 the Loss is 0.2644.\n",
      "For Iteration 2314 the Loss is 0.2644.\n",
      "For Iteration 2315 the Loss is 0.2644.\n",
      "For Iteration 2316 the Loss is 0.2644.\n",
      "For Iteration 2317 the Loss is 0.2644.\n",
      "For Iteration 2318 the Loss is 0.2644.\n",
      "For Iteration 2319 the Loss is 0.2644.\n",
      "For Iteration 2320 the Loss is 0.2643.\n",
      "For Iteration 2321 the Loss is 0.2643.\n",
      "For Iteration 2322 the Loss is 0.2643.\n",
      "For Iteration 2323 the Loss is 0.2643.\n",
      "For Iteration 2324 the Loss is 0.2643.\n",
      "For Iteration 2325 the Loss is 0.2643.\n",
      "For Iteration 2326 the Loss is 0.2643.\n",
      "For Iteration 2327 the Loss is 0.2643.\n",
      "For Iteration 2328 the Loss is 0.2643.\n",
      "For Iteration 2329 the Loss is 0.2643.\n",
      "For Iteration 2330 the Loss is 0.2643.\n",
      "For Iteration 2331 the Loss is 0.2643.\n",
      "For Iteration 2332 the Loss is 0.2643.\n",
      "For Iteration 2333 the Loss is 0.2643.\n",
      "For Iteration 2334 the Loss is 0.2643.\n",
      "For Iteration 2335 the Loss is 0.2643.\n",
      "For Iteration 2336 the Loss is 0.2643.\n",
      "For Iteration 2337 the Loss is 0.2643.\n",
      "For Iteration 2338 the Loss is 0.2643.\n",
      "For Iteration 2339 the Loss is 0.2643.\n",
      "For Iteration 2340 the Loss is 0.2643.\n",
      "For Iteration 2341 the Loss is 0.2643.\n",
      "For Iteration 2342 the Loss is 0.2643.\n",
      "For Iteration 2343 the Loss is 0.2643.\n",
      "For Iteration 2344 the Loss is 0.2643.\n",
      "For Iteration 2345 the Loss is 0.2643.\n",
      "For Iteration 2346 the Loss is 0.2643.\n",
      "For Iteration 2347 the Loss is 0.2643.\n",
      "For Iteration 2348 the Loss is 0.2643.\n",
      "For Iteration 2349 the Loss is 0.2643.\n",
      "For Iteration 2350 the Loss is 0.2643.\n",
      "For Iteration 2351 the Loss is 0.2643.\n",
      "For Iteration 2352 the Loss is 0.2643.\n",
      "For Iteration 2353 the Loss is 0.2643.\n",
      "For Iteration 2354 the Loss is 0.2643.\n",
      "For Iteration 2355 the Loss is 0.2643.\n",
      "For Iteration 2356 the Loss is 0.2643.\n",
      "For Iteration 2357 the Loss is 0.2643.\n",
      "For Iteration 2358 the Loss is 0.2643.\n",
      "For Iteration 2359 the Loss is 0.2643.\n",
      "For Iteration 2360 the Loss is 0.2643.\n",
      "For Iteration 2361 the Loss is 0.2643.\n",
      "For Iteration 2362 the Loss is 0.2643.\n",
      "For Iteration 2363 the Loss is 0.2643.\n",
      "For Iteration 2364 the Loss is 0.2643.\n",
      "For Iteration 2365 the Loss is 0.2643.\n",
      "For Iteration 2366 the Loss is 0.2643.\n",
      "For Iteration 2367 the Loss is 0.2643.\n",
      "For Iteration 2368 the Loss is 0.2643.\n",
      "For Iteration 2369 the Loss is 0.2643.\n",
      "For Iteration 2370 the Loss is 0.2643.\n",
      "For Iteration 2371 the Loss is 0.2643.\n",
      "For Iteration 2372 the Loss is 0.2643.\n",
      "For Iteration 2373 the Loss is 0.2643.\n",
      "For Iteration 2374 the Loss is 0.2643.\n",
      "For Iteration 2375 the Loss is 0.2643.\n",
      "For Iteration 2376 the Loss is 0.2643.\n",
      "For Iteration 2377 the Loss is 0.2643.\n",
      "For Iteration 2378 the Loss is 0.2643.\n",
      "For Iteration 2379 the Loss is 0.2643.\n",
      "For Iteration 2380 the Loss is 0.2643.\n",
      "For Iteration 2381 the Loss is 0.2643.\n",
      "For Iteration 2382 the Loss is 0.2643.\n",
      "For Iteration 2383 the Loss is 0.2643.\n",
      "For Iteration 2384 the Loss is 0.2642.\n",
      "For Iteration 2385 the Loss is 0.2642.\n",
      "For Iteration 2386 the Loss is 0.2642.\n",
      "For Iteration 2387 the Loss is 0.2642.\n",
      "For Iteration 2388 the Loss is 0.2642.\n",
      "For Iteration 2389 the Loss is 0.2642.\n",
      "For Iteration 2390 the Loss is 0.2642.\n",
      "For Iteration 2391 the Loss is 0.2642.\n",
      "For Iteration 2392 the Loss is 0.2642.\n",
      "For Iteration 2393 the Loss is 0.2642.\n",
      "For Iteration 2394 the Loss is 0.2642.\n",
      "For Iteration 2395 the Loss is 0.2642.\n",
      "For Iteration 2396 the Loss is 0.2642.\n",
      "For Iteration 2397 the Loss is 0.2642.\n",
      "For Iteration 2398 the Loss is 0.2642.\n",
      "For Iteration 2399 the Loss is 0.2642.\n",
      "For Iteration 2400 the Loss is 0.2642.\n",
      "For Iteration 2401 the Loss is 0.2642.\n",
      "For Iteration 2402 the Loss is 0.2642.\n",
      "For Iteration 2403 the Loss is 0.2642.\n",
      "For Iteration 2404 the Loss is 0.2642.\n",
      "For Iteration 2405 the Loss is 0.2642.\n",
      "For Iteration 2406 the Loss is 0.2642.\n",
      "For Iteration 2407 the Loss is 0.2642.\n",
      "For Iteration 2408 the Loss is 0.2642.\n",
      "For Iteration 2409 the Loss is 0.2642.\n",
      "For Iteration 2410 the Loss is 0.2642.\n",
      "For Iteration 2411 the Loss is 0.2642.\n",
      "For Iteration 2412 the Loss is 0.2642.\n",
      "For Iteration 2413 the Loss is 0.2642.\n",
      "For Iteration 2414 the Loss is 0.2642.\n",
      "For Iteration 2415 the Loss is 0.2642.\n",
      "For Iteration 2416 the Loss is 0.2642.\n",
      "For Iteration 2417 the Loss is 0.2642.\n",
      "For Iteration 2418 the Loss is 0.2642.\n",
      "For Iteration 2419 the Loss is 0.2642.\n",
      "For Iteration 2420 the Loss is 0.2642.\n",
      "For Iteration 2421 the Loss is 0.2642.\n",
      "For Iteration 2422 the Loss is 0.2642.\n",
      "For Iteration 2423 the Loss is 0.2642.\n",
      "For Iteration 2424 the Loss is 0.2642.\n",
      "For Iteration 2425 the Loss is 0.2642.\n",
      "For Iteration 2426 the Loss is 0.2642.\n",
      "For Iteration 2427 the Loss is 0.2642.\n",
      "For Iteration 2428 the Loss is 0.2642.\n",
      "For Iteration 2429 the Loss is 0.2642.\n",
      "For Iteration 2430 the Loss is 0.2642.\n",
      "For Iteration 2431 the Loss is 0.2642.\n",
      "For Iteration 2432 the Loss is 0.2642.\n",
      "For Iteration 2433 the Loss is 0.2642.\n",
      "For Iteration 2434 the Loss is 0.2642.\n",
      "For Iteration 2435 the Loss is 0.2642.\n",
      "For Iteration 2436 the Loss is 0.2642.\n",
      "For Iteration 2437 the Loss is 0.2642.\n",
      "For Iteration 2438 the Loss is 0.2642.\n",
      "For Iteration 2439 the Loss is 0.2642.\n",
      "For Iteration 2440 the Loss is 0.2642.\n",
      "For Iteration 2441 the Loss is 0.2642.\n",
      "For Iteration 2442 the Loss is 0.2642.\n",
      "For Iteration 2443 the Loss is 0.2642.\n",
      "For Iteration 2444 the Loss is 0.2642.\n",
      "For Iteration 2445 the Loss is 0.2642.\n",
      "For Iteration 2446 the Loss is 0.2642.\n",
      "For Iteration 2447 the Loss is 0.2642.\n",
      "For Iteration 2448 the Loss is 0.2642.\n",
      "For Iteration 2449 the Loss is 0.2642.\n",
      "For Iteration 2450 the Loss is 0.2642.\n",
      "For Iteration 2451 the Loss is 0.2641.\n",
      "For Iteration 2452 the Loss is 0.2641.\n",
      "For Iteration 2453 the Loss is 0.2641.\n",
      "For Iteration 2454 the Loss is 0.2641.\n",
      "For Iteration 2455 the Loss is 0.2641.\n",
      "For Iteration 2456 the Loss is 0.2641.\n",
      "For Iteration 2457 the Loss is 0.2641.\n",
      "For Iteration 2458 the Loss is 0.2641.\n",
      "For Iteration 2459 the Loss is 0.2641.\n",
      "For Iteration 2460 the Loss is 0.2641.\n",
      "For Iteration 2461 the Loss is 0.2641.\n",
      "For Iteration 2462 the Loss is 0.2641.\n",
      "For Iteration 2463 the Loss is 0.2641.\n",
      "For Iteration 2464 the Loss is 0.2641.\n",
      "For Iteration 2465 the Loss is 0.2641.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 2466 the Loss is 0.2641.\n",
      "For Iteration 2467 the Loss is 0.2641.\n",
      "For Iteration 2468 the Loss is 0.2641.\n",
      "For Iteration 2469 the Loss is 0.2641.\n",
      "For Iteration 2470 the Loss is 0.2641.\n",
      "For Iteration 2471 the Loss is 0.2641.\n",
      "For Iteration 2472 the Loss is 0.2641.\n",
      "For Iteration 2473 the Loss is 0.2641.\n",
      "For Iteration 2474 the Loss is 0.2641.\n",
      "For Iteration 2475 the Loss is 0.2641.\n",
      "For Iteration 2476 the Loss is 0.2641.\n",
      "For Iteration 2477 the Loss is 0.2641.\n",
      "For Iteration 2478 the Loss is 0.2641.\n",
      "For Iteration 2479 the Loss is 0.2641.\n",
      "For Iteration 2480 the Loss is 0.2641.\n",
      "For Iteration 2481 the Loss is 0.2641.\n",
      "For Iteration 2482 the Loss is 0.2641.\n",
      "For Iteration 2483 the Loss is 0.2641.\n",
      "For Iteration 2484 the Loss is 0.2641.\n",
      "For Iteration 2485 the Loss is 0.2641.\n",
      "For Iteration 2486 the Loss is 0.2641.\n",
      "For Iteration 2487 the Loss is 0.2641.\n",
      "For Iteration 2488 the Loss is 0.2641.\n",
      "For Iteration 2489 the Loss is 0.2641.\n",
      "For Iteration 2490 the Loss is 0.2641.\n",
      "For Iteration 2491 the Loss is 0.2641.\n",
      "For Iteration 2492 the Loss is 0.2641.\n",
      "For Iteration 2493 the Loss is 0.2641.\n",
      "For Iteration 2494 the Loss is 0.2641.\n",
      "For Iteration 2495 the Loss is 0.2641.\n",
      "For Iteration 2496 the Loss is 0.2641.\n",
      "For Iteration 2497 the Loss is 0.2641.\n",
      "For Iteration 2498 the Loss is 0.2641.\n",
      "For Iteration 2499 the Loss is 0.2641.\n",
      "For Iteration 2500 the Loss is 0.2641.\n",
      "For Iteration 2501 the Loss is 0.2641.\n",
      "For Iteration 2502 the Loss is 0.2641.\n",
      "For Iteration 2503 the Loss is 0.2641.\n",
      "For Iteration 2504 the Loss is 0.2641.\n",
      "For Iteration 2505 the Loss is 0.2641.\n",
      "For Iteration 2506 the Loss is 0.2641.\n",
      "For Iteration 2507 the Loss is 0.2641.\n",
      "For Iteration 2508 the Loss is 0.2641.\n",
      "For Iteration 2509 the Loss is 0.2641.\n",
      "For Iteration 2510 the Loss is 0.2641.\n",
      "For Iteration 2511 the Loss is 0.2641.\n",
      "For Iteration 2512 the Loss is 0.2641.\n",
      "For Iteration 2513 the Loss is 0.2641.\n",
      "For Iteration 2514 the Loss is 0.2641.\n",
      "For Iteration 2515 the Loss is 0.2641.\n",
      "For Iteration 2516 the Loss is 0.2641.\n",
      "For Iteration 2517 the Loss is 0.2641.\n",
      "For Iteration 2518 the Loss is 0.2641.\n",
      "For Iteration 2519 the Loss is 0.264.\n",
      "For Iteration 2520 the Loss is 0.264.\n",
      "For Iteration 2521 the Loss is 0.264.\n",
      "For Iteration 2522 the Loss is 0.264.\n",
      "For Iteration 2523 the Loss is 0.264.\n",
      "For Iteration 2524 the Loss is 0.264.\n",
      "For Iteration 2525 the Loss is 0.264.\n",
      "For Iteration 2526 the Loss is 0.264.\n",
      "For Iteration 2527 the Loss is 0.264.\n",
      "For Iteration 2528 the Loss is 0.264.\n",
      "For Iteration 2529 the Loss is 0.264.\n",
      "For Iteration 2530 the Loss is 0.264.\n",
      "For Iteration 2531 the Loss is 0.264.\n",
      "For Iteration 2532 the Loss is 0.264.\n",
      "For Iteration 2533 the Loss is 0.264.\n",
      "For Iteration 2534 the Loss is 0.264.\n",
      "For Iteration 2535 the Loss is 0.264.\n",
      "For Iteration 2536 the Loss is 0.264.\n",
      "For Iteration 2537 the Loss is 0.264.\n",
      "For Iteration 2538 the Loss is 0.264.\n",
      "For Iteration 2539 the Loss is 0.264.\n",
      "For Iteration 2540 the Loss is 0.264.\n",
      "For Iteration 2541 the Loss is 0.264.\n",
      "For Iteration 2542 the Loss is 0.264.\n",
      "For Iteration 2543 the Loss is 0.264.\n",
      "For Iteration 2544 the Loss is 0.264.\n",
      "For Iteration 2545 the Loss is 0.264.\n",
      "For Iteration 2546 the Loss is 0.264.\n",
      "For Iteration 2547 the Loss is 0.264.\n",
      "For Iteration 2548 the Loss is 0.264.\n",
      "For Iteration 2549 the Loss is 0.264.\n",
      "For Iteration 2550 the Loss is 0.264.\n",
      "For Iteration 2551 the Loss is 0.264.\n",
      "For Iteration 2552 the Loss is 0.264.\n",
      "For Iteration 2553 the Loss is 0.264.\n",
      "For Iteration 2554 the Loss is 0.264.\n",
      "For Iteration 2555 the Loss is 0.264.\n",
      "For Iteration 2556 the Loss is 0.264.\n",
      "For Iteration 2557 the Loss is 0.264.\n",
      "For Iteration 2558 the Loss is 0.264.\n",
      "For Iteration 2559 the Loss is 0.264.\n",
      "For Iteration 2560 the Loss is 0.264.\n",
      "For Iteration 2561 the Loss is 0.264.\n",
      "For Iteration 2562 the Loss is 0.264.\n",
      "For Iteration 2563 the Loss is 0.264.\n",
      "For Iteration 2564 the Loss is 0.264.\n",
      "For Iteration 2565 the Loss is 0.264.\n",
      "For Iteration 2566 the Loss is 0.264.\n",
      "For Iteration 2567 the Loss is 0.264.\n",
      "For Iteration 2568 the Loss is 0.264.\n",
      "For Iteration 2569 the Loss is 0.264.\n",
      "For Iteration 2570 the Loss is 0.264.\n",
      "For Iteration 2571 the Loss is 0.264.\n",
      "For Iteration 2572 the Loss is 0.264.\n",
      "For Iteration 2573 the Loss is 0.264.\n",
      "For Iteration 2574 the Loss is 0.264.\n",
      "For Iteration 2575 the Loss is 0.264.\n",
      "For Iteration 2576 the Loss is 0.264.\n",
      "For Iteration 2577 the Loss is 0.264.\n",
      "For Iteration 2578 the Loss is 0.264.\n",
      "For Iteration 2579 the Loss is 0.264.\n",
      "For Iteration 2580 the Loss is 0.264.\n",
      "For Iteration 2581 the Loss is 0.264.\n",
      "For Iteration 2582 the Loss is 0.264.\n",
      "For Iteration 2583 the Loss is 0.264.\n",
      "For Iteration 2584 the Loss is 0.264.\n",
      "For Iteration 2585 the Loss is 0.264.\n",
      "For Iteration 2586 the Loss is 0.264.\n",
      "For Iteration 2587 the Loss is 0.264.\n",
      "For Iteration 2588 the Loss is 0.264.\n",
      "For Iteration 2589 the Loss is 0.2639.\n",
      "For Iteration 2590 the Loss is 0.2639.\n",
      "For Iteration 2591 the Loss is 0.2639.\n",
      "For Iteration 2592 the Loss is 0.2639.\n",
      "For Iteration 2593 the Loss is 0.2639.\n",
      "For Iteration 2594 the Loss is 0.2639.\n",
      "For Iteration 2595 the Loss is 0.2639.\n",
      "For Iteration 2596 the Loss is 0.2639.\n",
      "For Iteration 2597 the Loss is 0.2639.\n",
      "For Iteration 2598 the Loss is 0.2639.\n",
      "For Iteration 2599 the Loss is 0.2639.\n",
      "For Iteration 2600 the Loss is 0.2639.\n",
      "For Iteration 2601 the Loss is 0.2639.\n",
      "For Iteration 2602 the Loss is 0.2639.\n",
      "For Iteration 2603 the Loss is 0.2639.\n",
      "For Iteration 2604 the Loss is 0.2639.\n",
      "For Iteration 2605 the Loss is 0.2639.\n",
      "For Iteration 2606 the Loss is 0.2639.\n",
      "For Iteration 2607 the Loss is 0.2639.\n",
      "For Iteration 2608 the Loss is 0.2639.\n",
      "For Iteration 2609 the Loss is 0.2639.\n",
      "For Iteration 2610 the Loss is 0.2639.\n",
      "For Iteration 2611 the Loss is 0.2639.\n",
      "For Iteration 2612 the Loss is 0.2639.\n",
      "For Iteration 2613 the Loss is 0.2639.\n",
      "For Iteration 2614 the Loss is 0.2639.\n",
      "For Iteration 2615 the Loss is 0.2639.\n",
      "For Iteration 2616 the Loss is 0.2639.\n",
      "For Iteration 2617 the Loss is 0.2639.\n",
      "For Iteration 2618 the Loss is 0.2639.\n",
      "For Iteration 2619 the Loss is 0.2639.\n",
      "For Iteration 2620 the Loss is 0.2639.\n",
      "For Iteration 2621 the Loss is 0.2639.\n",
      "For Iteration 2622 the Loss is 0.2639.\n",
      "For Iteration 2623 the Loss is 0.2639.\n",
      "For Iteration 2624 the Loss is 0.2639.\n",
      "For Iteration 2625 the Loss is 0.2639.\n",
      "For Iteration 2626 the Loss is 0.2639.\n",
      "For Iteration 2627 the Loss is 0.2639.\n",
      "For Iteration 2628 the Loss is 0.2639.\n",
      "For Iteration 2629 the Loss is 0.2639.\n",
      "For Iteration 2630 the Loss is 0.2639.\n",
      "For Iteration 2631 the Loss is 0.2639.\n",
      "For Iteration 2632 the Loss is 0.2639.\n",
      "For Iteration 2633 the Loss is 0.2639.\n",
      "For Iteration 2634 the Loss is 0.2639.\n",
      "For Iteration 2635 the Loss is 0.2639.\n",
      "For Iteration 2636 the Loss is 0.2639.\n",
      "For Iteration 2637 the Loss is 0.2639.\n",
      "For Iteration 2638 the Loss is 0.2639.\n",
      "For Iteration 2639 the Loss is 0.2639.\n",
      "For Iteration 2640 the Loss is 0.2639.\n",
      "For Iteration 2641 the Loss is 0.2639.\n",
      "For Iteration 2642 the Loss is 0.2639.\n",
      "For Iteration 2643 the Loss is 0.2639.\n",
      "For Iteration 2644 the Loss is 0.2639.\n",
      "For Iteration 2645 the Loss is 0.2639.\n",
      "For Iteration 2646 the Loss is 0.2639.\n",
      "For Iteration 2647 the Loss is 0.2639.\n",
      "For Iteration 2648 the Loss is 0.2639.\n",
      "For Iteration 2649 the Loss is 0.2639.\n",
      "For Iteration 2650 the Loss is 0.2639.\n",
      "For Iteration 2651 the Loss is 0.2639.\n",
      "For Iteration 2652 the Loss is 0.2639.\n",
      "For Iteration 2653 the Loss is 0.2639.\n",
      "For Iteration 2654 the Loss is 0.2639.\n",
      "For Iteration 2655 the Loss is 0.2639.\n",
      "For Iteration 2656 the Loss is 0.2639.\n",
      "For Iteration 2657 the Loss is 0.2639.\n",
      "For Iteration 2658 the Loss is 0.2639.\n",
      "For Iteration 2659 the Loss is 0.2639.\n",
      "For Iteration 2660 the Loss is 0.2639.\n",
      "For Iteration 2661 the Loss is 0.2639.\n",
      "For Iteration 2662 the Loss is 0.2638.\n",
      "For Iteration 2663 the Loss is 0.2638.\n",
      "For Iteration 2664 the Loss is 0.2638.\n",
      "For Iteration 2665 the Loss is 0.2638.\n",
      "For Iteration 2666 the Loss is 0.2638.\n",
      "For Iteration 2667 the Loss is 0.2638.\n",
      "For Iteration 2668 the Loss is 0.2638.\n",
      "For Iteration 2669 the Loss is 0.2638.\n",
      "For Iteration 2670 the Loss is 0.2638.\n",
      "For Iteration 2671 the Loss is 0.2638.\n",
      "For Iteration 2672 the Loss is 0.2638.\n",
      "For Iteration 2673 the Loss is 0.2638.\n",
      "For Iteration 2674 the Loss is 0.2638.\n",
      "For Iteration 2675 the Loss is 0.2638.\n",
      "For Iteration 2676 the Loss is 0.2638.\n",
      "For Iteration 2677 the Loss is 0.2638.\n",
      "For Iteration 2678 the Loss is 0.2638.\n",
      "For Iteration 2679 the Loss is 0.2638.\n",
      "For Iteration 2680 the Loss is 0.2638.\n",
      "For Iteration 2681 the Loss is 0.2638.\n",
      "For Iteration 2682 the Loss is 0.2638.\n",
      "For Iteration 2683 the Loss is 0.2638.\n",
      "For Iteration 2684 the Loss is 0.2638.\n",
      "For Iteration 2685 the Loss is 0.2638.\n",
      "For Iteration 2686 the Loss is 0.2638.\n",
      "For Iteration 2687 the Loss is 0.2638.\n",
      "For Iteration 2688 the Loss is 0.2638.\n",
      "For Iteration 2689 the Loss is 0.2638.\n",
      "For Iteration 2690 the Loss is 0.2638.\n",
      "For Iteration 2691 the Loss is 0.2638.\n",
      "For Iteration 2692 the Loss is 0.2638.\n",
      "For Iteration 2693 the Loss is 0.2638.\n",
      "For Iteration 2694 the Loss is 0.2638.\n",
      "For Iteration 2695 the Loss is 0.2638.\n",
      "For Iteration 2696 the Loss is 0.2638.\n",
      "For Iteration 2697 the Loss is 0.2638.\n",
      "For Iteration 2698 the Loss is 0.2638.\n",
      "For Iteration 2699 the Loss is 0.2638.\n",
      "For Iteration 2700 the Loss is 0.2638.\n",
      "For Iteration 2701 the Loss is 0.2638.\n",
      "For Iteration 2702 the Loss is 0.2638.\n",
      "For Iteration 2703 the Loss is 0.2638.\n",
      "For Iteration 2704 the Loss is 0.2638.\n",
      "For Iteration 2705 the Loss is 0.2638.\n",
      "For Iteration 2706 the Loss is 0.2638.\n",
      "For Iteration 2707 the Loss is 0.2638.\n",
      "For Iteration 2708 the Loss is 0.2638.\n",
      "For Iteration 2709 the Loss is 0.2638.\n",
      "For Iteration 2710 the Loss is 0.2638.\n",
      "For Iteration 2711 the Loss is 0.2638.\n",
      "For Iteration 2712 the Loss is 0.2638.\n",
      "For Iteration 2713 the Loss is 0.2638.\n",
      "For Iteration 2714 the Loss is 0.2638.\n",
      "For Iteration 2715 the Loss is 0.2638.\n",
      "For Iteration 2716 the Loss is 0.2638.\n",
      "For Iteration 2717 the Loss is 0.2638.\n",
      "For Iteration 2718 the Loss is 0.2638.\n",
      "For Iteration 2719 the Loss is 0.2638.\n",
      "For Iteration 2720 the Loss is 0.2638.\n",
      "For Iteration 2721 the Loss is 0.2638.\n",
      "For Iteration 2722 the Loss is 0.2638.\n",
      "For Iteration 2723 the Loss is 0.2638.\n",
      "For Iteration 2724 the Loss is 0.2638.\n",
      "For Iteration 2725 the Loss is 0.2638.\n",
      "For Iteration 2726 the Loss is 0.2638.\n",
      "For Iteration 2727 the Loss is 0.2638.\n",
      "For Iteration 2728 the Loss is 0.2638.\n",
      "For Iteration 2729 the Loss is 0.2638.\n",
      "For Iteration 2730 the Loss is 0.2638.\n",
      "For Iteration 2731 the Loss is 0.2638.\n",
      "For Iteration 2732 the Loss is 0.2638.\n",
      "For Iteration 2733 the Loss is 0.2638.\n",
      "For Iteration 2734 the Loss is 0.2638.\n",
      "For Iteration 2735 the Loss is 0.2638.\n",
      "For Iteration 2736 the Loss is 0.2638.\n",
      "For Iteration 2737 the Loss is 0.2637.\n",
      "For Iteration 2738 the Loss is 0.2637.\n",
      "For Iteration 2739 the Loss is 0.2637.\n",
      "For Iteration 2740 the Loss is 0.2637.\n",
      "For Iteration 2741 the Loss is 0.2637.\n",
      "For Iteration 2742 the Loss is 0.2637.\n",
      "For Iteration 2743 the Loss is 0.2637.\n",
      "For Iteration 2744 the Loss is 0.2637.\n",
      "For Iteration 2745 the Loss is 0.2637.\n",
      "For Iteration 2746 the Loss is 0.2637.\n",
      "For Iteration 2747 the Loss is 0.2637.\n",
      "For Iteration 2748 the Loss is 0.2637.\n",
      "For Iteration 2749 the Loss is 0.2637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 2750 the Loss is 0.2637.\n",
      "For Iteration 2751 the Loss is 0.2637.\n",
      "For Iteration 2752 the Loss is 0.2637.\n",
      "For Iteration 2753 the Loss is 0.2637.\n",
      "For Iteration 2754 the Loss is 0.2637.\n",
      "For Iteration 2755 the Loss is 0.2637.\n",
      "For Iteration 2756 the Loss is 0.2637.\n",
      "For Iteration 2757 the Loss is 0.2637.\n",
      "For Iteration 2758 the Loss is 0.2637.\n",
      "For Iteration 2759 the Loss is 0.2637.\n",
      "For Iteration 2760 the Loss is 0.2637.\n",
      "For Iteration 2761 the Loss is 0.2637.\n",
      "For Iteration 2762 the Loss is 0.2637.\n",
      "For Iteration 2763 the Loss is 0.2637.\n",
      "For Iteration 2764 the Loss is 0.2637.\n",
      "For Iteration 2765 the Loss is 0.2637.\n",
      "For Iteration 2766 the Loss is 0.2637.\n",
      "For Iteration 2767 the Loss is 0.2637.\n",
      "For Iteration 2768 the Loss is 0.2637.\n",
      "For Iteration 2769 the Loss is 0.2637.\n",
      "For Iteration 2770 the Loss is 0.2637.\n",
      "For Iteration 2771 the Loss is 0.2637.\n",
      "For Iteration 2772 the Loss is 0.2637.\n",
      "For Iteration 2773 the Loss is 0.2637.\n",
      "For Iteration 2774 the Loss is 0.2637.\n",
      "For Iteration 2775 the Loss is 0.2637.\n",
      "For Iteration 2776 the Loss is 0.2637.\n",
      "For Iteration 2777 the Loss is 0.2637.\n",
      "For Iteration 2778 the Loss is 0.2637.\n",
      "For Iteration 2779 the Loss is 0.2637.\n",
      "For Iteration 2780 the Loss is 0.2637.\n",
      "For Iteration 2781 the Loss is 0.2637.\n",
      "For Iteration 2782 the Loss is 0.2637.\n",
      "For Iteration 2783 the Loss is 0.2637.\n",
      "For Iteration 2784 the Loss is 0.2637.\n",
      "For Iteration 2785 the Loss is 0.2637.\n",
      "For Iteration 2786 the Loss is 0.2637.\n",
      "For Iteration 2787 the Loss is 0.2637.\n",
      "For Iteration 2788 the Loss is 0.2637.\n",
      "For Iteration 2789 the Loss is 0.2637.\n",
      "For Iteration 2790 the Loss is 0.2637.\n",
      "For Iteration 2791 the Loss is 0.2637.\n",
      "For Iteration 2792 the Loss is 0.2637.\n",
      "For Iteration 2793 the Loss is 0.2637.\n",
      "For Iteration 2794 the Loss is 0.2637.\n",
      "For Iteration 2795 the Loss is 0.2637.\n",
      "For Iteration 2796 the Loss is 0.2637.\n",
      "For Iteration 2797 the Loss is 0.2637.\n",
      "For Iteration 2798 the Loss is 0.2637.\n",
      "For Iteration 2799 the Loss is 0.2637.\n",
      "For Iteration 2800 the Loss is 0.2637.\n",
      "For Iteration 2801 the Loss is 0.2637.\n",
      "For Iteration 2802 the Loss is 0.2637.\n",
      "For Iteration 2803 the Loss is 0.2637.\n",
      "For Iteration 2804 the Loss is 0.2637.\n",
      "For Iteration 2805 the Loss is 0.2637.\n",
      "For Iteration 2806 the Loss is 0.2637.\n",
      "For Iteration 2807 the Loss is 0.2637.\n",
      "For Iteration 2808 the Loss is 0.2637.\n",
      "For Iteration 2809 the Loss is 0.2637.\n",
      "For Iteration 2810 the Loss is 0.2637.\n",
      "For Iteration 2811 the Loss is 0.2637.\n",
      "For Iteration 2812 the Loss is 0.2637.\n",
      "For Iteration 2813 the Loss is 0.2637.\n",
      "For Iteration 2814 the Loss is 0.2637.\n",
      "For Iteration 2815 the Loss is 0.2636.\n",
      "For Iteration 2816 the Loss is 0.2636.\n",
      "For Iteration 2817 the Loss is 0.2636.\n",
      "For Iteration 2818 the Loss is 0.2636.\n",
      "For Iteration 2819 the Loss is 0.2636.\n",
      "For Iteration 2820 the Loss is 0.2636.\n",
      "For Iteration 2821 the Loss is 0.2636.\n",
      "For Iteration 2822 the Loss is 0.2636.\n",
      "For Iteration 2823 the Loss is 0.2636.\n",
      "For Iteration 2824 the Loss is 0.2636.\n",
      "For Iteration 2825 the Loss is 0.2636.\n",
      "For Iteration 2826 the Loss is 0.2636.\n",
      "For Iteration 2827 the Loss is 0.2636.\n",
      "For Iteration 2828 the Loss is 0.2636.\n",
      "For Iteration 2829 the Loss is 0.2636.\n",
      "For Iteration 2830 the Loss is 0.2636.\n",
      "For Iteration 2831 the Loss is 0.2636.\n",
      "For Iteration 2832 the Loss is 0.2636.\n",
      "For Iteration 2833 the Loss is 0.2636.\n",
      "For Iteration 2834 the Loss is 0.2636.\n",
      "For Iteration 2835 the Loss is 0.2636.\n",
      "For Iteration 2836 the Loss is 0.2636.\n",
      "For Iteration 2837 the Loss is 0.2636.\n",
      "For Iteration 2838 the Loss is 0.2636.\n",
      "For Iteration 2839 the Loss is 0.2636.\n",
      "For Iteration 2840 the Loss is 0.2636.\n",
      "For Iteration 2841 the Loss is 0.2636.\n",
      "For Iteration 2842 the Loss is 0.2636.\n",
      "For Iteration 2843 the Loss is 0.2636.\n",
      "For Iteration 2844 the Loss is 0.2636.\n",
      "For Iteration 2845 the Loss is 0.2636.\n",
      "For Iteration 2846 the Loss is 0.2636.\n",
      "For Iteration 2847 the Loss is 0.2636.\n",
      "For Iteration 2848 the Loss is 0.2636.\n",
      "For Iteration 2849 the Loss is 0.2636.\n",
      "For Iteration 2850 the Loss is 0.2636.\n",
      "For Iteration 2851 the Loss is 0.2636.\n",
      "For Iteration 2852 the Loss is 0.2636.\n",
      "For Iteration 2853 the Loss is 0.2636.\n",
      "For Iteration 2854 the Loss is 0.2636.\n",
      "For Iteration 2855 the Loss is 0.2636.\n",
      "For Iteration 2856 the Loss is 0.2636.\n",
      "For Iteration 2857 the Loss is 0.2636.\n",
      "For Iteration 2858 the Loss is 0.2636.\n",
      "For Iteration 2859 the Loss is 0.2636.\n",
      "For Iteration 2860 the Loss is 0.2636.\n",
      "For Iteration 2861 the Loss is 0.2636.\n",
      "For Iteration 2862 the Loss is 0.2636.\n",
      "For Iteration 2863 the Loss is 0.2636.\n",
      "For Iteration 2864 the Loss is 0.2636.\n",
      "For Iteration 2865 the Loss is 0.2636.\n",
      "For Iteration 2866 the Loss is 0.2636.\n",
      "For Iteration 2867 the Loss is 0.2636.\n",
      "For Iteration 2868 the Loss is 0.2636.\n",
      "For Iteration 2869 the Loss is 0.2636.\n",
      "For Iteration 2870 the Loss is 0.2636.\n",
      "For Iteration 2871 the Loss is 0.2636.\n",
      "For Iteration 2872 the Loss is 0.2636.\n",
      "For Iteration 2873 the Loss is 0.2636.\n",
      "For Iteration 2874 the Loss is 0.2636.\n",
      "For Iteration 2875 the Loss is 0.2636.\n",
      "For Iteration 2876 the Loss is 0.2636.\n",
      "For Iteration 2877 the Loss is 0.2636.\n",
      "For Iteration 2878 the Loss is 0.2636.\n",
      "For Iteration 2879 the Loss is 0.2636.\n",
      "For Iteration 2880 the Loss is 0.2636.\n",
      "For Iteration 2881 the Loss is 0.2636.\n",
      "For Iteration 2882 the Loss is 0.2636.\n",
      "For Iteration 2883 the Loss is 0.2636.\n",
      "For Iteration 2884 the Loss is 0.2636.\n",
      "For Iteration 2885 the Loss is 0.2636.\n",
      "For Iteration 2886 the Loss is 0.2636.\n",
      "For Iteration 2887 the Loss is 0.2636.\n",
      "For Iteration 2888 the Loss is 0.2636.\n",
      "For Iteration 2889 the Loss is 0.2636.\n",
      "For Iteration 2890 the Loss is 0.2636.\n",
      "For Iteration 2891 the Loss is 0.2636.\n",
      "For Iteration 2892 the Loss is 0.2636.\n",
      "For Iteration 2893 the Loss is 0.2636.\n",
      "For Iteration 2894 the Loss is 0.2636.\n",
      "For Iteration 2895 the Loss is 0.2635.\n",
      "For Iteration 2896 the Loss is 0.2635.\n",
      "For Iteration 2897 the Loss is 0.2635.\n",
      "For Iteration 2898 the Loss is 0.2635.\n",
      "For Iteration 2899 the Loss is 0.2635.\n",
      "For Iteration 2900 the Loss is 0.2635.\n",
      "For Iteration 2901 the Loss is 0.2635.\n",
      "For Iteration 2902 the Loss is 0.2635.\n",
      "For Iteration 2903 the Loss is 0.2635.\n",
      "For Iteration 2904 the Loss is 0.2635.\n",
      "For Iteration 2905 the Loss is 0.2635.\n",
      "For Iteration 2906 the Loss is 0.2635.\n",
      "For Iteration 2907 the Loss is 0.2635.\n",
      "For Iteration 2908 the Loss is 0.2635.\n",
      "For Iteration 2909 the Loss is 0.2635.\n",
      "For Iteration 2910 the Loss is 0.2635.\n",
      "For Iteration 2911 the Loss is 0.2635.\n",
      "For Iteration 2912 the Loss is 0.2635.\n",
      "For Iteration 2913 the Loss is 0.2635.\n",
      "For Iteration 2914 the Loss is 0.2635.\n",
      "For Iteration 2915 the Loss is 0.2635.\n",
      "For Iteration 2916 the Loss is 0.2635.\n",
      "For Iteration 2917 the Loss is 0.2635.\n",
      "For Iteration 2918 the Loss is 0.2635.\n",
      "For Iteration 2919 the Loss is 0.2635.\n",
      "For Iteration 2920 the Loss is 0.2635.\n",
      "For Iteration 2921 the Loss is 0.2635.\n",
      "For Iteration 2922 the Loss is 0.2635.\n",
      "For Iteration 2923 the Loss is 0.2635.\n",
      "For Iteration 2924 the Loss is 0.2635.\n",
      "For Iteration 2925 the Loss is 0.2635.\n",
      "For Iteration 2926 the Loss is 0.2635.\n",
      "For Iteration 2927 the Loss is 0.2635.\n",
      "For Iteration 2928 the Loss is 0.2635.\n",
      "For Iteration 2929 the Loss is 0.2635.\n",
      "For Iteration 2930 the Loss is 0.2635.\n",
      "For Iteration 2931 the Loss is 0.2635.\n",
      "For Iteration 2932 the Loss is 0.2635.\n",
      "For Iteration 2933 the Loss is 0.2635.\n",
      "For Iteration 2934 the Loss is 0.2635.\n",
      "For Iteration 2935 the Loss is 0.2635.\n",
      "For Iteration 2936 the Loss is 0.2635.\n",
      "For Iteration 2937 the Loss is 0.2635.\n",
      "For Iteration 2938 the Loss is 0.2635.\n",
      "For Iteration 2939 the Loss is 0.2635.\n",
      "For Iteration 2940 the Loss is 0.2635.\n",
      "For Iteration 2941 the Loss is 0.2635.\n",
      "For Iteration 2942 the Loss is 0.2635.\n",
      "For Iteration 2943 the Loss is 0.2635.\n",
      "For Iteration 2944 the Loss is 0.2635.\n",
      "For Iteration 2945 the Loss is 0.2635.\n",
      "For Iteration 2946 the Loss is 0.2635.\n",
      "For Iteration 2947 the Loss is 0.2635.\n",
      "For Iteration 2948 the Loss is 0.2635.\n",
      "For Iteration 2949 the Loss is 0.2635.\n",
      "For Iteration 2950 the Loss is 0.2635.\n",
      "For Iteration 2951 the Loss is 0.2635.\n",
      "For Iteration 2952 the Loss is 0.2635.\n",
      "For Iteration 2953 the Loss is 0.2635.\n",
      "For Iteration 2954 the Loss is 0.2635.\n",
      "For Iteration 2955 the Loss is 0.2635.\n",
      "For Iteration 2956 the Loss is 0.2635.\n",
      "For Iteration 2957 the Loss is 0.2635.\n",
      "For Iteration 2958 the Loss is 0.2635.\n",
      "For Iteration 2959 the Loss is 0.2635.\n",
      "For Iteration 2960 the Loss is 0.2635.\n",
      "For Iteration 2961 the Loss is 0.2635.\n",
      "For Iteration 2962 the Loss is 0.2635.\n",
      "For Iteration 2963 the Loss is 0.2635.\n",
      "For Iteration 2964 the Loss is 0.2635.\n",
      "For Iteration 2965 the Loss is 0.2635.\n",
      "For Iteration 2966 the Loss is 0.2635.\n",
      "For Iteration 2967 the Loss is 0.2635.\n",
      "For Iteration 2968 the Loss is 0.2635.\n",
      "For Iteration 2969 the Loss is 0.2635.\n",
      "For Iteration 2970 the Loss is 0.2635.\n",
      "For Iteration 2971 the Loss is 0.2635.\n",
      "For Iteration 2972 the Loss is 0.2635.\n",
      "For Iteration 2973 the Loss is 0.2635.\n",
      "For Iteration 2974 the Loss is 0.2635.\n",
      "For Iteration 2975 the Loss is 0.2635.\n",
      "For Iteration 2976 the Loss is 0.2635.\n",
      "For Iteration 2977 the Loss is 0.2635.\n",
      "For Iteration 2978 the Loss is 0.2634.\n",
      "For Iteration 2979 the Loss is 0.2634.\n",
      "For Iteration 2980 the Loss is 0.2634.\n",
      "For Iteration 2981 the Loss is 0.2634.\n",
      "For Iteration 2982 the Loss is 0.2634.\n",
      "For Iteration 2983 the Loss is 0.2634.\n",
      "For Iteration 2984 the Loss is 0.2634.\n",
      "For Iteration 2985 the Loss is 0.2634.\n",
      "For Iteration 2986 the Loss is 0.2634.\n",
      "For Iteration 2987 the Loss is 0.2634.\n",
      "For Iteration 2988 the Loss is 0.2634.\n",
      "For Iteration 2989 the Loss is 0.2634.\n",
      "For Iteration 2990 the Loss is 0.2634.\n",
      "For Iteration 2991 the Loss is 0.2634.\n",
      "For Iteration 2992 the Loss is 0.2634.\n",
      "For Iteration 2993 the Loss is 0.2634.\n",
      "For Iteration 2994 the Loss is 0.2634.\n",
      "For Iteration 2995 the Loss is 0.2634.\n",
      "For Iteration 2996 the Loss is 0.2634.\n",
      "For Iteration 2997 the Loss is 0.2634.\n",
      "For Iteration 2998 the Loss is 0.2634.\n",
      "For Iteration 2999 the Loss is 0.2634.\n",
      "For Iteration 3000 the Loss is 0.2634.\n",
      "For Iteration 3001 the Loss is 0.2634.\n",
      "For Iteration 3002 the Loss is 0.2634.\n",
      "For Iteration 3003 the Loss is 0.2634.\n",
      "For Iteration 3004 the Loss is 0.2634.\n",
      "For Iteration 3005 the Loss is 0.2634.\n",
      "For Iteration 3006 the Loss is 0.2634.\n",
      "For Iteration 3007 the Loss is 0.2634.\n",
      "For Iteration 3008 the Loss is 0.2634.\n",
      "For Iteration 3009 the Loss is 0.2634.\n",
      "For Iteration 3010 the Loss is 0.2634.\n",
      "For Iteration 3011 the Loss is 0.2634.\n",
      "For Iteration 3012 the Loss is 0.2634.\n",
      "For Iteration 3013 the Loss is 0.2634.\n",
      "For Iteration 3014 the Loss is 0.2634.\n",
      "For Iteration 3015 the Loss is 0.2634.\n",
      "For Iteration 3016 the Loss is 0.2634.\n",
      "For Iteration 3017 the Loss is 0.2634.\n",
      "For Iteration 3018 the Loss is 0.2634.\n",
      "For Iteration 3019 the Loss is 0.2634.\n",
      "For Iteration 3020 the Loss is 0.2634.\n",
      "For Iteration 3021 the Loss is 0.2634.\n",
      "For Iteration 3022 the Loss is 0.2634.\n",
      "For Iteration 3023 the Loss is 0.2634.\n",
      "For Iteration 3024 the Loss is 0.2634.\n",
      "For Iteration 3025 the Loss is 0.2634.\n",
      "For Iteration 3026 the Loss is 0.2634.\n",
      "For Iteration 3027 the Loss is 0.2634.\n",
      "For Iteration 3028 the Loss is 0.2634.\n",
      "For Iteration 3029 the Loss is 0.2634.\n",
      "For Iteration 3030 the Loss is 0.2634.\n",
      "For Iteration 3031 the Loss is 0.2634.\n",
      "For Iteration 3032 the Loss is 0.2634.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 3033 the Loss is 0.2634.\n",
      "For Iteration 3034 the Loss is 0.2634.\n",
      "For Iteration 3035 the Loss is 0.2634.\n",
      "For Iteration 3036 the Loss is 0.2634.\n",
      "For Iteration 3037 the Loss is 0.2634.\n",
      "For Iteration 3038 the Loss is 0.2634.\n",
      "For Iteration 3039 the Loss is 0.2634.\n",
      "For Iteration 3040 the Loss is 0.2634.\n",
      "For Iteration 3041 the Loss is 0.2634.\n",
      "For Iteration 3042 the Loss is 0.2634.\n",
      "For Iteration 3043 the Loss is 0.2634.\n",
      "For Iteration 3044 the Loss is 0.2634.\n",
      "For Iteration 3045 the Loss is 0.2634.\n",
      "For Iteration 3046 the Loss is 0.2634.\n",
      "For Iteration 3047 the Loss is 0.2634.\n",
      "For Iteration 3048 the Loss is 0.2634.\n",
      "For Iteration 3049 the Loss is 0.2634.\n",
      "For Iteration 3050 the Loss is 0.2634.\n",
      "For Iteration 3051 the Loss is 0.2634.\n",
      "For Iteration 3052 the Loss is 0.2634.\n",
      "For Iteration 3053 the Loss is 0.2634.\n",
      "For Iteration 3054 the Loss is 0.2634.\n",
      "For Iteration 3055 the Loss is 0.2634.\n",
      "For Iteration 3056 the Loss is 0.2634.\n",
      "For Iteration 3057 the Loss is 0.2634.\n",
      "For Iteration 3058 the Loss is 0.2634.\n",
      "For Iteration 3059 the Loss is 0.2634.\n",
      "For Iteration 3060 the Loss is 0.2634.\n",
      "For Iteration 3061 the Loss is 0.2634.\n",
      "For Iteration 3062 the Loss is 0.2634.\n",
      "For Iteration 3063 the Loss is 0.2634.\n",
      "For Iteration 3064 the Loss is 0.2633.\n",
      "For Iteration 3065 the Loss is 0.2633.\n",
      "For Iteration 3066 the Loss is 0.2633.\n",
      "For Iteration 3067 the Loss is 0.2633.\n",
      "For Iteration 3068 the Loss is 0.2633.\n",
      "For Iteration 3069 the Loss is 0.2633.\n",
      "For Iteration 3070 the Loss is 0.2633.\n",
      "For Iteration 3071 the Loss is 0.2633.\n",
      "For Iteration 3072 the Loss is 0.2633.\n",
      "For Iteration 3073 the Loss is 0.2633.\n",
      "For Iteration 3074 the Loss is 0.2633.\n",
      "For Iteration 3075 the Loss is 0.2633.\n",
      "For Iteration 3076 the Loss is 0.2633.\n",
      "For Iteration 3077 the Loss is 0.2633.\n",
      "For Iteration 3078 the Loss is 0.2633.\n",
      "For Iteration 3079 the Loss is 0.2633.\n",
      "For Iteration 3080 the Loss is 0.2633.\n",
      "For Iteration 3081 the Loss is 0.2633.\n",
      "For Iteration 3082 the Loss is 0.2633.\n",
      "For Iteration 3083 the Loss is 0.2633.\n",
      "For Iteration 3084 the Loss is 0.2633.\n",
      "For Iteration 3085 the Loss is 0.2633.\n",
      "For Iteration 3086 the Loss is 0.2633.\n",
      "For Iteration 3087 the Loss is 0.2633.\n",
      "For Iteration 3088 the Loss is 0.2633.\n",
      "For Iteration 3089 the Loss is 0.2633.\n",
      "For Iteration 3090 the Loss is 0.2633.\n",
      "For Iteration 3091 the Loss is 0.2633.\n",
      "For Iteration 3092 the Loss is 0.2633.\n",
      "For Iteration 3093 the Loss is 0.2633.\n",
      "For Iteration 3094 the Loss is 0.2633.\n",
      "For Iteration 3095 the Loss is 0.2633.\n",
      "For Iteration 3096 the Loss is 0.2633.\n",
      "For Iteration 3097 the Loss is 0.2633.\n",
      "For Iteration 3098 the Loss is 0.2633.\n",
      "For Iteration 3099 the Loss is 0.2633.\n",
      "For Iteration 3100 the Loss is 0.2633.\n",
      "For Iteration 3101 the Loss is 0.2633.\n",
      "For Iteration 3102 the Loss is 0.2633.\n",
      "For Iteration 3103 the Loss is 0.2633.\n",
      "For Iteration 3104 the Loss is 0.2633.\n",
      "For Iteration 3105 the Loss is 0.2633.\n",
      "For Iteration 3106 the Loss is 0.2633.\n",
      "For Iteration 3107 the Loss is 0.2633.\n",
      "For Iteration 3108 the Loss is 0.2633.\n",
      "For Iteration 3109 the Loss is 0.2633.\n",
      "For Iteration 3110 the Loss is 0.2633.\n",
      "For Iteration 3111 the Loss is 0.2633.\n",
      "For Iteration 3112 the Loss is 0.2633.\n",
      "For Iteration 3113 the Loss is 0.2633.\n",
      "For Iteration 3114 the Loss is 0.2633.\n",
      "For Iteration 3115 the Loss is 0.2633.\n",
      "For Iteration 3116 the Loss is 0.2633.\n",
      "For Iteration 3117 the Loss is 0.2633.\n",
      "For Iteration 3118 the Loss is 0.2633.\n",
      "For Iteration 3119 the Loss is 0.2633.\n",
      "For Iteration 3120 the Loss is 0.2633.\n",
      "For Iteration 3121 the Loss is 0.2633.\n",
      "For Iteration 3122 the Loss is 0.2633.\n",
      "For Iteration 3123 the Loss is 0.2633.\n",
      "For Iteration 3124 the Loss is 0.2633.\n",
      "For Iteration 3125 the Loss is 0.2633.\n",
      "For Iteration 3126 the Loss is 0.2633.\n",
      "For Iteration 3127 the Loss is 0.2633.\n",
      "For Iteration 3128 the Loss is 0.2633.\n",
      "For Iteration 3129 the Loss is 0.2633.\n",
      "For Iteration 3130 the Loss is 0.2633.\n",
      "For Iteration 3131 the Loss is 0.2633.\n",
      "For Iteration 3132 the Loss is 0.2633.\n",
      "For Iteration 3133 the Loss is 0.2633.\n",
      "For Iteration 3134 the Loss is 0.2633.\n",
      "For Iteration 3135 the Loss is 0.2633.\n",
      "For Iteration 3136 the Loss is 0.2633.\n",
      "For Iteration 3137 the Loss is 0.2633.\n",
      "For Iteration 3138 the Loss is 0.2633.\n",
      "For Iteration 3139 the Loss is 0.2633.\n",
      "For Iteration 3140 the Loss is 0.2633.\n",
      "For Iteration 3141 the Loss is 0.2633.\n",
      "For Iteration 3142 the Loss is 0.2633.\n",
      "For Iteration 3143 the Loss is 0.2633.\n",
      "For Iteration 3144 the Loss is 0.2633.\n",
      "For Iteration 3145 the Loss is 0.2633.\n",
      "For Iteration 3146 the Loss is 0.2633.\n",
      "For Iteration 3147 the Loss is 0.2633.\n",
      "For Iteration 3148 the Loss is 0.2633.\n",
      "For Iteration 3149 the Loss is 0.2633.\n",
      "For Iteration 3150 the Loss is 0.2633.\n",
      "For Iteration 3151 the Loss is 0.2633.\n",
      "For Iteration 3152 the Loss is 0.2633.\n",
      "For Iteration 3153 the Loss is 0.2632.\n",
      "For Iteration 3154 the Loss is 0.2632.\n",
      "For Iteration 3155 the Loss is 0.2632.\n",
      "For Iteration 3156 the Loss is 0.2632.\n",
      "For Iteration 3157 the Loss is 0.2632.\n",
      "For Iteration 3158 the Loss is 0.2632.\n",
      "For Iteration 3159 the Loss is 0.2632.\n",
      "For Iteration 3160 the Loss is 0.2632.\n",
      "For Iteration 3161 the Loss is 0.2632.\n",
      "For Iteration 3162 the Loss is 0.2632.\n",
      "For Iteration 3163 the Loss is 0.2632.\n",
      "For Iteration 3164 the Loss is 0.2632.\n",
      "For Iteration 3165 the Loss is 0.2632.\n",
      "For Iteration 3166 the Loss is 0.2632.\n",
      "For Iteration 3167 the Loss is 0.2632.\n",
      "For Iteration 3168 the Loss is 0.2632.\n",
      "For Iteration 3169 the Loss is 0.2632.\n",
      "For Iteration 3170 the Loss is 0.2632.\n",
      "For Iteration 3171 the Loss is 0.2632.\n",
      "For Iteration 3172 the Loss is 0.2632.\n",
      "For Iteration 3173 the Loss is 0.2632.\n",
      "For Iteration 3174 the Loss is 0.2632.\n",
      "For Iteration 3175 the Loss is 0.2632.\n",
      "For Iteration 3176 the Loss is 0.2632.\n",
      "For Iteration 3177 the Loss is 0.2632.\n",
      "For Iteration 3178 the Loss is 0.2632.\n",
      "For Iteration 3179 the Loss is 0.2632.\n",
      "For Iteration 3180 the Loss is 0.2632.\n",
      "For Iteration 3181 the Loss is 0.2632.\n",
      "For Iteration 3182 the Loss is 0.2632.\n",
      "For Iteration 3183 the Loss is 0.2632.\n",
      "For Iteration 3184 the Loss is 0.2632.\n",
      "For Iteration 3185 the Loss is 0.2632.\n",
      "For Iteration 3186 the Loss is 0.2632.\n",
      "For Iteration 3187 the Loss is 0.2632.\n",
      "For Iteration 3188 the Loss is 0.2632.\n",
      "For Iteration 3189 the Loss is 0.2632.\n",
      "For Iteration 3190 the Loss is 0.2632.\n",
      "For Iteration 3191 the Loss is 0.2632.\n",
      "For Iteration 3192 the Loss is 0.2632.\n",
      "For Iteration 3193 the Loss is 0.2632.\n",
      "For Iteration 3194 the Loss is 0.2632.\n",
      "For Iteration 3195 the Loss is 0.2632.\n",
      "For Iteration 3196 the Loss is 0.2632.\n",
      "For Iteration 3197 the Loss is 0.2632.\n",
      "For Iteration 3198 the Loss is 0.2632.\n",
      "For Iteration 3199 the Loss is 0.2632.\n",
      "For Iteration 3200 the Loss is 0.2632.\n",
      "For Iteration 3201 the Loss is 0.2632.\n",
      "For Iteration 3202 the Loss is 0.2632.\n",
      "For Iteration 3203 the Loss is 0.2632.\n",
      "For Iteration 3204 the Loss is 0.2632.\n",
      "For Iteration 3205 the Loss is 0.2632.\n",
      "For Iteration 3206 the Loss is 0.2632.\n",
      "For Iteration 3207 the Loss is 0.2632.\n",
      "For Iteration 3208 the Loss is 0.2632.\n",
      "For Iteration 3209 the Loss is 0.2632.\n",
      "For Iteration 3210 the Loss is 0.2632.\n",
      "For Iteration 3211 the Loss is 0.2632.\n",
      "For Iteration 3212 the Loss is 0.2632.\n",
      "For Iteration 3213 the Loss is 0.2632.\n",
      "For Iteration 3214 the Loss is 0.2632.\n",
      "For Iteration 3215 the Loss is 0.2632.\n",
      "For Iteration 3216 the Loss is 0.2632.\n",
      "For Iteration 3217 the Loss is 0.2632.\n",
      "For Iteration 3218 the Loss is 0.2632.\n",
      "For Iteration 3219 the Loss is 0.2632.\n",
      "For Iteration 3220 the Loss is 0.2632.\n",
      "For Iteration 3221 the Loss is 0.2632.\n",
      "For Iteration 3222 the Loss is 0.2632.\n",
      "For Iteration 3223 the Loss is 0.2632.\n",
      "For Iteration 3224 the Loss is 0.2632.\n",
      "For Iteration 3225 the Loss is 0.2632.\n",
      "For Iteration 3226 the Loss is 0.2632.\n",
      "For Iteration 3227 the Loss is 0.2632.\n",
      "For Iteration 3228 the Loss is 0.2632.\n",
      "For Iteration 3229 the Loss is 0.2632.\n",
      "For Iteration 3230 the Loss is 0.2632.\n",
      "For Iteration 3231 the Loss is 0.2632.\n",
      "For Iteration 3232 the Loss is 0.2632.\n",
      "For Iteration 3233 the Loss is 0.2632.\n",
      "For Iteration 3234 the Loss is 0.2632.\n",
      "For Iteration 3235 the Loss is 0.2632.\n",
      "For Iteration 3236 the Loss is 0.2632.\n",
      "For Iteration 3237 the Loss is 0.2632.\n",
      "For Iteration 3238 the Loss is 0.2632.\n",
      "For Iteration 3239 the Loss is 0.2632.\n",
      "For Iteration 3240 the Loss is 0.2632.\n",
      "For Iteration 3241 the Loss is 0.2632.\n",
      "For Iteration 3242 the Loss is 0.2632.\n",
      "For Iteration 3243 the Loss is 0.2632.\n",
      "For Iteration 3244 the Loss is 0.2632.\n",
      "For Iteration 3245 the Loss is 0.2631.\n",
      "For Iteration 3246 the Loss is 0.2631.\n",
      "For Iteration 3247 the Loss is 0.2631.\n",
      "For Iteration 3248 the Loss is 0.2631.\n",
      "For Iteration 3249 the Loss is 0.2631.\n",
      "For Iteration 3250 the Loss is 0.2631.\n",
      "For Iteration 3251 the Loss is 0.2631.\n",
      "For Iteration 3252 the Loss is 0.2631.\n",
      "For Iteration 3253 the Loss is 0.2631.\n",
      "For Iteration 3254 the Loss is 0.2631.\n",
      "For Iteration 3255 the Loss is 0.2631.\n",
      "For Iteration 3256 the Loss is 0.2631.\n",
      "For Iteration 3257 the Loss is 0.2631.\n",
      "For Iteration 3258 the Loss is 0.2631.\n",
      "For Iteration 3259 the Loss is 0.2631.\n",
      "For Iteration 3260 the Loss is 0.2631.\n",
      "For Iteration 3261 the Loss is 0.2631.\n",
      "For Iteration 3262 the Loss is 0.2631.\n",
      "For Iteration 3263 the Loss is 0.2631.\n",
      "For Iteration 3264 the Loss is 0.2631.\n",
      "For Iteration 3265 the Loss is 0.2631.\n",
      "For Iteration 3266 the Loss is 0.2631.\n",
      "For Iteration 3267 the Loss is 0.2631.\n",
      "For Iteration 3268 the Loss is 0.2631.\n",
      "For Iteration 3269 the Loss is 0.2631.\n",
      "For Iteration 3270 the Loss is 0.2631.\n",
      "For Iteration 3271 the Loss is 0.2631.\n",
      "For Iteration 3272 the Loss is 0.2631.\n",
      "For Iteration 3273 the Loss is 0.2631.\n",
      "For Iteration 3274 the Loss is 0.2631.\n",
      "For Iteration 3275 the Loss is 0.2631.\n",
      "For Iteration 3276 the Loss is 0.2631.\n",
      "For Iteration 3277 the Loss is 0.2631.\n",
      "For Iteration 3278 the Loss is 0.2631.\n",
      "For Iteration 3279 the Loss is 0.2631.\n",
      "For Iteration 3280 the Loss is 0.2631.\n",
      "For Iteration 3281 the Loss is 0.2631.\n",
      "For Iteration 3282 the Loss is 0.2631.\n",
      "For Iteration 3283 the Loss is 0.2631.\n",
      "For Iteration 3284 the Loss is 0.2631.\n",
      "For Iteration 3285 the Loss is 0.2631.\n",
      "For Iteration 3286 the Loss is 0.2631.\n",
      "For Iteration 3287 the Loss is 0.2631.\n",
      "For Iteration 3288 the Loss is 0.2631.\n",
      "For Iteration 3289 the Loss is 0.2631.\n",
      "For Iteration 3290 the Loss is 0.2631.\n",
      "For Iteration 3291 the Loss is 0.2631.\n",
      "For Iteration 3292 the Loss is 0.2631.\n",
      "For Iteration 3293 the Loss is 0.2631.\n",
      "For Iteration 3294 the Loss is 0.2631.\n",
      "For Iteration 3295 the Loss is 0.2631.\n",
      "For Iteration 3296 the Loss is 0.2631.\n",
      "For Iteration 3297 the Loss is 0.2631.\n",
      "For Iteration 3298 the Loss is 0.2631.\n",
      "For Iteration 3299 the Loss is 0.2631.\n",
      "For Iteration 3300 the Loss is 0.2631.\n",
      "For Iteration 3301 the Loss is 0.2631.\n",
      "For Iteration 3302 the Loss is 0.2631.\n",
      "For Iteration 3303 the Loss is 0.2631.\n",
      "For Iteration 3304 the Loss is 0.2631.\n",
      "For Iteration 3305 the Loss is 0.2631.\n",
      "For Iteration 3306 the Loss is 0.2631.\n",
      "For Iteration 3307 the Loss is 0.2631.\n",
      "For Iteration 3308 the Loss is 0.2631.\n",
      "For Iteration 3309 the Loss is 0.2631.\n",
      "For Iteration 3310 the Loss is 0.2631.\n",
      "For Iteration 3311 the Loss is 0.2631.\n",
      "For Iteration 3312 the Loss is 0.2631.\n",
      "For Iteration 3313 the Loss is 0.2631.\n",
      "For Iteration 3314 the Loss is 0.2631.\n",
      "For Iteration 3315 the Loss is 0.2631.\n",
      "For Iteration 3316 the Loss is 0.2631.\n",
      "For Iteration 3317 the Loss is 0.2631.\n",
      "For Iteration 3318 the Loss is 0.2631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 3319 the Loss is 0.2631.\n",
      "For Iteration 3320 the Loss is 0.2631.\n",
      "For Iteration 3321 the Loss is 0.2631.\n",
      "For Iteration 3322 the Loss is 0.2631.\n",
      "For Iteration 3323 the Loss is 0.2631.\n",
      "For Iteration 3324 the Loss is 0.2631.\n",
      "For Iteration 3325 the Loss is 0.2631.\n",
      "For Iteration 3326 the Loss is 0.2631.\n",
      "For Iteration 3327 the Loss is 0.2631.\n",
      "For Iteration 3328 the Loss is 0.2631.\n",
      "For Iteration 3329 the Loss is 0.2631.\n",
      "For Iteration 3330 the Loss is 0.2631.\n",
      "For Iteration 3331 the Loss is 0.2631.\n",
      "For Iteration 3332 the Loss is 0.2631.\n",
      "For Iteration 3333 the Loss is 0.2631.\n",
      "For Iteration 3334 the Loss is 0.2631.\n",
      "For Iteration 3335 the Loss is 0.2631.\n",
      "For Iteration 3336 the Loss is 0.2631.\n",
      "For Iteration 3337 the Loss is 0.2631.\n",
      "For Iteration 3338 the Loss is 0.2631.\n",
      "For Iteration 3339 the Loss is 0.2631.\n",
      "For Iteration 3340 the Loss is 0.2631.\n",
      "For Iteration 3341 the Loss is 0.263.\n",
      "For Iteration 3342 the Loss is 0.263.\n",
      "For Iteration 3343 the Loss is 0.263.\n",
      "For Iteration 3344 the Loss is 0.263.\n",
      "For Iteration 3345 the Loss is 0.263.\n",
      "For Iteration 3346 the Loss is 0.263.\n",
      "For Iteration 3347 the Loss is 0.263.\n",
      "For Iteration 3348 the Loss is 0.263.\n",
      "For Iteration 3349 the Loss is 0.263.\n",
      "For Iteration 3350 the Loss is 0.263.\n",
      "For Iteration 3351 the Loss is 0.263.\n",
      "For Iteration 3352 the Loss is 0.263.\n",
      "For Iteration 3353 the Loss is 0.263.\n",
      "For Iteration 3354 the Loss is 0.263.\n",
      "For Iteration 3355 the Loss is 0.263.\n",
      "For Iteration 3356 the Loss is 0.263.\n",
      "For Iteration 3357 the Loss is 0.263.\n",
      "For Iteration 3358 the Loss is 0.263.\n",
      "For Iteration 3359 the Loss is 0.263.\n",
      "For Iteration 3360 the Loss is 0.263.\n",
      "For Iteration 3361 the Loss is 0.263.\n",
      "For Iteration 3362 the Loss is 0.263.\n",
      "For Iteration 3363 the Loss is 0.263.\n",
      "For Iteration 3364 the Loss is 0.263.\n",
      "For Iteration 3365 the Loss is 0.263.\n",
      "For Iteration 3366 the Loss is 0.263.\n",
      "For Iteration 3367 the Loss is 0.263.\n",
      "For Iteration 3368 the Loss is 0.263.\n",
      "For Iteration 3369 the Loss is 0.263.\n",
      "For Iteration 3370 the Loss is 0.263.\n",
      "For Iteration 3371 the Loss is 0.263.\n",
      "For Iteration 3372 the Loss is 0.263.\n",
      "For Iteration 3373 the Loss is 0.263.\n",
      "For Iteration 3374 the Loss is 0.263.\n",
      "For Iteration 3375 the Loss is 0.263.\n",
      "For Iteration 3376 the Loss is 0.263.\n",
      "For Iteration 3377 the Loss is 0.263.\n",
      "For Iteration 3378 the Loss is 0.263.\n",
      "For Iteration 3379 the Loss is 0.263.\n",
      "For Iteration 3380 the Loss is 0.263.\n",
      "For Iteration 3381 the Loss is 0.263.\n",
      "For Iteration 3382 the Loss is 0.263.\n",
      "For Iteration 3383 the Loss is 0.263.\n",
      "For Iteration 3384 the Loss is 0.263.\n",
      "For Iteration 3385 the Loss is 0.263.\n",
      "For Iteration 3386 the Loss is 0.263.\n",
      "For Iteration 3387 the Loss is 0.263.\n",
      "For Iteration 3388 the Loss is 0.263.\n",
      "For Iteration 3389 the Loss is 0.263.\n",
      "For Iteration 3390 the Loss is 0.263.\n",
      "For Iteration 3391 the Loss is 0.263.\n",
      "For Iteration 3392 the Loss is 0.263.\n",
      "For Iteration 3393 the Loss is 0.263.\n",
      "For Iteration 3394 the Loss is 0.263.\n",
      "For Iteration 3395 the Loss is 0.263.\n",
      "For Iteration 3396 the Loss is 0.263.\n",
      "For Iteration 3397 the Loss is 0.263.\n",
      "For Iteration 3398 the Loss is 0.263.\n",
      "For Iteration 3399 the Loss is 0.263.\n",
      "For Iteration 3400 the Loss is 0.263.\n",
      "For Iteration 3401 the Loss is 0.263.\n",
      "For Iteration 3402 the Loss is 0.263.\n",
      "For Iteration 3403 the Loss is 0.263.\n",
      "For Iteration 3404 the Loss is 0.263.\n",
      "For Iteration 3405 the Loss is 0.263.\n",
      "For Iteration 3406 the Loss is 0.263.\n",
      "For Iteration 3407 the Loss is 0.263.\n",
      "For Iteration 3408 the Loss is 0.263.\n",
      "For Iteration 3409 the Loss is 0.263.\n",
      "For Iteration 3410 the Loss is 0.263.\n",
      "For Iteration 3411 the Loss is 0.263.\n",
      "For Iteration 3412 the Loss is 0.263.\n",
      "For Iteration 3413 the Loss is 0.263.\n",
      "For Iteration 3414 the Loss is 0.263.\n",
      "For Iteration 3415 the Loss is 0.263.\n",
      "For Iteration 3416 the Loss is 0.263.\n",
      "For Iteration 3417 the Loss is 0.263.\n",
      "For Iteration 3418 the Loss is 0.263.\n",
      "For Iteration 3419 the Loss is 0.263.\n",
      "For Iteration 3420 the Loss is 0.263.\n",
      "For Iteration 3421 the Loss is 0.263.\n",
      "For Iteration 3422 the Loss is 0.263.\n",
      "For Iteration 3423 the Loss is 0.263.\n",
      "For Iteration 3424 the Loss is 0.263.\n",
      "For Iteration 3425 the Loss is 0.263.\n",
      "For Iteration 3426 the Loss is 0.263.\n",
      "For Iteration 3427 the Loss is 0.263.\n",
      "For Iteration 3428 the Loss is 0.263.\n",
      "For Iteration 3429 the Loss is 0.263.\n",
      "For Iteration 3430 the Loss is 0.263.\n",
      "For Iteration 3431 the Loss is 0.263.\n",
      "For Iteration 3432 the Loss is 0.263.\n",
      "For Iteration 3433 the Loss is 0.263.\n",
      "For Iteration 3434 the Loss is 0.263.\n",
      "For Iteration 3435 the Loss is 0.263.\n",
      "For Iteration 3436 the Loss is 0.263.\n",
      "For Iteration 3437 the Loss is 0.263.\n",
      "For Iteration 3438 the Loss is 0.263.\n",
      "For Iteration 3439 the Loss is 0.263.\n",
      "For Iteration 3440 the Loss is 0.263.\n",
      "For Iteration 3441 the Loss is 0.2629.\n",
      "For Iteration 3442 the Loss is 0.2629.\n",
      "For Iteration 3443 the Loss is 0.2629.\n",
      "For Iteration 3444 the Loss is 0.2629.\n",
      "For Iteration 3445 the Loss is 0.2629.\n",
      "For Iteration 3446 the Loss is 0.2629.\n",
      "For Iteration 3447 the Loss is 0.2629.\n",
      "For Iteration 3448 the Loss is 0.2629.\n",
      "For Iteration 3449 the Loss is 0.2629.\n",
      "For Iteration 3450 the Loss is 0.2629.\n",
      "For Iteration 3451 the Loss is 0.2629.\n",
      "For Iteration 3452 the Loss is 0.2629.\n",
      "For Iteration 3453 the Loss is 0.2629.\n",
      "For Iteration 3454 the Loss is 0.2629.\n",
      "For Iteration 3455 the Loss is 0.2629.\n",
      "For Iteration 3456 the Loss is 0.2629.\n",
      "For Iteration 3457 the Loss is 0.2629.\n",
      "For Iteration 3458 the Loss is 0.2629.\n",
      "For Iteration 3459 the Loss is 0.2629.\n",
      "For Iteration 3460 the Loss is 0.2629.\n",
      "For Iteration 3461 the Loss is 0.2629.\n",
      "For Iteration 3462 the Loss is 0.2629.\n",
      "For Iteration 3463 the Loss is 0.2629.\n",
      "For Iteration 3464 the Loss is 0.2629.\n",
      "For Iteration 3465 the Loss is 0.2629.\n",
      "For Iteration 3466 the Loss is 0.2629.\n",
      "For Iteration 3467 the Loss is 0.2629.\n",
      "For Iteration 3468 the Loss is 0.2629.\n",
      "For Iteration 3469 the Loss is 0.2629.\n",
      "For Iteration 3470 the Loss is 0.2629.\n",
      "For Iteration 3471 the Loss is 0.2629.\n",
      "For Iteration 3472 the Loss is 0.2629.\n",
      "For Iteration 3473 the Loss is 0.2629.\n",
      "For Iteration 3474 the Loss is 0.2629.\n",
      "For Iteration 3475 the Loss is 0.2629.\n",
      "For Iteration 3476 the Loss is 0.2629.\n",
      "For Iteration 3477 the Loss is 0.2629.\n",
      "For Iteration 3478 the Loss is 0.2629.\n",
      "For Iteration 3479 the Loss is 0.2629.\n",
      "For Iteration 3480 the Loss is 0.2629.\n",
      "For Iteration 3481 the Loss is 0.2629.\n",
      "For Iteration 3482 the Loss is 0.2629.\n",
      "For Iteration 3483 the Loss is 0.2629.\n",
      "For Iteration 3484 the Loss is 0.2629.\n",
      "For Iteration 3485 the Loss is 0.2629.\n",
      "For Iteration 3486 the Loss is 0.2629.\n",
      "For Iteration 3487 the Loss is 0.2629.\n",
      "For Iteration 3488 the Loss is 0.2629.\n",
      "For Iteration 3489 the Loss is 0.2629.\n",
      "For Iteration 3490 the Loss is 0.2629.\n",
      "For Iteration 3491 the Loss is 0.2629.\n",
      "For Iteration 3492 the Loss is 0.2629.\n",
      "For Iteration 3493 the Loss is 0.2629.\n",
      "For Iteration 3494 the Loss is 0.2629.\n",
      "For Iteration 3495 the Loss is 0.2629.\n",
      "For Iteration 3496 the Loss is 0.2629.\n",
      "For Iteration 3497 the Loss is 0.2629.\n",
      "For Iteration 3498 the Loss is 0.2629.\n",
      "For Iteration 3499 the Loss is 0.2629.\n",
      "For Iteration 3500 the Loss is 0.2629.\n",
      "For Iteration 3501 the Loss is 0.2629.\n",
      "For Iteration 3502 the Loss is 0.2629.\n",
      "For Iteration 3503 the Loss is 0.2629.\n",
      "For Iteration 3504 the Loss is 0.2629.\n",
      "For Iteration 3505 the Loss is 0.2629.\n",
      "For Iteration 3506 the Loss is 0.2629.\n",
      "For Iteration 3507 the Loss is 0.2629.\n",
      "For Iteration 3508 the Loss is 0.2629.\n",
      "For Iteration 3509 the Loss is 0.2629.\n",
      "For Iteration 3510 the Loss is 0.2629.\n",
      "For Iteration 3511 the Loss is 0.2629.\n",
      "For Iteration 3512 the Loss is 0.2629.\n",
      "For Iteration 3513 the Loss is 0.2629.\n",
      "For Iteration 3514 the Loss is 0.2629.\n",
      "For Iteration 3515 the Loss is 0.2629.\n",
      "For Iteration 3516 the Loss is 0.2629.\n",
      "For Iteration 3517 the Loss is 0.2629.\n",
      "For Iteration 3518 the Loss is 0.2629.\n",
      "For Iteration 3519 the Loss is 0.2629.\n",
      "For Iteration 3520 the Loss is 0.2629.\n",
      "For Iteration 3521 the Loss is 0.2629.\n",
      "For Iteration 3522 the Loss is 0.2629.\n",
      "For Iteration 3523 the Loss is 0.2629.\n",
      "For Iteration 3524 the Loss is 0.2629.\n",
      "For Iteration 3525 the Loss is 0.2629.\n",
      "For Iteration 3526 the Loss is 0.2629.\n",
      "For Iteration 3527 the Loss is 0.2629.\n",
      "For Iteration 3528 the Loss is 0.2629.\n",
      "For Iteration 3529 the Loss is 0.2629.\n",
      "For Iteration 3530 the Loss is 0.2629.\n",
      "For Iteration 3531 the Loss is 0.2629.\n",
      "For Iteration 3532 the Loss is 0.2629.\n",
      "For Iteration 3533 the Loss is 0.2629.\n",
      "For Iteration 3534 the Loss is 0.2629.\n",
      "For Iteration 3535 the Loss is 0.2629.\n",
      "For Iteration 3536 the Loss is 0.2629.\n",
      "For Iteration 3537 the Loss is 0.2629.\n",
      "For Iteration 3538 the Loss is 0.2629.\n",
      "For Iteration 3539 the Loss is 0.2629.\n",
      "For Iteration 3540 the Loss is 0.2629.\n",
      "For Iteration 3541 the Loss is 0.2629.\n",
      "For Iteration 3542 the Loss is 0.2629.\n",
      "For Iteration 3543 the Loss is 0.2629.\n",
      "For Iteration 3544 the Loss is 0.2629.\n",
      "For Iteration 3545 the Loss is 0.2628.\n",
      "For Iteration 3546 the Loss is 0.2628.\n",
      "For Iteration 3547 the Loss is 0.2628.\n",
      "For Iteration 3548 the Loss is 0.2628.\n",
      "For Iteration 3549 the Loss is 0.2628.\n",
      "For Iteration 3550 the Loss is 0.2628.\n",
      "For Iteration 3551 the Loss is 0.2628.\n",
      "For Iteration 3552 the Loss is 0.2628.\n",
      "For Iteration 3553 the Loss is 0.2628.\n",
      "For Iteration 3554 the Loss is 0.2628.\n",
      "For Iteration 3555 the Loss is 0.2628.\n",
      "For Iteration 3556 the Loss is 0.2628.\n",
      "For Iteration 3557 the Loss is 0.2628.\n",
      "For Iteration 3558 the Loss is 0.2628.\n",
      "For Iteration 3559 the Loss is 0.2628.\n",
      "For Iteration 3560 the Loss is 0.2628.\n",
      "For Iteration 3561 the Loss is 0.2628.\n",
      "For Iteration 3562 the Loss is 0.2628.\n",
      "For Iteration 3563 the Loss is 0.2628.\n",
      "For Iteration 3564 the Loss is 0.2628.\n",
      "For Iteration 3565 the Loss is 0.2628.\n",
      "For Iteration 3566 the Loss is 0.2628.\n",
      "For Iteration 3567 the Loss is 0.2628.\n",
      "For Iteration 3568 the Loss is 0.2628.\n",
      "For Iteration 3569 the Loss is 0.2628.\n",
      "For Iteration 3570 the Loss is 0.2628.\n",
      "For Iteration 3571 the Loss is 0.2628.\n",
      "For Iteration 3572 the Loss is 0.2628.\n",
      "For Iteration 3573 the Loss is 0.2628.\n",
      "For Iteration 3574 the Loss is 0.2628.\n",
      "For Iteration 3575 the Loss is 0.2628.\n",
      "For Iteration 3576 the Loss is 0.2628.\n",
      "For Iteration 3577 the Loss is 0.2628.\n",
      "For Iteration 3578 the Loss is 0.2628.\n",
      "For Iteration 3579 the Loss is 0.2628.\n",
      "For Iteration 3580 the Loss is 0.2628.\n",
      "For Iteration 3581 the Loss is 0.2628.\n",
      "For Iteration 3582 the Loss is 0.2628.\n",
      "For Iteration 3583 the Loss is 0.2628.\n",
      "For Iteration 3584 the Loss is 0.2628.\n",
      "For Iteration 3585 the Loss is 0.2628.\n",
      "For Iteration 3586 the Loss is 0.2628.\n",
      "For Iteration 3587 the Loss is 0.2628.\n",
      "For Iteration 3588 the Loss is 0.2628.\n",
      "For Iteration 3589 the Loss is 0.2628.\n",
      "For Iteration 3590 the Loss is 0.2628.\n",
      "For Iteration 3591 the Loss is 0.2628.\n",
      "For Iteration 3592 the Loss is 0.2628.\n",
      "For Iteration 3593 the Loss is 0.2628.\n",
      "For Iteration 3594 the Loss is 0.2628.\n",
      "For Iteration 3595 the Loss is 0.2628.\n",
      "For Iteration 3596 the Loss is 0.2628.\n",
      "For Iteration 3597 the Loss is 0.2628.\n",
      "For Iteration 3598 the Loss is 0.2628.\n",
      "For Iteration 3599 the Loss is 0.2628.\n",
      "For Iteration 3600 the Loss is 0.2628.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 3601 the Loss is 0.2628.\n",
      "For Iteration 3602 the Loss is 0.2628.\n",
      "For Iteration 3603 the Loss is 0.2628.\n",
      "For Iteration 3604 the Loss is 0.2628.\n",
      "For Iteration 3605 the Loss is 0.2628.\n",
      "For Iteration 3606 the Loss is 0.2628.\n",
      "For Iteration 3607 the Loss is 0.2628.\n",
      "For Iteration 3608 the Loss is 0.2628.\n",
      "For Iteration 3609 the Loss is 0.2628.\n",
      "For Iteration 3610 the Loss is 0.2628.\n",
      "For Iteration 3611 the Loss is 0.2628.\n",
      "For Iteration 3612 the Loss is 0.2628.\n",
      "For Iteration 3613 the Loss is 0.2628.\n",
      "For Iteration 3614 the Loss is 0.2628.\n",
      "For Iteration 3615 the Loss is 0.2628.\n",
      "For Iteration 3616 the Loss is 0.2628.\n",
      "For Iteration 3617 the Loss is 0.2628.\n",
      "For Iteration 3618 the Loss is 0.2628.\n",
      "For Iteration 3619 the Loss is 0.2628.\n",
      "For Iteration 3620 the Loss is 0.2628.\n",
      "For Iteration 3621 the Loss is 0.2628.\n",
      "For Iteration 3622 the Loss is 0.2628.\n",
      "For Iteration 3623 the Loss is 0.2628.\n",
      "For Iteration 3624 the Loss is 0.2628.\n",
      "For Iteration 3625 the Loss is 0.2628.\n",
      "For Iteration 3626 the Loss is 0.2628.\n",
      "For Iteration 3627 the Loss is 0.2628.\n",
      "For Iteration 3628 the Loss is 0.2628.\n",
      "For Iteration 3629 the Loss is 0.2628.\n",
      "For Iteration 3630 the Loss is 0.2628.\n",
      "For Iteration 3631 the Loss is 0.2628.\n",
      "For Iteration 3632 the Loss is 0.2628.\n",
      "For Iteration 3633 the Loss is 0.2628.\n",
      "For Iteration 3634 the Loss is 0.2628.\n",
      "For Iteration 3635 the Loss is 0.2628.\n",
      "For Iteration 3636 the Loss is 0.2628.\n",
      "For Iteration 3637 the Loss is 0.2628.\n",
      "For Iteration 3638 the Loss is 0.2628.\n",
      "For Iteration 3639 the Loss is 0.2628.\n",
      "For Iteration 3640 the Loss is 0.2628.\n",
      "For Iteration 3641 the Loss is 0.2628.\n",
      "For Iteration 3642 the Loss is 0.2628.\n",
      "For Iteration 3643 the Loss is 0.2628.\n",
      "For Iteration 3644 the Loss is 0.2628.\n",
      "For Iteration 3645 the Loss is 0.2628.\n",
      "For Iteration 3646 the Loss is 0.2628.\n",
      "For Iteration 3647 the Loss is 0.2628.\n",
      "For Iteration 3648 the Loss is 0.2628.\n",
      "For Iteration 3649 the Loss is 0.2628.\n",
      "For Iteration 3650 the Loss is 0.2628.\n",
      "For Iteration 3651 the Loss is 0.2628.\n",
      "For Iteration 3652 the Loss is 0.2628.\n",
      "For Iteration 3653 the Loss is 0.2627.\n",
      "For Iteration 3654 the Loss is 0.2627.\n",
      "For Iteration 3655 the Loss is 0.2627.\n",
      "For Iteration 3656 the Loss is 0.2627.\n",
      "For Iteration 3657 the Loss is 0.2627.\n",
      "For Iteration 3658 the Loss is 0.2627.\n",
      "For Iteration 3659 the Loss is 0.2627.\n",
      "For Iteration 3660 the Loss is 0.2627.\n",
      "For Iteration 3661 the Loss is 0.2627.\n",
      "For Iteration 3662 the Loss is 0.2627.\n",
      "For Iteration 3663 the Loss is 0.2627.\n",
      "For Iteration 3664 the Loss is 0.2627.\n",
      "For Iteration 3665 the Loss is 0.2627.\n",
      "For Iteration 3666 the Loss is 0.2627.\n",
      "For Iteration 3667 the Loss is 0.2627.\n",
      "For Iteration 3668 the Loss is 0.2627.\n",
      "For Iteration 3669 the Loss is 0.2627.\n",
      "For Iteration 3670 the Loss is 0.2627.\n",
      "For Iteration 3671 the Loss is 0.2627.\n",
      "For Iteration 3672 the Loss is 0.2627.\n",
      "For Iteration 3673 the Loss is 0.2627.\n",
      "For Iteration 3674 the Loss is 0.2627.\n",
      "For Iteration 3675 the Loss is 0.2627.\n",
      "For Iteration 3676 the Loss is 0.2627.\n",
      "For Iteration 3677 the Loss is 0.2627.\n",
      "For Iteration 3678 the Loss is 0.2627.\n",
      "For Iteration 3679 the Loss is 0.2627.\n",
      "For Iteration 3680 the Loss is 0.2627.\n",
      "For Iteration 3681 the Loss is 0.2627.\n",
      "For Iteration 3682 the Loss is 0.2627.\n",
      "For Iteration 3683 the Loss is 0.2627.\n",
      "For Iteration 3684 the Loss is 0.2627.\n",
      "For Iteration 3685 the Loss is 0.2627.\n",
      "For Iteration 3686 the Loss is 0.2627.\n",
      "For Iteration 3687 the Loss is 0.2627.\n",
      "For Iteration 3688 the Loss is 0.2627.\n",
      "For Iteration 3689 the Loss is 0.2627.\n",
      "For Iteration 3690 the Loss is 0.2627.\n",
      "For Iteration 3691 the Loss is 0.2627.\n",
      "For Iteration 3692 the Loss is 0.2627.\n",
      "For Iteration 3693 the Loss is 0.2627.\n",
      "For Iteration 3694 the Loss is 0.2627.\n",
      "For Iteration 3695 the Loss is 0.2627.\n",
      "For Iteration 3696 the Loss is 0.2627.\n",
      "For Iteration 3697 the Loss is 0.2627.\n",
      "For Iteration 3698 the Loss is 0.2627.\n",
      "For Iteration 3699 the Loss is 0.2627.\n",
      "For Iteration 3700 the Loss is 0.2627.\n",
      "For Iteration 3701 the Loss is 0.2627.\n",
      "For Iteration 3702 the Loss is 0.2627.\n",
      "For Iteration 3703 the Loss is 0.2627.\n",
      "For Iteration 3704 the Loss is 0.2627.\n",
      "For Iteration 3705 the Loss is 0.2627.\n",
      "For Iteration 3706 the Loss is 0.2627.\n",
      "For Iteration 3707 the Loss is 0.2627.\n",
      "For Iteration 3708 the Loss is 0.2627.\n",
      "For Iteration 3709 the Loss is 0.2627.\n",
      "For Iteration 3710 the Loss is 0.2627.\n",
      "For Iteration 3711 the Loss is 0.2627.\n",
      "For Iteration 3712 the Loss is 0.2627.\n",
      "For Iteration 3713 the Loss is 0.2627.\n",
      "For Iteration 3714 the Loss is 0.2627.\n",
      "For Iteration 3715 the Loss is 0.2627.\n",
      "For Iteration 3716 the Loss is 0.2627.\n",
      "For Iteration 3717 the Loss is 0.2627.\n",
      "For Iteration 3718 the Loss is 0.2627.\n",
      "For Iteration 3719 the Loss is 0.2627.\n",
      "For Iteration 3720 the Loss is 0.2627.\n",
      "For Iteration 3721 the Loss is 0.2627.\n",
      "For Iteration 3722 the Loss is 0.2627.\n",
      "For Iteration 3723 the Loss is 0.2627.\n",
      "For Iteration 3724 the Loss is 0.2627.\n",
      "For Iteration 3725 the Loss is 0.2627.\n",
      "For Iteration 3726 the Loss is 0.2627.\n",
      "For Iteration 3727 the Loss is 0.2627.\n",
      "For Iteration 3728 the Loss is 0.2627.\n",
      "For Iteration 3729 the Loss is 0.2627.\n",
      "For Iteration 3730 the Loss is 0.2627.\n",
      "For Iteration 3731 the Loss is 0.2627.\n",
      "For Iteration 3732 the Loss is 0.2627.\n",
      "For Iteration 3733 the Loss is 0.2627.\n",
      "For Iteration 3734 the Loss is 0.2627.\n",
      "For Iteration 3735 the Loss is 0.2627.\n",
      "For Iteration 3736 the Loss is 0.2627.\n",
      "For Iteration 3737 the Loss is 0.2627.\n",
      "For Iteration 3738 the Loss is 0.2627.\n",
      "For Iteration 3739 the Loss is 0.2627.\n",
      "For Iteration 3740 the Loss is 0.2627.\n",
      "For Iteration 3741 the Loss is 0.2627.\n",
      "For Iteration 3742 the Loss is 0.2627.\n",
      "For Iteration 3743 the Loss is 0.2627.\n",
      "For Iteration 3744 the Loss is 0.2627.\n",
      "For Iteration 3745 the Loss is 0.2627.\n",
      "For Iteration 3746 the Loss is 0.2627.\n",
      "For Iteration 3747 the Loss is 0.2627.\n",
      "For Iteration 3748 the Loss is 0.2627.\n",
      "For Iteration 3749 the Loss is 0.2627.\n",
      "For Iteration 3750 the Loss is 0.2627.\n",
      "For Iteration 3751 the Loss is 0.2627.\n",
      "For Iteration 3752 the Loss is 0.2627.\n",
      "For Iteration 3753 the Loss is 0.2627.\n",
      "For Iteration 3754 the Loss is 0.2627.\n",
      "For Iteration 3755 the Loss is 0.2627.\n",
      "For Iteration 3756 the Loss is 0.2627.\n",
      "For Iteration 3757 the Loss is 0.2627.\n",
      "For Iteration 3758 the Loss is 0.2627.\n",
      "For Iteration 3759 the Loss is 0.2627.\n",
      "For Iteration 3760 the Loss is 0.2627.\n",
      "For Iteration 3761 the Loss is 0.2627.\n",
      "For Iteration 3762 the Loss is 0.2627.\n",
      "For Iteration 3763 the Loss is 0.2627.\n",
      "For Iteration 3764 the Loss is 0.2627.\n",
      "For Iteration 3765 the Loss is 0.2627.\n",
      "For Iteration 3766 the Loss is 0.2626.\n",
      "For Iteration 3767 the Loss is 0.2626.\n",
      "For Iteration 3768 the Loss is 0.2626.\n",
      "For Iteration 3769 the Loss is 0.2626.\n",
      "For Iteration 3770 the Loss is 0.2626.\n",
      "For Iteration 3771 the Loss is 0.2626.\n",
      "For Iteration 3772 the Loss is 0.2626.\n",
      "For Iteration 3773 the Loss is 0.2626.\n",
      "For Iteration 3774 the Loss is 0.2626.\n",
      "For Iteration 3775 the Loss is 0.2626.\n",
      "For Iteration 3776 the Loss is 0.2626.\n",
      "For Iteration 3777 the Loss is 0.2626.\n",
      "For Iteration 3778 the Loss is 0.2626.\n",
      "For Iteration 3779 the Loss is 0.2626.\n",
      "For Iteration 3780 the Loss is 0.2626.\n",
      "For Iteration 3781 the Loss is 0.2626.\n",
      "For Iteration 3782 the Loss is 0.2626.\n",
      "For Iteration 3783 the Loss is 0.2626.\n",
      "For Iteration 3784 the Loss is 0.2626.\n",
      "For Iteration 3785 the Loss is 0.2626.\n",
      "For Iteration 3786 the Loss is 0.2626.\n",
      "For Iteration 3787 the Loss is 0.2626.\n",
      "For Iteration 3788 the Loss is 0.2626.\n",
      "For Iteration 3789 the Loss is 0.2626.\n",
      "For Iteration 3790 the Loss is 0.2626.\n",
      "For Iteration 3791 the Loss is 0.2626.\n",
      "For Iteration 3792 the Loss is 0.2626.\n",
      "For Iteration 3793 the Loss is 0.2626.\n",
      "For Iteration 3794 the Loss is 0.2626.\n",
      "For Iteration 3795 the Loss is 0.2626.\n",
      "For Iteration 3796 the Loss is 0.2626.\n",
      "For Iteration 3797 the Loss is 0.2626.\n",
      "For Iteration 3798 the Loss is 0.2626.\n",
      "For Iteration 3799 the Loss is 0.2626.\n",
      "For Iteration 3800 the Loss is 0.2626.\n",
      "For Iteration 3801 the Loss is 0.2626.\n",
      "For Iteration 3802 the Loss is 0.2626.\n",
      "For Iteration 3803 the Loss is 0.2626.\n",
      "For Iteration 3804 the Loss is 0.2626.\n",
      "For Iteration 3805 the Loss is 0.2626.\n",
      "For Iteration 3806 the Loss is 0.2626.\n",
      "For Iteration 3807 the Loss is 0.2626.\n",
      "For Iteration 3808 the Loss is 0.2626.\n",
      "For Iteration 3809 the Loss is 0.2626.\n",
      "For Iteration 3810 the Loss is 0.2626.\n",
      "For Iteration 3811 the Loss is 0.2626.\n",
      "For Iteration 3812 the Loss is 0.2626.\n",
      "For Iteration 3813 the Loss is 0.2626.\n",
      "For Iteration 3814 the Loss is 0.2626.\n",
      "For Iteration 3815 the Loss is 0.2626.\n",
      "For Iteration 3816 the Loss is 0.2626.\n",
      "For Iteration 3817 the Loss is 0.2626.\n",
      "For Iteration 3818 the Loss is 0.2626.\n",
      "For Iteration 3819 the Loss is 0.2626.\n",
      "For Iteration 3820 the Loss is 0.2626.\n",
      "For Iteration 3821 the Loss is 0.2626.\n",
      "For Iteration 3822 the Loss is 0.2626.\n",
      "For Iteration 3823 the Loss is 0.2626.\n",
      "For Iteration 3824 the Loss is 0.2626.\n",
      "For Iteration 3825 the Loss is 0.2626.\n",
      "For Iteration 3826 the Loss is 0.2626.\n",
      "For Iteration 3827 the Loss is 0.2626.\n",
      "For Iteration 3828 the Loss is 0.2626.\n",
      "For Iteration 3829 the Loss is 0.2626.\n",
      "For Iteration 3830 the Loss is 0.2626.\n",
      "For Iteration 3831 the Loss is 0.2626.\n",
      "For Iteration 3832 the Loss is 0.2626.\n",
      "For Iteration 3833 the Loss is 0.2626.\n",
      "For Iteration 3834 the Loss is 0.2626.\n",
      "For Iteration 3835 the Loss is 0.2626.\n",
      "For Iteration 3836 the Loss is 0.2626.\n",
      "For Iteration 3837 the Loss is 0.2626.\n",
      "For Iteration 3838 the Loss is 0.2626.\n",
      "For Iteration 3839 the Loss is 0.2626.\n",
      "For Iteration 3840 the Loss is 0.2626.\n",
      "For Iteration 3841 the Loss is 0.2626.\n",
      "For Iteration 3842 the Loss is 0.2626.\n",
      "For Iteration 3843 the Loss is 0.2626.\n",
      "For Iteration 3844 the Loss is 0.2626.\n",
      "For Iteration 3845 the Loss is 0.2626.\n",
      "For Iteration 3846 the Loss is 0.2626.\n",
      "For Iteration 3847 the Loss is 0.2626.\n",
      "For Iteration 3848 the Loss is 0.2626.\n",
      "For Iteration 3849 the Loss is 0.2626.\n",
      "For Iteration 3850 the Loss is 0.2626.\n",
      "For Iteration 3851 the Loss is 0.2626.\n",
      "For Iteration 3852 the Loss is 0.2626.\n",
      "For Iteration 3853 the Loss is 0.2626.\n",
      "For Iteration 3854 the Loss is 0.2626.\n",
      "For Iteration 3855 the Loss is 0.2626.\n",
      "For Iteration 3856 the Loss is 0.2626.\n",
      "For Iteration 3857 the Loss is 0.2626.\n",
      "For Iteration 3858 the Loss is 0.2626.\n",
      "For Iteration 3859 the Loss is 0.2626.\n",
      "For Iteration 3860 the Loss is 0.2626.\n",
      "For Iteration 3861 the Loss is 0.2626.\n",
      "For Iteration 3862 the Loss is 0.2626.\n",
      "For Iteration 3863 the Loss is 0.2626.\n",
      "For Iteration 3864 the Loss is 0.2626.\n",
      "For Iteration 3865 the Loss is 0.2626.\n",
      "For Iteration 3866 the Loss is 0.2626.\n",
      "For Iteration 3867 the Loss is 0.2626.\n",
      "For Iteration 3868 the Loss is 0.2626.\n",
      "For Iteration 3869 the Loss is 0.2626.\n",
      "For Iteration 3870 the Loss is 0.2626.\n",
      "For Iteration 3871 the Loss is 0.2626.\n",
      "For Iteration 3872 the Loss is 0.2626.\n",
      "For Iteration 3873 the Loss is 0.2626.\n",
      "For Iteration 3874 the Loss is 0.2626.\n",
      "For Iteration 3875 the Loss is 0.2626.\n",
      "For Iteration 3876 the Loss is 0.2626.\n",
      "For Iteration 3877 the Loss is 0.2626.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 3878 the Loss is 0.2626.\n",
      "For Iteration 3879 the Loss is 0.2626.\n",
      "For Iteration 3880 the Loss is 0.2626.\n",
      "For Iteration 3881 the Loss is 0.2626.\n",
      "For Iteration 3882 the Loss is 0.2626.\n",
      "For Iteration 3883 the Loss is 0.2626.\n",
      "For Iteration 3884 the Loss is 0.2625.\n",
      "For Iteration 3885 the Loss is 0.2625.\n",
      "For Iteration 3886 the Loss is 0.2625.\n",
      "For Iteration 3887 the Loss is 0.2625.\n",
      "For Iteration 3888 the Loss is 0.2625.\n",
      "For Iteration 3889 the Loss is 0.2625.\n",
      "For Iteration 3890 the Loss is 0.2625.\n",
      "For Iteration 3891 the Loss is 0.2625.\n",
      "For Iteration 3892 the Loss is 0.2625.\n",
      "For Iteration 3893 the Loss is 0.2625.\n",
      "For Iteration 3894 the Loss is 0.2625.\n",
      "For Iteration 3895 the Loss is 0.2625.\n",
      "For Iteration 3896 the Loss is 0.2625.\n",
      "For Iteration 3897 the Loss is 0.2625.\n",
      "For Iteration 3898 the Loss is 0.2625.\n",
      "For Iteration 3899 the Loss is 0.2625.\n",
      "For Iteration 3900 the Loss is 0.2625.\n",
      "For Iteration 3901 the Loss is 0.2625.\n",
      "For Iteration 3902 the Loss is 0.2625.\n",
      "For Iteration 3903 the Loss is 0.2625.\n",
      "For Iteration 3904 the Loss is 0.2625.\n",
      "For Iteration 3905 the Loss is 0.2625.\n",
      "For Iteration 3906 the Loss is 0.2625.\n",
      "For Iteration 3907 the Loss is 0.2625.\n",
      "For Iteration 3908 the Loss is 0.2625.\n",
      "For Iteration 3909 the Loss is 0.2625.\n",
      "For Iteration 3910 the Loss is 0.2625.\n",
      "For Iteration 3911 the Loss is 0.2625.\n",
      "For Iteration 3912 the Loss is 0.2625.\n",
      "For Iteration 3913 the Loss is 0.2625.\n",
      "For Iteration 3914 the Loss is 0.2625.\n",
      "For Iteration 3915 the Loss is 0.2625.\n",
      "For Iteration 3916 the Loss is 0.2625.\n",
      "For Iteration 3917 the Loss is 0.2625.\n",
      "For Iteration 3918 the Loss is 0.2625.\n",
      "For Iteration 3919 the Loss is 0.2625.\n",
      "For Iteration 3920 the Loss is 0.2625.\n",
      "For Iteration 3921 the Loss is 0.2625.\n",
      "For Iteration 3922 the Loss is 0.2625.\n",
      "For Iteration 3923 the Loss is 0.2625.\n",
      "For Iteration 3924 the Loss is 0.2625.\n",
      "For Iteration 3925 the Loss is 0.2625.\n",
      "For Iteration 3926 the Loss is 0.2625.\n",
      "For Iteration 3927 the Loss is 0.2625.\n",
      "For Iteration 3928 the Loss is 0.2625.\n",
      "For Iteration 3929 the Loss is 0.2625.\n",
      "For Iteration 3930 the Loss is 0.2625.\n",
      "For Iteration 3931 the Loss is 0.2625.\n",
      "For Iteration 3932 the Loss is 0.2625.\n",
      "For Iteration 3933 the Loss is 0.2625.\n",
      "For Iteration 3934 the Loss is 0.2625.\n",
      "For Iteration 3935 the Loss is 0.2625.\n",
      "For Iteration 3936 the Loss is 0.2625.\n",
      "For Iteration 3937 the Loss is 0.2625.\n",
      "For Iteration 3938 the Loss is 0.2625.\n",
      "For Iteration 3939 the Loss is 0.2625.\n",
      "For Iteration 3940 the Loss is 0.2625.\n",
      "For Iteration 3941 the Loss is 0.2625.\n",
      "For Iteration 3942 the Loss is 0.2625.\n",
      "For Iteration 3943 the Loss is 0.2625.\n",
      "For Iteration 3944 the Loss is 0.2625.\n",
      "For Iteration 3945 the Loss is 0.2625.\n",
      "For Iteration 3946 the Loss is 0.2625.\n",
      "For Iteration 3947 the Loss is 0.2625.\n",
      "For Iteration 3948 the Loss is 0.2625.\n",
      "For Iteration 3949 the Loss is 0.2625.\n",
      "For Iteration 3950 the Loss is 0.2625.\n",
      "For Iteration 3951 the Loss is 0.2625.\n",
      "For Iteration 3952 the Loss is 0.2625.\n",
      "For Iteration 3953 the Loss is 0.2625.\n",
      "For Iteration 3954 the Loss is 0.2625.\n",
      "For Iteration 3955 the Loss is 0.2625.\n",
      "For Iteration 3956 the Loss is 0.2625.\n",
      "For Iteration 3957 the Loss is 0.2625.\n",
      "For Iteration 3958 the Loss is 0.2625.\n",
      "For Iteration 3959 the Loss is 0.2625.\n",
      "For Iteration 3960 the Loss is 0.2625.\n",
      "For Iteration 3961 the Loss is 0.2625.\n",
      "For Iteration 3962 the Loss is 0.2625.\n",
      "For Iteration 3963 the Loss is 0.2625.\n",
      "For Iteration 3964 the Loss is 0.2625.\n",
      "For Iteration 3965 the Loss is 0.2625.\n",
      "For Iteration 3966 the Loss is 0.2625.\n",
      "For Iteration 3967 the Loss is 0.2625.\n",
      "For Iteration 3968 the Loss is 0.2625.\n",
      "For Iteration 3969 the Loss is 0.2625.\n",
      "For Iteration 3970 the Loss is 0.2625.\n",
      "For Iteration 3971 the Loss is 0.2625.\n",
      "For Iteration 3972 the Loss is 0.2625.\n",
      "For Iteration 3973 the Loss is 0.2625.\n",
      "For Iteration 3974 the Loss is 0.2625.\n",
      "For Iteration 3975 the Loss is 0.2625.\n",
      "For Iteration 3976 the Loss is 0.2625.\n",
      "For Iteration 3977 the Loss is 0.2625.\n",
      "For Iteration 3978 the Loss is 0.2625.\n",
      "For Iteration 3979 the Loss is 0.2625.\n",
      "For Iteration 3980 the Loss is 0.2625.\n",
      "For Iteration 3981 the Loss is 0.2625.\n",
      "For Iteration 3982 the Loss is 0.2625.\n",
      "For Iteration 3983 the Loss is 0.2625.\n",
      "For Iteration 3984 the Loss is 0.2625.\n",
      "For Iteration 3985 the Loss is 0.2625.\n",
      "For Iteration 3986 the Loss is 0.2625.\n",
      "For Iteration 3987 the Loss is 0.2625.\n",
      "For Iteration 3988 the Loss is 0.2625.\n",
      "For Iteration 3989 the Loss is 0.2625.\n",
      "For Iteration 3990 the Loss is 0.2625.\n",
      "For Iteration 3991 the Loss is 0.2625.\n",
      "For Iteration 3992 the Loss is 0.2625.\n",
      "For Iteration 3993 the Loss is 0.2625.\n",
      "For Iteration 3994 the Loss is 0.2625.\n",
      "For Iteration 3995 the Loss is 0.2625.\n",
      "For Iteration 3996 the Loss is 0.2625.\n",
      "For Iteration 3997 the Loss is 0.2625.\n",
      "For Iteration 3998 the Loss is 0.2625.\n",
      "For Iteration 3999 the Loss is 0.2625.\n",
      "For Iteration 4000 the Loss is 0.2625.\n",
      "For Iteration 4001 the Loss is 0.2625.\n",
      "For Iteration 4002 the Loss is 0.2625.\n",
      "For Iteration 4003 the Loss is 0.2625.\n",
      "For Iteration 4004 the Loss is 0.2625.\n",
      "For Iteration 4005 the Loss is 0.2625.\n",
      "For Iteration 4006 the Loss is 0.2625.\n",
      "For Iteration 4007 the Loss is 0.2625.\n",
      "For Iteration 4008 the Loss is 0.2624.\n",
      "For Iteration 4009 the Loss is 0.2624.\n",
      "For Iteration 4010 the Loss is 0.2624.\n",
      "For Iteration 4011 the Loss is 0.2624.\n",
      "For Iteration 4012 the Loss is 0.2624.\n",
      "For Iteration 4013 the Loss is 0.2624.\n",
      "For Iteration 4014 the Loss is 0.2624.\n",
      "For Iteration 4015 the Loss is 0.2624.\n",
      "For Iteration 4016 the Loss is 0.2624.\n",
      "For Iteration 4017 the Loss is 0.2624.\n",
      "For Iteration 4018 the Loss is 0.2624.\n",
      "For Iteration 4019 the Loss is 0.2624.\n",
      "For Iteration 4020 the Loss is 0.2624.\n",
      "For Iteration 4021 the Loss is 0.2624.\n",
      "For Iteration 4022 the Loss is 0.2624.\n",
      "For Iteration 4023 the Loss is 0.2624.\n",
      "For Iteration 4024 the Loss is 0.2624.\n",
      "For Iteration 4025 the Loss is 0.2624.\n",
      "For Iteration 4026 the Loss is 0.2624.\n",
      "For Iteration 4027 the Loss is 0.2624.\n",
      "For Iteration 4028 the Loss is 0.2624.\n",
      "For Iteration 4029 the Loss is 0.2624.\n",
      "For Iteration 4030 the Loss is 0.2624.\n",
      "For Iteration 4031 the Loss is 0.2624.\n",
      "For Iteration 4032 the Loss is 0.2624.\n",
      "For Iteration 4033 the Loss is 0.2624.\n",
      "For Iteration 4034 the Loss is 0.2624.\n",
      "For Iteration 4035 the Loss is 0.2624.\n",
      "For Iteration 4036 the Loss is 0.2624.\n",
      "For Iteration 4037 the Loss is 0.2624.\n",
      "For Iteration 4038 the Loss is 0.2624.\n",
      "For Iteration 4039 the Loss is 0.2624.\n",
      "For Iteration 4040 the Loss is 0.2624.\n",
      "For Iteration 4041 the Loss is 0.2624.\n",
      "For Iteration 4042 the Loss is 0.2624.\n",
      "For Iteration 4043 the Loss is 0.2624.\n",
      "For Iteration 4044 the Loss is 0.2624.\n",
      "For Iteration 4045 the Loss is 0.2624.\n",
      "For Iteration 4046 the Loss is 0.2624.\n",
      "For Iteration 4047 the Loss is 0.2624.\n",
      "For Iteration 4048 the Loss is 0.2624.\n",
      "For Iteration 4049 the Loss is 0.2624.\n",
      "For Iteration 4050 the Loss is 0.2624.\n",
      "For Iteration 4051 the Loss is 0.2624.\n",
      "For Iteration 4052 the Loss is 0.2624.\n",
      "For Iteration 4053 the Loss is 0.2624.\n",
      "For Iteration 4054 the Loss is 0.2624.\n",
      "For Iteration 4055 the Loss is 0.2624.\n",
      "For Iteration 4056 the Loss is 0.2624.\n",
      "For Iteration 4057 the Loss is 0.2624.\n",
      "For Iteration 4058 the Loss is 0.2624.\n",
      "For Iteration 4059 the Loss is 0.2624.\n",
      "For Iteration 4060 the Loss is 0.2624.\n",
      "For Iteration 4061 the Loss is 0.2624.\n",
      "For Iteration 4062 the Loss is 0.2624.\n",
      "For Iteration 4063 the Loss is 0.2624.\n",
      "For Iteration 4064 the Loss is 0.2624.\n",
      "For Iteration 4065 the Loss is 0.2624.\n",
      "For Iteration 4066 the Loss is 0.2624.\n",
      "For Iteration 4067 the Loss is 0.2624.\n",
      "For Iteration 4068 the Loss is 0.2624.\n",
      "For Iteration 4069 the Loss is 0.2624.\n",
      "For Iteration 4070 the Loss is 0.2624.\n",
      "For Iteration 4071 the Loss is 0.2624.\n",
      "For Iteration 4072 the Loss is 0.2624.\n",
      "For Iteration 4073 the Loss is 0.2624.\n",
      "For Iteration 4074 the Loss is 0.2624.\n",
      "For Iteration 4075 the Loss is 0.2624.\n",
      "For Iteration 4076 the Loss is 0.2624.\n",
      "For Iteration 4077 the Loss is 0.2624.\n",
      "For Iteration 4078 the Loss is 0.2624.\n",
      "For Iteration 4079 the Loss is 0.2624.\n",
      "For Iteration 4080 the Loss is 0.2624.\n",
      "For Iteration 4081 the Loss is 0.2624.\n",
      "For Iteration 4082 the Loss is 0.2624.\n",
      "For Iteration 4083 the Loss is 0.2624.\n",
      "For Iteration 4084 the Loss is 0.2624.\n",
      "For Iteration 4085 the Loss is 0.2624.\n",
      "For Iteration 4086 the Loss is 0.2624.\n",
      "For Iteration 4087 the Loss is 0.2624.\n",
      "For Iteration 4088 the Loss is 0.2624.\n",
      "For Iteration 4089 the Loss is 0.2624.\n",
      "For Iteration 4090 the Loss is 0.2624.\n",
      "For Iteration 4091 the Loss is 0.2624.\n",
      "For Iteration 4092 the Loss is 0.2624.\n",
      "For Iteration 4093 the Loss is 0.2624.\n",
      "For Iteration 4094 the Loss is 0.2624.\n",
      "For Iteration 4095 the Loss is 0.2624.\n",
      "For Iteration 4096 the Loss is 0.2624.\n",
      "For Iteration 4097 the Loss is 0.2624.\n",
      "For Iteration 4098 the Loss is 0.2624.\n",
      "For Iteration 4099 the Loss is 0.2624.\n",
      "For Iteration 4100 the Loss is 0.2624.\n",
      "For Iteration 4101 the Loss is 0.2624.\n",
      "For Iteration 4102 the Loss is 0.2624.\n",
      "For Iteration 4103 the Loss is 0.2624.\n",
      "For Iteration 4104 the Loss is 0.2624.\n",
      "For Iteration 4105 the Loss is 0.2624.\n",
      "For Iteration 4106 the Loss is 0.2624.\n",
      "For Iteration 4107 the Loss is 0.2624.\n",
      "For Iteration 4108 the Loss is 0.2624.\n",
      "For Iteration 4109 the Loss is 0.2624.\n",
      "For Iteration 4110 the Loss is 0.2624.\n",
      "For Iteration 4111 the Loss is 0.2624.\n",
      "For Iteration 4112 the Loss is 0.2624.\n",
      "For Iteration 4113 the Loss is 0.2624.\n",
      "For Iteration 4114 the Loss is 0.2624.\n",
      "For Iteration 4115 the Loss is 0.2624.\n",
      "For Iteration 4116 the Loss is 0.2624.\n",
      "For Iteration 4117 the Loss is 0.2624.\n",
      "For Iteration 4118 the Loss is 0.2624.\n",
      "For Iteration 4119 the Loss is 0.2624.\n",
      "For Iteration 4120 the Loss is 0.2624.\n",
      "For Iteration 4121 the Loss is 0.2624.\n",
      "For Iteration 4122 the Loss is 0.2624.\n",
      "For Iteration 4123 the Loss is 0.2624.\n",
      "For Iteration 4124 the Loss is 0.2624.\n",
      "For Iteration 4125 the Loss is 0.2624.\n",
      "For Iteration 4126 the Loss is 0.2624.\n",
      "For Iteration 4127 the Loss is 0.2624.\n",
      "For Iteration 4128 the Loss is 0.2624.\n",
      "For Iteration 4129 the Loss is 0.2624.\n",
      "For Iteration 4130 the Loss is 0.2624.\n",
      "For Iteration 4131 the Loss is 0.2624.\n",
      "For Iteration 4132 the Loss is 0.2624.\n",
      "For Iteration 4133 the Loss is 0.2624.\n",
      "For Iteration 4134 the Loss is 0.2624.\n",
      "For Iteration 4135 the Loss is 0.2624.\n",
      "For Iteration 4136 the Loss is 0.2624.\n",
      "For Iteration 4137 the Loss is 0.2624.\n",
      "For Iteration 4138 the Loss is 0.2623.\n",
      "For Iteration 4139 the Loss is 0.2623.\n",
      "For Iteration 4140 the Loss is 0.2623.\n",
      "For Iteration 4141 the Loss is 0.2623.\n",
      "For Iteration 4142 the Loss is 0.2623.\n",
      "For Iteration 4143 the Loss is 0.2623.\n",
      "For Iteration 4144 the Loss is 0.2623.\n",
      "For Iteration 4145 the Loss is 0.2623.\n",
      "For Iteration 4146 the Loss is 0.2623.\n",
      "For Iteration 4147 the Loss is 0.2623.\n",
      "For Iteration 4148 the Loss is 0.2623.\n",
      "For Iteration 4149 the Loss is 0.2623.\n",
      "For Iteration 4150 the Loss is 0.2623.\n",
      "For Iteration 4151 the Loss is 0.2623.\n",
      "For Iteration 4152 the Loss is 0.2623.\n",
      "For Iteration 4153 the Loss is 0.2623.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 4154 the Loss is 0.2623.\n",
      "For Iteration 4155 the Loss is 0.2623.\n",
      "For Iteration 4156 the Loss is 0.2623.\n",
      "For Iteration 4157 the Loss is 0.2623.\n",
      "For Iteration 4158 the Loss is 0.2623.\n",
      "For Iteration 4159 the Loss is 0.2623.\n",
      "For Iteration 4160 the Loss is 0.2623.\n",
      "For Iteration 4161 the Loss is 0.2623.\n",
      "For Iteration 4162 the Loss is 0.2623.\n",
      "For Iteration 4163 the Loss is 0.2623.\n",
      "For Iteration 4164 the Loss is 0.2623.\n",
      "For Iteration 4165 the Loss is 0.2623.\n",
      "For Iteration 4166 the Loss is 0.2623.\n",
      "For Iteration 4167 the Loss is 0.2623.\n",
      "For Iteration 4168 the Loss is 0.2623.\n",
      "For Iteration 4169 the Loss is 0.2623.\n",
      "For Iteration 4170 the Loss is 0.2623.\n",
      "For Iteration 4171 the Loss is 0.2623.\n",
      "For Iteration 4172 the Loss is 0.2623.\n",
      "For Iteration 4173 the Loss is 0.2623.\n",
      "For Iteration 4174 the Loss is 0.2623.\n",
      "For Iteration 4175 the Loss is 0.2623.\n",
      "For Iteration 4176 the Loss is 0.2623.\n",
      "For Iteration 4177 the Loss is 0.2623.\n",
      "For Iteration 4178 the Loss is 0.2623.\n",
      "For Iteration 4179 the Loss is 0.2623.\n",
      "For Iteration 4180 the Loss is 0.2623.\n",
      "For Iteration 4181 the Loss is 0.2623.\n",
      "For Iteration 4182 the Loss is 0.2623.\n",
      "For Iteration 4183 the Loss is 0.2623.\n",
      "For Iteration 4184 the Loss is 0.2623.\n",
      "For Iteration 4185 the Loss is 0.2623.\n",
      "For Iteration 4186 the Loss is 0.2623.\n",
      "For Iteration 4187 the Loss is 0.2623.\n",
      "For Iteration 4188 the Loss is 0.2623.\n",
      "For Iteration 4189 the Loss is 0.2623.\n",
      "For Iteration 4190 the Loss is 0.2623.\n",
      "For Iteration 4191 the Loss is 0.2623.\n",
      "For Iteration 4192 the Loss is 0.2623.\n",
      "For Iteration 4193 the Loss is 0.2623.\n",
      "For Iteration 4194 the Loss is 0.2623.\n",
      "For Iteration 4195 the Loss is 0.2623.\n",
      "For Iteration 4196 the Loss is 0.2623.\n",
      "For Iteration 4197 the Loss is 0.2623.\n",
      "For Iteration 4198 the Loss is 0.2623.\n",
      "For Iteration 4199 the Loss is 0.2623.\n",
      "For Iteration 4200 the Loss is 0.2623.\n",
      "For Iteration 4201 the Loss is 0.2623.\n",
      "For Iteration 4202 the Loss is 0.2623.\n",
      "For Iteration 4203 the Loss is 0.2623.\n",
      "For Iteration 4204 the Loss is 0.2623.\n",
      "For Iteration 4205 the Loss is 0.2623.\n",
      "For Iteration 4206 the Loss is 0.2623.\n",
      "For Iteration 4207 the Loss is 0.2623.\n",
      "For Iteration 4208 the Loss is 0.2623.\n",
      "For Iteration 4209 the Loss is 0.2623.\n",
      "For Iteration 4210 the Loss is 0.2623.\n",
      "For Iteration 4211 the Loss is 0.2623.\n",
      "For Iteration 4212 the Loss is 0.2623.\n",
      "For Iteration 4213 the Loss is 0.2623.\n",
      "For Iteration 4214 the Loss is 0.2623.\n",
      "For Iteration 4215 the Loss is 0.2623.\n",
      "For Iteration 4216 the Loss is 0.2623.\n",
      "For Iteration 4217 the Loss is 0.2623.\n",
      "For Iteration 4218 the Loss is 0.2623.\n",
      "For Iteration 4219 the Loss is 0.2623.\n",
      "For Iteration 4220 the Loss is 0.2623.\n",
      "For Iteration 4221 the Loss is 0.2623.\n",
      "For Iteration 4222 the Loss is 0.2623.\n",
      "For Iteration 4223 the Loss is 0.2623.\n",
      "For Iteration 4224 the Loss is 0.2623.\n",
      "For Iteration 4225 the Loss is 0.2623.\n",
      "For Iteration 4226 the Loss is 0.2623.\n",
      "For Iteration 4227 the Loss is 0.2623.\n",
      "For Iteration 4228 the Loss is 0.2623.\n",
      "For Iteration 4229 the Loss is 0.2623.\n",
      "For Iteration 4230 the Loss is 0.2623.\n",
      "For Iteration 4231 the Loss is 0.2623.\n",
      "For Iteration 4232 the Loss is 0.2623.\n",
      "For Iteration 4233 the Loss is 0.2623.\n",
      "For Iteration 4234 the Loss is 0.2623.\n",
      "For Iteration 4235 the Loss is 0.2623.\n",
      "For Iteration 4236 the Loss is 0.2623.\n",
      "For Iteration 4237 the Loss is 0.2623.\n",
      "For Iteration 4238 the Loss is 0.2623.\n",
      "For Iteration 4239 the Loss is 0.2623.\n",
      "For Iteration 4240 the Loss is 0.2623.\n",
      "For Iteration 4241 the Loss is 0.2623.\n",
      "For Iteration 4242 the Loss is 0.2623.\n",
      "For Iteration 4243 the Loss is 0.2623.\n",
      "For Iteration 4244 the Loss is 0.2623.\n",
      "For Iteration 4245 the Loss is 0.2623.\n",
      "For Iteration 4246 the Loss is 0.2623.\n",
      "For Iteration 4247 the Loss is 0.2623.\n",
      "For Iteration 4248 the Loss is 0.2623.\n",
      "For Iteration 4249 the Loss is 0.2623.\n",
      "For Iteration 4250 the Loss is 0.2623.\n",
      "For Iteration 4251 the Loss is 0.2623.\n",
      "For Iteration 4252 the Loss is 0.2623.\n",
      "For Iteration 4253 the Loss is 0.2623.\n",
      "For Iteration 4254 the Loss is 0.2623.\n",
      "For Iteration 4255 the Loss is 0.2623.\n",
      "For Iteration 4256 the Loss is 0.2623.\n",
      "For Iteration 4257 the Loss is 0.2623.\n",
      "For Iteration 4258 the Loss is 0.2623.\n",
      "For Iteration 4259 the Loss is 0.2623.\n",
      "For Iteration 4260 the Loss is 0.2623.\n",
      "For Iteration 4261 the Loss is 0.2623.\n",
      "For Iteration 4262 the Loss is 0.2623.\n",
      "For Iteration 4263 the Loss is 0.2623.\n",
      "For Iteration 4264 the Loss is 0.2623.\n",
      "For Iteration 4265 the Loss is 0.2623.\n",
      "For Iteration 4266 the Loss is 0.2623.\n",
      "For Iteration 4267 the Loss is 0.2623.\n",
      "For Iteration 4268 the Loss is 0.2623.\n",
      "For Iteration 4269 the Loss is 0.2623.\n",
      "For Iteration 4270 the Loss is 0.2623.\n",
      "For Iteration 4271 the Loss is 0.2623.\n",
      "For Iteration 4272 the Loss is 0.2623.\n",
      "For Iteration 4273 the Loss is 0.2623.\n",
      "For Iteration 4274 the Loss is 0.2622.\n",
      "For Iteration 4275 the Loss is 0.2622.\n",
      "For Iteration 4276 the Loss is 0.2622.\n",
      "For Iteration 4277 the Loss is 0.2622.\n",
      "For Iteration 4278 the Loss is 0.2622.\n",
      "For Iteration 4279 the Loss is 0.2622.\n",
      "For Iteration 4280 the Loss is 0.2622.\n",
      "For Iteration 4281 the Loss is 0.2622.\n",
      "For Iteration 4282 the Loss is 0.2622.\n",
      "For Iteration 4283 the Loss is 0.2622.\n",
      "For Iteration 4284 the Loss is 0.2622.\n",
      "For Iteration 4285 the Loss is 0.2622.\n",
      "For Iteration 4286 the Loss is 0.2622.\n",
      "For Iteration 4287 the Loss is 0.2622.\n",
      "For Iteration 4288 the Loss is 0.2622.\n",
      "For Iteration 4289 the Loss is 0.2622.\n",
      "For Iteration 4290 the Loss is 0.2622.\n",
      "For Iteration 4291 the Loss is 0.2622.\n",
      "For Iteration 4292 the Loss is 0.2622.\n",
      "For Iteration 4293 the Loss is 0.2622.\n",
      "For Iteration 4294 the Loss is 0.2622.\n",
      "For Iteration 4295 the Loss is 0.2622.\n",
      "For Iteration 4296 the Loss is 0.2622.\n",
      "For Iteration 4297 the Loss is 0.2622.\n",
      "For Iteration 4298 the Loss is 0.2622.\n",
      "For Iteration 4299 the Loss is 0.2622.\n",
      "For Iteration 4300 the Loss is 0.2622.\n",
      "For Iteration 4301 the Loss is 0.2622.\n",
      "For Iteration 4302 the Loss is 0.2622.\n",
      "For Iteration 4303 the Loss is 0.2622.\n",
      "For Iteration 4304 the Loss is 0.2622.\n",
      "For Iteration 4305 the Loss is 0.2622.\n",
      "For Iteration 4306 the Loss is 0.2622.\n",
      "For Iteration 4307 the Loss is 0.2622.\n",
      "For Iteration 4308 the Loss is 0.2622.\n",
      "For Iteration 4309 the Loss is 0.2622.\n",
      "For Iteration 4310 the Loss is 0.2622.\n",
      "For Iteration 4311 the Loss is 0.2622.\n",
      "For Iteration 4312 the Loss is 0.2622.\n",
      "For Iteration 4313 the Loss is 0.2622.\n",
      "For Iteration 4314 the Loss is 0.2622.\n",
      "For Iteration 4315 the Loss is 0.2622.\n",
      "For Iteration 4316 the Loss is 0.2622.\n",
      "For Iteration 4317 the Loss is 0.2622.\n",
      "For Iteration 4318 the Loss is 0.2622.\n",
      "For Iteration 4319 the Loss is 0.2622.\n",
      "For Iteration 4320 the Loss is 0.2622.\n",
      "For Iteration 4321 the Loss is 0.2622.\n",
      "For Iteration 4322 the Loss is 0.2622.\n",
      "For Iteration 4323 the Loss is 0.2622.\n",
      "For Iteration 4324 the Loss is 0.2622.\n",
      "For Iteration 4325 the Loss is 0.2622.\n",
      "For Iteration 4326 the Loss is 0.2622.\n",
      "For Iteration 4327 the Loss is 0.2622.\n",
      "For Iteration 4328 the Loss is 0.2622.\n",
      "For Iteration 4329 the Loss is 0.2622.\n",
      "For Iteration 4330 the Loss is 0.2622.\n",
      "For Iteration 4331 the Loss is 0.2622.\n",
      "For Iteration 4332 the Loss is 0.2622.\n",
      "For Iteration 4333 the Loss is 0.2622.\n",
      "For Iteration 4334 the Loss is 0.2622.\n",
      "For Iteration 4335 the Loss is 0.2622.\n",
      "For Iteration 4336 the Loss is 0.2622.\n",
      "For Iteration 4337 the Loss is 0.2622.\n",
      "For Iteration 4338 the Loss is 0.2622.\n",
      "For Iteration 4339 the Loss is 0.2622.\n",
      "For Iteration 4340 the Loss is 0.2622.\n",
      "For Iteration 4341 the Loss is 0.2622.\n",
      "For Iteration 4342 the Loss is 0.2622.\n",
      "For Iteration 4343 the Loss is 0.2622.\n",
      "For Iteration 4344 the Loss is 0.2622.\n",
      "For Iteration 4345 the Loss is 0.2622.\n",
      "For Iteration 4346 the Loss is 0.2622.\n",
      "For Iteration 4347 the Loss is 0.2622.\n",
      "For Iteration 4348 the Loss is 0.2622.\n",
      "For Iteration 4349 the Loss is 0.2622.\n",
      "For Iteration 4350 the Loss is 0.2622.\n",
      "For Iteration 4351 the Loss is 0.2622.\n",
      "For Iteration 4352 the Loss is 0.2622.\n",
      "For Iteration 4353 the Loss is 0.2622.\n",
      "For Iteration 4354 the Loss is 0.2622.\n",
      "For Iteration 4355 the Loss is 0.2622.\n",
      "For Iteration 4356 the Loss is 0.2622.\n",
      "For Iteration 4357 the Loss is 0.2622.\n",
      "For Iteration 4358 the Loss is 0.2622.\n",
      "For Iteration 4359 the Loss is 0.2622.\n",
      "For Iteration 4360 the Loss is 0.2622.\n",
      "For Iteration 4361 the Loss is 0.2622.\n",
      "For Iteration 4362 the Loss is 0.2622.\n",
      "For Iteration 4363 the Loss is 0.2622.\n",
      "For Iteration 4364 the Loss is 0.2622.\n",
      "For Iteration 4365 the Loss is 0.2622.\n",
      "For Iteration 4366 the Loss is 0.2622.\n",
      "For Iteration 4367 the Loss is 0.2622.\n",
      "For Iteration 4368 the Loss is 0.2622.\n",
      "For Iteration 4369 the Loss is 0.2622.\n",
      "For Iteration 4370 the Loss is 0.2622.\n",
      "For Iteration 4371 the Loss is 0.2622.\n",
      "For Iteration 4372 the Loss is 0.2622.\n",
      "For Iteration 4373 the Loss is 0.2622.\n",
      "For Iteration 4374 the Loss is 0.2622.\n",
      "For Iteration 4375 the Loss is 0.2622.\n",
      "For Iteration 4376 the Loss is 0.2622.\n",
      "For Iteration 4377 the Loss is 0.2622.\n",
      "For Iteration 4378 the Loss is 0.2622.\n",
      "For Iteration 4379 the Loss is 0.2622.\n",
      "For Iteration 4380 the Loss is 0.2622.\n",
      "For Iteration 4381 the Loss is 0.2622.\n",
      "For Iteration 4382 the Loss is 0.2622.\n",
      "For Iteration 4383 the Loss is 0.2622.\n",
      "For Iteration 4384 the Loss is 0.2622.\n",
      "For Iteration 4385 the Loss is 0.2622.\n",
      "For Iteration 4386 the Loss is 0.2622.\n",
      "For Iteration 4387 the Loss is 0.2622.\n",
      "For Iteration 4388 the Loss is 0.2622.\n",
      "For Iteration 4389 the Loss is 0.2622.\n",
      "For Iteration 4390 the Loss is 0.2622.\n",
      "For Iteration 4391 the Loss is 0.2622.\n",
      "For Iteration 4392 the Loss is 0.2622.\n",
      "For Iteration 4393 the Loss is 0.2622.\n",
      "For Iteration 4394 the Loss is 0.2622.\n",
      "For Iteration 4395 the Loss is 0.2622.\n",
      "For Iteration 4396 the Loss is 0.2622.\n",
      "For Iteration 4397 the Loss is 0.2622.\n",
      "For Iteration 4398 the Loss is 0.2622.\n",
      "For Iteration 4399 the Loss is 0.2622.\n",
      "For Iteration 4400 the Loss is 0.2622.\n",
      "For Iteration 4401 the Loss is 0.2622.\n",
      "For Iteration 4402 the Loss is 0.2622.\n",
      "For Iteration 4403 the Loss is 0.2622.\n",
      "For Iteration 4404 the Loss is 0.2622.\n",
      "For Iteration 4405 the Loss is 0.2622.\n",
      "For Iteration 4406 the Loss is 0.2622.\n",
      "For Iteration 4407 the Loss is 0.2622.\n",
      "For Iteration 4408 the Loss is 0.2622.\n",
      "For Iteration 4409 the Loss is 0.2622.\n",
      "For Iteration 4410 the Loss is 0.2622.\n",
      "For Iteration 4411 the Loss is 0.2622.\n",
      "For Iteration 4412 the Loss is 0.2622.\n",
      "For Iteration 4413 the Loss is 0.2622.\n",
      "For Iteration 4414 the Loss is 0.2622.\n",
      "For Iteration 4415 the Loss is 0.2622.\n",
      "For Iteration 4416 the Loss is 0.2622.\n",
      "For Iteration 4417 the Loss is 0.2622.\n",
      "For Iteration 4418 the Loss is 0.2621.\n",
      "For Iteration 4419 the Loss is 0.2621.\n",
      "For Iteration 4420 the Loss is 0.2621.\n",
      "For Iteration 4421 the Loss is 0.2621.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 4422 the Loss is 0.2621.\n",
      "For Iteration 4423 the Loss is 0.2621.\n",
      "For Iteration 4424 the Loss is 0.2621.\n",
      "For Iteration 4425 the Loss is 0.2621.\n",
      "For Iteration 4426 the Loss is 0.2621.\n",
      "For Iteration 4427 the Loss is 0.2621.\n",
      "For Iteration 4428 the Loss is 0.2621.\n",
      "For Iteration 4429 the Loss is 0.2621.\n",
      "For Iteration 4430 the Loss is 0.2621.\n",
      "For Iteration 4431 the Loss is 0.2621.\n",
      "For Iteration 4432 the Loss is 0.2621.\n",
      "For Iteration 4433 the Loss is 0.2621.\n",
      "For Iteration 4434 the Loss is 0.2621.\n",
      "For Iteration 4435 the Loss is 0.2621.\n",
      "For Iteration 4436 the Loss is 0.2621.\n",
      "For Iteration 4437 the Loss is 0.2621.\n",
      "For Iteration 4438 the Loss is 0.2621.\n",
      "For Iteration 4439 the Loss is 0.2621.\n",
      "For Iteration 4440 the Loss is 0.2621.\n",
      "For Iteration 4441 the Loss is 0.2621.\n",
      "For Iteration 4442 the Loss is 0.2621.\n",
      "For Iteration 4443 the Loss is 0.2621.\n",
      "For Iteration 4444 the Loss is 0.2621.\n",
      "For Iteration 4445 the Loss is 0.2621.\n",
      "For Iteration 4446 the Loss is 0.2621.\n",
      "For Iteration 4447 the Loss is 0.2621.\n",
      "For Iteration 4448 the Loss is 0.2621.\n",
      "For Iteration 4449 the Loss is 0.2621.\n",
      "For Iteration 4450 the Loss is 0.2621.\n",
      "For Iteration 4451 the Loss is 0.2621.\n",
      "For Iteration 4452 the Loss is 0.2621.\n",
      "For Iteration 4453 the Loss is 0.2621.\n",
      "For Iteration 4454 the Loss is 0.2621.\n",
      "For Iteration 4455 the Loss is 0.2621.\n",
      "For Iteration 4456 the Loss is 0.2621.\n",
      "For Iteration 4457 the Loss is 0.2621.\n",
      "For Iteration 4458 the Loss is 0.2621.\n",
      "For Iteration 4459 the Loss is 0.2621.\n",
      "For Iteration 4460 the Loss is 0.2621.\n",
      "For Iteration 4461 the Loss is 0.2621.\n",
      "For Iteration 4462 the Loss is 0.2621.\n",
      "For Iteration 4463 the Loss is 0.2621.\n",
      "For Iteration 4464 the Loss is 0.2621.\n",
      "For Iteration 4465 the Loss is 0.2621.\n",
      "For Iteration 4466 the Loss is 0.2621.\n",
      "For Iteration 4467 the Loss is 0.2621.\n",
      "For Iteration 4468 the Loss is 0.2621.\n",
      "For Iteration 4469 the Loss is 0.2621.\n",
      "For Iteration 4470 the Loss is 0.2621.\n",
      "For Iteration 4471 the Loss is 0.2621.\n",
      "For Iteration 4472 the Loss is 0.2621.\n",
      "For Iteration 4473 the Loss is 0.2621.\n",
      "For Iteration 4474 the Loss is 0.2621.\n",
      "For Iteration 4475 the Loss is 0.2621.\n",
      "For Iteration 4476 the Loss is 0.2621.\n",
      "For Iteration 4477 the Loss is 0.2621.\n",
      "For Iteration 4478 the Loss is 0.2621.\n",
      "For Iteration 4479 the Loss is 0.2621.\n",
      "For Iteration 4480 the Loss is 0.2621.\n",
      "For Iteration 4481 the Loss is 0.2621.\n",
      "For Iteration 4482 the Loss is 0.2621.\n",
      "For Iteration 4483 the Loss is 0.2621.\n",
      "For Iteration 4484 the Loss is 0.2621.\n",
      "For Iteration 4485 the Loss is 0.2621.\n",
      "For Iteration 4486 the Loss is 0.2621.\n",
      "For Iteration 4487 the Loss is 0.2621.\n",
      "For Iteration 4488 the Loss is 0.2621.\n",
      "For Iteration 4489 the Loss is 0.2621.\n",
      "For Iteration 4490 the Loss is 0.2621.\n",
      "For Iteration 4491 the Loss is 0.2621.\n",
      "For Iteration 4492 the Loss is 0.2621.\n",
      "For Iteration 4493 the Loss is 0.2621.\n",
      "For Iteration 4494 the Loss is 0.2621.\n",
      "For Iteration 4495 the Loss is 0.2621.\n",
      "For Iteration 4496 the Loss is 0.2621.\n",
      "For Iteration 4497 the Loss is 0.2621.\n",
      "For Iteration 4498 the Loss is 0.2621.\n",
      "For Iteration 4499 the Loss is 0.2621.\n",
      "For Iteration 4500 the Loss is 0.2621.\n",
      "For Iteration 4501 the Loss is 0.2621.\n",
      "For Iteration 4502 the Loss is 0.2621.\n",
      "For Iteration 4503 the Loss is 0.2621.\n",
      "For Iteration 4504 the Loss is 0.2621.\n",
      "For Iteration 4505 the Loss is 0.2621.\n",
      "For Iteration 4506 the Loss is 0.2621.\n",
      "For Iteration 4507 the Loss is 0.2621.\n",
      "For Iteration 4508 the Loss is 0.2621.\n",
      "For Iteration 4509 the Loss is 0.2621.\n",
      "For Iteration 4510 the Loss is 0.2621.\n",
      "For Iteration 4511 the Loss is 0.2621.\n",
      "For Iteration 4512 the Loss is 0.2621.\n",
      "For Iteration 4513 the Loss is 0.2621.\n",
      "For Iteration 4514 the Loss is 0.2621.\n",
      "For Iteration 4515 the Loss is 0.2621.\n",
      "For Iteration 4516 the Loss is 0.2621.\n",
      "For Iteration 4517 the Loss is 0.2621.\n",
      "For Iteration 4518 the Loss is 0.2621.\n",
      "For Iteration 4519 the Loss is 0.2621.\n",
      "For Iteration 4520 the Loss is 0.2621.\n",
      "For Iteration 4521 the Loss is 0.2621.\n",
      "For Iteration 4522 the Loss is 0.2621.\n",
      "For Iteration 4523 the Loss is 0.2621.\n",
      "For Iteration 4524 the Loss is 0.2621.\n",
      "For Iteration 4525 the Loss is 0.2621.\n",
      "For Iteration 4526 the Loss is 0.2621.\n",
      "For Iteration 4527 the Loss is 0.2621.\n",
      "For Iteration 4528 the Loss is 0.2621.\n",
      "For Iteration 4529 the Loss is 0.2621.\n",
      "For Iteration 4530 the Loss is 0.2621.\n",
      "For Iteration 4531 the Loss is 0.2621.\n",
      "For Iteration 4532 the Loss is 0.2621.\n",
      "For Iteration 4533 the Loss is 0.2621.\n",
      "For Iteration 4534 the Loss is 0.2621.\n",
      "For Iteration 4535 the Loss is 0.2621.\n",
      "For Iteration 4536 the Loss is 0.2621.\n",
      "For Iteration 4537 the Loss is 0.2621.\n",
      "For Iteration 4538 the Loss is 0.2621.\n",
      "For Iteration 4539 the Loss is 0.2621.\n",
      "For Iteration 4540 the Loss is 0.2621.\n",
      "For Iteration 4541 the Loss is 0.2621.\n",
      "For Iteration 4542 the Loss is 0.2621.\n",
      "For Iteration 4543 the Loss is 0.2621.\n",
      "For Iteration 4544 the Loss is 0.2621.\n",
      "For Iteration 4545 the Loss is 0.2621.\n",
      "For Iteration 4546 the Loss is 0.2621.\n",
      "For Iteration 4547 the Loss is 0.2621.\n",
      "For Iteration 4548 the Loss is 0.2621.\n",
      "For Iteration 4549 the Loss is 0.2621.\n",
      "For Iteration 4550 the Loss is 0.2621.\n",
      "For Iteration 4551 the Loss is 0.2621.\n",
      "For Iteration 4552 the Loss is 0.2621.\n",
      "For Iteration 4553 the Loss is 0.2621.\n",
      "For Iteration 4554 the Loss is 0.2621.\n",
      "For Iteration 4555 the Loss is 0.2621.\n",
      "For Iteration 4556 the Loss is 0.2621.\n",
      "For Iteration 4557 the Loss is 0.2621.\n",
      "For Iteration 4558 the Loss is 0.2621.\n",
      "For Iteration 4559 the Loss is 0.2621.\n",
      "For Iteration 4560 the Loss is 0.2621.\n",
      "For Iteration 4561 the Loss is 0.2621.\n",
      "For Iteration 4562 the Loss is 0.2621.\n",
      "For Iteration 4563 the Loss is 0.2621.\n",
      "For Iteration 4564 the Loss is 0.2621.\n",
      "For Iteration 4565 the Loss is 0.2621.\n",
      "For Iteration 4566 the Loss is 0.2621.\n",
      "For Iteration 4567 the Loss is 0.2621.\n",
      "For Iteration 4568 the Loss is 0.2621.\n",
      "For Iteration 4569 the Loss is 0.2621.\n",
      "For Iteration 4570 the Loss is 0.262.\n",
      "For Iteration 4571 the Loss is 0.262.\n",
      "For Iteration 4572 the Loss is 0.262.\n",
      "For Iteration 4573 the Loss is 0.262.\n",
      "For Iteration 4574 the Loss is 0.262.\n",
      "For Iteration 4575 the Loss is 0.262.\n",
      "For Iteration 4576 the Loss is 0.262.\n",
      "For Iteration 4577 the Loss is 0.262.\n",
      "For Iteration 4578 the Loss is 0.262.\n",
      "For Iteration 4579 the Loss is 0.262.\n",
      "For Iteration 4580 the Loss is 0.262.\n",
      "For Iteration 4581 the Loss is 0.262.\n",
      "For Iteration 4582 the Loss is 0.262.\n",
      "For Iteration 4583 the Loss is 0.262.\n",
      "For Iteration 4584 the Loss is 0.262.\n",
      "For Iteration 4585 the Loss is 0.262.\n",
      "For Iteration 4586 the Loss is 0.262.\n",
      "For Iteration 4587 the Loss is 0.262.\n",
      "For Iteration 4588 the Loss is 0.262.\n",
      "For Iteration 4589 the Loss is 0.262.\n",
      "For Iteration 4590 the Loss is 0.262.\n",
      "For Iteration 4591 the Loss is 0.262.\n",
      "For Iteration 4592 the Loss is 0.262.\n",
      "For Iteration 4593 the Loss is 0.262.\n",
      "For Iteration 4594 the Loss is 0.262.\n",
      "For Iteration 4595 the Loss is 0.262.\n",
      "For Iteration 4596 the Loss is 0.262.\n",
      "For Iteration 4597 the Loss is 0.262.\n",
      "For Iteration 4598 the Loss is 0.262.\n",
      "For Iteration 4599 the Loss is 0.262.\n",
      "For Iteration 4600 the Loss is 0.262.\n",
      "For Iteration 4601 the Loss is 0.262.\n",
      "For Iteration 4602 the Loss is 0.262.\n",
      "For Iteration 4603 the Loss is 0.262.\n",
      "For Iteration 4604 the Loss is 0.262.\n",
      "For Iteration 4605 the Loss is 0.262.\n",
      "For Iteration 4606 the Loss is 0.262.\n",
      "For Iteration 4607 the Loss is 0.262.\n",
      "For Iteration 4608 the Loss is 0.262.\n",
      "For Iteration 4609 the Loss is 0.262.\n",
      "For Iteration 4610 the Loss is 0.262.\n",
      "For Iteration 4611 the Loss is 0.262.\n",
      "For Iteration 4612 the Loss is 0.262.\n",
      "For Iteration 4613 the Loss is 0.262.\n",
      "For Iteration 4614 the Loss is 0.262.\n",
      "For Iteration 4615 the Loss is 0.262.\n",
      "For Iteration 4616 the Loss is 0.262.\n",
      "For Iteration 4617 the Loss is 0.262.\n",
      "For Iteration 4618 the Loss is 0.262.\n",
      "For Iteration 4619 the Loss is 0.262.\n",
      "For Iteration 4620 the Loss is 0.262.\n",
      "For Iteration 4621 the Loss is 0.262.\n",
      "For Iteration 4622 the Loss is 0.262.\n",
      "For Iteration 4623 the Loss is 0.262.\n",
      "For Iteration 4624 the Loss is 0.262.\n",
      "For Iteration 4625 the Loss is 0.262.\n",
      "For Iteration 4626 the Loss is 0.262.\n",
      "For Iteration 4627 the Loss is 0.262.\n",
      "For Iteration 4628 the Loss is 0.262.\n",
      "For Iteration 4629 the Loss is 0.262.\n",
      "For Iteration 4630 the Loss is 0.262.\n",
      "For Iteration 4631 the Loss is 0.262.\n",
      "For Iteration 4632 the Loss is 0.262.\n",
      "For Iteration 4633 the Loss is 0.262.\n",
      "For Iteration 4634 the Loss is 0.262.\n",
      "For Iteration 4635 the Loss is 0.262.\n",
      "For Iteration 4636 the Loss is 0.262.\n",
      "For Iteration 4637 the Loss is 0.262.\n",
      "For Iteration 4638 the Loss is 0.262.\n",
      "For Iteration 4639 the Loss is 0.262.\n",
      "For Iteration 4640 the Loss is 0.262.\n",
      "For Iteration 4641 the Loss is 0.262.\n",
      "For Iteration 4642 the Loss is 0.262.\n",
      "For Iteration 4643 the Loss is 0.262.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 4644 the Loss is 0.262.\n",
      "For Iteration 4645 the Loss is 0.262.\n",
      "For Iteration 4646 the Loss is 0.262.\n",
      "For Iteration 4647 the Loss is 0.262.\n",
      "For Iteration 4648 the Loss is 0.262.\n",
      "For Iteration 4649 the Loss is 0.262.\n",
      "For Iteration 4650 the Loss is 0.262.\n",
      "For Iteration 4651 the Loss is 0.262.\n",
      "For Iteration 4652 the Loss is 0.262.\n",
      "For Iteration 4653 the Loss is 0.262.\n",
      "For Iteration 4654 the Loss is 0.262.\n",
      "For Iteration 4655 the Loss is 0.262.\n",
      "For Iteration 4656 the Loss is 0.262.\n",
      "For Iteration 4657 the Loss is 0.262.\n",
      "For Iteration 4658 the Loss is 0.262.\n",
      "For Iteration 4659 the Loss is 0.262.\n",
      "For Iteration 4660 the Loss is 0.262.\n",
      "For Iteration 4661 the Loss is 0.262.\n",
      "For Iteration 4662 the Loss is 0.262.\n",
      "For Iteration 4663 the Loss is 0.262.\n",
      "For Iteration 4664 the Loss is 0.262.\n",
      "For Iteration 4665 the Loss is 0.262.\n",
      "For Iteration 4666 the Loss is 0.262.\n",
      "For Iteration 4667 the Loss is 0.262.\n",
      "For Iteration 4668 the Loss is 0.262.\n",
      "For Iteration 4669 the Loss is 0.262.\n",
      "For Iteration 4670 the Loss is 0.262.\n",
      "For Iteration 4671 the Loss is 0.262.\n",
      "For Iteration 4672 the Loss is 0.262.\n",
      "For Iteration 4673 the Loss is 0.262.\n",
      "For Iteration 4674 the Loss is 0.262.\n",
      "For Iteration 4675 the Loss is 0.262.\n",
      "For Iteration 4676 the Loss is 0.262.\n",
      "For Iteration 4677 the Loss is 0.262.\n",
      "For Iteration 4678 the Loss is 0.262.\n",
      "For Iteration 4679 the Loss is 0.262.\n",
      "For Iteration 4680 the Loss is 0.262.\n",
      "For Iteration 4681 the Loss is 0.262.\n",
      "For Iteration 4682 the Loss is 0.262.\n",
      "For Iteration 4683 the Loss is 0.262.\n",
      "For Iteration 4684 the Loss is 0.262.\n",
      "For Iteration 4685 the Loss is 0.262.\n",
      "For Iteration 4686 the Loss is 0.262.\n",
      "For Iteration 4687 the Loss is 0.262.\n",
      "For Iteration 4688 the Loss is 0.262.\n",
      "For Iteration 4689 the Loss is 0.262.\n",
      "For Iteration 4690 the Loss is 0.262.\n",
      "For Iteration 4691 the Loss is 0.262.\n",
      "For Iteration 4692 the Loss is 0.262.\n",
      "For Iteration 4693 the Loss is 0.262.\n",
      "For Iteration 4694 the Loss is 0.262.\n",
      "For Iteration 4695 the Loss is 0.262.\n",
      "For Iteration 4696 the Loss is 0.262.\n",
      "For Iteration 4697 the Loss is 0.262.\n",
      "For Iteration 4698 the Loss is 0.262.\n",
      "For Iteration 4699 the Loss is 0.262.\n",
      "For Iteration 4700 the Loss is 0.262.\n",
      "For Iteration 4701 the Loss is 0.262.\n",
      "For Iteration 4702 the Loss is 0.262.\n",
      "For Iteration 4703 the Loss is 0.262.\n",
      "For Iteration 4704 the Loss is 0.262.\n",
      "For Iteration 4705 the Loss is 0.262.\n",
      "For Iteration 4706 the Loss is 0.262.\n",
      "For Iteration 4707 the Loss is 0.262.\n",
      "For Iteration 4708 the Loss is 0.262.\n",
      "For Iteration 4709 the Loss is 0.262.\n",
      "For Iteration 4710 the Loss is 0.262.\n",
      "For Iteration 4711 the Loss is 0.262.\n",
      "For Iteration 4712 the Loss is 0.262.\n",
      "For Iteration 4713 the Loss is 0.262.\n",
      "For Iteration 4714 the Loss is 0.262.\n",
      "For Iteration 4715 the Loss is 0.262.\n",
      "For Iteration 4716 the Loss is 0.262.\n",
      "For Iteration 4717 the Loss is 0.262.\n",
      "For Iteration 4718 the Loss is 0.262.\n",
      "For Iteration 4719 the Loss is 0.262.\n",
      "For Iteration 4720 the Loss is 0.262.\n",
      "For Iteration 4721 the Loss is 0.262.\n",
      "For Iteration 4722 the Loss is 0.262.\n",
      "For Iteration 4723 the Loss is 0.262.\n",
      "For Iteration 4724 the Loss is 0.262.\n",
      "For Iteration 4725 the Loss is 0.262.\n",
      "For Iteration 4726 the Loss is 0.262.\n",
      "For Iteration 4727 the Loss is 0.262.\n",
      "For Iteration 4728 the Loss is 0.262.\n",
      "For Iteration 4729 the Loss is 0.262.\n",
      "For Iteration 4730 the Loss is 0.2619.\n",
      "For Iteration 4731 the Loss is 0.2619.\n",
      "For Iteration 4732 the Loss is 0.2619.\n",
      "For Iteration 4733 the Loss is 0.2619.\n",
      "For Iteration 4734 the Loss is 0.2619.\n",
      "For Iteration 4735 the Loss is 0.2619.\n",
      "For Iteration 4736 the Loss is 0.2619.\n",
      "For Iteration 4737 the Loss is 0.2619.\n",
      "For Iteration 4738 the Loss is 0.2619.\n",
      "For Iteration 4739 the Loss is 0.2619.\n",
      "For Iteration 4740 the Loss is 0.2619.\n",
      "For Iteration 4741 the Loss is 0.2619.\n",
      "For Iteration 4742 the Loss is 0.2619.\n",
      "For Iteration 4743 the Loss is 0.2619.\n",
      "For Iteration 4744 the Loss is 0.2619.\n",
      "For Iteration 4745 the Loss is 0.2619.\n",
      "For Iteration 4746 the Loss is 0.2619.\n",
      "For Iteration 4747 the Loss is 0.2619.\n",
      "For Iteration 4748 the Loss is 0.2619.\n",
      "For Iteration 4749 the Loss is 0.2619.\n",
      "For Iteration 4750 the Loss is 0.2619.\n",
      "For Iteration 4751 the Loss is 0.2619.\n",
      "For Iteration 4752 the Loss is 0.2619.\n",
      "For Iteration 4753 the Loss is 0.2619.\n",
      "For Iteration 4754 the Loss is 0.2619.\n",
      "For Iteration 4755 the Loss is 0.2619.\n",
      "For Iteration 4756 the Loss is 0.2619.\n",
      "For Iteration 4757 the Loss is 0.2619.\n",
      "For Iteration 4758 the Loss is 0.2619.\n",
      "For Iteration 4759 the Loss is 0.2619.\n",
      "For Iteration 4760 the Loss is 0.2619.\n",
      "For Iteration 4761 the Loss is 0.2619.\n",
      "For Iteration 4762 the Loss is 0.2619.\n",
      "For Iteration 4763 the Loss is 0.2619.\n",
      "For Iteration 4764 the Loss is 0.2619.\n",
      "For Iteration 4765 the Loss is 0.2619.\n",
      "For Iteration 4766 the Loss is 0.2619.\n",
      "For Iteration 4767 the Loss is 0.2619.\n",
      "For Iteration 4768 the Loss is 0.2619.\n",
      "For Iteration 4769 the Loss is 0.2619.\n",
      "For Iteration 4770 the Loss is 0.2619.\n",
      "For Iteration 4771 the Loss is 0.2619.\n",
      "For Iteration 4772 the Loss is 0.2619.\n",
      "For Iteration 4773 the Loss is 0.2619.\n",
      "For Iteration 4774 the Loss is 0.2619.\n",
      "For Iteration 4775 the Loss is 0.2619.\n",
      "For Iteration 4776 the Loss is 0.2619.\n",
      "For Iteration 4777 the Loss is 0.2619.\n",
      "For Iteration 4778 the Loss is 0.2619.\n",
      "For Iteration 4779 the Loss is 0.2619.\n",
      "For Iteration 4780 the Loss is 0.2619.\n",
      "For Iteration 4781 the Loss is 0.2619.\n",
      "For Iteration 4782 the Loss is 0.2619.\n",
      "For Iteration 4783 the Loss is 0.2619.\n",
      "For Iteration 4784 the Loss is 0.2619.\n",
      "For Iteration 4785 the Loss is 0.2619.\n",
      "For Iteration 4786 the Loss is 0.2619.\n",
      "For Iteration 4787 the Loss is 0.2619.\n",
      "For Iteration 4788 the Loss is 0.2619.\n",
      "For Iteration 4789 the Loss is 0.2619.\n",
      "For Iteration 4790 the Loss is 0.2619.\n",
      "For Iteration 4791 the Loss is 0.2619.\n",
      "For Iteration 4792 the Loss is 0.2619.\n",
      "For Iteration 4793 the Loss is 0.2619.\n",
      "For Iteration 4794 the Loss is 0.2619.\n",
      "For Iteration 4795 the Loss is 0.2619.\n",
      "For Iteration 4796 the Loss is 0.2619.\n",
      "For Iteration 4797 the Loss is 0.2619.\n",
      "For Iteration 4798 the Loss is 0.2619.\n",
      "For Iteration 4799 the Loss is 0.2619.\n",
      "For Iteration 4800 the Loss is 0.2619.\n",
      "For Iteration 4801 the Loss is 0.2619.\n",
      "For Iteration 4802 the Loss is 0.2619.\n",
      "For Iteration 4803 the Loss is 0.2619.\n",
      "For Iteration 4804 the Loss is 0.2619.\n",
      "For Iteration 4805 the Loss is 0.2619.\n",
      "For Iteration 4806 the Loss is 0.2619.\n",
      "For Iteration 4807 the Loss is 0.2619.\n",
      "For Iteration 4808 the Loss is 0.2619.\n",
      "For Iteration 4809 the Loss is 0.2619.\n",
      "For Iteration 4810 the Loss is 0.2619.\n",
      "For Iteration 4811 the Loss is 0.2619.\n",
      "For Iteration 4812 the Loss is 0.2619.\n",
      "For Iteration 4813 the Loss is 0.2619.\n",
      "For Iteration 4814 the Loss is 0.2619.\n",
      "For Iteration 4815 the Loss is 0.2619.\n",
      "For Iteration 4816 the Loss is 0.2619.\n",
      "For Iteration 4817 the Loss is 0.2619.\n",
      "For Iteration 4818 the Loss is 0.2619.\n",
      "For Iteration 4819 the Loss is 0.2619.\n",
      "For Iteration 4820 the Loss is 0.2619.\n",
      "For Iteration 4821 the Loss is 0.2619.\n",
      "For Iteration 4822 the Loss is 0.2619.\n",
      "For Iteration 4823 the Loss is 0.2619.\n",
      "For Iteration 4824 the Loss is 0.2619.\n",
      "For Iteration 4825 the Loss is 0.2619.\n",
      "For Iteration 4826 the Loss is 0.2619.\n",
      "For Iteration 4827 the Loss is 0.2619.\n",
      "For Iteration 4828 the Loss is 0.2619.\n",
      "For Iteration 4829 the Loss is 0.2619.\n",
      "For Iteration 4830 the Loss is 0.2619.\n",
      "For Iteration 4831 the Loss is 0.2619.\n",
      "For Iteration 4832 the Loss is 0.2619.\n",
      "For Iteration 4833 the Loss is 0.2619.\n",
      "For Iteration 4834 the Loss is 0.2619.\n",
      "For Iteration 4835 the Loss is 0.2619.\n",
      "For Iteration 4836 the Loss is 0.2619.\n",
      "For Iteration 4837 the Loss is 0.2619.\n",
      "For Iteration 4838 the Loss is 0.2619.\n",
      "For Iteration 4839 the Loss is 0.2619.\n",
      "For Iteration 4840 the Loss is 0.2619.\n",
      "For Iteration 4841 the Loss is 0.2619.\n",
      "For Iteration 4842 the Loss is 0.2619.\n",
      "For Iteration 4843 the Loss is 0.2619.\n",
      "For Iteration 4844 the Loss is 0.2619.\n",
      "For Iteration 4845 the Loss is 0.2619.\n",
      "For Iteration 4846 the Loss is 0.2619.\n",
      "For Iteration 4847 the Loss is 0.2619.\n",
      "For Iteration 4848 the Loss is 0.2619.\n",
      "For Iteration 4849 the Loss is 0.2619.\n",
      "For Iteration 4850 the Loss is 0.2619.\n",
      "For Iteration 4851 the Loss is 0.2619.\n",
      "For Iteration 4852 the Loss is 0.2619.\n",
      "For Iteration 4853 the Loss is 0.2619.\n",
      "For Iteration 4854 the Loss is 0.2619.\n",
      "For Iteration 4855 the Loss is 0.2619.\n",
      "For Iteration 4856 the Loss is 0.2619.\n",
      "For Iteration 4857 the Loss is 0.2619.\n",
      "For Iteration 4858 the Loss is 0.2619.\n",
      "For Iteration 4859 the Loss is 0.2619.\n",
      "For Iteration 4860 the Loss is 0.2619.\n",
      "For Iteration 4861 the Loss is 0.2619.\n",
      "For Iteration 4862 the Loss is 0.2619.\n",
      "For Iteration 4863 the Loss is 0.2619.\n",
      "For Iteration 4864 the Loss is 0.2619.\n",
      "For Iteration 4865 the Loss is 0.2619.\n",
      "For Iteration 4866 the Loss is 0.2619.\n",
      "For Iteration 4867 the Loss is 0.2619.\n",
      "For Iteration 4868 the Loss is 0.2619.\n",
      "For Iteration 4869 the Loss is 0.2619.\n",
      "For Iteration 4870 the Loss is 0.2619.\n",
      "For Iteration 4871 the Loss is 0.2619.\n",
      "For Iteration 4872 the Loss is 0.2619.\n",
      "For Iteration 4873 the Loss is 0.2619.\n",
      "For Iteration 4874 the Loss is 0.2619.\n",
      "For Iteration 4875 the Loss is 0.2619.\n",
      "For Iteration 4876 the Loss is 0.2619.\n",
      "For Iteration 4877 the Loss is 0.2619.\n",
      "For Iteration 4878 the Loss is 0.2619.\n",
      "For Iteration 4879 the Loss is 0.2619.\n",
      "For Iteration 4880 the Loss is 0.2619.\n",
      "For Iteration 4881 the Loss is 0.2619.\n",
      "For Iteration 4882 the Loss is 0.2619.\n",
      "For Iteration 4883 the Loss is 0.2619.\n",
      "For Iteration 4884 the Loss is 0.2619.\n",
      "For Iteration 4885 the Loss is 0.2619.\n",
      "For Iteration 4886 the Loss is 0.2619.\n",
      "For Iteration 4887 the Loss is 0.2619.\n",
      "For Iteration 4888 the Loss is 0.2619.\n",
      "For Iteration 4889 the Loss is 0.2619.\n",
      "For Iteration 4890 the Loss is 0.2619.\n",
      "For Iteration 4891 the Loss is 0.2619.\n",
      "For Iteration 4892 the Loss is 0.2619.\n",
      "For Iteration 4893 the Loss is 0.2619.\n",
      "For Iteration 4894 the Loss is 0.2619.\n",
      "For Iteration 4895 the Loss is 0.2619.\n",
      "For Iteration 4896 the Loss is 0.2619.\n",
      "For Iteration 4897 the Loss is 0.2619.\n",
      "For Iteration 4898 the Loss is 0.2619.\n",
      "For Iteration 4899 the Loss is 0.2619.\n",
      "For Iteration 4900 the Loss is 0.2619.\n",
      "For Iteration 4901 the Loss is 0.2618.\n",
      "For Iteration 4902 the Loss is 0.2618.\n",
      "For Iteration 4903 the Loss is 0.2618.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 4904 the Loss is 0.2618.\n",
      "For Iteration 4905 the Loss is 0.2618.\n",
      "For Iteration 4906 the Loss is 0.2618.\n",
      "For Iteration 4907 the Loss is 0.2618.\n",
      "For Iteration 4908 the Loss is 0.2618.\n",
      "For Iteration 4909 the Loss is 0.2618.\n",
      "For Iteration 4910 the Loss is 0.2618.\n",
      "For Iteration 4911 the Loss is 0.2618.\n",
      "For Iteration 4912 the Loss is 0.2618.\n",
      "For Iteration 4913 the Loss is 0.2618.\n",
      "For Iteration 4914 the Loss is 0.2618.\n",
      "For Iteration 4915 the Loss is 0.2618.\n",
      "For Iteration 4916 the Loss is 0.2618.\n",
      "For Iteration 4917 the Loss is 0.2618.\n",
      "For Iteration 4918 the Loss is 0.2618.\n",
      "For Iteration 4919 the Loss is 0.2618.\n",
      "For Iteration 4920 the Loss is 0.2618.\n",
      "For Iteration 4921 the Loss is 0.2618.\n",
      "For Iteration 4922 the Loss is 0.2618.\n",
      "For Iteration 4923 the Loss is 0.2618.\n",
      "For Iteration 4924 the Loss is 0.2618.\n",
      "For Iteration 4925 the Loss is 0.2618.\n",
      "For Iteration 4926 the Loss is 0.2618.\n",
      "For Iteration 4927 the Loss is 0.2618.\n",
      "For Iteration 4928 the Loss is 0.2618.\n",
      "For Iteration 4929 the Loss is 0.2618.\n",
      "For Iteration 4930 the Loss is 0.2618.\n",
      "For Iteration 4931 the Loss is 0.2618.\n",
      "For Iteration 4932 the Loss is 0.2618.\n",
      "For Iteration 4933 the Loss is 0.2618.\n",
      "For Iteration 4934 the Loss is 0.2618.\n",
      "For Iteration 4935 the Loss is 0.2618.\n",
      "For Iteration 4936 the Loss is 0.2618.\n",
      "For Iteration 4937 the Loss is 0.2618.\n",
      "For Iteration 4938 the Loss is 0.2618.\n",
      "For Iteration 4939 the Loss is 0.2618.\n",
      "For Iteration 4940 the Loss is 0.2618.\n",
      "For Iteration 4941 the Loss is 0.2618.\n",
      "For Iteration 4942 the Loss is 0.2618.\n",
      "For Iteration 4943 the Loss is 0.2618.\n",
      "For Iteration 4944 the Loss is 0.2618.\n",
      "For Iteration 4945 the Loss is 0.2618.\n",
      "For Iteration 4946 the Loss is 0.2618.\n",
      "For Iteration 4947 the Loss is 0.2618.\n",
      "For Iteration 4948 the Loss is 0.2618.\n",
      "For Iteration 4949 the Loss is 0.2618.\n",
      "For Iteration 4950 the Loss is 0.2618.\n",
      "For Iteration 4951 the Loss is 0.2618.\n",
      "For Iteration 4952 the Loss is 0.2618.\n",
      "For Iteration 4953 the Loss is 0.2618.\n",
      "For Iteration 4954 the Loss is 0.2618.\n",
      "For Iteration 4955 the Loss is 0.2618.\n",
      "For Iteration 4956 the Loss is 0.2618.\n",
      "For Iteration 4957 the Loss is 0.2618.\n",
      "For Iteration 4958 the Loss is 0.2618.\n",
      "For Iteration 4959 the Loss is 0.2618.\n",
      "For Iteration 4960 the Loss is 0.2618.\n",
      "For Iteration 4961 the Loss is 0.2618.\n",
      "For Iteration 4962 the Loss is 0.2618.\n",
      "For Iteration 4963 the Loss is 0.2618.\n",
      "For Iteration 4964 the Loss is 0.2618.\n",
      "For Iteration 4965 the Loss is 0.2618.\n",
      "For Iteration 4966 the Loss is 0.2618.\n",
      "For Iteration 4967 the Loss is 0.2618.\n",
      "For Iteration 4968 the Loss is 0.2618.\n",
      "For Iteration 4969 the Loss is 0.2618.\n",
      "For Iteration 4970 the Loss is 0.2618.\n",
      "For Iteration 4971 the Loss is 0.2618.\n",
      "For Iteration 4972 the Loss is 0.2618.\n",
      "For Iteration 4973 the Loss is 0.2618.\n",
      "For Iteration 4974 the Loss is 0.2618.\n",
      "For Iteration 4975 the Loss is 0.2618.\n",
      "For Iteration 4976 the Loss is 0.2618.\n",
      "For Iteration 4977 the Loss is 0.2618.\n",
      "For Iteration 4978 the Loss is 0.2618.\n",
      "For Iteration 4979 the Loss is 0.2618.\n",
      "For Iteration 4980 the Loss is 0.2618.\n",
      "For Iteration 4981 the Loss is 0.2618.\n",
      "For Iteration 4982 the Loss is 0.2618.\n",
      "For Iteration 4983 the Loss is 0.2618.\n",
      "For Iteration 4984 the Loss is 0.2618.\n",
      "For Iteration 4985 the Loss is 0.2618.\n",
      "For Iteration 4986 the Loss is 0.2618.\n",
      "For Iteration 4987 the Loss is 0.2618.\n",
      "For Iteration 4988 the Loss is 0.2618.\n",
      "For Iteration 4989 the Loss is 0.2618.\n",
      "For Iteration 4990 the Loss is 0.2618.\n",
      "For Iteration 4991 the Loss is 0.2618.\n",
      "For Iteration 4992 the Loss is 0.2618.\n",
      "For Iteration 4993 the Loss is 0.2618.\n",
      "For Iteration 4994 the Loss is 0.2618.\n",
      "For Iteration 4995 the Loss is 0.2618.\n",
      "For Iteration 4996 the Loss is 0.2618.\n",
      "For Iteration 4997 the Loss is 0.2618.\n",
      "For Iteration 4998 the Loss is 0.2618.\n",
      "For Iteration 4999 the Loss is 0.2618.\n",
      "For Iteration 5000 the Loss is 0.2618.\n",
      "For Iteration 5001 the Loss is 0.2618.\n",
      "For Iteration 5002 the Loss is 0.2618.\n",
      "For Iteration 5003 the Loss is 0.2618.\n",
      "For Iteration 5004 the Loss is 0.2618.\n",
      "For Iteration 5005 the Loss is 0.2618.\n",
      "For Iteration 5006 the Loss is 0.2618.\n",
      "For Iteration 5007 the Loss is 0.2618.\n",
      "For Iteration 5008 the Loss is 0.2618.\n",
      "For Iteration 5009 the Loss is 0.2618.\n",
      "For Iteration 5010 the Loss is 0.2618.\n",
      "For Iteration 5011 the Loss is 0.2618.\n",
      "For Iteration 5012 the Loss is 0.2618.\n",
      "For Iteration 5013 the Loss is 0.2618.\n",
      "For Iteration 5014 the Loss is 0.2618.\n",
      "For Iteration 5015 the Loss is 0.2618.\n",
      "For Iteration 5016 the Loss is 0.2618.\n",
      "For Iteration 5017 the Loss is 0.2618.\n",
      "For Iteration 5018 the Loss is 0.2618.\n",
      "For Iteration 5019 the Loss is 0.2618.\n",
      "For Iteration 5020 the Loss is 0.2618.\n",
      "For Iteration 5021 the Loss is 0.2618.\n",
      "For Iteration 5022 the Loss is 0.2618.\n",
      "For Iteration 5023 the Loss is 0.2618.\n",
      "For Iteration 5024 the Loss is 0.2618.\n",
      "For Iteration 5025 the Loss is 0.2618.\n",
      "For Iteration 5026 the Loss is 0.2618.\n",
      "For Iteration 5027 the Loss is 0.2618.\n",
      "For Iteration 5028 the Loss is 0.2618.\n",
      "For Iteration 5029 the Loss is 0.2618.\n",
      "For Iteration 5030 the Loss is 0.2618.\n",
      "For Iteration 5031 the Loss is 0.2618.\n",
      "For Iteration 5032 the Loss is 0.2618.\n",
      "For Iteration 5033 the Loss is 0.2618.\n",
      "For Iteration 5034 the Loss is 0.2618.\n",
      "For Iteration 5035 the Loss is 0.2618.\n",
      "For Iteration 5036 the Loss is 0.2618.\n",
      "For Iteration 5037 the Loss is 0.2618.\n",
      "For Iteration 5038 the Loss is 0.2618.\n",
      "For Iteration 5039 the Loss is 0.2618.\n",
      "For Iteration 5040 the Loss is 0.2618.\n",
      "For Iteration 5041 the Loss is 0.2618.\n",
      "For Iteration 5042 the Loss is 0.2618.\n",
      "For Iteration 5043 the Loss is 0.2618.\n",
      "For Iteration 5044 the Loss is 0.2618.\n",
      "For Iteration 5045 the Loss is 0.2618.\n",
      "For Iteration 5046 the Loss is 0.2618.\n",
      "For Iteration 5047 the Loss is 0.2618.\n",
      "For Iteration 5048 the Loss is 0.2618.\n",
      "For Iteration 5049 the Loss is 0.2618.\n",
      "For Iteration 5050 the Loss is 0.2618.\n",
      "For Iteration 5051 the Loss is 0.2618.\n",
      "For Iteration 5052 the Loss is 0.2618.\n",
      "For Iteration 5053 the Loss is 0.2618.\n",
      "For Iteration 5054 the Loss is 0.2618.\n",
      "For Iteration 5055 the Loss is 0.2618.\n",
      "For Iteration 5056 the Loss is 0.2618.\n",
      "For Iteration 5057 the Loss is 0.2618.\n",
      "For Iteration 5058 the Loss is 0.2618.\n",
      "For Iteration 5059 the Loss is 0.2618.\n",
      "For Iteration 5060 the Loss is 0.2618.\n",
      "For Iteration 5061 the Loss is 0.2618.\n",
      "For Iteration 5062 the Loss is 0.2618.\n",
      "For Iteration 5063 the Loss is 0.2618.\n",
      "For Iteration 5064 the Loss is 0.2618.\n",
      "For Iteration 5065 the Loss is 0.2618.\n",
      "For Iteration 5066 the Loss is 0.2618.\n",
      "For Iteration 5067 the Loss is 0.2618.\n",
      "For Iteration 5068 the Loss is 0.2618.\n",
      "For Iteration 5069 the Loss is 0.2618.\n",
      "For Iteration 5070 the Loss is 0.2618.\n",
      "For Iteration 5071 the Loss is 0.2618.\n",
      "For Iteration 5072 the Loss is 0.2618.\n",
      "For Iteration 5073 the Loss is 0.2618.\n",
      "For Iteration 5074 the Loss is 0.2618.\n",
      "For Iteration 5075 the Loss is 0.2618.\n",
      "For Iteration 5076 the Loss is 0.2618.\n",
      "For Iteration 5077 the Loss is 0.2618.\n",
      "For Iteration 5078 the Loss is 0.2618.\n",
      "For Iteration 5079 the Loss is 0.2618.\n",
      "For Iteration 5080 the Loss is 0.2618.\n",
      "For Iteration 5081 the Loss is 0.2618.\n",
      "For Iteration 5082 the Loss is 0.2617.\n",
      "For Iteration 5083 the Loss is 0.2617.\n",
      "For Iteration 5084 the Loss is 0.2617.\n",
      "For Iteration 5085 the Loss is 0.2617.\n",
      "For Iteration 5086 the Loss is 0.2617.\n",
      "For Iteration 5087 the Loss is 0.2617.\n",
      "For Iteration 5088 the Loss is 0.2617.\n",
      "For Iteration 5089 the Loss is 0.2617.\n",
      "For Iteration 5090 the Loss is 0.2617.\n",
      "For Iteration 5091 the Loss is 0.2617.\n",
      "For Iteration 5092 the Loss is 0.2617.\n",
      "For Iteration 5093 the Loss is 0.2617.\n",
      "For Iteration 5094 the Loss is 0.2617.\n",
      "For Iteration 5095 the Loss is 0.2617.\n",
      "For Iteration 5096 the Loss is 0.2617.\n",
      "For Iteration 5097 the Loss is 0.2617.\n",
      "For Iteration 5098 the Loss is 0.2617.\n",
      "For Iteration 5099 the Loss is 0.2617.\n",
      "For Iteration 5100 the Loss is 0.2617.\n",
      "For Iteration 5101 the Loss is 0.2617.\n",
      "For Iteration 5102 the Loss is 0.2617.\n",
      "For Iteration 5103 the Loss is 0.2617.\n",
      "For Iteration 5104 the Loss is 0.2617.\n",
      "For Iteration 5105 the Loss is 0.2617.\n",
      "For Iteration 5106 the Loss is 0.2617.\n",
      "For Iteration 5107 the Loss is 0.2617.\n",
      "For Iteration 5108 the Loss is 0.2617.\n",
      "For Iteration 5109 the Loss is 0.2617.\n",
      "For Iteration 5110 the Loss is 0.2617.\n",
      "For Iteration 5111 the Loss is 0.2617.\n",
      "For Iteration 5112 the Loss is 0.2617.\n",
      "For Iteration 5113 the Loss is 0.2617.\n",
      "For Iteration 5114 the Loss is 0.2617.\n",
      "For Iteration 5115 the Loss is 0.2617.\n",
      "For Iteration 5116 the Loss is 0.2617.\n",
      "For Iteration 5117 the Loss is 0.2617.\n",
      "For Iteration 5118 the Loss is 0.2617.\n",
      "For Iteration 5119 the Loss is 0.2617.\n",
      "For Iteration 5120 the Loss is 0.2617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 5121 the Loss is 0.2617.\n",
      "For Iteration 5122 the Loss is 0.2617.\n",
      "For Iteration 5123 the Loss is 0.2617.\n",
      "For Iteration 5124 the Loss is 0.2617.\n",
      "For Iteration 5125 the Loss is 0.2617.\n",
      "For Iteration 5126 the Loss is 0.2617.\n",
      "For Iteration 5127 the Loss is 0.2617.\n",
      "For Iteration 5128 the Loss is 0.2617.\n",
      "For Iteration 5129 the Loss is 0.2617.\n",
      "For Iteration 5130 the Loss is 0.2617.\n",
      "For Iteration 5131 the Loss is 0.2617.\n",
      "For Iteration 5132 the Loss is 0.2617.\n",
      "For Iteration 5133 the Loss is 0.2617.\n",
      "For Iteration 5134 the Loss is 0.2617.\n",
      "For Iteration 5135 the Loss is 0.2617.\n",
      "For Iteration 5136 the Loss is 0.2617.\n",
      "For Iteration 5137 the Loss is 0.2617.\n",
      "For Iteration 5138 the Loss is 0.2617.\n",
      "For Iteration 5139 the Loss is 0.2617.\n",
      "For Iteration 5140 the Loss is 0.2617.\n",
      "For Iteration 5141 the Loss is 0.2617.\n",
      "For Iteration 5142 the Loss is 0.2617.\n",
      "For Iteration 5143 the Loss is 0.2617.\n",
      "For Iteration 5144 the Loss is 0.2617.\n",
      "For Iteration 5145 the Loss is 0.2617.\n",
      "For Iteration 5146 the Loss is 0.2617.\n",
      "For Iteration 5147 the Loss is 0.2617.\n",
      "For Iteration 5148 the Loss is 0.2617.\n",
      "For Iteration 5149 the Loss is 0.2617.\n",
      "For Iteration 5150 the Loss is 0.2617.\n",
      "For Iteration 5151 the Loss is 0.2617.\n",
      "For Iteration 5152 the Loss is 0.2617.\n",
      "For Iteration 5153 the Loss is 0.2617.\n",
      "For Iteration 5154 the Loss is 0.2617.\n",
      "For Iteration 5155 the Loss is 0.2617.\n",
      "For Iteration 5156 the Loss is 0.2617.\n",
      "For Iteration 5157 the Loss is 0.2617.\n",
      "For Iteration 5158 the Loss is 0.2617.\n",
      "For Iteration 5159 the Loss is 0.2617.\n",
      "For Iteration 5160 the Loss is 0.2617.\n",
      "For Iteration 5161 the Loss is 0.2617.\n",
      "For Iteration 5162 the Loss is 0.2617.\n",
      "For Iteration 5163 the Loss is 0.2617.\n",
      "For Iteration 5164 the Loss is 0.2617.\n",
      "For Iteration 5165 the Loss is 0.2617.\n",
      "For Iteration 5166 the Loss is 0.2617.\n",
      "For Iteration 5167 the Loss is 0.2617.\n",
      "For Iteration 5168 the Loss is 0.2617.\n",
      "For Iteration 5169 the Loss is 0.2617.\n",
      "For Iteration 5170 the Loss is 0.2617.\n",
      "For Iteration 5171 the Loss is 0.2617.\n",
      "For Iteration 5172 the Loss is 0.2617.\n",
      "For Iteration 5173 the Loss is 0.2617.\n",
      "For Iteration 5174 the Loss is 0.2617.\n",
      "For Iteration 5175 the Loss is 0.2617.\n",
      "For Iteration 5176 the Loss is 0.2617.\n",
      "For Iteration 5177 the Loss is 0.2617.\n",
      "For Iteration 5178 the Loss is 0.2617.\n",
      "For Iteration 5179 the Loss is 0.2617.\n",
      "For Iteration 5180 the Loss is 0.2617.\n",
      "For Iteration 5181 the Loss is 0.2617.\n",
      "For Iteration 5182 the Loss is 0.2617.\n",
      "For Iteration 5183 the Loss is 0.2617.\n",
      "For Iteration 5184 the Loss is 0.2617.\n",
      "For Iteration 5185 the Loss is 0.2617.\n",
      "For Iteration 5186 the Loss is 0.2617.\n",
      "For Iteration 5187 the Loss is 0.2617.\n",
      "For Iteration 5188 the Loss is 0.2617.\n",
      "For Iteration 5189 the Loss is 0.2617.\n",
      "For Iteration 5190 the Loss is 0.2617.\n",
      "For Iteration 5191 the Loss is 0.2617.\n",
      "For Iteration 5192 the Loss is 0.2617.\n",
      "For Iteration 5193 the Loss is 0.2617.\n",
      "For Iteration 5194 the Loss is 0.2617.\n",
      "For Iteration 5195 the Loss is 0.2617.\n",
      "For Iteration 5196 the Loss is 0.2617.\n",
      "For Iteration 5197 the Loss is 0.2617.\n",
      "For Iteration 5198 the Loss is 0.2617.\n",
      "For Iteration 5199 the Loss is 0.2617.\n",
      "For Iteration 5200 the Loss is 0.2617.\n",
      "For Iteration 5201 the Loss is 0.2617.\n",
      "For Iteration 5202 the Loss is 0.2617.\n",
      "For Iteration 5203 the Loss is 0.2617.\n",
      "For Iteration 5204 the Loss is 0.2617.\n",
      "For Iteration 5205 the Loss is 0.2617.\n",
      "For Iteration 5206 the Loss is 0.2617.\n",
      "For Iteration 5207 the Loss is 0.2617.\n",
      "For Iteration 5208 the Loss is 0.2617.\n",
      "For Iteration 5209 the Loss is 0.2617.\n",
      "For Iteration 5210 the Loss is 0.2617.\n",
      "For Iteration 5211 the Loss is 0.2617.\n",
      "For Iteration 5212 the Loss is 0.2617.\n",
      "For Iteration 5213 the Loss is 0.2617.\n",
      "For Iteration 5214 the Loss is 0.2617.\n",
      "For Iteration 5215 the Loss is 0.2617.\n",
      "For Iteration 5216 the Loss is 0.2617.\n",
      "For Iteration 5217 the Loss is 0.2617.\n",
      "For Iteration 5218 the Loss is 0.2617.\n",
      "For Iteration 5219 the Loss is 0.2617.\n",
      "For Iteration 5220 the Loss is 0.2617.\n",
      "For Iteration 5221 the Loss is 0.2617.\n",
      "For Iteration 5222 the Loss is 0.2617.\n",
      "For Iteration 5223 the Loss is 0.2617.\n",
      "For Iteration 5224 the Loss is 0.2617.\n",
      "For Iteration 5225 the Loss is 0.2617.\n",
      "For Iteration 5226 the Loss is 0.2617.\n",
      "For Iteration 5227 the Loss is 0.2617.\n",
      "For Iteration 5228 the Loss is 0.2617.\n",
      "For Iteration 5229 the Loss is 0.2617.\n",
      "For Iteration 5230 the Loss is 0.2617.\n",
      "For Iteration 5231 the Loss is 0.2617.\n",
      "For Iteration 5232 the Loss is 0.2617.\n",
      "For Iteration 5233 the Loss is 0.2617.\n",
      "For Iteration 5234 the Loss is 0.2617.\n",
      "For Iteration 5235 the Loss is 0.2617.\n",
      "For Iteration 5236 the Loss is 0.2617.\n",
      "For Iteration 5237 the Loss is 0.2617.\n",
      "For Iteration 5238 the Loss is 0.2617.\n",
      "For Iteration 5239 the Loss is 0.2617.\n",
      "For Iteration 5240 the Loss is 0.2617.\n",
      "For Iteration 5241 the Loss is 0.2617.\n",
      "For Iteration 5242 the Loss is 0.2617.\n",
      "For Iteration 5243 the Loss is 0.2617.\n",
      "For Iteration 5244 the Loss is 0.2617.\n",
      "For Iteration 5245 the Loss is 0.2617.\n",
      "For Iteration 5246 the Loss is 0.2617.\n",
      "For Iteration 5247 the Loss is 0.2617.\n",
      "For Iteration 5248 the Loss is 0.2617.\n",
      "For Iteration 5249 the Loss is 0.2617.\n",
      "For Iteration 5250 the Loss is 0.2617.\n",
      "For Iteration 5251 the Loss is 0.2617.\n",
      "For Iteration 5252 the Loss is 0.2617.\n",
      "For Iteration 5253 the Loss is 0.2617.\n",
      "For Iteration 5254 the Loss is 0.2617.\n",
      "For Iteration 5255 the Loss is 0.2617.\n",
      "For Iteration 5256 the Loss is 0.2617.\n",
      "For Iteration 5257 the Loss is 0.2617.\n",
      "For Iteration 5258 the Loss is 0.2617.\n",
      "For Iteration 5259 the Loss is 0.2617.\n",
      "For Iteration 5260 the Loss is 0.2617.\n",
      "For Iteration 5261 the Loss is 0.2617.\n",
      "For Iteration 5262 the Loss is 0.2617.\n",
      "For Iteration 5263 the Loss is 0.2617.\n",
      "For Iteration 5264 the Loss is 0.2617.\n",
      "For Iteration 5265 the Loss is 0.2617.\n",
      "For Iteration 5266 the Loss is 0.2617.\n",
      "For Iteration 5267 the Loss is 0.2617.\n",
      "For Iteration 5268 the Loss is 0.2617.\n",
      "For Iteration 5269 the Loss is 0.2617.\n",
      "For Iteration 5270 the Loss is 0.2617.\n",
      "For Iteration 5271 the Loss is 0.2617.\n",
      "For Iteration 5272 the Loss is 0.2617.\n",
      "For Iteration 5273 the Loss is 0.2617.\n",
      "For Iteration 5274 the Loss is 0.2617.\n",
      "For Iteration 5275 the Loss is 0.2617.\n",
      "For Iteration 5276 the Loss is 0.2616.\n",
      "For Iteration 5277 the Loss is 0.2616.\n",
      "For Iteration 5278 the Loss is 0.2616.\n",
      "For Iteration 5279 the Loss is 0.2616.\n",
      "For Iteration 5280 the Loss is 0.2616.\n",
      "For Iteration 5281 the Loss is 0.2616.\n",
      "For Iteration 5282 the Loss is 0.2616.\n",
      "For Iteration 5283 the Loss is 0.2616.\n",
      "For Iteration 5284 the Loss is 0.2616.\n",
      "For Iteration 5285 the Loss is 0.2616.\n",
      "For Iteration 5286 the Loss is 0.2616.\n",
      "For Iteration 5287 the Loss is 0.2616.\n",
      "For Iteration 5288 the Loss is 0.2616.\n",
      "For Iteration 5289 the Loss is 0.2616.\n",
      "For Iteration 5290 the Loss is 0.2616.\n",
      "For Iteration 5291 the Loss is 0.2616.\n",
      "For Iteration 5292 the Loss is 0.2616.\n",
      "For Iteration 5293 the Loss is 0.2616.\n",
      "For Iteration 5294 the Loss is 0.2616.\n",
      "For Iteration 5295 the Loss is 0.2616.\n",
      "For Iteration 5296 the Loss is 0.2616.\n",
      "For Iteration 5297 the Loss is 0.2616.\n",
      "For Iteration 5298 the Loss is 0.2616.\n",
      "For Iteration 5299 the Loss is 0.2616.\n",
      "For Iteration 5300 the Loss is 0.2616.\n",
      "For Iteration 5301 the Loss is 0.2616.\n",
      "For Iteration 5302 the Loss is 0.2616.\n",
      "For Iteration 5303 the Loss is 0.2616.\n",
      "For Iteration 5304 the Loss is 0.2616.\n",
      "For Iteration 5305 the Loss is 0.2616.\n",
      "For Iteration 5306 the Loss is 0.2616.\n",
      "For Iteration 5307 the Loss is 0.2616.\n",
      "For Iteration 5308 the Loss is 0.2616.\n",
      "For Iteration 5309 the Loss is 0.2616.\n",
      "For Iteration 5310 the Loss is 0.2616.\n",
      "For Iteration 5311 the Loss is 0.2616.\n",
      "For Iteration 5312 the Loss is 0.2616.\n",
      "For Iteration 5313 the Loss is 0.2616.\n",
      "For Iteration 5314 the Loss is 0.2616.\n",
      "For Iteration 5315 the Loss is 0.2616.\n",
      "For Iteration 5316 the Loss is 0.2616.\n",
      "For Iteration 5317 the Loss is 0.2616.\n",
      "For Iteration 5318 the Loss is 0.2616.\n",
      "For Iteration 5319 the Loss is 0.2616.\n",
      "For Iteration 5320 the Loss is 0.2616.\n",
      "For Iteration 5321 the Loss is 0.2616.\n",
      "For Iteration 5322 the Loss is 0.2616.\n",
      "For Iteration 5323 the Loss is 0.2616.\n",
      "For Iteration 5324 the Loss is 0.2616.\n",
      "For Iteration 5325 the Loss is 0.2616.\n",
      "For Iteration 5326 the Loss is 0.2616.\n",
      "For Iteration 5327 the Loss is 0.2616.\n",
      "For Iteration 5328 the Loss is 0.2616.\n",
      "For Iteration 5329 the Loss is 0.2616.\n",
      "For Iteration 5330 the Loss is 0.2616.\n",
      "For Iteration 5331 the Loss is 0.2616.\n",
      "For Iteration 5332 the Loss is 0.2616.\n",
      "For Iteration 5333 the Loss is 0.2616.\n",
      "For Iteration 5334 the Loss is 0.2616.\n",
      "For Iteration 5335 the Loss is 0.2616.\n",
      "For Iteration 5336 the Loss is 0.2616.\n",
      "For Iteration 5337 the Loss is 0.2616.\n",
      "For Iteration 5338 the Loss is 0.2616.\n",
      "For Iteration 5339 the Loss is 0.2616.\n",
      "For Iteration 5340 the Loss is 0.2616.\n",
      "For Iteration 5341 the Loss is 0.2616.\n",
      "For Iteration 5342 the Loss is 0.2616.\n",
      "For Iteration 5343 the Loss is 0.2616.\n",
      "For Iteration 5344 the Loss is 0.2616.\n",
      "For Iteration 5345 the Loss is 0.2616.\n",
      "For Iteration 5346 the Loss is 0.2616.\n",
      "For Iteration 5347 the Loss is 0.2616.\n",
      "For Iteration 5348 the Loss is 0.2616.\n",
      "For Iteration 5349 the Loss is 0.2616.\n",
      "For Iteration 5350 the Loss is 0.2616.\n",
      "For Iteration 5351 the Loss is 0.2616.\n",
      "For Iteration 5352 the Loss is 0.2616.\n",
      "For Iteration 5353 the Loss is 0.2616.\n",
      "For Iteration 5354 the Loss is 0.2616.\n",
      "For Iteration 5355 the Loss is 0.2616.\n",
      "For Iteration 5356 the Loss is 0.2616.\n",
      "For Iteration 5357 the Loss is 0.2616.\n",
      "For Iteration 5358 the Loss is 0.2616.\n",
      "For Iteration 5359 the Loss is 0.2616.\n",
      "For Iteration 5360 the Loss is 0.2616.\n",
      "For Iteration 5361 the Loss is 0.2616.\n",
      "For Iteration 5362 the Loss is 0.2616.\n",
      "For Iteration 5363 the Loss is 0.2616.\n",
      "For Iteration 5364 the Loss is 0.2616.\n",
      "For Iteration 5365 the Loss is 0.2616.\n",
      "For Iteration 5366 the Loss is 0.2616.\n",
      "For Iteration 5367 the Loss is 0.2616.\n",
      "For Iteration 5368 the Loss is 0.2616.\n",
      "For Iteration 5369 the Loss is 0.2616.\n",
      "For Iteration 5370 the Loss is 0.2616.\n",
      "For Iteration 5371 the Loss is 0.2616.\n",
      "For Iteration 5372 the Loss is 0.2616.\n",
      "For Iteration 5373 the Loss is 0.2616.\n",
      "For Iteration 5374 the Loss is 0.2616.\n",
      "For Iteration 5375 the Loss is 0.2616.\n",
      "For Iteration 5376 the Loss is 0.2616.\n",
      "For Iteration 5377 the Loss is 0.2616.\n",
      "For Iteration 5378 the Loss is 0.2616.\n",
      "For Iteration 5379 the Loss is 0.2616.\n",
      "For Iteration 5380 the Loss is 0.2616.\n",
      "For Iteration 5381 the Loss is 0.2616.\n",
      "For Iteration 5382 the Loss is 0.2616.\n",
      "For Iteration 5383 the Loss is 0.2616.\n",
      "For Iteration 5384 the Loss is 0.2616.\n",
      "For Iteration 5385 the Loss is 0.2616.\n",
      "For Iteration 5386 the Loss is 0.2616.\n",
      "For Iteration 5387 the Loss is 0.2616.\n",
      "For Iteration 5388 the Loss is 0.2616.\n",
      "For Iteration 5389 the Loss is 0.2616.\n",
      "For Iteration 5390 the Loss is 0.2616.\n",
      "For Iteration 5391 the Loss is 0.2616.\n",
      "For Iteration 5392 the Loss is 0.2616.\n",
      "For Iteration 5393 the Loss is 0.2616.\n",
      "For Iteration 5394 the Loss is 0.2616.\n",
      "For Iteration 5395 the Loss is 0.2616.\n",
      "For Iteration 5396 the Loss is 0.2616.\n",
      "For Iteration 5397 the Loss is 0.2616.\n",
      "For Iteration 5398 the Loss is 0.2616.\n",
      "For Iteration 5399 the Loss is 0.2616.\n",
      "For Iteration 5400 the Loss is 0.2616.\n",
      "For Iteration 5401 the Loss is 0.2616.\n",
      "For Iteration 5402 the Loss is 0.2616.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 5403 the Loss is 0.2616.\n",
      "For Iteration 5404 the Loss is 0.2616.\n",
      "For Iteration 5405 the Loss is 0.2616.\n",
      "For Iteration 5406 the Loss is 0.2616.\n",
      "For Iteration 5407 the Loss is 0.2616.\n",
      "For Iteration 5408 the Loss is 0.2616.\n",
      "For Iteration 5409 the Loss is 0.2616.\n",
      "For Iteration 5410 the Loss is 0.2616.\n",
      "For Iteration 5411 the Loss is 0.2616.\n",
      "For Iteration 5412 the Loss is 0.2616.\n",
      "For Iteration 5413 the Loss is 0.2616.\n",
      "For Iteration 5414 the Loss is 0.2616.\n",
      "For Iteration 5415 the Loss is 0.2616.\n",
      "For Iteration 5416 the Loss is 0.2616.\n",
      "For Iteration 5417 the Loss is 0.2616.\n",
      "For Iteration 5418 the Loss is 0.2616.\n",
      "For Iteration 5419 the Loss is 0.2616.\n",
      "For Iteration 5420 the Loss is 0.2616.\n",
      "For Iteration 5421 the Loss is 0.2616.\n",
      "For Iteration 5422 the Loss is 0.2616.\n",
      "For Iteration 5423 the Loss is 0.2616.\n",
      "For Iteration 5424 the Loss is 0.2616.\n",
      "For Iteration 5425 the Loss is 0.2616.\n",
      "For Iteration 5426 the Loss is 0.2616.\n",
      "For Iteration 5427 the Loss is 0.2616.\n",
      "For Iteration 5428 the Loss is 0.2616.\n",
      "For Iteration 5429 the Loss is 0.2616.\n",
      "For Iteration 5430 the Loss is 0.2616.\n",
      "For Iteration 5431 the Loss is 0.2616.\n",
      "For Iteration 5432 the Loss is 0.2616.\n",
      "For Iteration 5433 the Loss is 0.2616.\n",
      "For Iteration 5434 the Loss is 0.2616.\n",
      "For Iteration 5435 the Loss is 0.2616.\n",
      "For Iteration 5436 the Loss is 0.2616.\n",
      "For Iteration 5437 the Loss is 0.2616.\n",
      "For Iteration 5438 the Loss is 0.2616.\n",
      "For Iteration 5439 the Loss is 0.2616.\n",
      "For Iteration 5440 the Loss is 0.2616.\n",
      "For Iteration 5441 the Loss is 0.2616.\n",
      "For Iteration 5442 the Loss is 0.2616.\n",
      "For Iteration 5443 the Loss is 0.2616.\n",
      "For Iteration 5444 the Loss is 0.2616.\n",
      "For Iteration 5445 the Loss is 0.2616.\n",
      "For Iteration 5446 the Loss is 0.2616.\n",
      "For Iteration 5447 the Loss is 0.2616.\n",
      "For Iteration 5448 the Loss is 0.2616.\n",
      "For Iteration 5449 the Loss is 0.2616.\n",
      "For Iteration 5450 the Loss is 0.2616.\n",
      "For Iteration 5451 the Loss is 0.2616.\n",
      "For Iteration 5452 the Loss is 0.2616.\n",
      "For Iteration 5453 the Loss is 0.2616.\n",
      "For Iteration 5454 the Loss is 0.2616.\n",
      "For Iteration 5455 the Loss is 0.2616.\n",
      "For Iteration 5456 the Loss is 0.2616.\n",
      "For Iteration 5457 the Loss is 0.2616.\n",
      "For Iteration 5458 the Loss is 0.2616.\n",
      "For Iteration 5459 the Loss is 0.2616.\n",
      "For Iteration 5460 the Loss is 0.2616.\n",
      "For Iteration 5461 the Loss is 0.2616.\n",
      "For Iteration 5462 the Loss is 0.2616.\n",
      "For Iteration 5463 the Loss is 0.2616.\n",
      "For Iteration 5464 the Loss is 0.2616.\n",
      "For Iteration 5465 the Loss is 0.2616.\n",
      "For Iteration 5466 the Loss is 0.2616.\n",
      "For Iteration 5467 the Loss is 0.2616.\n",
      "For Iteration 5468 the Loss is 0.2616.\n",
      "For Iteration 5469 the Loss is 0.2616.\n",
      "For Iteration 5470 the Loss is 0.2616.\n",
      "For Iteration 5471 the Loss is 0.2616.\n",
      "For Iteration 5472 the Loss is 0.2616.\n",
      "For Iteration 5473 the Loss is 0.2616.\n",
      "For Iteration 5474 the Loss is 0.2616.\n",
      "For Iteration 5475 the Loss is 0.2616.\n",
      "For Iteration 5476 the Loss is 0.2616.\n",
      "For Iteration 5477 the Loss is 0.2616.\n",
      "For Iteration 5478 the Loss is 0.2616.\n",
      "For Iteration 5479 the Loss is 0.2616.\n",
      "For Iteration 5480 the Loss is 0.2616.\n",
      "For Iteration 5481 the Loss is 0.2616.\n",
      "For Iteration 5482 the Loss is 0.2616.\n",
      "For Iteration 5483 the Loss is 0.2615.\n",
      "For Iteration 5484 the Loss is 0.2615.\n",
      "For Iteration 5485 the Loss is 0.2615.\n",
      "For Iteration 5486 the Loss is 0.2615.\n",
      "For Iteration 5487 the Loss is 0.2615.\n",
      "For Iteration 5488 the Loss is 0.2615.\n",
      "For Iteration 5489 the Loss is 0.2615.\n",
      "For Iteration 5490 the Loss is 0.2615.\n",
      "For Iteration 5491 the Loss is 0.2615.\n",
      "For Iteration 5492 the Loss is 0.2615.\n",
      "For Iteration 5493 the Loss is 0.2615.\n",
      "For Iteration 5494 the Loss is 0.2615.\n",
      "For Iteration 5495 the Loss is 0.2615.\n",
      "For Iteration 5496 the Loss is 0.2615.\n",
      "For Iteration 5497 the Loss is 0.2615.\n",
      "For Iteration 5498 the Loss is 0.2615.\n",
      "For Iteration 5499 the Loss is 0.2615.\n",
      "For Iteration 5500 the Loss is 0.2615.\n",
      "For Iteration 5501 the Loss is 0.2615.\n",
      "For Iteration 5502 the Loss is 0.2615.\n",
      "For Iteration 5503 the Loss is 0.2615.\n",
      "For Iteration 5504 the Loss is 0.2615.\n",
      "For Iteration 5505 the Loss is 0.2615.\n",
      "For Iteration 5506 the Loss is 0.2615.\n",
      "For Iteration 5507 the Loss is 0.2615.\n",
      "For Iteration 5508 the Loss is 0.2615.\n",
      "For Iteration 5509 the Loss is 0.2615.\n",
      "For Iteration 5510 the Loss is 0.2615.\n",
      "For Iteration 5511 the Loss is 0.2615.\n",
      "For Iteration 5512 the Loss is 0.2615.\n",
      "For Iteration 5513 the Loss is 0.2615.\n",
      "For Iteration 5514 the Loss is 0.2615.\n",
      "For Iteration 5515 the Loss is 0.2615.\n",
      "For Iteration 5516 the Loss is 0.2615.\n",
      "For Iteration 5517 the Loss is 0.2615.\n",
      "For Iteration 5518 the Loss is 0.2615.\n",
      "For Iteration 5519 the Loss is 0.2615.\n",
      "For Iteration 5520 the Loss is 0.2615.\n",
      "For Iteration 5521 the Loss is 0.2615.\n",
      "For Iteration 5522 the Loss is 0.2615.\n",
      "For Iteration 5523 the Loss is 0.2615.\n",
      "For Iteration 5524 the Loss is 0.2615.\n",
      "For Iteration 5525 the Loss is 0.2615.\n",
      "For Iteration 5526 the Loss is 0.2615.\n",
      "For Iteration 5527 the Loss is 0.2615.\n",
      "For Iteration 5528 the Loss is 0.2615.\n",
      "For Iteration 5529 the Loss is 0.2615.\n",
      "For Iteration 5530 the Loss is 0.2615.\n",
      "For Iteration 5531 the Loss is 0.2615.\n",
      "For Iteration 5532 the Loss is 0.2615.\n",
      "For Iteration 5533 the Loss is 0.2615.\n",
      "For Iteration 5534 the Loss is 0.2615.\n",
      "For Iteration 5535 the Loss is 0.2615.\n",
      "For Iteration 5536 the Loss is 0.2615.\n",
      "For Iteration 5537 the Loss is 0.2615.\n",
      "For Iteration 5538 the Loss is 0.2615.\n",
      "For Iteration 5539 the Loss is 0.2615.\n",
      "For Iteration 5540 the Loss is 0.2615.\n",
      "For Iteration 5541 the Loss is 0.2615.\n",
      "For Iteration 5542 the Loss is 0.2615.\n",
      "For Iteration 5543 the Loss is 0.2615.\n",
      "For Iteration 5544 the Loss is 0.2615.\n",
      "For Iteration 5545 the Loss is 0.2615.\n",
      "For Iteration 5546 the Loss is 0.2615.\n",
      "For Iteration 5547 the Loss is 0.2615.\n",
      "For Iteration 5548 the Loss is 0.2615.\n",
      "For Iteration 5549 the Loss is 0.2615.\n",
      "For Iteration 5550 the Loss is 0.2615.\n",
      "For Iteration 5551 the Loss is 0.2615.\n",
      "For Iteration 5552 the Loss is 0.2615.\n",
      "For Iteration 5553 the Loss is 0.2615.\n",
      "For Iteration 5554 the Loss is 0.2615.\n",
      "For Iteration 5555 the Loss is 0.2615.\n",
      "For Iteration 5556 the Loss is 0.2615.\n",
      "For Iteration 5557 the Loss is 0.2615.\n",
      "For Iteration 5558 the Loss is 0.2615.\n",
      "For Iteration 5559 the Loss is 0.2615.\n",
      "For Iteration 5560 the Loss is 0.2615.\n",
      "For Iteration 5561 the Loss is 0.2615.\n",
      "For Iteration 5562 the Loss is 0.2615.\n",
      "For Iteration 5563 the Loss is 0.2615.\n",
      "For Iteration 5564 the Loss is 0.2615.\n",
      "For Iteration 5565 the Loss is 0.2615.\n",
      "For Iteration 5566 the Loss is 0.2615.\n",
      "For Iteration 5567 the Loss is 0.2615.\n",
      "For Iteration 5568 the Loss is 0.2615.\n",
      "For Iteration 5569 the Loss is 0.2615.\n",
      "For Iteration 5570 the Loss is 0.2615.\n",
      "For Iteration 5571 the Loss is 0.2615.\n",
      "For Iteration 5572 the Loss is 0.2615.\n",
      "For Iteration 5573 the Loss is 0.2615.\n",
      "For Iteration 5574 the Loss is 0.2615.\n",
      "For Iteration 5575 the Loss is 0.2615.\n",
      "For Iteration 5576 the Loss is 0.2615.\n",
      "For Iteration 5577 the Loss is 0.2615.\n",
      "For Iteration 5578 the Loss is 0.2615.\n",
      "For Iteration 5579 the Loss is 0.2615.\n",
      "For Iteration 5580 the Loss is 0.2615.\n",
      "For Iteration 5581 the Loss is 0.2615.\n",
      "For Iteration 5582 the Loss is 0.2615.\n",
      "For Iteration 5583 the Loss is 0.2615.\n",
      "For Iteration 5584 the Loss is 0.2615.\n",
      "For Iteration 5585 the Loss is 0.2615.\n",
      "For Iteration 5586 the Loss is 0.2615.\n",
      "For Iteration 5587 the Loss is 0.2615.\n",
      "For Iteration 5588 the Loss is 0.2615.\n",
      "For Iteration 5589 the Loss is 0.2615.\n",
      "For Iteration 5590 the Loss is 0.2615.\n",
      "For Iteration 5591 the Loss is 0.2615.\n",
      "For Iteration 5592 the Loss is 0.2615.\n",
      "For Iteration 5593 the Loss is 0.2615.\n",
      "For Iteration 5594 the Loss is 0.2615.\n",
      "For Iteration 5595 the Loss is 0.2615.\n",
      "For Iteration 5596 the Loss is 0.2615.\n",
      "For Iteration 5597 the Loss is 0.2615.\n",
      "For Iteration 5598 the Loss is 0.2615.\n",
      "For Iteration 5599 the Loss is 0.2615.\n",
      "For Iteration 5600 the Loss is 0.2615.\n",
      "For Iteration 5601 the Loss is 0.2615.\n",
      "For Iteration 5602 the Loss is 0.2615.\n",
      "For Iteration 5603 the Loss is 0.2615.\n",
      "For Iteration 5604 the Loss is 0.2615.\n",
      "For Iteration 5605 the Loss is 0.2615.\n",
      "For Iteration 5606 the Loss is 0.2615.\n",
      "For Iteration 5607 the Loss is 0.2615.\n",
      "For Iteration 5608 the Loss is 0.2615.\n",
      "For Iteration 5609 the Loss is 0.2615.\n",
      "For Iteration 5610 the Loss is 0.2615.\n",
      "For Iteration 5611 the Loss is 0.2615.\n",
      "For Iteration 5612 the Loss is 0.2615.\n",
      "For Iteration 5613 the Loss is 0.2615.\n",
      "For Iteration 5614 the Loss is 0.2615.\n",
      "For Iteration 5615 the Loss is 0.2615.\n",
      "For Iteration 5616 the Loss is 0.2615.\n",
      "For Iteration 5617 the Loss is 0.2615.\n",
      "For Iteration 5618 the Loss is 0.2615.\n",
      "For Iteration 5619 the Loss is 0.2615.\n",
      "For Iteration 5620 the Loss is 0.2615.\n",
      "For Iteration 5621 the Loss is 0.2615.\n",
      "For Iteration 5622 the Loss is 0.2615.\n",
      "For Iteration 5623 the Loss is 0.2615.\n",
      "For Iteration 5624 the Loss is 0.2615.\n",
      "For Iteration 5625 the Loss is 0.2615.\n",
      "For Iteration 5626 the Loss is 0.2615.\n",
      "For Iteration 5627 the Loss is 0.2615.\n",
      "For Iteration 5628 the Loss is 0.2615.\n",
      "For Iteration 5629 the Loss is 0.2615.\n",
      "For Iteration 5630 the Loss is 0.2615.\n",
      "For Iteration 5631 the Loss is 0.2615.\n",
      "For Iteration 5632 the Loss is 0.2615.\n",
      "For Iteration 5633 the Loss is 0.2615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 5634 the Loss is 0.2615.\n",
      "For Iteration 5635 the Loss is 0.2615.\n",
      "For Iteration 5636 the Loss is 0.2615.\n",
      "For Iteration 5637 the Loss is 0.2615.\n",
      "For Iteration 5638 the Loss is 0.2615.\n",
      "For Iteration 5639 the Loss is 0.2615.\n",
      "For Iteration 5640 the Loss is 0.2615.\n",
      "For Iteration 5641 the Loss is 0.2615.\n",
      "For Iteration 5642 the Loss is 0.2615.\n",
      "For Iteration 5643 the Loss is 0.2615.\n",
      "For Iteration 5644 the Loss is 0.2615.\n",
      "For Iteration 5645 the Loss is 0.2615.\n",
      "For Iteration 5646 the Loss is 0.2615.\n",
      "For Iteration 5647 the Loss is 0.2615.\n",
      "For Iteration 5648 the Loss is 0.2615.\n",
      "For Iteration 5649 the Loss is 0.2615.\n",
      "For Iteration 5650 the Loss is 0.2615.\n",
      "For Iteration 5651 the Loss is 0.2615.\n",
      "For Iteration 5652 the Loss is 0.2615.\n",
      "For Iteration 5653 the Loss is 0.2615.\n",
      "For Iteration 5654 the Loss is 0.2615.\n",
      "For Iteration 5655 the Loss is 0.2615.\n",
      "For Iteration 5656 the Loss is 0.2615.\n",
      "For Iteration 5657 the Loss is 0.2615.\n",
      "For Iteration 5658 the Loss is 0.2615.\n",
      "For Iteration 5659 the Loss is 0.2615.\n",
      "For Iteration 5660 the Loss is 0.2615.\n",
      "For Iteration 5661 the Loss is 0.2615.\n",
      "For Iteration 5662 the Loss is 0.2615.\n",
      "For Iteration 5663 the Loss is 0.2615.\n",
      "For Iteration 5664 the Loss is 0.2615.\n",
      "For Iteration 5665 the Loss is 0.2615.\n",
      "For Iteration 5666 the Loss is 0.2615.\n",
      "For Iteration 5667 the Loss is 0.2615.\n",
      "For Iteration 5668 the Loss is 0.2615.\n",
      "For Iteration 5669 the Loss is 0.2615.\n",
      "For Iteration 5670 the Loss is 0.2615.\n",
      "For Iteration 5671 the Loss is 0.2615.\n",
      "For Iteration 5672 the Loss is 0.2615.\n",
      "For Iteration 5673 the Loss is 0.2615.\n",
      "For Iteration 5674 the Loss is 0.2615.\n",
      "For Iteration 5675 the Loss is 0.2615.\n",
      "For Iteration 5676 the Loss is 0.2615.\n",
      "For Iteration 5677 the Loss is 0.2615.\n",
      "For Iteration 5678 the Loss is 0.2615.\n",
      "For Iteration 5679 the Loss is 0.2615.\n",
      "For Iteration 5680 the Loss is 0.2615.\n",
      "For Iteration 5681 the Loss is 0.2615.\n",
      "For Iteration 5682 the Loss is 0.2615.\n",
      "For Iteration 5683 the Loss is 0.2615.\n",
      "For Iteration 5684 the Loss is 0.2615.\n",
      "For Iteration 5685 the Loss is 0.2615.\n",
      "For Iteration 5686 the Loss is 0.2615.\n",
      "For Iteration 5687 the Loss is 0.2615.\n",
      "For Iteration 5688 the Loss is 0.2615.\n",
      "For Iteration 5689 the Loss is 0.2615.\n",
      "For Iteration 5690 the Loss is 0.2615.\n",
      "For Iteration 5691 the Loss is 0.2615.\n",
      "For Iteration 5692 the Loss is 0.2615.\n",
      "For Iteration 5693 the Loss is 0.2615.\n",
      "For Iteration 5694 the Loss is 0.2615.\n",
      "For Iteration 5695 the Loss is 0.2615.\n",
      "For Iteration 5696 the Loss is 0.2615.\n",
      "For Iteration 5697 the Loss is 0.2615.\n",
      "For Iteration 5698 the Loss is 0.2615.\n",
      "For Iteration 5699 the Loss is 0.2615.\n",
      "For Iteration 5700 the Loss is 0.2615.\n",
      "For Iteration 5701 the Loss is 0.2615.\n",
      "For Iteration 5702 the Loss is 0.2615.\n",
      "For Iteration 5703 the Loss is 0.2615.\n",
      "For Iteration 5704 the Loss is 0.2615.\n",
      "For Iteration 5705 the Loss is 0.2615.\n",
      "For Iteration 5706 the Loss is 0.2615.\n",
      "For Iteration 5707 the Loss is 0.2614.\n",
      "For Iteration 5708 the Loss is 0.2614.\n",
      "For Iteration 5709 the Loss is 0.2614.\n",
      "For Iteration 5710 the Loss is 0.2614.\n",
      "For Iteration 5711 the Loss is 0.2614.\n",
      "For Iteration 5712 the Loss is 0.2614.\n",
      "For Iteration 5713 the Loss is 0.2614.\n",
      "For Iteration 5714 the Loss is 0.2614.\n",
      "For Iteration 5715 the Loss is 0.2614.\n",
      "For Iteration 5716 the Loss is 0.2614.\n",
      "For Iteration 5717 the Loss is 0.2614.\n",
      "For Iteration 5718 the Loss is 0.2614.\n",
      "For Iteration 5719 the Loss is 0.2614.\n",
      "For Iteration 5720 the Loss is 0.2614.\n",
      "For Iteration 5721 the Loss is 0.2614.\n",
      "For Iteration 5722 the Loss is 0.2614.\n",
      "For Iteration 5723 the Loss is 0.2614.\n",
      "For Iteration 5724 the Loss is 0.2614.\n",
      "For Iteration 5725 the Loss is 0.2614.\n",
      "For Iteration 5726 the Loss is 0.2614.\n",
      "For Iteration 5727 the Loss is 0.2614.\n",
      "For Iteration 5728 the Loss is 0.2614.\n",
      "For Iteration 5729 the Loss is 0.2614.\n",
      "For Iteration 5730 the Loss is 0.2614.\n",
      "For Iteration 5731 the Loss is 0.2614.\n",
      "For Iteration 5732 the Loss is 0.2614.\n",
      "For Iteration 5733 the Loss is 0.2614.\n",
      "For Iteration 5734 the Loss is 0.2614.\n",
      "For Iteration 5735 the Loss is 0.2614.\n",
      "For Iteration 5736 the Loss is 0.2614.\n",
      "For Iteration 5737 the Loss is 0.2614.\n",
      "For Iteration 5738 the Loss is 0.2614.\n",
      "For Iteration 5739 the Loss is 0.2614.\n",
      "For Iteration 5740 the Loss is 0.2614.\n",
      "For Iteration 5741 the Loss is 0.2614.\n",
      "For Iteration 5742 the Loss is 0.2614.\n",
      "For Iteration 5743 the Loss is 0.2614.\n",
      "For Iteration 5744 the Loss is 0.2614.\n",
      "For Iteration 5745 the Loss is 0.2614.\n",
      "For Iteration 5746 the Loss is 0.2614.\n",
      "For Iteration 5747 the Loss is 0.2614.\n",
      "For Iteration 5748 the Loss is 0.2614.\n",
      "For Iteration 5749 the Loss is 0.2614.\n",
      "For Iteration 5750 the Loss is 0.2614.\n",
      "For Iteration 5751 the Loss is 0.2614.\n",
      "For Iteration 5752 the Loss is 0.2614.\n",
      "For Iteration 5753 the Loss is 0.2614.\n",
      "For Iteration 5754 the Loss is 0.2614.\n",
      "For Iteration 5755 the Loss is 0.2614.\n",
      "For Iteration 5756 the Loss is 0.2614.\n",
      "For Iteration 5757 the Loss is 0.2614.\n",
      "For Iteration 5758 the Loss is 0.2614.\n",
      "For Iteration 5759 the Loss is 0.2614.\n",
      "For Iteration 5760 the Loss is 0.2614.\n",
      "For Iteration 5761 the Loss is 0.2614.\n",
      "For Iteration 5762 the Loss is 0.2614.\n",
      "For Iteration 5763 the Loss is 0.2614.\n",
      "For Iteration 5764 the Loss is 0.2614.\n",
      "For Iteration 5765 the Loss is 0.2614.\n",
      "For Iteration 5766 the Loss is 0.2614.\n",
      "For Iteration 5767 the Loss is 0.2614.\n",
      "For Iteration 5768 the Loss is 0.2614.\n",
      "For Iteration 5769 the Loss is 0.2614.\n",
      "For Iteration 5770 the Loss is 0.2614.\n",
      "For Iteration 5771 the Loss is 0.2614.\n",
      "For Iteration 5772 the Loss is 0.2614.\n",
      "For Iteration 5773 the Loss is 0.2614.\n",
      "For Iteration 5774 the Loss is 0.2614.\n",
      "For Iteration 5775 the Loss is 0.2614.\n",
      "For Iteration 5776 the Loss is 0.2614.\n",
      "For Iteration 5777 the Loss is 0.2614.\n",
      "For Iteration 5778 the Loss is 0.2614.\n",
      "For Iteration 5779 the Loss is 0.2614.\n",
      "For Iteration 5780 the Loss is 0.2614.\n",
      "For Iteration 5781 the Loss is 0.2614.\n",
      "For Iteration 5782 the Loss is 0.2614.\n",
      "For Iteration 5783 the Loss is 0.2614.\n",
      "For Iteration 5784 the Loss is 0.2614.\n",
      "For Iteration 5785 the Loss is 0.2614.\n",
      "For Iteration 5786 the Loss is 0.2614.\n",
      "For Iteration 5787 the Loss is 0.2614.\n",
      "For Iteration 5788 the Loss is 0.2614.\n",
      "For Iteration 5789 the Loss is 0.2614.\n",
      "For Iteration 5790 the Loss is 0.2614.\n",
      "For Iteration 5791 the Loss is 0.2614.\n",
      "For Iteration 5792 the Loss is 0.2614.\n",
      "For Iteration 5793 the Loss is 0.2614.\n",
      "For Iteration 5794 the Loss is 0.2614.\n",
      "For Iteration 5795 the Loss is 0.2614.\n",
      "For Iteration 5796 the Loss is 0.2614.\n",
      "For Iteration 5797 the Loss is 0.2614.\n",
      "For Iteration 5798 the Loss is 0.2614.\n",
      "For Iteration 5799 the Loss is 0.2614.\n",
      "For Iteration 5800 the Loss is 0.2614.\n",
      "For Iteration 5801 the Loss is 0.2614.\n",
      "For Iteration 5802 the Loss is 0.2614.\n",
      "For Iteration 5803 the Loss is 0.2614.\n",
      "For Iteration 5804 the Loss is 0.2614.\n",
      "For Iteration 5805 the Loss is 0.2614.\n",
      "For Iteration 5806 the Loss is 0.2614.\n",
      "For Iteration 5807 the Loss is 0.2614.\n",
      "For Iteration 5808 the Loss is 0.2614.\n",
      "For Iteration 5809 the Loss is 0.2614.\n",
      "For Iteration 5810 the Loss is 0.2614.\n",
      "For Iteration 5811 the Loss is 0.2614.\n",
      "For Iteration 5812 the Loss is 0.2614.\n",
      "For Iteration 5813 the Loss is 0.2614.\n",
      "For Iteration 5814 the Loss is 0.2614.\n",
      "For Iteration 5815 the Loss is 0.2614.\n",
      "For Iteration 5816 the Loss is 0.2614.\n",
      "For Iteration 5817 the Loss is 0.2614.\n",
      "For Iteration 5818 the Loss is 0.2614.\n",
      "For Iteration 5819 the Loss is 0.2614.\n",
      "For Iteration 5820 the Loss is 0.2614.\n",
      "For Iteration 5821 the Loss is 0.2614.\n",
      "For Iteration 5822 the Loss is 0.2614.\n",
      "For Iteration 5823 the Loss is 0.2614.\n",
      "For Iteration 5824 the Loss is 0.2614.\n",
      "For Iteration 5825 the Loss is 0.2614.\n",
      "For Iteration 5826 the Loss is 0.2614.\n",
      "For Iteration 5827 the Loss is 0.2614.\n",
      "For Iteration 5828 the Loss is 0.2614.\n",
      "For Iteration 5829 the Loss is 0.2614.\n",
      "For Iteration 5830 the Loss is 0.2614.\n",
      "For Iteration 5831 the Loss is 0.2614.\n",
      "For Iteration 5832 the Loss is 0.2614.\n",
      "For Iteration 5833 the Loss is 0.2614.\n",
      "For Iteration 5834 the Loss is 0.2614.\n",
      "For Iteration 5835 the Loss is 0.2614.\n",
      "For Iteration 5836 the Loss is 0.2614.\n",
      "For Iteration 5837 the Loss is 0.2614.\n",
      "For Iteration 5838 the Loss is 0.2614.\n",
      "For Iteration 5839 the Loss is 0.2614.\n",
      "For Iteration 5840 the Loss is 0.2614.\n",
      "For Iteration 5841 the Loss is 0.2614.\n",
      "For Iteration 5842 the Loss is 0.2614.\n",
      "For Iteration 5843 the Loss is 0.2614.\n",
      "For Iteration 5844 the Loss is 0.2614.\n",
      "For Iteration 5845 the Loss is 0.2614.\n",
      "For Iteration 5846 the Loss is 0.2614.\n",
      "For Iteration 5847 the Loss is 0.2614.\n",
      "For Iteration 5848 the Loss is 0.2614.\n",
      "For Iteration 5849 the Loss is 0.2614.\n",
      "For Iteration 5850 the Loss is 0.2614.\n",
      "For Iteration 5851 the Loss is 0.2614.\n",
      "For Iteration 5852 the Loss is 0.2614.\n",
      "For Iteration 5853 the Loss is 0.2614.\n",
      "For Iteration 5854 the Loss is 0.2614.\n",
      "For Iteration 5855 the Loss is 0.2614.\n",
      "For Iteration 5856 the Loss is 0.2614.\n",
      "For Iteration 5857 the Loss is 0.2614.\n",
      "For Iteration 5858 the Loss is 0.2614.\n",
      "For Iteration 5859 the Loss is 0.2614.\n",
      "For Iteration 5860 the Loss is 0.2614.\n",
      "For Iteration 5861 the Loss is 0.2614.\n",
      "For Iteration 5862 the Loss is 0.2614.\n",
      "For Iteration 5863 the Loss is 0.2614.\n",
      "For Iteration 5864 the Loss is 0.2614.\n",
      "For Iteration 5865 the Loss is 0.2614.\n",
      "For Iteration 5866 the Loss is 0.2614.\n",
      "For Iteration 5867 the Loss is 0.2614.\n",
      "For Iteration 5868 the Loss is 0.2614.\n",
      "For Iteration 5869 the Loss is 0.2614.\n",
      "For Iteration 5870 the Loss is 0.2614.\n",
      "For Iteration 5871 the Loss is 0.2614.\n",
      "For Iteration 5872 the Loss is 0.2614.\n",
      "For Iteration 5873 the Loss is 0.2614.\n",
      "For Iteration 5874 the Loss is 0.2614.\n",
      "For Iteration 5875 the Loss is 0.2614.\n",
      "For Iteration 5876 the Loss is 0.2614.\n",
      "For Iteration 5877 the Loss is 0.2614.\n",
      "For Iteration 5878 the Loss is 0.2614.\n",
      "For Iteration 5879 the Loss is 0.2614.\n",
      "For Iteration 5880 the Loss is 0.2614.\n",
      "For Iteration 5881 the Loss is 0.2614.\n",
      "For Iteration 5882 the Loss is 0.2614.\n",
      "For Iteration 5883 the Loss is 0.2614.\n",
      "For Iteration 5884 the Loss is 0.2614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 5885 the Loss is 0.2614.\n",
      "For Iteration 5886 the Loss is 0.2614.\n",
      "For Iteration 5887 the Loss is 0.2614.\n",
      "For Iteration 5888 the Loss is 0.2614.\n",
      "For Iteration 5889 the Loss is 0.2614.\n",
      "For Iteration 5890 the Loss is 0.2614.\n",
      "For Iteration 5891 the Loss is 0.2614.\n",
      "For Iteration 5892 the Loss is 0.2614.\n",
      "For Iteration 5893 the Loss is 0.2614.\n",
      "For Iteration 5894 the Loss is 0.2614.\n",
      "For Iteration 5895 the Loss is 0.2614.\n",
      "For Iteration 5896 the Loss is 0.2614.\n",
      "For Iteration 5897 the Loss is 0.2614.\n",
      "For Iteration 5898 the Loss is 0.2614.\n",
      "For Iteration 5899 the Loss is 0.2614.\n",
      "For Iteration 5900 the Loss is 0.2614.\n",
      "For Iteration 5901 the Loss is 0.2614.\n",
      "For Iteration 5902 the Loss is 0.2614.\n",
      "For Iteration 5903 the Loss is 0.2614.\n",
      "For Iteration 5904 the Loss is 0.2614.\n",
      "For Iteration 5905 the Loss is 0.2614.\n",
      "For Iteration 5906 the Loss is 0.2614.\n",
      "For Iteration 5907 the Loss is 0.2614.\n",
      "For Iteration 5908 the Loss is 0.2614.\n",
      "For Iteration 5909 the Loss is 0.2614.\n",
      "For Iteration 5910 the Loss is 0.2614.\n",
      "For Iteration 5911 the Loss is 0.2614.\n",
      "For Iteration 5912 the Loss is 0.2614.\n",
      "For Iteration 5913 the Loss is 0.2614.\n",
      "For Iteration 5914 the Loss is 0.2614.\n",
      "For Iteration 5915 the Loss is 0.2614.\n",
      "For Iteration 5916 the Loss is 0.2614.\n",
      "For Iteration 5917 the Loss is 0.2614.\n",
      "For Iteration 5918 the Loss is 0.2614.\n",
      "For Iteration 5919 the Loss is 0.2614.\n",
      "For Iteration 5920 the Loss is 0.2614.\n",
      "For Iteration 5921 the Loss is 0.2614.\n",
      "For Iteration 5922 the Loss is 0.2614.\n",
      "For Iteration 5923 the Loss is 0.2614.\n",
      "For Iteration 5924 the Loss is 0.2614.\n",
      "For Iteration 5925 the Loss is 0.2614.\n",
      "For Iteration 5926 the Loss is 0.2614.\n",
      "For Iteration 5927 the Loss is 0.2614.\n",
      "For Iteration 5928 the Loss is 0.2614.\n",
      "For Iteration 5929 the Loss is 0.2614.\n",
      "For Iteration 5930 the Loss is 0.2614.\n",
      "For Iteration 5931 the Loss is 0.2614.\n",
      "For Iteration 5932 the Loss is 0.2614.\n",
      "For Iteration 5933 the Loss is 0.2614.\n",
      "For Iteration 5934 the Loss is 0.2614.\n",
      "For Iteration 5935 the Loss is 0.2614.\n",
      "For Iteration 5936 the Loss is 0.2614.\n",
      "For Iteration 5937 the Loss is 0.2614.\n",
      "For Iteration 5938 the Loss is 0.2614.\n",
      "For Iteration 5939 the Loss is 0.2614.\n",
      "For Iteration 5940 the Loss is 0.2614.\n",
      "For Iteration 5941 the Loss is 0.2614.\n",
      "For Iteration 5942 the Loss is 0.2614.\n",
      "For Iteration 5943 the Loss is 0.2614.\n",
      "For Iteration 5944 the Loss is 0.2614.\n",
      "For Iteration 5945 the Loss is 0.2614.\n",
      "For Iteration 5946 the Loss is 0.2614.\n",
      "For Iteration 5947 the Loss is 0.2614.\n",
      "For Iteration 5948 the Loss is 0.2614.\n",
      "For Iteration 5949 the Loss is 0.2613.\n",
      "For Iteration 5950 the Loss is 0.2613.\n",
      "For Iteration 5951 the Loss is 0.2613.\n",
      "For Iteration 5952 the Loss is 0.2613.\n",
      "For Iteration 5953 the Loss is 0.2613.\n",
      "For Iteration 5954 the Loss is 0.2613.\n",
      "For Iteration 5955 the Loss is 0.2613.\n",
      "For Iteration 5956 the Loss is 0.2613.\n",
      "For Iteration 5957 the Loss is 0.2613.\n",
      "For Iteration 5958 the Loss is 0.2613.\n",
      "For Iteration 5959 the Loss is 0.2613.\n",
      "For Iteration 5960 the Loss is 0.2613.\n",
      "For Iteration 5961 the Loss is 0.2613.\n",
      "For Iteration 5962 the Loss is 0.2613.\n",
      "For Iteration 5963 the Loss is 0.2613.\n",
      "For Iteration 5964 the Loss is 0.2613.\n",
      "For Iteration 5965 the Loss is 0.2613.\n",
      "For Iteration 5966 the Loss is 0.2613.\n",
      "For Iteration 5967 the Loss is 0.2613.\n",
      "For Iteration 5968 the Loss is 0.2613.\n",
      "For Iteration 5969 the Loss is 0.2613.\n",
      "For Iteration 5970 the Loss is 0.2613.\n",
      "For Iteration 5971 the Loss is 0.2613.\n",
      "For Iteration 5972 the Loss is 0.2613.\n",
      "For Iteration 5973 the Loss is 0.2613.\n",
      "For Iteration 5974 the Loss is 0.2613.\n",
      "For Iteration 5975 the Loss is 0.2613.\n",
      "For Iteration 5976 the Loss is 0.2613.\n",
      "For Iteration 5977 the Loss is 0.2613.\n",
      "For Iteration 5978 the Loss is 0.2613.\n",
      "For Iteration 5979 the Loss is 0.2613.\n",
      "For Iteration 5980 the Loss is 0.2613.\n",
      "For Iteration 5981 the Loss is 0.2613.\n",
      "For Iteration 5982 the Loss is 0.2613.\n",
      "For Iteration 5983 the Loss is 0.2613.\n",
      "For Iteration 5984 the Loss is 0.2613.\n",
      "For Iteration 5985 the Loss is 0.2613.\n",
      "For Iteration 5986 the Loss is 0.2613.\n",
      "For Iteration 5987 the Loss is 0.2613.\n",
      "For Iteration 5988 the Loss is 0.2613.\n",
      "For Iteration 5989 the Loss is 0.2613.\n",
      "For Iteration 5990 the Loss is 0.2613.\n",
      "For Iteration 5991 the Loss is 0.2613.\n",
      "For Iteration 5992 the Loss is 0.2613.\n",
      "For Iteration 5993 the Loss is 0.2613.\n",
      "For Iteration 5994 the Loss is 0.2613.\n",
      "For Iteration 5995 the Loss is 0.2613.\n",
      "For Iteration 5996 the Loss is 0.2613.\n",
      "For Iteration 5997 the Loss is 0.2613.\n",
      "For Iteration 5998 the Loss is 0.2613.\n",
      "For Iteration 5999 the Loss is 0.2613.\n",
      "For Iteration 6000 the Loss is 0.2613.\n",
      "For Iteration 6001 the Loss is 0.2613.\n",
      "For Iteration 6002 the Loss is 0.2613.\n",
      "For Iteration 6003 the Loss is 0.2613.\n",
      "For Iteration 6004 the Loss is 0.2613.\n",
      "For Iteration 6005 the Loss is 0.2613.\n",
      "For Iteration 6006 the Loss is 0.2613.\n",
      "For Iteration 6007 the Loss is 0.2613.\n",
      "For Iteration 6008 the Loss is 0.2613.\n",
      "For Iteration 6009 the Loss is 0.2613.\n",
      "For Iteration 6010 the Loss is 0.2613.\n",
      "For Iteration 6011 the Loss is 0.2613.\n",
      "For Iteration 6012 the Loss is 0.2613.\n",
      "For Iteration 6013 the Loss is 0.2613.\n",
      "For Iteration 6014 the Loss is 0.2613.\n",
      "For Iteration 6015 the Loss is 0.2613.\n",
      "For Iteration 6016 the Loss is 0.2613.\n",
      "For Iteration 6017 the Loss is 0.2613.\n",
      "For Iteration 6018 the Loss is 0.2613.\n",
      "For Iteration 6019 the Loss is 0.2613.\n",
      "For Iteration 6020 the Loss is 0.2613.\n",
      "For Iteration 6021 the Loss is 0.2613.\n",
      "For Iteration 6022 the Loss is 0.2613.\n",
      "For Iteration 6023 the Loss is 0.2613.\n",
      "For Iteration 6024 the Loss is 0.2613.\n",
      "For Iteration 6025 the Loss is 0.2613.\n",
      "For Iteration 6026 the Loss is 0.2613.\n",
      "For Iteration 6027 the Loss is 0.2613.\n",
      "For Iteration 6028 the Loss is 0.2613.\n",
      "For Iteration 6029 the Loss is 0.2613.\n",
      "For Iteration 6030 the Loss is 0.2613.\n",
      "For Iteration 6031 the Loss is 0.2613.\n",
      "For Iteration 6032 the Loss is 0.2613.\n",
      "For Iteration 6033 the Loss is 0.2613.\n",
      "For Iteration 6034 the Loss is 0.2613.\n",
      "For Iteration 6035 the Loss is 0.2613.\n",
      "For Iteration 6036 the Loss is 0.2613.\n",
      "For Iteration 6037 the Loss is 0.2613.\n",
      "For Iteration 6038 the Loss is 0.2613.\n",
      "For Iteration 6039 the Loss is 0.2613.\n",
      "For Iteration 6040 the Loss is 0.2613.\n",
      "For Iteration 6041 the Loss is 0.2613.\n",
      "For Iteration 6042 the Loss is 0.2613.\n",
      "For Iteration 6043 the Loss is 0.2613.\n",
      "For Iteration 6044 the Loss is 0.2613.\n",
      "For Iteration 6045 the Loss is 0.2613.\n",
      "For Iteration 6046 the Loss is 0.2613.\n",
      "For Iteration 6047 the Loss is 0.2613.\n",
      "For Iteration 6048 the Loss is 0.2613.\n",
      "For Iteration 6049 the Loss is 0.2613.\n",
      "For Iteration 6050 the Loss is 0.2613.\n",
      "For Iteration 6051 the Loss is 0.2613.\n",
      "For Iteration 6052 the Loss is 0.2613.\n",
      "For Iteration 6053 the Loss is 0.2613.\n",
      "For Iteration 6054 the Loss is 0.2613.\n",
      "For Iteration 6055 the Loss is 0.2613.\n",
      "For Iteration 6056 the Loss is 0.2613.\n",
      "For Iteration 6057 the Loss is 0.2613.\n",
      "For Iteration 6058 the Loss is 0.2613.\n",
      "For Iteration 6059 the Loss is 0.2613.\n",
      "For Iteration 6060 the Loss is 0.2613.\n",
      "For Iteration 6061 the Loss is 0.2613.\n",
      "For Iteration 6062 the Loss is 0.2613.\n",
      "For Iteration 6063 the Loss is 0.2613.\n",
      "For Iteration 6064 the Loss is 0.2613.\n",
      "For Iteration 6065 the Loss is 0.2613.\n",
      "For Iteration 6066 the Loss is 0.2613.\n",
      "For Iteration 6067 the Loss is 0.2613.\n",
      "For Iteration 6068 the Loss is 0.2613.\n",
      "For Iteration 6069 the Loss is 0.2613.\n",
      "For Iteration 6070 the Loss is 0.2613.\n",
      "For Iteration 6071 the Loss is 0.2613.\n",
      "For Iteration 6072 the Loss is 0.2613.\n",
      "For Iteration 6073 the Loss is 0.2613.\n",
      "For Iteration 6074 the Loss is 0.2613.\n",
      "For Iteration 6075 the Loss is 0.2613.\n",
      "For Iteration 6076 the Loss is 0.2613.\n",
      "For Iteration 6077 the Loss is 0.2613.\n",
      "For Iteration 6078 the Loss is 0.2613.\n",
      "For Iteration 6079 the Loss is 0.2613.\n",
      "For Iteration 6080 the Loss is 0.2613.\n",
      "For Iteration 6081 the Loss is 0.2613.\n",
      "For Iteration 6082 the Loss is 0.2613.\n",
      "For Iteration 6083 the Loss is 0.2613.\n",
      "For Iteration 6084 the Loss is 0.2613.\n",
      "For Iteration 6085 the Loss is 0.2613.\n",
      "For Iteration 6086 the Loss is 0.2613.\n",
      "For Iteration 6087 the Loss is 0.2613.\n",
      "For Iteration 6088 the Loss is 0.2613.\n",
      "For Iteration 6089 the Loss is 0.2613.\n",
      "For Iteration 6090 the Loss is 0.2613.\n",
      "For Iteration 6091 the Loss is 0.2613.\n",
      "For Iteration 6092 the Loss is 0.2613.\n",
      "For Iteration 6093 the Loss is 0.2613.\n",
      "For Iteration 6094 the Loss is 0.2613.\n",
      "For Iteration 6095 the Loss is 0.2613.\n",
      "For Iteration 6096 the Loss is 0.2613.\n",
      "For Iteration 6097 the Loss is 0.2613.\n",
      "For Iteration 6098 the Loss is 0.2613.\n",
      "For Iteration 6099 the Loss is 0.2613.\n",
      "For Iteration 6100 the Loss is 0.2613.\n",
      "For Iteration 6101 the Loss is 0.2613.\n",
      "For Iteration 6102 the Loss is 0.2613.\n",
      "For Iteration 6103 the Loss is 0.2613.\n",
      "For Iteration 6104 the Loss is 0.2613.\n",
      "For Iteration 6105 the Loss is 0.2613.\n",
      "For Iteration 6106 the Loss is 0.2613.\n",
      "For Iteration 6107 the Loss is 0.2613.\n",
      "For Iteration 6108 the Loss is 0.2613.\n",
      "For Iteration 6109 the Loss is 0.2613.\n",
      "For Iteration 6110 the Loss is 0.2613.\n",
      "For Iteration 6111 the Loss is 0.2613.\n",
      "For Iteration 6112 the Loss is 0.2613.\n",
      "For Iteration 6113 the Loss is 0.2613.\n",
      "For Iteration 6114 the Loss is 0.2613.\n",
      "For Iteration 6115 the Loss is 0.2613.\n",
      "For Iteration 6116 the Loss is 0.2613.\n",
      "For Iteration 6117 the Loss is 0.2613.\n",
      "For Iteration 6118 the Loss is 0.2613.\n",
      "For Iteration 6119 the Loss is 0.2613.\n",
      "For Iteration 6120 the Loss is 0.2613.\n",
      "For Iteration 6121 the Loss is 0.2613.\n",
      "For Iteration 6122 the Loss is 0.2613.\n",
      "For Iteration 6123 the Loss is 0.2613.\n",
      "For Iteration 6124 the Loss is 0.2613.\n",
      "For Iteration 6125 the Loss is 0.2613.\n",
      "For Iteration 6126 the Loss is 0.2613.\n",
      "For Iteration 6127 the Loss is 0.2613.\n",
      "For Iteration 6128 the Loss is 0.2613.\n",
      "For Iteration 6129 the Loss is 0.2613.\n",
      "For Iteration 6130 the Loss is 0.2613.\n",
      "For Iteration 6131 the Loss is 0.2613.\n",
      "For Iteration 6132 the Loss is 0.2613.\n",
      "For Iteration 6133 the Loss is 0.2613.\n",
      "For Iteration 6134 the Loss is 0.2613.\n",
      "For Iteration 6135 the Loss is 0.2613.\n",
      "For Iteration 6136 the Loss is 0.2613.\n",
      "For Iteration 6137 the Loss is 0.2613.\n",
      "For Iteration 6138 the Loss is 0.2613.\n",
      "For Iteration 6139 the Loss is 0.2613.\n",
      "For Iteration 6140 the Loss is 0.2613.\n",
      "For Iteration 6141 the Loss is 0.2613.\n",
      "For Iteration 6142 the Loss is 0.2613.\n",
      "For Iteration 6143 the Loss is 0.2613.\n",
      "For Iteration 6144 the Loss is 0.2613.\n",
      "For Iteration 6145 the Loss is 0.2613.\n",
      "For Iteration 6146 the Loss is 0.2613.\n",
      "For Iteration 6147 the Loss is 0.2613.\n",
      "For Iteration 6148 the Loss is 0.2613.\n",
      "For Iteration 6149 the Loss is 0.2613.\n",
      "For Iteration 6150 the Loss is 0.2613.\n",
      "For Iteration 6151 the Loss is 0.2613.\n",
      "For Iteration 6152 the Loss is 0.2613.\n",
      "For Iteration 6153 the Loss is 0.2613.\n",
      "For Iteration 6154 the Loss is 0.2613.\n",
      "For Iteration 6155 the Loss is 0.2613.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 6156 the Loss is 0.2613.\n",
      "For Iteration 6157 the Loss is 0.2613.\n",
      "For Iteration 6158 the Loss is 0.2613.\n",
      "For Iteration 6159 the Loss is 0.2613.\n",
      "For Iteration 6160 the Loss is 0.2613.\n",
      "For Iteration 6161 the Loss is 0.2613.\n",
      "For Iteration 6162 the Loss is 0.2613.\n",
      "For Iteration 6163 the Loss is 0.2613.\n",
      "For Iteration 6164 the Loss is 0.2613.\n",
      "For Iteration 6165 the Loss is 0.2613.\n",
      "For Iteration 6166 the Loss is 0.2613.\n",
      "For Iteration 6167 the Loss is 0.2613.\n",
      "For Iteration 6168 the Loss is 0.2613.\n",
      "For Iteration 6169 the Loss is 0.2613.\n",
      "For Iteration 6170 the Loss is 0.2613.\n",
      "For Iteration 6171 the Loss is 0.2613.\n",
      "For Iteration 6172 the Loss is 0.2613.\n",
      "For Iteration 6173 the Loss is 0.2613.\n",
      "For Iteration 6174 the Loss is 0.2613.\n",
      "For Iteration 6175 the Loss is 0.2613.\n",
      "For Iteration 6176 the Loss is 0.2613.\n",
      "For Iteration 6177 the Loss is 0.2613.\n",
      "For Iteration 6178 the Loss is 0.2613.\n",
      "For Iteration 6179 the Loss is 0.2613.\n",
      "For Iteration 6180 the Loss is 0.2613.\n",
      "For Iteration 6181 the Loss is 0.2613.\n",
      "For Iteration 6182 the Loss is 0.2613.\n",
      "For Iteration 6183 the Loss is 0.2613.\n",
      "For Iteration 6184 the Loss is 0.2613.\n",
      "For Iteration 6185 the Loss is 0.2613.\n",
      "For Iteration 6186 the Loss is 0.2613.\n",
      "For Iteration 6187 the Loss is 0.2613.\n",
      "For Iteration 6188 the Loss is 0.2613.\n",
      "For Iteration 6189 the Loss is 0.2613.\n",
      "For Iteration 6190 the Loss is 0.2613.\n",
      "For Iteration 6191 the Loss is 0.2613.\n",
      "For Iteration 6192 the Loss is 0.2613.\n",
      "For Iteration 6193 the Loss is 0.2613.\n",
      "For Iteration 6194 the Loss is 0.2613.\n",
      "For Iteration 6195 the Loss is 0.2613.\n",
      "For Iteration 6196 the Loss is 0.2613.\n",
      "For Iteration 6197 the Loss is 0.2613.\n",
      "For Iteration 6198 the Loss is 0.2613.\n",
      "For Iteration 6199 the Loss is 0.2613.\n",
      "For Iteration 6200 the Loss is 0.2613.\n",
      "For Iteration 6201 the Loss is 0.2613.\n",
      "For Iteration 6202 the Loss is 0.2613.\n",
      "For Iteration 6203 the Loss is 0.2613.\n",
      "For Iteration 6204 the Loss is 0.2613.\n",
      "For Iteration 6205 the Loss is 0.2613.\n",
      "For Iteration 6206 the Loss is 0.2613.\n",
      "For Iteration 6207 the Loss is 0.2613.\n",
      "For Iteration 6208 the Loss is 0.2613.\n",
      "For Iteration 6209 the Loss is 0.2613.\n",
      "For Iteration 6210 the Loss is 0.2613.\n",
      "For Iteration 6211 the Loss is 0.2613.\n",
      "For Iteration 6212 the Loss is 0.2613.\n",
      "For Iteration 6213 the Loss is 0.2612.\n",
      "For Iteration 6214 the Loss is 0.2612.\n",
      "For Iteration 6215 the Loss is 0.2612.\n",
      "For Iteration 6216 the Loss is 0.2612.\n",
      "For Iteration 6217 the Loss is 0.2612.\n",
      "For Iteration 6218 the Loss is 0.2612.\n",
      "For Iteration 6219 the Loss is 0.2612.\n",
      "For Iteration 6220 the Loss is 0.2612.\n",
      "For Iteration 6221 the Loss is 0.2612.\n",
      "For Iteration 6222 the Loss is 0.2612.\n",
      "For Iteration 6223 the Loss is 0.2612.\n",
      "For Iteration 6224 the Loss is 0.2612.\n",
      "For Iteration 6225 the Loss is 0.2612.\n",
      "For Iteration 6226 the Loss is 0.2612.\n",
      "For Iteration 6227 the Loss is 0.2612.\n",
      "For Iteration 6228 the Loss is 0.2612.\n",
      "For Iteration 6229 the Loss is 0.2612.\n",
      "For Iteration 6230 the Loss is 0.2612.\n",
      "For Iteration 6231 the Loss is 0.2612.\n",
      "For Iteration 6232 the Loss is 0.2612.\n",
      "For Iteration 6233 the Loss is 0.2612.\n",
      "For Iteration 6234 the Loss is 0.2612.\n",
      "For Iteration 6235 the Loss is 0.2612.\n",
      "For Iteration 6236 the Loss is 0.2612.\n",
      "For Iteration 6237 the Loss is 0.2612.\n",
      "For Iteration 6238 the Loss is 0.2612.\n",
      "For Iteration 6239 the Loss is 0.2612.\n",
      "For Iteration 6240 the Loss is 0.2612.\n",
      "For Iteration 6241 the Loss is 0.2612.\n",
      "For Iteration 6242 the Loss is 0.2612.\n",
      "For Iteration 6243 the Loss is 0.2612.\n",
      "For Iteration 6244 the Loss is 0.2612.\n",
      "For Iteration 6245 the Loss is 0.2612.\n",
      "For Iteration 6246 the Loss is 0.2612.\n",
      "For Iteration 6247 the Loss is 0.2612.\n",
      "For Iteration 6248 the Loss is 0.2612.\n",
      "For Iteration 6249 the Loss is 0.2612.\n",
      "For Iteration 6250 the Loss is 0.2612.\n",
      "For Iteration 6251 the Loss is 0.2612.\n",
      "For Iteration 6252 the Loss is 0.2612.\n",
      "For Iteration 6253 the Loss is 0.2612.\n",
      "For Iteration 6254 the Loss is 0.2612.\n",
      "For Iteration 6255 the Loss is 0.2612.\n",
      "For Iteration 6256 the Loss is 0.2612.\n",
      "For Iteration 6257 the Loss is 0.2612.\n",
      "For Iteration 6258 the Loss is 0.2612.\n",
      "For Iteration 6259 the Loss is 0.2612.\n",
      "For Iteration 6260 the Loss is 0.2612.\n",
      "For Iteration 6261 the Loss is 0.2612.\n",
      "For Iteration 6262 the Loss is 0.2612.\n",
      "For Iteration 6263 the Loss is 0.2612.\n",
      "For Iteration 6264 the Loss is 0.2612.\n",
      "For Iteration 6265 the Loss is 0.2612.\n",
      "For Iteration 6266 the Loss is 0.2612.\n",
      "For Iteration 6267 the Loss is 0.2612.\n",
      "For Iteration 6268 the Loss is 0.2612.\n",
      "For Iteration 6269 the Loss is 0.2612.\n",
      "For Iteration 6270 the Loss is 0.2612.\n",
      "For Iteration 6271 the Loss is 0.2612.\n",
      "For Iteration 6272 the Loss is 0.2612.\n",
      "For Iteration 6273 the Loss is 0.2612.\n",
      "For Iteration 6274 the Loss is 0.2612.\n",
      "For Iteration 6275 the Loss is 0.2612.\n",
      "For Iteration 6276 the Loss is 0.2612.\n",
      "For Iteration 6277 the Loss is 0.2612.\n",
      "For Iteration 6278 the Loss is 0.2612.\n",
      "For Iteration 6279 the Loss is 0.2612.\n",
      "For Iteration 6280 the Loss is 0.2612.\n",
      "For Iteration 6281 the Loss is 0.2612.\n",
      "For Iteration 6282 the Loss is 0.2612.\n",
      "For Iteration 6283 the Loss is 0.2612.\n",
      "For Iteration 6284 the Loss is 0.2612.\n",
      "For Iteration 6285 the Loss is 0.2612.\n",
      "For Iteration 6286 the Loss is 0.2612.\n",
      "For Iteration 6287 the Loss is 0.2612.\n",
      "For Iteration 6288 the Loss is 0.2612.\n",
      "For Iteration 6289 the Loss is 0.2612.\n",
      "For Iteration 6290 the Loss is 0.2612.\n",
      "For Iteration 6291 the Loss is 0.2612.\n",
      "For Iteration 6292 the Loss is 0.2612.\n",
      "For Iteration 6293 the Loss is 0.2612.\n",
      "For Iteration 6294 the Loss is 0.2612.\n",
      "For Iteration 6295 the Loss is 0.2612.\n",
      "For Iteration 6296 the Loss is 0.2612.\n",
      "For Iteration 6297 the Loss is 0.2612.\n",
      "For Iteration 6298 the Loss is 0.2612.\n",
      "For Iteration 6299 the Loss is 0.2612.\n",
      "For Iteration 6300 the Loss is 0.2612.\n",
      "For Iteration 6301 the Loss is 0.2612.\n",
      "For Iteration 6302 the Loss is 0.2612.\n",
      "For Iteration 6303 the Loss is 0.2612.\n",
      "For Iteration 6304 the Loss is 0.2612.\n",
      "For Iteration 6305 the Loss is 0.2612.\n",
      "For Iteration 6306 the Loss is 0.2612.\n",
      "For Iteration 6307 the Loss is 0.2612.\n",
      "For Iteration 6308 the Loss is 0.2612.\n",
      "For Iteration 6309 the Loss is 0.2612.\n",
      "For Iteration 6310 the Loss is 0.2612.\n",
      "For Iteration 6311 the Loss is 0.2612.\n",
      "For Iteration 6312 the Loss is 0.2612.\n",
      "For Iteration 6313 the Loss is 0.2612.\n",
      "For Iteration 6314 the Loss is 0.2612.\n",
      "For Iteration 6315 the Loss is 0.2612.\n",
      "For Iteration 6316 the Loss is 0.2612.\n",
      "For Iteration 6317 the Loss is 0.2612.\n",
      "For Iteration 6318 the Loss is 0.2612.\n",
      "For Iteration 6319 the Loss is 0.2612.\n",
      "For Iteration 6320 the Loss is 0.2612.\n",
      "For Iteration 6321 the Loss is 0.2612.\n",
      "For Iteration 6322 the Loss is 0.2612.\n",
      "For Iteration 6323 the Loss is 0.2612.\n",
      "For Iteration 6324 the Loss is 0.2612.\n",
      "For Iteration 6325 the Loss is 0.2612.\n",
      "For Iteration 6326 the Loss is 0.2612.\n",
      "For Iteration 6327 the Loss is 0.2612.\n",
      "For Iteration 6328 the Loss is 0.2612.\n",
      "For Iteration 6329 the Loss is 0.2612.\n",
      "For Iteration 6330 the Loss is 0.2612.\n",
      "For Iteration 6331 the Loss is 0.2612.\n",
      "For Iteration 6332 the Loss is 0.2612.\n",
      "For Iteration 6333 the Loss is 0.2612.\n",
      "For Iteration 6334 the Loss is 0.2612.\n",
      "For Iteration 6335 the Loss is 0.2612.\n",
      "For Iteration 6336 the Loss is 0.2612.\n",
      "For Iteration 6337 the Loss is 0.2612.\n",
      "For Iteration 6338 the Loss is 0.2612.\n",
      "For Iteration 6339 the Loss is 0.2612.\n",
      "For Iteration 6340 the Loss is 0.2612.\n",
      "For Iteration 6341 the Loss is 0.2612.\n",
      "For Iteration 6342 the Loss is 0.2612.\n",
      "For Iteration 6343 the Loss is 0.2612.\n",
      "For Iteration 6344 the Loss is 0.2612.\n",
      "For Iteration 6345 the Loss is 0.2612.\n",
      "For Iteration 6346 the Loss is 0.2612.\n",
      "For Iteration 6347 the Loss is 0.2612.\n",
      "For Iteration 6348 the Loss is 0.2612.\n",
      "For Iteration 6349 the Loss is 0.2612.\n",
      "For Iteration 6350 the Loss is 0.2612.\n",
      "For Iteration 6351 the Loss is 0.2612.\n",
      "For Iteration 6352 the Loss is 0.2612.\n",
      "For Iteration 6353 the Loss is 0.2612.\n",
      "For Iteration 6354 the Loss is 0.2612.\n",
      "For Iteration 6355 the Loss is 0.2612.\n",
      "For Iteration 6356 the Loss is 0.2612.\n",
      "For Iteration 6357 the Loss is 0.2612.\n",
      "For Iteration 6358 the Loss is 0.2612.\n",
      "For Iteration 6359 the Loss is 0.2612.\n",
      "For Iteration 6360 the Loss is 0.2612.\n",
      "For Iteration 6361 the Loss is 0.2612.\n",
      "For Iteration 6362 the Loss is 0.2612.\n",
      "For Iteration 6363 the Loss is 0.2612.\n",
      "For Iteration 6364 the Loss is 0.2612.\n",
      "For Iteration 6365 the Loss is 0.2612.\n",
      "For Iteration 6366 the Loss is 0.2612.\n",
      "For Iteration 6367 the Loss is 0.2612.\n",
      "For Iteration 6368 the Loss is 0.2612.\n",
      "For Iteration 6369 the Loss is 0.2612.\n",
      "For Iteration 6370 the Loss is 0.2612.\n",
      "For Iteration 6371 the Loss is 0.2612.\n",
      "For Iteration 6372 the Loss is 0.2612.\n",
      "For Iteration 6373 the Loss is 0.2612.\n",
      "For Iteration 6374 the Loss is 0.2612.\n",
      "For Iteration 6375 the Loss is 0.2612.\n",
      "For Iteration 6376 the Loss is 0.2612.\n",
      "For Iteration 6377 the Loss is 0.2612.\n",
      "For Iteration 6378 the Loss is 0.2612.\n",
      "For Iteration 6379 the Loss is 0.2612.\n",
      "For Iteration 6380 the Loss is 0.2612.\n",
      "For Iteration 6381 the Loss is 0.2612.\n",
      "For Iteration 6382 the Loss is 0.2612.\n",
      "For Iteration 6383 the Loss is 0.2612.\n",
      "For Iteration 6384 the Loss is 0.2612.\n",
      "For Iteration 6385 the Loss is 0.2612.\n",
      "For Iteration 6386 the Loss is 0.2612.\n",
      "For Iteration 6387 the Loss is 0.2612.\n",
      "For Iteration 6388 the Loss is 0.2612.\n",
      "For Iteration 6389 the Loss is 0.2612.\n",
      "For Iteration 6390 the Loss is 0.2612.\n",
      "For Iteration 6391 the Loss is 0.2612.\n",
      "For Iteration 6392 the Loss is 0.2612.\n",
      "For Iteration 6393 the Loss is 0.2612.\n",
      "For Iteration 6394 the Loss is 0.2612.\n",
      "For Iteration 6395 the Loss is 0.2612.\n",
      "For Iteration 6396 the Loss is 0.2612.\n",
      "For Iteration 6397 the Loss is 0.2612.\n",
      "For Iteration 6398 the Loss is 0.2612.\n",
      "For Iteration 6399 the Loss is 0.2612.\n",
      "For Iteration 6400 the Loss is 0.2612.\n",
      "For Iteration 6401 the Loss is 0.2612.\n",
      "For Iteration 6402 the Loss is 0.2612.\n",
      "For Iteration 6403 the Loss is 0.2612.\n",
      "For Iteration 6404 the Loss is 0.2612.\n",
      "For Iteration 6405 the Loss is 0.2612.\n",
      "For Iteration 6406 the Loss is 0.2612.\n",
      "For Iteration 6407 the Loss is 0.2612.\n",
      "For Iteration 6408 the Loss is 0.2612.\n",
      "For Iteration 6409 the Loss is 0.2612.\n",
      "For Iteration 6410 the Loss is 0.2612.\n",
      "For Iteration 6411 the Loss is 0.2612.\n",
      "For Iteration 6412 the Loss is 0.2612.\n",
      "For Iteration 6413 the Loss is 0.2612.\n",
      "For Iteration 6414 the Loss is 0.2612.\n",
      "For Iteration 6415 the Loss is 0.2612.\n",
      "For Iteration 6416 the Loss is 0.2612.\n",
      "For Iteration 6417 the Loss is 0.2612.\n",
      "For Iteration 6418 the Loss is 0.2612.\n",
      "For Iteration 6419 the Loss is 0.2612.\n",
      "For Iteration 6420 the Loss is 0.2612.\n",
      "For Iteration 6421 the Loss is 0.2612.\n",
      "For Iteration 6422 the Loss is 0.2612.\n",
      "For Iteration 6423 the Loss is 0.2612.\n",
      "For Iteration 6424 the Loss is 0.2612.\n",
      "For Iteration 6425 the Loss is 0.2612.\n",
      "For Iteration 6426 the Loss is 0.2612.\n",
      "For Iteration 6427 the Loss is 0.2612.\n",
      "For Iteration 6428 the Loss is 0.2612.\n",
      "For Iteration 6429 the Loss is 0.2612.\n",
      "For Iteration 6430 the Loss is 0.2612.\n",
      "For Iteration 6431 the Loss is 0.2612.\n",
      "For Iteration 6432 the Loss is 0.2612.\n",
      "For Iteration 6433 the Loss is 0.2612.\n",
      "For Iteration 6434 the Loss is 0.2612.\n",
      "For Iteration 6435 the Loss is 0.2612.\n",
      "For Iteration 6436 the Loss is 0.2612.\n",
      "For Iteration 6437 the Loss is 0.2612.\n",
      "For Iteration 6438 the Loss is 0.2612.\n",
      "For Iteration 6439 the Loss is 0.2612.\n",
      "For Iteration 6440 the Loss is 0.2612.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 6441 the Loss is 0.2612.\n",
      "For Iteration 6442 the Loss is 0.2612.\n",
      "For Iteration 6443 the Loss is 0.2612.\n",
      "For Iteration 6444 the Loss is 0.2612.\n",
      "For Iteration 6445 the Loss is 0.2612.\n",
      "For Iteration 6446 the Loss is 0.2612.\n",
      "For Iteration 6447 the Loss is 0.2612.\n",
      "For Iteration 6448 the Loss is 0.2612.\n",
      "For Iteration 6449 the Loss is 0.2612.\n",
      "For Iteration 6450 the Loss is 0.2612.\n",
      "For Iteration 6451 the Loss is 0.2612.\n",
      "For Iteration 6452 the Loss is 0.2612.\n",
      "For Iteration 6453 the Loss is 0.2612.\n",
      "For Iteration 6454 the Loss is 0.2612.\n",
      "For Iteration 6455 the Loss is 0.2612.\n",
      "For Iteration 6456 the Loss is 0.2612.\n",
      "For Iteration 6457 the Loss is 0.2612.\n",
      "For Iteration 6458 the Loss is 0.2612.\n",
      "For Iteration 6459 the Loss is 0.2612.\n",
      "For Iteration 6460 the Loss is 0.2612.\n",
      "For Iteration 6461 the Loss is 0.2612.\n",
      "For Iteration 6462 the Loss is 0.2612.\n",
      "For Iteration 6463 the Loss is 0.2612.\n",
      "For Iteration 6464 the Loss is 0.2612.\n",
      "For Iteration 6465 the Loss is 0.2612.\n",
      "For Iteration 6466 the Loss is 0.2612.\n",
      "For Iteration 6467 the Loss is 0.2612.\n",
      "For Iteration 6468 the Loss is 0.2612.\n",
      "For Iteration 6469 the Loss is 0.2612.\n",
      "For Iteration 6470 the Loss is 0.2612.\n",
      "For Iteration 6471 the Loss is 0.2612.\n",
      "For Iteration 6472 the Loss is 0.2612.\n",
      "For Iteration 6473 the Loss is 0.2612.\n",
      "For Iteration 6474 the Loss is 0.2612.\n",
      "For Iteration 6475 the Loss is 0.2612.\n",
      "For Iteration 6476 the Loss is 0.2612.\n",
      "For Iteration 6477 the Loss is 0.2612.\n",
      "For Iteration 6478 the Loss is 0.2612.\n",
      "For Iteration 6479 the Loss is 0.2612.\n",
      "For Iteration 6480 the Loss is 0.2612.\n",
      "For Iteration 6481 the Loss is 0.2612.\n",
      "For Iteration 6482 the Loss is 0.2612.\n",
      "For Iteration 6483 the Loss is 0.2612.\n",
      "For Iteration 6484 the Loss is 0.2612.\n",
      "For Iteration 6485 the Loss is 0.2612.\n",
      "For Iteration 6486 the Loss is 0.2612.\n",
      "For Iteration 6487 the Loss is 0.2612.\n",
      "For Iteration 6488 the Loss is 0.2612.\n",
      "For Iteration 6489 the Loss is 0.2612.\n",
      "For Iteration 6490 the Loss is 0.2612.\n",
      "For Iteration 6491 the Loss is 0.2612.\n",
      "For Iteration 6492 the Loss is 0.2612.\n",
      "For Iteration 6493 the Loss is 0.2612.\n",
      "For Iteration 6494 the Loss is 0.2612.\n",
      "For Iteration 6495 the Loss is 0.2612.\n",
      "For Iteration 6496 the Loss is 0.2612.\n",
      "For Iteration 6497 the Loss is 0.2612.\n",
      "For Iteration 6498 the Loss is 0.2612.\n",
      "For Iteration 6499 the Loss is 0.2612.\n",
      "For Iteration 6500 the Loss is 0.2612.\n",
      "For Iteration 6501 the Loss is 0.2612.\n",
      "For Iteration 6502 the Loss is 0.2611.\n",
      "For Iteration 6503 the Loss is 0.2611.\n",
      "For Iteration 6504 the Loss is 0.2611.\n",
      "For Iteration 6505 the Loss is 0.2611.\n",
      "For Iteration 6506 the Loss is 0.2611.\n",
      "For Iteration 6507 the Loss is 0.2611.\n",
      "For Iteration 6508 the Loss is 0.2611.\n",
      "For Iteration 6509 the Loss is 0.2611.\n",
      "For Iteration 6510 the Loss is 0.2611.\n",
      "For Iteration 6511 the Loss is 0.2611.\n",
      "For Iteration 6512 the Loss is 0.2611.\n",
      "For Iteration 6513 the Loss is 0.2611.\n",
      "For Iteration 6514 the Loss is 0.2611.\n",
      "For Iteration 6515 the Loss is 0.2611.\n",
      "For Iteration 6516 the Loss is 0.2611.\n",
      "For Iteration 6517 the Loss is 0.2611.\n",
      "For Iteration 6518 the Loss is 0.2611.\n",
      "For Iteration 6519 the Loss is 0.2611.\n",
      "For Iteration 6520 the Loss is 0.2611.\n",
      "For Iteration 6521 the Loss is 0.2611.\n",
      "For Iteration 6522 the Loss is 0.2611.\n",
      "For Iteration 6523 the Loss is 0.2611.\n",
      "For Iteration 6524 the Loss is 0.2611.\n",
      "For Iteration 6525 the Loss is 0.2611.\n",
      "For Iteration 6526 the Loss is 0.2611.\n",
      "For Iteration 6527 the Loss is 0.2611.\n",
      "For Iteration 6528 the Loss is 0.2611.\n",
      "For Iteration 6529 the Loss is 0.2611.\n",
      "For Iteration 6530 the Loss is 0.2611.\n",
      "For Iteration 6531 the Loss is 0.2611.\n",
      "For Iteration 6532 the Loss is 0.2611.\n",
      "For Iteration 6533 the Loss is 0.2611.\n",
      "For Iteration 6534 the Loss is 0.2611.\n",
      "For Iteration 6535 the Loss is 0.2611.\n",
      "For Iteration 6536 the Loss is 0.2611.\n",
      "For Iteration 6537 the Loss is 0.2611.\n",
      "For Iteration 6538 the Loss is 0.2611.\n",
      "For Iteration 6539 the Loss is 0.2611.\n",
      "For Iteration 6540 the Loss is 0.2611.\n",
      "For Iteration 6541 the Loss is 0.2611.\n",
      "For Iteration 6542 the Loss is 0.2611.\n",
      "For Iteration 6543 the Loss is 0.2611.\n",
      "For Iteration 6544 the Loss is 0.2611.\n",
      "For Iteration 6545 the Loss is 0.2611.\n",
      "For Iteration 6546 the Loss is 0.2611.\n",
      "For Iteration 6547 the Loss is 0.2611.\n",
      "For Iteration 6548 the Loss is 0.2611.\n",
      "For Iteration 6549 the Loss is 0.2611.\n",
      "For Iteration 6550 the Loss is 0.2611.\n",
      "For Iteration 6551 the Loss is 0.2611.\n",
      "For Iteration 6552 the Loss is 0.2611.\n",
      "For Iteration 6553 the Loss is 0.2611.\n",
      "For Iteration 6554 the Loss is 0.2611.\n",
      "For Iteration 6555 the Loss is 0.2611.\n",
      "For Iteration 6556 the Loss is 0.2611.\n",
      "For Iteration 6557 the Loss is 0.2611.\n",
      "For Iteration 6558 the Loss is 0.2611.\n",
      "For Iteration 6559 the Loss is 0.2611.\n",
      "For Iteration 6560 the Loss is 0.2611.\n",
      "For Iteration 6561 the Loss is 0.2611.\n",
      "For Iteration 6562 the Loss is 0.2611.\n",
      "For Iteration 6563 the Loss is 0.2611.\n",
      "For Iteration 6564 the Loss is 0.2611.\n",
      "For Iteration 6565 the Loss is 0.2611.\n",
      "For Iteration 6566 the Loss is 0.2611.\n",
      "For Iteration 6567 the Loss is 0.2611.\n",
      "For Iteration 6568 the Loss is 0.2611.\n",
      "For Iteration 6569 the Loss is 0.2611.\n",
      "For Iteration 6570 the Loss is 0.2611.\n",
      "For Iteration 6571 the Loss is 0.2611.\n",
      "For Iteration 6572 the Loss is 0.2611.\n",
      "For Iteration 6573 the Loss is 0.2611.\n",
      "For Iteration 6574 the Loss is 0.2611.\n",
      "For Iteration 6575 the Loss is 0.2611.\n",
      "For Iteration 6576 the Loss is 0.2611.\n",
      "For Iteration 6577 the Loss is 0.2611.\n",
      "For Iteration 6578 the Loss is 0.2611.\n",
      "For Iteration 6579 the Loss is 0.2611.\n",
      "For Iteration 6580 the Loss is 0.2611.\n",
      "For Iteration 6581 the Loss is 0.2611.\n",
      "For Iteration 6582 the Loss is 0.2611.\n",
      "For Iteration 6583 the Loss is 0.2611.\n",
      "For Iteration 6584 the Loss is 0.2611.\n",
      "For Iteration 6585 the Loss is 0.2611.\n",
      "For Iteration 6586 the Loss is 0.2611.\n",
      "For Iteration 6587 the Loss is 0.2611.\n",
      "For Iteration 6588 the Loss is 0.2611.\n",
      "For Iteration 6589 the Loss is 0.2611.\n",
      "For Iteration 6590 the Loss is 0.2611.\n",
      "For Iteration 6591 the Loss is 0.2611.\n",
      "For Iteration 6592 the Loss is 0.2611.\n",
      "For Iteration 6593 the Loss is 0.2611.\n",
      "For Iteration 6594 the Loss is 0.2611.\n",
      "For Iteration 6595 the Loss is 0.2611.\n",
      "For Iteration 6596 the Loss is 0.2611.\n",
      "For Iteration 6597 the Loss is 0.2611.\n",
      "For Iteration 6598 the Loss is 0.2611.\n",
      "For Iteration 6599 the Loss is 0.2611.\n",
      "For Iteration 6600 the Loss is 0.2611.\n",
      "For Iteration 6601 the Loss is 0.2611.\n",
      "For Iteration 6602 the Loss is 0.2611.\n",
      "For Iteration 6603 the Loss is 0.2611.\n",
      "For Iteration 6604 the Loss is 0.2611.\n",
      "For Iteration 6605 the Loss is 0.2611.\n",
      "For Iteration 6606 the Loss is 0.2611.\n",
      "For Iteration 6607 the Loss is 0.2611.\n",
      "For Iteration 6608 the Loss is 0.2611.\n",
      "For Iteration 6609 the Loss is 0.2611.\n",
      "For Iteration 6610 the Loss is 0.2611.\n",
      "For Iteration 6611 the Loss is 0.2611.\n",
      "For Iteration 6612 the Loss is 0.2611.\n",
      "For Iteration 6613 the Loss is 0.2611.\n",
      "For Iteration 6614 the Loss is 0.2611.\n",
      "For Iteration 6615 the Loss is 0.2611.\n",
      "For Iteration 6616 the Loss is 0.2611.\n",
      "For Iteration 6617 the Loss is 0.2611.\n",
      "For Iteration 6618 the Loss is 0.2611.\n",
      "For Iteration 6619 the Loss is 0.2611.\n",
      "For Iteration 6620 the Loss is 0.2611.\n",
      "For Iteration 6621 the Loss is 0.2611.\n",
      "For Iteration 6622 the Loss is 0.2611.\n",
      "For Iteration 6623 the Loss is 0.2611.\n",
      "For Iteration 6624 the Loss is 0.2611.\n",
      "For Iteration 6625 the Loss is 0.2611.\n",
      "For Iteration 6626 the Loss is 0.2611.\n",
      "For Iteration 6627 the Loss is 0.2611.\n",
      "For Iteration 6628 the Loss is 0.2611.\n",
      "For Iteration 6629 the Loss is 0.2611.\n",
      "For Iteration 6630 the Loss is 0.2611.\n",
      "For Iteration 6631 the Loss is 0.2611.\n",
      "For Iteration 6632 the Loss is 0.2611.\n",
      "For Iteration 6633 the Loss is 0.2611.\n",
      "For Iteration 6634 the Loss is 0.2611.\n",
      "For Iteration 6635 the Loss is 0.2611.\n",
      "For Iteration 6636 the Loss is 0.2611.\n",
      "For Iteration 6637 the Loss is 0.2611.\n",
      "For Iteration 6638 the Loss is 0.2611.\n",
      "For Iteration 6639 the Loss is 0.2611.\n",
      "For Iteration 6640 the Loss is 0.2611.\n",
      "For Iteration 6641 the Loss is 0.2611.\n",
      "For Iteration 6642 the Loss is 0.2611.\n",
      "For Iteration 6643 the Loss is 0.2611.\n",
      "For Iteration 6644 the Loss is 0.2611.\n",
      "For Iteration 6645 the Loss is 0.2611.\n",
      "For Iteration 6646 the Loss is 0.2611.\n",
      "For Iteration 6647 the Loss is 0.2611.\n",
      "For Iteration 6648 the Loss is 0.2611.\n",
      "For Iteration 6649 the Loss is 0.2611.\n",
      "For Iteration 6650 the Loss is 0.2611.\n",
      "For Iteration 6651 the Loss is 0.2611.\n",
      "For Iteration 6652 the Loss is 0.2611.\n",
      "For Iteration 6653 the Loss is 0.2611.\n",
      "For Iteration 6654 the Loss is 0.2611.\n",
      "For Iteration 6655 the Loss is 0.2611.\n",
      "For Iteration 6656 the Loss is 0.2611.\n",
      "For Iteration 6657 the Loss is 0.2611.\n",
      "For Iteration 6658 the Loss is 0.2611.\n",
      "For Iteration 6659 the Loss is 0.2611.\n",
      "For Iteration 6660 the Loss is 0.2611.\n",
      "For Iteration 6661 the Loss is 0.2611.\n",
      "For Iteration 6662 the Loss is 0.2611.\n",
      "For Iteration 6663 the Loss is 0.2611.\n",
      "For Iteration 6664 the Loss is 0.2611.\n",
      "For Iteration 6665 the Loss is 0.2611.\n",
      "For Iteration 6666 the Loss is 0.2611.\n",
      "For Iteration 6667 the Loss is 0.2611.\n",
      "For Iteration 6668 the Loss is 0.2611.\n",
      "For Iteration 6669 the Loss is 0.2611.\n",
      "For Iteration 6670 the Loss is 0.2611.\n",
      "For Iteration 6671 the Loss is 0.2611.\n",
      "For Iteration 6672 the Loss is 0.2611.\n",
      "For Iteration 6673 the Loss is 0.2611.\n",
      "For Iteration 6674 the Loss is 0.2611.\n",
      "For Iteration 6675 the Loss is 0.2611.\n",
      "For Iteration 6676 the Loss is 0.2611.\n",
      "For Iteration 6677 the Loss is 0.2611.\n",
      "For Iteration 6678 the Loss is 0.2611.\n",
      "For Iteration 6679 the Loss is 0.2611.\n",
      "For Iteration 6680 the Loss is 0.2611.\n",
      "For Iteration 6681 the Loss is 0.2611.\n",
      "For Iteration 6682 the Loss is 0.2611.\n",
      "For Iteration 6683 the Loss is 0.2611.\n",
      "For Iteration 6684 the Loss is 0.2611.\n",
      "For Iteration 6685 the Loss is 0.2611.\n",
      "For Iteration 6686 the Loss is 0.2611.\n",
      "For Iteration 6687 the Loss is 0.2611.\n",
      "For Iteration 6688 the Loss is 0.2611.\n",
      "For Iteration 6689 the Loss is 0.2611.\n",
      "For Iteration 6690 the Loss is 0.2611.\n",
      "For Iteration 6691 the Loss is 0.2611.\n",
      "For Iteration 6692 the Loss is 0.2611.\n",
      "For Iteration 6693 the Loss is 0.2611.\n",
      "For Iteration 6694 the Loss is 0.2611.\n",
      "For Iteration 6695 the Loss is 0.2611.\n",
      "For Iteration 6696 the Loss is 0.2611.\n",
      "For Iteration 6697 the Loss is 0.2611.\n",
      "For Iteration 6698 the Loss is 0.2611.\n",
      "For Iteration 6699 the Loss is 0.2611.\n",
      "For Iteration 6700 the Loss is 0.2611.\n",
      "For Iteration 6701 the Loss is 0.2611.\n",
      "For Iteration 6702 the Loss is 0.2611.\n",
      "For Iteration 6703 the Loss is 0.2611.\n",
      "For Iteration 6704 the Loss is 0.2611.\n",
      "For Iteration 6705 the Loss is 0.2611.\n",
      "For Iteration 6706 the Loss is 0.2611.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 6707 the Loss is 0.2611.\n",
      "For Iteration 6708 the Loss is 0.2611.\n",
      "For Iteration 6709 the Loss is 0.2611.\n",
      "For Iteration 6710 the Loss is 0.2611.\n",
      "For Iteration 6711 the Loss is 0.2611.\n",
      "For Iteration 6712 the Loss is 0.2611.\n",
      "For Iteration 6713 the Loss is 0.2611.\n",
      "For Iteration 6714 the Loss is 0.2611.\n",
      "For Iteration 6715 the Loss is 0.2611.\n",
      "For Iteration 6716 the Loss is 0.2611.\n",
      "For Iteration 6717 the Loss is 0.2611.\n",
      "For Iteration 6718 the Loss is 0.2611.\n",
      "For Iteration 6719 the Loss is 0.2611.\n",
      "For Iteration 6720 the Loss is 0.2611.\n",
      "For Iteration 6721 the Loss is 0.2611.\n",
      "For Iteration 6722 the Loss is 0.2611.\n",
      "For Iteration 6723 the Loss is 0.2611.\n",
      "For Iteration 6724 the Loss is 0.2611.\n",
      "For Iteration 6725 the Loss is 0.2611.\n",
      "For Iteration 6726 the Loss is 0.2611.\n",
      "For Iteration 6727 the Loss is 0.2611.\n",
      "For Iteration 6728 the Loss is 0.2611.\n",
      "For Iteration 6729 the Loss is 0.2611.\n",
      "For Iteration 6730 the Loss is 0.2611.\n",
      "For Iteration 6731 the Loss is 0.2611.\n",
      "For Iteration 6732 the Loss is 0.2611.\n",
      "For Iteration 6733 the Loss is 0.2611.\n",
      "For Iteration 6734 the Loss is 0.2611.\n",
      "For Iteration 6735 the Loss is 0.2611.\n",
      "For Iteration 6736 the Loss is 0.2611.\n",
      "For Iteration 6737 the Loss is 0.2611.\n",
      "For Iteration 6738 the Loss is 0.2611.\n",
      "For Iteration 6739 the Loss is 0.2611.\n",
      "For Iteration 6740 the Loss is 0.2611.\n",
      "For Iteration 6741 the Loss is 0.2611.\n",
      "For Iteration 6742 the Loss is 0.2611.\n",
      "For Iteration 6743 the Loss is 0.2611.\n",
      "For Iteration 6744 the Loss is 0.2611.\n",
      "For Iteration 6745 the Loss is 0.2611.\n",
      "For Iteration 6746 the Loss is 0.2611.\n",
      "For Iteration 6747 the Loss is 0.2611.\n",
      "For Iteration 6748 the Loss is 0.2611.\n",
      "For Iteration 6749 the Loss is 0.2611.\n",
      "For Iteration 6750 the Loss is 0.2611.\n",
      "For Iteration 6751 the Loss is 0.2611.\n",
      "For Iteration 6752 the Loss is 0.2611.\n",
      "For Iteration 6753 the Loss is 0.2611.\n",
      "For Iteration 6754 the Loss is 0.2611.\n",
      "For Iteration 6755 the Loss is 0.2611.\n",
      "For Iteration 6756 the Loss is 0.2611.\n",
      "For Iteration 6757 the Loss is 0.2611.\n",
      "For Iteration 6758 the Loss is 0.2611.\n",
      "For Iteration 6759 the Loss is 0.2611.\n",
      "For Iteration 6760 the Loss is 0.2611.\n",
      "For Iteration 6761 the Loss is 0.2611.\n",
      "For Iteration 6762 the Loss is 0.2611.\n",
      "For Iteration 6763 the Loss is 0.2611.\n",
      "For Iteration 6764 the Loss is 0.2611.\n",
      "For Iteration 6765 the Loss is 0.2611.\n",
      "For Iteration 6766 the Loss is 0.2611.\n",
      "For Iteration 6767 the Loss is 0.2611.\n",
      "For Iteration 6768 the Loss is 0.2611.\n",
      "For Iteration 6769 the Loss is 0.2611.\n",
      "For Iteration 6770 the Loss is 0.2611.\n",
      "For Iteration 6771 the Loss is 0.2611.\n",
      "For Iteration 6772 the Loss is 0.2611.\n",
      "For Iteration 6773 the Loss is 0.2611.\n",
      "For Iteration 6774 the Loss is 0.2611.\n",
      "For Iteration 6775 the Loss is 0.2611.\n",
      "For Iteration 6776 the Loss is 0.2611.\n",
      "For Iteration 6777 the Loss is 0.2611.\n",
      "For Iteration 6778 the Loss is 0.2611.\n",
      "For Iteration 6779 the Loss is 0.2611.\n",
      "For Iteration 6780 the Loss is 0.2611.\n",
      "For Iteration 6781 the Loss is 0.2611.\n",
      "For Iteration 6782 the Loss is 0.2611.\n",
      "For Iteration 6783 the Loss is 0.2611.\n",
      "For Iteration 6784 the Loss is 0.2611.\n",
      "For Iteration 6785 the Loss is 0.2611.\n",
      "For Iteration 6786 the Loss is 0.2611.\n",
      "For Iteration 6787 the Loss is 0.2611.\n",
      "For Iteration 6788 the Loss is 0.2611.\n",
      "For Iteration 6789 the Loss is 0.2611.\n",
      "For Iteration 6790 the Loss is 0.2611.\n",
      "For Iteration 6791 the Loss is 0.2611.\n",
      "For Iteration 6792 the Loss is 0.2611.\n",
      "For Iteration 6793 the Loss is 0.2611.\n",
      "For Iteration 6794 the Loss is 0.2611.\n",
      "For Iteration 6795 the Loss is 0.2611.\n",
      "For Iteration 6796 the Loss is 0.2611.\n",
      "For Iteration 6797 the Loss is 0.2611.\n",
      "For Iteration 6798 the Loss is 0.2611.\n",
      "For Iteration 6799 the Loss is 0.2611.\n",
      "For Iteration 6800 the Loss is 0.2611.\n",
      "For Iteration 6801 the Loss is 0.2611.\n",
      "For Iteration 6802 the Loss is 0.2611.\n",
      "For Iteration 6803 the Loss is 0.2611.\n",
      "For Iteration 6804 the Loss is 0.2611.\n",
      "For Iteration 6805 the Loss is 0.2611.\n",
      "For Iteration 6806 the Loss is 0.2611.\n",
      "For Iteration 6807 the Loss is 0.2611.\n",
      "For Iteration 6808 the Loss is 0.2611.\n",
      "For Iteration 6809 the Loss is 0.2611.\n",
      "For Iteration 6810 the Loss is 0.2611.\n",
      "For Iteration 6811 the Loss is 0.2611.\n",
      "For Iteration 6812 the Loss is 0.2611.\n",
      "For Iteration 6813 the Loss is 0.2611.\n",
      "For Iteration 6814 the Loss is 0.2611.\n",
      "For Iteration 6815 the Loss is 0.2611.\n",
      "For Iteration 6816 the Loss is 0.2611.\n",
      "For Iteration 6817 the Loss is 0.2611.\n",
      "For Iteration 6818 the Loss is 0.2611.\n",
      "For Iteration 6819 the Loss is 0.2611.\n",
      "For Iteration 6820 the Loss is 0.2611.\n",
      "For Iteration 6821 the Loss is 0.261.\n",
      "For Iteration 6822 the Loss is 0.261.\n",
      "For Iteration 6823 the Loss is 0.261.\n",
      "For Iteration 6824 the Loss is 0.261.\n",
      "For Iteration 6825 the Loss is 0.261.\n",
      "For Iteration 6826 the Loss is 0.261.\n",
      "For Iteration 6827 the Loss is 0.261.\n",
      "For Iteration 6828 the Loss is 0.261.\n",
      "For Iteration 6829 the Loss is 0.261.\n",
      "For Iteration 6830 the Loss is 0.261.\n",
      "For Iteration 6831 the Loss is 0.261.\n",
      "For Iteration 6832 the Loss is 0.261.\n",
      "For Iteration 6833 the Loss is 0.261.\n",
      "For Iteration 6834 the Loss is 0.261.\n",
      "For Iteration 6835 the Loss is 0.261.\n",
      "For Iteration 6836 the Loss is 0.261.\n",
      "For Iteration 6837 the Loss is 0.261.\n",
      "For Iteration 6838 the Loss is 0.261.\n",
      "For Iteration 6839 the Loss is 0.261.\n",
      "For Iteration 6840 the Loss is 0.261.\n",
      "For Iteration 6841 the Loss is 0.261.\n",
      "For Iteration 6842 the Loss is 0.261.\n",
      "For Iteration 6843 the Loss is 0.261.\n",
      "For Iteration 6844 the Loss is 0.261.\n",
      "For Iteration 6845 the Loss is 0.261.\n",
      "For Iteration 6846 the Loss is 0.261.\n",
      "For Iteration 6847 the Loss is 0.261.\n",
      "For Iteration 6848 the Loss is 0.261.\n",
      "For Iteration 6849 the Loss is 0.261.\n",
      "For Iteration 6850 the Loss is 0.261.\n",
      "For Iteration 6851 the Loss is 0.261.\n",
      "For Iteration 6852 the Loss is 0.261.\n",
      "For Iteration 6853 the Loss is 0.261.\n",
      "For Iteration 6854 the Loss is 0.261.\n",
      "For Iteration 6855 the Loss is 0.261.\n",
      "For Iteration 6856 the Loss is 0.261.\n",
      "For Iteration 6857 the Loss is 0.261.\n",
      "For Iteration 6858 the Loss is 0.261.\n",
      "For Iteration 6859 the Loss is 0.261.\n",
      "For Iteration 6860 the Loss is 0.261.\n",
      "For Iteration 6861 the Loss is 0.261.\n",
      "For Iteration 6862 the Loss is 0.261.\n",
      "For Iteration 6863 the Loss is 0.261.\n",
      "For Iteration 6864 the Loss is 0.261.\n",
      "For Iteration 6865 the Loss is 0.261.\n",
      "For Iteration 6866 the Loss is 0.261.\n",
      "For Iteration 6867 the Loss is 0.261.\n",
      "For Iteration 6868 the Loss is 0.261.\n",
      "For Iteration 6869 the Loss is 0.261.\n",
      "For Iteration 6870 the Loss is 0.261.\n",
      "For Iteration 6871 the Loss is 0.261.\n",
      "For Iteration 6872 the Loss is 0.261.\n",
      "For Iteration 6873 the Loss is 0.261.\n",
      "For Iteration 6874 the Loss is 0.261.\n",
      "For Iteration 6875 the Loss is 0.261.\n",
      "For Iteration 6876 the Loss is 0.261.\n",
      "For Iteration 6877 the Loss is 0.261.\n",
      "For Iteration 6878 the Loss is 0.261.\n",
      "For Iteration 6879 the Loss is 0.261.\n",
      "For Iteration 6880 the Loss is 0.261.\n",
      "For Iteration 6881 the Loss is 0.261.\n",
      "For Iteration 6882 the Loss is 0.261.\n",
      "For Iteration 6883 the Loss is 0.261.\n",
      "For Iteration 6884 the Loss is 0.261.\n",
      "For Iteration 6885 the Loss is 0.261.\n",
      "For Iteration 6886 the Loss is 0.261.\n",
      "For Iteration 6887 the Loss is 0.261.\n",
      "For Iteration 6888 the Loss is 0.261.\n",
      "For Iteration 6889 the Loss is 0.261.\n",
      "For Iteration 6890 the Loss is 0.261.\n",
      "For Iteration 6891 the Loss is 0.261.\n",
      "For Iteration 6892 the Loss is 0.261.\n",
      "For Iteration 6893 the Loss is 0.261.\n",
      "For Iteration 6894 the Loss is 0.261.\n",
      "For Iteration 6895 the Loss is 0.261.\n",
      "For Iteration 6896 the Loss is 0.261.\n",
      "For Iteration 6897 the Loss is 0.261.\n",
      "For Iteration 6898 the Loss is 0.261.\n",
      "For Iteration 6899 the Loss is 0.261.\n",
      "For Iteration 6900 the Loss is 0.261.\n",
      "For Iteration 6901 the Loss is 0.261.\n",
      "For Iteration 6902 the Loss is 0.261.\n",
      "For Iteration 6903 the Loss is 0.261.\n",
      "For Iteration 6904 the Loss is 0.261.\n",
      "For Iteration 6905 the Loss is 0.261.\n",
      "For Iteration 6906 the Loss is 0.261.\n",
      "For Iteration 6907 the Loss is 0.261.\n",
      "For Iteration 6908 the Loss is 0.261.\n",
      "For Iteration 6909 the Loss is 0.261.\n",
      "For Iteration 6910 the Loss is 0.261.\n",
      "For Iteration 6911 the Loss is 0.261.\n",
      "For Iteration 6912 the Loss is 0.261.\n",
      "For Iteration 6913 the Loss is 0.261.\n",
      "For Iteration 6914 the Loss is 0.261.\n",
      "For Iteration 6915 the Loss is 0.261.\n",
      "For Iteration 6916 the Loss is 0.261.\n",
      "For Iteration 6917 the Loss is 0.261.\n",
      "For Iteration 6918 the Loss is 0.261.\n",
      "For Iteration 6919 the Loss is 0.261.\n",
      "For Iteration 6920 the Loss is 0.261.\n",
      "For Iteration 6921 the Loss is 0.261.\n",
      "For Iteration 6922 the Loss is 0.261.\n",
      "For Iteration 6923 the Loss is 0.261.\n",
      "For Iteration 6924 the Loss is 0.261.\n",
      "For Iteration 6925 the Loss is 0.261.\n",
      "For Iteration 6926 the Loss is 0.261.\n",
      "For Iteration 6927 the Loss is 0.261.\n",
      "For Iteration 6928 the Loss is 0.261.\n",
      "For Iteration 6929 the Loss is 0.261.\n",
      "For Iteration 6930 the Loss is 0.261.\n",
      "For Iteration 6931 the Loss is 0.261.\n",
      "For Iteration 6932 the Loss is 0.261.\n",
      "For Iteration 6933 the Loss is 0.261.\n",
      "For Iteration 6934 the Loss is 0.261.\n",
      "For Iteration 6935 the Loss is 0.261.\n",
      "For Iteration 6936 the Loss is 0.261.\n",
      "For Iteration 6937 the Loss is 0.261.\n",
      "For Iteration 6938 the Loss is 0.261.\n",
      "For Iteration 6939 the Loss is 0.261.\n",
      "For Iteration 6940 the Loss is 0.261.\n",
      "For Iteration 6941 the Loss is 0.261.\n",
      "For Iteration 6942 the Loss is 0.261.\n",
      "For Iteration 6943 the Loss is 0.261.\n",
      "For Iteration 6944 the Loss is 0.261.\n",
      "For Iteration 6945 the Loss is 0.261.\n",
      "For Iteration 6946 the Loss is 0.261.\n",
      "For Iteration 6947 the Loss is 0.261.\n",
      "For Iteration 6948 the Loss is 0.261.\n",
      "For Iteration 6949 the Loss is 0.261.\n",
      "For Iteration 6950 the Loss is 0.261.\n",
      "For Iteration 6951 the Loss is 0.261.\n",
      "For Iteration 6952 the Loss is 0.261.\n",
      "For Iteration 6953 the Loss is 0.261.\n",
      "For Iteration 6954 the Loss is 0.261.\n",
      "For Iteration 6955 the Loss is 0.261.\n",
      "For Iteration 6956 the Loss is 0.261.\n",
      "For Iteration 6957 the Loss is 0.261.\n",
      "For Iteration 6958 the Loss is 0.261.\n",
      "For Iteration 6959 the Loss is 0.261.\n",
      "For Iteration 6960 the Loss is 0.261.\n",
      "For Iteration 6961 the Loss is 0.261.\n",
      "For Iteration 6962 the Loss is 0.261.\n",
      "For Iteration 6963 the Loss is 0.261.\n",
      "For Iteration 6964 the Loss is 0.261.\n",
      "For Iteration 6965 the Loss is 0.261.\n",
      "For Iteration 6966 the Loss is 0.261.\n",
      "For Iteration 6967 the Loss is 0.261.\n",
      "For Iteration 6968 the Loss is 0.261.\n",
      "For Iteration 6969 the Loss is 0.261.\n",
      "For Iteration 6970 the Loss is 0.261.\n",
      "For Iteration 6971 the Loss is 0.261.\n",
      "For Iteration 6972 the Loss is 0.261.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 6973 the Loss is 0.261.\n",
      "For Iteration 6974 the Loss is 0.261.\n",
      "For Iteration 6975 the Loss is 0.261.\n",
      "For Iteration 6976 the Loss is 0.261.\n",
      "For Iteration 6977 the Loss is 0.261.\n",
      "For Iteration 6978 the Loss is 0.261.\n",
      "For Iteration 6979 the Loss is 0.261.\n",
      "For Iteration 6980 the Loss is 0.261.\n",
      "For Iteration 6981 the Loss is 0.261.\n",
      "For Iteration 6982 the Loss is 0.261.\n",
      "For Iteration 6983 the Loss is 0.261.\n",
      "For Iteration 6984 the Loss is 0.261.\n",
      "For Iteration 6985 the Loss is 0.261.\n",
      "For Iteration 6986 the Loss is 0.261.\n",
      "For Iteration 6987 the Loss is 0.261.\n",
      "For Iteration 6988 the Loss is 0.261.\n",
      "For Iteration 6989 the Loss is 0.261.\n",
      "For Iteration 6990 the Loss is 0.261.\n",
      "For Iteration 6991 the Loss is 0.261.\n",
      "For Iteration 6992 the Loss is 0.261.\n",
      "For Iteration 6993 the Loss is 0.261.\n",
      "For Iteration 6994 the Loss is 0.261.\n",
      "For Iteration 6995 the Loss is 0.261.\n",
      "For Iteration 6996 the Loss is 0.261.\n",
      "For Iteration 6997 the Loss is 0.261.\n",
      "For Iteration 6998 the Loss is 0.261.\n",
      "For Iteration 6999 the Loss is 0.261.\n",
      "For Iteration 7000 the Loss is 0.261.\n",
      "For Iteration 7001 the Loss is 0.261.\n",
      "For Iteration 7002 the Loss is 0.261.\n",
      "For Iteration 7003 the Loss is 0.261.\n",
      "For Iteration 7004 the Loss is 0.261.\n",
      "For Iteration 7005 the Loss is 0.261.\n",
      "For Iteration 7006 the Loss is 0.261.\n",
      "For Iteration 7007 the Loss is 0.261.\n",
      "For Iteration 7008 the Loss is 0.261.\n",
      "For Iteration 7009 the Loss is 0.261.\n",
      "For Iteration 7010 the Loss is 0.261.\n",
      "For Iteration 7011 the Loss is 0.261.\n",
      "For Iteration 7012 the Loss is 0.261.\n",
      "For Iteration 7013 the Loss is 0.261.\n",
      "For Iteration 7014 the Loss is 0.261.\n",
      "For Iteration 7015 the Loss is 0.261.\n",
      "For Iteration 7016 the Loss is 0.261.\n",
      "For Iteration 7017 the Loss is 0.261.\n",
      "For Iteration 7018 the Loss is 0.261.\n",
      "For Iteration 7019 the Loss is 0.261.\n",
      "For Iteration 7020 the Loss is 0.261.\n",
      "For Iteration 7021 the Loss is 0.261.\n",
      "For Iteration 7022 the Loss is 0.261.\n",
      "For Iteration 7023 the Loss is 0.261.\n",
      "For Iteration 7024 the Loss is 0.261.\n",
      "For Iteration 7025 the Loss is 0.261.\n",
      "For Iteration 7026 the Loss is 0.261.\n",
      "For Iteration 7027 the Loss is 0.261.\n",
      "For Iteration 7028 the Loss is 0.261.\n",
      "For Iteration 7029 the Loss is 0.261.\n",
      "For Iteration 7030 the Loss is 0.261.\n",
      "For Iteration 7031 the Loss is 0.261.\n",
      "For Iteration 7032 the Loss is 0.261.\n",
      "For Iteration 7033 the Loss is 0.261.\n",
      "For Iteration 7034 the Loss is 0.261.\n",
      "For Iteration 7035 the Loss is 0.261.\n",
      "For Iteration 7036 the Loss is 0.261.\n",
      "For Iteration 7037 the Loss is 0.261.\n",
      "For Iteration 7038 the Loss is 0.261.\n",
      "For Iteration 7039 the Loss is 0.261.\n",
      "For Iteration 7040 the Loss is 0.261.\n",
      "For Iteration 7041 the Loss is 0.261.\n",
      "For Iteration 7042 the Loss is 0.261.\n",
      "For Iteration 7043 the Loss is 0.261.\n",
      "For Iteration 7044 the Loss is 0.261.\n",
      "For Iteration 7045 the Loss is 0.261.\n",
      "For Iteration 7046 the Loss is 0.261.\n",
      "For Iteration 7047 the Loss is 0.261.\n",
      "For Iteration 7048 the Loss is 0.261.\n",
      "For Iteration 7049 the Loss is 0.261.\n",
      "For Iteration 7050 the Loss is 0.261.\n",
      "For Iteration 7051 the Loss is 0.261.\n",
      "For Iteration 7052 the Loss is 0.261.\n",
      "For Iteration 7053 the Loss is 0.261.\n",
      "For Iteration 7054 the Loss is 0.261.\n",
      "For Iteration 7055 the Loss is 0.261.\n",
      "For Iteration 7056 the Loss is 0.261.\n",
      "For Iteration 7057 the Loss is 0.261.\n",
      "For Iteration 7058 the Loss is 0.261.\n",
      "For Iteration 7059 the Loss is 0.261.\n",
      "For Iteration 7060 the Loss is 0.261.\n",
      "For Iteration 7061 the Loss is 0.261.\n",
      "For Iteration 7062 the Loss is 0.261.\n",
      "For Iteration 7063 the Loss is 0.261.\n",
      "For Iteration 7064 the Loss is 0.261.\n",
      "For Iteration 7065 the Loss is 0.261.\n",
      "For Iteration 7066 the Loss is 0.261.\n",
      "For Iteration 7067 the Loss is 0.261.\n",
      "For Iteration 7068 the Loss is 0.261.\n",
      "For Iteration 7069 the Loss is 0.261.\n",
      "For Iteration 7070 the Loss is 0.261.\n",
      "For Iteration 7071 the Loss is 0.261.\n",
      "For Iteration 7072 the Loss is 0.261.\n",
      "For Iteration 7073 the Loss is 0.261.\n",
      "For Iteration 7074 the Loss is 0.261.\n",
      "For Iteration 7075 the Loss is 0.261.\n",
      "For Iteration 7076 the Loss is 0.261.\n",
      "For Iteration 7077 the Loss is 0.261.\n",
      "For Iteration 7078 the Loss is 0.261.\n",
      "For Iteration 7079 the Loss is 0.261.\n",
      "For Iteration 7080 the Loss is 0.261.\n",
      "For Iteration 7081 the Loss is 0.261.\n",
      "For Iteration 7082 the Loss is 0.261.\n",
      "For Iteration 7083 the Loss is 0.261.\n",
      "For Iteration 7084 the Loss is 0.261.\n",
      "For Iteration 7085 the Loss is 0.261.\n",
      "For Iteration 7086 the Loss is 0.261.\n",
      "For Iteration 7087 the Loss is 0.261.\n",
      "For Iteration 7088 the Loss is 0.261.\n",
      "For Iteration 7089 the Loss is 0.261.\n",
      "For Iteration 7090 the Loss is 0.261.\n",
      "For Iteration 7091 the Loss is 0.261.\n",
      "For Iteration 7092 the Loss is 0.261.\n",
      "For Iteration 7093 the Loss is 0.261.\n",
      "For Iteration 7094 the Loss is 0.261.\n",
      "For Iteration 7095 the Loss is 0.261.\n",
      "For Iteration 7096 the Loss is 0.261.\n",
      "For Iteration 7097 the Loss is 0.261.\n",
      "For Iteration 7098 the Loss is 0.261.\n",
      "For Iteration 7099 the Loss is 0.261.\n",
      "For Iteration 7100 the Loss is 0.261.\n",
      "For Iteration 7101 the Loss is 0.261.\n",
      "For Iteration 7102 the Loss is 0.261.\n",
      "For Iteration 7103 the Loss is 0.261.\n",
      "For Iteration 7104 the Loss is 0.261.\n",
      "For Iteration 7105 the Loss is 0.261.\n",
      "For Iteration 7106 the Loss is 0.261.\n",
      "For Iteration 7107 the Loss is 0.261.\n",
      "For Iteration 7108 the Loss is 0.261.\n",
      "For Iteration 7109 the Loss is 0.261.\n",
      "For Iteration 7110 the Loss is 0.261.\n",
      "For Iteration 7111 the Loss is 0.261.\n",
      "For Iteration 7112 the Loss is 0.261.\n",
      "For Iteration 7113 the Loss is 0.261.\n",
      "For Iteration 7114 the Loss is 0.261.\n",
      "For Iteration 7115 the Loss is 0.261.\n",
      "For Iteration 7116 the Loss is 0.261.\n",
      "For Iteration 7117 the Loss is 0.261.\n",
      "For Iteration 7118 the Loss is 0.261.\n",
      "For Iteration 7119 the Loss is 0.261.\n",
      "For Iteration 7120 the Loss is 0.261.\n",
      "For Iteration 7121 the Loss is 0.261.\n",
      "For Iteration 7122 the Loss is 0.261.\n",
      "For Iteration 7123 the Loss is 0.261.\n",
      "For Iteration 7124 the Loss is 0.261.\n",
      "For Iteration 7125 the Loss is 0.261.\n",
      "For Iteration 7126 the Loss is 0.261.\n",
      "For Iteration 7127 the Loss is 0.261.\n",
      "For Iteration 7128 the Loss is 0.261.\n",
      "For Iteration 7129 the Loss is 0.261.\n",
      "For Iteration 7130 the Loss is 0.261.\n",
      "For Iteration 7131 the Loss is 0.261.\n",
      "For Iteration 7132 the Loss is 0.261.\n",
      "For Iteration 7133 the Loss is 0.261.\n",
      "For Iteration 7134 the Loss is 0.261.\n",
      "For Iteration 7135 the Loss is 0.261.\n",
      "For Iteration 7136 the Loss is 0.261.\n",
      "For Iteration 7137 the Loss is 0.261.\n",
      "For Iteration 7138 the Loss is 0.261.\n",
      "For Iteration 7139 the Loss is 0.261.\n",
      "For Iteration 7140 the Loss is 0.261.\n",
      "For Iteration 7141 the Loss is 0.261.\n",
      "For Iteration 7142 the Loss is 0.261.\n",
      "For Iteration 7143 the Loss is 0.261.\n",
      "For Iteration 7144 the Loss is 0.261.\n",
      "For Iteration 7145 the Loss is 0.261.\n",
      "For Iteration 7146 the Loss is 0.261.\n",
      "For Iteration 7147 the Loss is 0.261.\n",
      "For Iteration 7148 the Loss is 0.261.\n",
      "For Iteration 7149 the Loss is 0.261.\n",
      "For Iteration 7150 the Loss is 0.261.\n",
      "For Iteration 7151 the Loss is 0.261.\n",
      "For Iteration 7152 the Loss is 0.261.\n",
      "For Iteration 7153 the Loss is 0.261.\n",
      "For Iteration 7154 the Loss is 0.261.\n",
      "For Iteration 7155 the Loss is 0.261.\n",
      "For Iteration 7156 the Loss is 0.261.\n",
      "For Iteration 7157 the Loss is 0.261.\n",
      "For Iteration 7158 the Loss is 0.261.\n",
      "For Iteration 7159 the Loss is 0.261.\n",
      "For Iteration 7160 the Loss is 0.261.\n",
      "For Iteration 7161 the Loss is 0.261.\n",
      "For Iteration 7162 the Loss is 0.261.\n",
      "For Iteration 7163 the Loss is 0.261.\n",
      "For Iteration 7164 the Loss is 0.261.\n",
      "For Iteration 7165 the Loss is 0.261.\n",
      "For Iteration 7166 the Loss is 0.261.\n",
      "For Iteration 7167 the Loss is 0.261.\n",
      "For Iteration 7168 the Loss is 0.261.\n",
      "For Iteration 7169 the Loss is 0.261.\n",
      "For Iteration 7170 the Loss is 0.261.\n",
      "For Iteration 7171 the Loss is 0.261.\n",
      "For Iteration 7172 the Loss is 0.261.\n",
      "For Iteration 7173 the Loss is 0.261.\n",
      "For Iteration 7174 the Loss is 0.261.\n",
      "For Iteration 7175 the Loss is 0.261.\n",
      "For Iteration 7176 the Loss is 0.2609.\n",
      "For Iteration 7177 the Loss is 0.2609.\n",
      "For Iteration 7178 the Loss is 0.2609.\n",
      "For Iteration 7179 the Loss is 0.2609.\n",
      "For Iteration 7180 the Loss is 0.2609.\n",
      "For Iteration 7181 the Loss is 0.2609.\n",
      "For Iteration 7182 the Loss is 0.2609.\n",
      "For Iteration 7183 the Loss is 0.2609.\n",
      "For Iteration 7184 the Loss is 0.2609.\n",
      "For Iteration 7185 the Loss is 0.2609.\n",
      "For Iteration 7186 the Loss is 0.2609.\n",
      "For Iteration 7187 the Loss is 0.2609.\n",
      "For Iteration 7188 the Loss is 0.2609.\n",
      "For Iteration 7189 the Loss is 0.2609.\n",
      "For Iteration 7190 the Loss is 0.2609.\n",
      "For Iteration 7191 the Loss is 0.2609.\n",
      "For Iteration 7192 the Loss is 0.2609.\n",
      "For Iteration 7193 the Loss is 0.2609.\n",
      "For Iteration 7194 the Loss is 0.2609.\n",
      "For Iteration 7195 the Loss is 0.2609.\n",
      "For Iteration 7196 the Loss is 0.2609.\n",
      "For Iteration 7197 the Loss is 0.2609.\n",
      "For Iteration 7198 the Loss is 0.2609.\n",
      "For Iteration 7199 the Loss is 0.2609.\n",
      "For Iteration 7200 the Loss is 0.2609.\n",
      "For Iteration 7201 the Loss is 0.2609.\n",
      "For Iteration 7202 the Loss is 0.2609.\n",
      "For Iteration 7203 the Loss is 0.2609.\n",
      "For Iteration 7204 the Loss is 0.2609.\n",
      "For Iteration 7205 the Loss is 0.2609.\n",
      "For Iteration 7206 the Loss is 0.2609.\n",
      "For Iteration 7207 the Loss is 0.2609.\n",
      "For Iteration 7208 the Loss is 0.2609.\n",
      "For Iteration 7209 the Loss is 0.2609.\n",
      "For Iteration 7210 the Loss is 0.2609.\n",
      "For Iteration 7211 the Loss is 0.2609.\n",
      "For Iteration 7212 the Loss is 0.2609.\n",
      "For Iteration 7213 the Loss is 0.2609.\n",
      "For Iteration 7214 the Loss is 0.2609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 7215 the Loss is 0.2609.\n",
      "For Iteration 7216 the Loss is 0.2609.\n",
      "For Iteration 7217 the Loss is 0.2609.\n",
      "For Iteration 7218 the Loss is 0.2609.\n",
      "For Iteration 7219 the Loss is 0.2609.\n",
      "For Iteration 7220 the Loss is 0.2609.\n",
      "For Iteration 7221 the Loss is 0.2609.\n",
      "For Iteration 7222 the Loss is 0.2609.\n",
      "For Iteration 7223 the Loss is 0.2609.\n",
      "For Iteration 7224 the Loss is 0.2609.\n",
      "For Iteration 7225 the Loss is 0.2609.\n",
      "For Iteration 7226 the Loss is 0.2609.\n",
      "For Iteration 7227 the Loss is 0.2609.\n",
      "For Iteration 7228 the Loss is 0.2609.\n",
      "For Iteration 7229 the Loss is 0.2609.\n",
      "For Iteration 7230 the Loss is 0.2609.\n",
      "For Iteration 7231 the Loss is 0.2609.\n",
      "For Iteration 7232 the Loss is 0.2609.\n",
      "For Iteration 7233 the Loss is 0.2609.\n",
      "For Iteration 7234 the Loss is 0.2609.\n",
      "For Iteration 7235 the Loss is 0.2609.\n",
      "For Iteration 7236 the Loss is 0.2609.\n",
      "For Iteration 7237 the Loss is 0.2609.\n",
      "For Iteration 7238 the Loss is 0.2609.\n",
      "For Iteration 7239 the Loss is 0.2609.\n",
      "For Iteration 7240 the Loss is 0.2609.\n",
      "For Iteration 7241 the Loss is 0.2609.\n",
      "For Iteration 7242 the Loss is 0.2609.\n",
      "For Iteration 7243 the Loss is 0.2609.\n",
      "For Iteration 7244 the Loss is 0.2609.\n",
      "For Iteration 7245 the Loss is 0.2609.\n",
      "For Iteration 7246 the Loss is 0.2609.\n",
      "For Iteration 7247 the Loss is 0.2609.\n",
      "For Iteration 7248 the Loss is 0.2609.\n",
      "For Iteration 7249 the Loss is 0.2609.\n",
      "For Iteration 7250 the Loss is 0.2609.\n",
      "For Iteration 7251 the Loss is 0.2609.\n",
      "For Iteration 7252 the Loss is 0.2609.\n",
      "For Iteration 7253 the Loss is 0.2609.\n",
      "For Iteration 7254 the Loss is 0.2609.\n",
      "For Iteration 7255 the Loss is 0.2609.\n",
      "For Iteration 7256 the Loss is 0.2609.\n",
      "For Iteration 7257 the Loss is 0.2609.\n",
      "For Iteration 7258 the Loss is 0.2609.\n",
      "For Iteration 7259 the Loss is 0.2609.\n",
      "For Iteration 7260 the Loss is 0.2609.\n",
      "For Iteration 7261 the Loss is 0.2609.\n",
      "For Iteration 7262 the Loss is 0.2609.\n",
      "For Iteration 7263 the Loss is 0.2609.\n",
      "For Iteration 7264 the Loss is 0.2609.\n",
      "For Iteration 7265 the Loss is 0.2609.\n",
      "For Iteration 7266 the Loss is 0.2609.\n",
      "For Iteration 7267 the Loss is 0.2609.\n",
      "For Iteration 7268 the Loss is 0.2609.\n",
      "For Iteration 7269 the Loss is 0.2609.\n",
      "For Iteration 7270 the Loss is 0.2609.\n",
      "For Iteration 7271 the Loss is 0.2609.\n",
      "For Iteration 7272 the Loss is 0.2609.\n",
      "For Iteration 7273 the Loss is 0.2609.\n",
      "For Iteration 7274 the Loss is 0.2609.\n",
      "For Iteration 7275 the Loss is 0.2609.\n",
      "For Iteration 7276 the Loss is 0.2609.\n",
      "For Iteration 7277 the Loss is 0.2609.\n",
      "For Iteration 7278 the Loss is 0.2609.\n",
      "For Iteration 7279 the Loss is 0.2609.\n",
      "For Iteration 7280 the Loss is 0.2609.\n",
      "For Iteration 7281 the Loss is 0.2609.\n",
      "For Iteration 7282 the Loss is 0.2609.\n",
      "For Iteration 7283 the Loss is 0.2609.\n",
      "For Iteration 7284 the Loss is 0.2609.\n",
      "For Iteration 7285 the Loss is 0.2609.\n",
      "For Iteration 7286 the Loss is 0.2609.\n",
      "For Iteration 7287 the Loss is 0.2609.\n",
      "For Iteration 7288 the Loss is 0.2609.\n",
      "For Iteration 7289 the Loss is 0.2609.\n",
      "For Iteration 7290 the Loss is 0.2609.\n",
      "For Iteration 7291 the Loss is 0.2609.\n",
      "For Iteration 7292 the Loss is 0.2609.\n",
      "For Iteration 7293 the Loss is 0.2609.\n",
      "For Iteration 7294 the Loss is 0.2609.\n",
      "For Iteration 7295 the Loss is 0.2609.\n",
      "For Iteration 7296 the Loss is 0.2609.\n",
      "For Iteration 7297 the Loss is 0.2609.\n",
      "For Iteration 7298 the Loss is 0.2609.\n",
      "For Iteration 7299 the Loss is 0.2609.\n",
      "For Iteration 7300 the Loss is 0.2609.\n",
      "For Iteration 7301 the Loss is 0.2609.\n",
      "For Iteration 7302 the Loss is 0.2609.\n",
      "For Iteration 7303 the Loss is 0.2609.\n",
      "For Iteration 7304 the Loss is 0.2609.\n",
      "For Iteration 7305 the Loss is 0.2609.\n",
      "For Iteration 7306 the Loss is 0.2609.\n",
      "For Iteration 7307 the Loss is 0.2609.\n",
      "For Iteration 7308 the Loss is 0.2609.\n",
      "For Iteration 7309 the Loss is 0.2609.\n",
      "For Iteration 7310 the Loss is 0.2609.\n",
      "For Iteration 7311 the Loss is 0.2609.\n",
      "For Iteration 7312 the Loss is 0.2609.\n",
      "For Iteration 7313 the Loss is 0.2609.\n",
      "For Iteration 7314 the Loss is 0.2609.\n",
      "For Iteration 7315 the Loss is 0.2609.\n",
      "For Iteration 7316 the Loss is 0.2609.\n",
      "For Iteration 7317 the Loss is 0.2609.\n",
      "For Iteration 7318 the Loss is 0.2609.\n",
      "For Iteration 7319 the Loss is 0.2609.\n",
      "For Iteration 7320 the Loss is 0.2609.\n",
      "For Iteration 7321 the Loss is 0.2609.\n",
      "For Iteration 7322 the Loss is 0.2609.\n",
      "For Iteration 7323 the Loss is 0.2609.\n",
      "For Iteration 7324 the Loss is 0.2609.\n",
      "For Iteration 7325 the Loss is 0.2609.\n",
      "For Iteration 7326 the Loss is 0.2609.\n",
      "For Iteration 7327 the Loss is 0.2609.\n",
      "For Iteration 7328 the Loss is 0.2609.\n",
      "For Iteration 7329 the Loss is 0.2609.\n",
      "For Iteration 7330 the Loss is 0.2609.\n",
      "For Iteration 7331 the Loss is 0.2609.\n",
      "For Iteration 7332 the Loss is 0.2609.\n",
      "For Iteration 7333 the Loss is 0.2609.\n",
      "For Iteration 7334 the Loss is 0.2609.\n",
      "For Iteration 7335 the Loss is 0.2609.\n",
      "For Iteration 7336 the Loss is 0.2609.\n",
      "For Iteration 7337 the Loss is 0.2609.\n",
      "For Iteration 7338 the Loss is 0.2609.\n",
      "For Iteration 7339 the Loss is 0.2609.\n",
      "For Iteration 7340 the Loss is 0.2609.\n",
      "For Iteration 7341 the Loss is 0.2609.\n",
      "For Iteration 7342 the Loss is 0.2609.\n",
      "For Iteration 7343 the Loss is 0.2609.\n",
      "For Iteration 7344 the Loss is 0.2609.\n",
      "For Iteration 7345 the Loss is 0.2609.\n",
      "For Iteration 7346 the Loss is 0.2609.\n",
      "For Iteration 7347 the Loss is 0.2609.\n",
      "For Iteration 7348 the Loss is 0.2609.\n",
      "For Iteration 7349 the Loss is 0.2609.\n",
      "For Iteration 7350 the Loss is 0.2609.\n",
      "For Iteration 7351 the Loss is 0.2609.\n",
      "For Iteration 7352 the Loss is 0.2609.\n",
      "For Iteration 7353 the Loss is 0.2609.\n",
      "For Iteration 7354 the Loss is 0.2609.\n",
      "For Iteration 7355 the Loss is 0.2609.\n",
      "For Iteration 7356 the Loss is 0.2609.\n",
      "For Iteration 7357 the Loss is 0.2609.\n",
      "For Iteration 7358 the Loss is 0.2609.\n",
      "For Iteration 7359 the Loss is 0.2609.\n",
      "For Iteration 7360 the Loss is 0.2609.\n",
      "For Iteration 7361 the Loss is 0.2609.\n",
      "For Iteration 7362 the Loss is 0.2609.\n",
      "For Iteration 7363 the Loss is 0.2609.\n",
      "For Iteration 7364 the Loss is 0.2609.\n",
      "For Iteration 7365 the Loss is 0.2609.\n",
      "For Iteration 7366 the Loss is 0.2609.\n",
      "For Iteration 7367 the Loss is 0.2609.\n",
      "For Iteration 7368 the Loss is 0.2609.\n",
      "For Iteration 7369 the Loss is 0.2609.\n",
      "For Iteration 7370 the Loss is 0.2609.\n",
      "For Iteration 7371 the Loss is 0.2609.\n",
      "For Iteration 7372 the Loss is 0.2609.\n",
      "For Iteration 7373 the Loss is 0.2609.\n",
      "For Iteration 7374 the Loss is 0.2609.\n",
      "For Iteration 7375 the Loss is 0.2609.\n",
      "For Iteration 7376 the Loss is 0.2609.\n",
      "For Iteration 7377 the Loss is 0.2609.\n",
      "For Iteration 7378 the Loss is 0.2609.\n",
      "For Iteration 7379 the Loss is 0.2609.\n",
      "For Iteration 7380 the Loss is 0.2609.\n",
      "For Iteration 7381 the Loss is 0.2609.\n",
      "For Iteration 7382 the Loss is 0.2609.\n",
      "For Iteration 7383 the Loss is 0.2609.\n",
      "For Iteration 7384 the Loss is 0.2609.\n",
      "For Iteration 7385 the Loss is 0.2609.\n",
      "For Iteration 7386 the Loss is 0.2609.\n",
      "For Iteration 7387 the Loss is 0.2609.\n",
      "For Iteration 7388 the Loss is 0.2609.\n",
      "For Iteration 7389 the Loss is 0.2609.\n",
      "For Iteration 7390 the Loss is 0.2609.\n",
      "For Iteration 7391 the Loss is 0.2609.\n",
      "For Iteration 7392 the Loss is 0.2609.\n",
      "For Iteration 7393 the Loss is 0.2609.\n",
      "For Iteration 7394 the Loss is 0.2609.\n",
      "For Iteration 7395 the Loss is 0.2609.\n",
      "For Iteration 7396 the Loss is 0.2609.\n",
      "For Iteration 7397 the Loss is 0.2609.\n",
      "For Iteration 7398 the Loss is 0.2609.\n",
      "For Iteration 7399 the Loss is 0.2609.\n",
      "For Iteration 7400 the Loss is 0.2609.\n",
      "For Iteration 7401 the Loss is 0.2609.\n",
      "For Iteration 7402 the Loss is 0.2609.\n",
      "For Iteration 7403 the Loss is 0.2609.\n",
      "For Iteration 7404 the Loss is 0.2609.\n",
      "For Iteration 7405 the Loss is 0.2609.\n",
      "For Iteration 7406 the Loss is 0.2609.\n",
      "For Iteration 7407 the Loss is 0.2609.\n",
      "For Iteration 7408 the Loss is 0.2609.\n",
      "For Iteration 7409 the Loss is 0.2609.\n",
      "For Iteration 7410 the Loss is 0.2609.\n",
      "For Iteration 7411 the Loss is 0.2609.\n",
      "For Iteration 7412 the Loss is 0.2609.\n",
      "For Iteration 7413 the Loss is 0.2609.\n",
      "For Iteration 7414 the Loss is 0.2609.\n",
      "For Iteration 7415 the Loss is 0.2609.\n",
      "For Iteration 7416 the Loss is 0.2609.\n",
      "For Iteration 7417 the Loss is 0.2609.\n",
      "For Iteration 7418 the Loss is 0.2609.\n",
      "For Iteration 7419 the Loss is 0.2609.\n",
      "For Iteration 7420 the Loss is 0.2609.\n",
      "For Iteration 7421 the Loss is 0.2609.\n",
      "For Iteration 7422 the Loss is 0.2609.\n",
      "For Iteration 7423 the Loss is 0.2609.\n",
      "For Iteration 7424 the Loss is 0.2609.\n",
      "For Iteration 7425 the Loss is 0.2609.\n",
      "For Iteration 7426 the Loss is 0.2609.\n",
      "For Iteration 7427 the Loss is 0.2609.\n",
      "For Iteration 7428 the Loss is 0.2609.\n",
      "For Iteration 7429 the Loss is 0.2609.\n",
      "For Iteration 7430 the Loss is 0.2609.\n",
      "For Iteration 7431 the Loss is 0.2609.\n",
      "For Iteration 7432 the Loss is 0.2609.\n",
      "For Iteration 7433 the Loss is 0.2609.\n",
      "For Iteration 7434 the Loss is 0.2609.\n",
      "For Iteration 7435 the Loss is 0.2609.\n",
      "For Iteration 7436 the Loss is 0.2609.\n",
      "For Iteration 7437 the Loss is 0.2609.\n",
      "For Iteration 7438 the Loss is 0.2609.\n",
      "For Iteration 7439 the Loss is 0.2609.\n",
      "For Iteration 7440 the Loss is 0.2609.\n",
      "For Iteration 7441 the Loss is 0.2609.\n",
      "For Iteration 7442 the Loss is 0.2609.\n",
      "For Iteration 7443 the Loss is 0.2609.\n",
      "For Iteration 7444 the Loss is 0.2609.\n",
      "For Iteration 7445 the Loss is 0.2609.\n",
      "For Iteration 7446 the Loss is 0.2609.\n",
      "For Iteration 7447 the Loss is 0.2609.\n",
      "For Iteration 7448 the Loss is 0.2609.\n",
      "For Iteration 7449 the Loss is 0.2609.\n",
      "For Iteration 7450 the Loss is 0.2609.\n",
      "For Iteration 7451 the Loss is 0.2609.\n",
      "For Iteration 7452 the Loss is 0.2609.\n",
      "For Iteration 7453 the Loss is 0.2609.\n",
      "For Iteration 7454 the Loss is 0.2609.\n",
      "For Iteration 7455 the Loss is 0.2609.\n",
      "For Iteration 7456 the Loss is 0.2609.\n",
      "For Iteration 7457 the Loss is 0.2609.\n",
      "For Iteration 7458 the Loss is 0.2609.\n",
      "For Iteration 7459 the Loss is 0.2609.\n",
      "For Iteration 7460 the Loss is 0.2609.\n",
      "For Iteration 7461 the Loss is 0.2609.\n",
      "For Iteration 7462 the Loss is 0.2609.\n",
      "For Iteration 7463 the Loss is 0.2609.\n",
      "For Iteration 7464 the Loss is 0.2609.\n",
      "For Iteration 7465 the Loss is 0.2609.\n",
      "For Iteration 7466 the Loss is 0.2609.\n",
      "For Iteration 7467 the Loss is 0.2609.\n",
      "For Iteration 7468 the Loss is 0.2609.\n",
      "For Iteration 7469 the Loss is 0.2609.\n",
      "For Iteration 7470 the Loss is 0.2609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 7471 the Loss is 0.2609.\n",
      "For Iteration 7472 the Loss is 0.2609.\n",
      "For Iteration 7473 the Loss is 0.2609.\n",
      "For Iteration 7474 the Loss is 0.2609.\n",
      "For Iteration 7475 the Loss is 0.2609.\n",
      "For Iteration 7476 the Loss is 0.2609.\n",
      "For Iteration 7477 the Loss is 0.2609.\n",
      "For Iteration 7478 the Loss is 0.2609.\n",
      "For Iteration 7479 the Loss is 0.2609.\n",
      "For Iteration 7480 the Loss is 0.2609.\n",
      "For Iteration 7481 the Loss is 0.2609.\n",
      "For Iteration 7482 the Loss is 0.2609.\n",
      "For Iteration 7483 the Loss is 0.2609.\n",
      "For Iteration 7484 the Loss is 0.2609.\n",
      "For Iteration 7485 the Loss is 0.2609.\n",
      "For Iteration 7486 the Loss is 0.2609.\n",
      "For Iteration 7487 the Loss is 0.2609.\n",
      "For Iteration 7488 the Loss is 0.2609.\n",
      "For Iteration 7489 the Loss is 0.2609.\n",
      "For Iteration 7490 the Loss is 0.2609.\n",
      "For Iteration 7491 the Loss is 0.2609.\n",
      "For Iteration 7492 the Loss is 0.2609.\n",
      "For Iteration 7493 the Loss is 0.2609.\n",
      "For Iteration 7494 the Loss is 0.2609.\n",
      "For Iteration 7495 the Loss is 0.2609.\n",
      "For Iteration 7496 the Loss is 0.2609.\n",
      "For Iteration 7497 the Loss is 0.2609.\n",
      "For Iteration 7498 the Loss is 0.2609.\n",
      "For Iteration 7499 the Loss is 0.2609.\n",
      "For Iteration 7500 the Loss is 0.2609.\n",
      "For Iteration 7501 the Loss is 0.2609.\n",
      "For Iteration 7502 the Loss is 0.2609.\n",
      "For Iteration 7503 the Loss is 0.2609.\n",
      "For Iteration 7504 the Loss is 0.2609.\n",
      "For Iteration 7505 the Loss is 0.2609.\n",
      "For Iteration 7506 the Loss is 0.2609.\n",
      "For Iteration 7507 the Loss is 0.2609.\n",
      "For Iteration 7508 the Loss is 0.2609.\n",
      "For Iteration 7509 the Loss is 0.2609.\n",
      "For Iteration 7510 the Loss is 0.2609.\n",
      "For Iteration 7511 the Loss is 0.2609.\n",
      "For Iteration 7512 the Loss is 0.2609.\n",
      "For Iteration 7513 the Loss is 0.2609.\n",
      "For Iteration 7514 the Loss is 0.2609.\n",
      "For Iteration 7515 the Loss is 0.2609.\n",
      "For Iteration 7516 the Loss is 0.2609.\n",
      "For Iteration 7517 the Loss is 0.2609.\n",
      "For Iteration 7518 the Loss is 0.2609.\n",
      "For Iteration 7519 the Loss is 0.2609.\n",
      "For Iteration 7520 the Loss is 0.2609.\n",
      "For Iteration 7521 the Loss is 0.2609.\n",
      "For Iteration 7522 the Loss is 0.2609.\n",
      "For Iteration 7523 the Loss is 0.2609.\n",
      "For Iteration 7524 the Loss is 0.2609.\n",
      "For Iteration 7525 the Loss is 0.2609.\n",
      "For Iteration 7526 the Loss is 0.2609.\n",
      "For Iteration 7527 the Loss is 0.2609.\n",
      "For Iteration 7528 the Loss is 0.2609.\n",
      "For Iteration 7529 the Loss is 0.2609.\n",
      "For Iteration 7530 the Loss is 0.2609.\n",
      "For Iteration 7531 the Loss is 0.2609.\n",
      "For Iteration 7532 the Loss is 0.2609.\n",
      "For Iteration 7533 the Loss is 0.2609.\n",
      "For Iteration 7534 the Loss is 0.2609.\n",
      "For Iteration 7535 the Loss is 0.2609.\n",
      "For Iteration 7536 the Loss is 0.2609.\n",
      "For Iteration 7537 the Loss is 0.2609.\n",
      "For Iteration 7538 the Loss is 0.2609.\n",
      "For Iteration 7539 the Loss is 0.2609.\n",
      "For Iteration 7540 the Loss is 0.2609.\n",
      "For Iteration 7541 the Loss is 0.2609.\n",
      "For Iteration 7542 the Loss is 0.2609.\n",
      "For Iteration 7543 the Loss is 0.2609.\n",
      "For Iteration 7544 the Loss is 0.2609.\n",
      "For Iteration 7545 the Loss is 0.2609.\n",
      "For Iteration 7546 the Loss is 0.2609.\n",
      "For Iteration 7547 the Loss is 0.2609.\n",
      "For Iteration 7548 the Loss is 0.2609.\n",
      "For Iteration 7549 the Loss is 0.2609.\n",
      "For Iteration 7550 the Loss is 0.2609.\n",
      "For Iteration 7551 the Loss is 0.2609.\n",
      "For Iteration 7552 the Loss is 0.2609.\n",
      "For Iteration 7553 the Loss is 0.2609.\n",
      "For Iteration 7554 the Loss is 0.2609.\n",
      "For Iteration 7555 the Loss is 0.2609.\n",
      "For Iteration 7556 the Loss is 0.2609.\n",
      "For Iteration 7557 the Loss is 0.2609.\n",
      "For Iteration 7558 the Loss is 0.2609.\n",
      "For Iteration 7559 the Loss is 0.2609.\n",
      "For Iteration 7560 the Loss is 0.2609.\n",
      "For Iteration 7561 the Loss is 0.2609.\n",
      "For Iteration 7562 the Loss is 0.2609.\n",
      "For Iteration 7563 the Loss is 0.2609.\n",
      "For Iteration 7564 the Loss is 0.2609.\n",
      "For Iteration 7565 the Loss is 0.2609.\n",
      "For Iteration 7566 the Loss is 0.2609.\n",
      "For Iteration 7567 the Loss is 0.2609.\n",
      "For Iteration 7568 the Loss is 0.2609.\n",
      "For Iteration 7569 the Loss is 0.2609.\n",
      "For Iteration 7570 the Loss is 0.2609.\n",
      "For Iteration 7571 the Loss is 0.2609.\n",
      "For Iteration 7572 the Loss is 0.2609.\n",
      "For Iteration 7573 the Loss is 0.2609.\n",
      "For Iteration 7574 the Loss is 0.2609.\n",
      "For Iteration 7575 the Loss is 0.2608.\n",
      "For Iteration 7576 the Loss is 0.2608.\n",
      "For Iteration 7577 the Loss is 0.2608.\n",
      "For Iteration 7578 the Loss is 0.2608.\n",
      "For Iteration 7579 the Loss is 0.2608.\n",
      "For Iteration 7580 the Loss is 0.2608.\n",
      "For Iteration 7581 the Loss is 0.2608.\n",
      "For Iteration 7582 the Loss is 0.2608.\n",
      "For Iteration 7583 the Loss is 0.2608.\n",
      "For Iteration 7584 the Loss is 0.2608.\n",
      "For Iteration 7585 the Loss is 0.2608.\n",
      "For Iteration 7586 the Loss is 0.2608.\n",
      "For Iteration 7587 the Loss is 0.2608.\n",
      "For Iteration 7588 the Loss is 0.2608.\n",
      "For Iteration 7589 the Loss is 0.2608.\n",
      "For Iteration 7590 the Loss is 0.2608.\n",
      "For Iteration 7591 the Loss is 0.2608.\n",
      "For Iteration 7592 the Loss is 0.2608.\n",
      "For Iteration 7593 the Loss is 0.2608.\n",
      "For Iteration 7594 the Loss is 0.2608.\n",
      "For Iteration 7595 the Loss is 0.2608.\n",
      "For Iteration 7596 the Loss is 0.2608.\n",
      "For Iteration 7597 the Loss is 0.2608.\n",
      "For Iteration 7598 the Loss is 0.2608.\n",
      "For Iteration 7599 the Loss is 0.2608.\n",
      "For Iteration 7600 the Loss is 0.2608.\n",
      "For Iteration 7601 the Loss is 0.2608.\n",
      "For Iteration 7602 the Loss is 0.2608.\n",
      "For Iteration 7603 the Loss is 0.2608.\n",
      "For Iteration 7604 the Loss is 0.2608.\n",
      "For Iteration 7605 the Loss is 0.2608.\n",
      "For Iteration 7606 the Loss is 0.2608.\n",
      "For Iteration 7607 the Loss is 0.2608.\n",
      "For Iteration 7608 the Loss is 0.2608.\n",
      "For Iteration 7609 the Loss is 0.2608.\n",
      "For Iteration 7610 the Loss is 0.2608.\n",
      "For Iteration 7611 the Loss is 0.2608.\n",
      "For Iteration 7612 the Loss is 0.2608.\n",
      "For Iteration 7613 the Loss is 0.2608.\n",
      "For Iteration 7614 the Loss is 0.2608.\n",
      "For Iteration 7615 the Loss is 0.2608.\n",
      "For Iteration 7616 the Loss is 0.2608.\n",
      "For Iteration 7617 the Loss is 0.2608.\n",
      "For Iteration 7618 the Loss is 0.2608.\n",
      "For Iteration 7619 the Loss is 0.2608.\n",
      "For Iteration 7620 the Loss is 0.2608.\n",
      "For Iteration 7621 the Loss is 0.2608.\n",
      "For Iteration 7622 the Loss is 0.2608.\n",
      "For Iteration 7623 the Loss is 0.2608.\n",
      "For Iteration 7624 the Loss is 0.2608.\n",
      "For Iteration 7625 the Loss is 0.2608.\n",
      "For Iteration 7626 the Loss is 0.2608.\n",
      "For Iteration 7627 the Loss is 0.2608.\n",
      "For Iteration 7628 the Loss is 0.2608.\n",
      "For Iteration 7629 the Loss is 0.2608.\n",
      "For Iteration 7630 the Loss is 0.2608.\n",
      "For Iteration 7631 the Loss is 0.2608.\n",
      "For Iteration 7632 the Loss is 0.2608.\n",
      "For Iteration 7633 the Loss is 0.2608.\n",
      "For Iteration 7634 the Loss is 0.2608.\n",
      "For Iteration 7635 the Loss is 0.2608.\n",
      "For Iteration 7636 the Loss is 0.2608.\n",
      "For Iteration 7637 the Loss is 0.2608.\n",
      "For Iteration 7638 the Loss is 0.2608.\n",
      "For Iteration 7639 the Loss is 0.2608.\n",
      "For Iteration 7640 the Loss is 0.2608.\n",
      "For Iteration 7641 the Loss is 0.2608.\n",
      "For Iteration 7642 the Loss is 0.2608.\n",
      "For Iteration 7643 the Loss is 0.2608.\n",
      "For Iteration 7644 the Loss is 0.2608.\n",
      "For Iteration 7645 the Loss is 0.2608.\n",
      "For Iteration 7646 the Loss is 0.2608.\n",
      "For Iteration 7647 the Loss is 0.2608.\n",
      "For Iteration 7648 the Loss is 0.2608.\n",
      "For Iteration 7649 the Loss is 0.2608.\n",
      "For Iteration 7650 the Loss is 0.2608.\n",
      "For Iteration 7651 the Loss is 0.2608.\n",
      "For Iteration 7652 the Loss is 0.2608.\n",
      "For Iteration 7653 the Loss is 0.2608.\n",
      "For Iteration 7654 the Loss is 0.2608.\n",
      "For Iteration 7655 the Loss is 0.2608.\n",
      "For Iteration 7656 the Loss is 0.2608.\n",
      "For Iteration 7657 the Loss is 0.2608.\n",
      "For Iteration 7658 the Loss is 0.2608.\n",
      "For Iteration 7659 the Loss is 0.2608.\n",
      "For Iteration 7660 the Loss is 0.2608.\n",
      "For Iteration 7661 the Loss is 0.2608.\n",
      "For Iteration 7662 the Loss is 0.2608.\n",
      "For Iteration 7663 the Loss is 0.2608.\n",
      "For Iteration 7664 the Loss is 0.2608.\n",
      "For Iteration 7665 the Loss is 0.2608.\n",
      "For Iteration 7666 the Loss is 0.2608.\n",
      "For Iteration 7667 the Loss is 0.2608.\n",
      "For Iteration 7668 the Loss is 0.2608.\n",
      "For Iteration 7669 the Loss is 0.2608.\n",
      "For Iteration 7670 the Loss is 0.2608.\n",
      "For Iteration 7671 the Loss is 0.2608.\n",
      "For Iteration 7672 the Loss is 0.2608.\n",
      "For Iteration 7673 the Loss is 0.2608.\n",
      "For Iteration 7674 the Loss is 0.2608.\n",
      "For Iteration 7675 the Loss is 0.2608.\n",
      "For Iteration 7676 the Loss is 0.2608.\n",
      "For Iteration 7677 the Loss is 0.2608.\n",
      "For Iteration 7678 the Loss is 0.2608.\n",
      "For Iteration 7679 the Loss is 0.2608.\n",
      "For Iteration 7680 the Loss is 0.2608.\n",
      "For Iteration 7681 the Loss is 0.2608.\n",
      "For Iteration 7682 the Loss is 0.2608.\n",
      "For Iteration 7683 the Loss is 0.2608.\n",
      "For Iteration 7684 the Loss is 0.2608.\n",
      "For Iteration 7685 the Loss is 0.2608.\n",
      "For Iteration 7686 the Loss is 0.2608.\n",
      "For Iteration 7687 the Loss is 0.2608.\n",
      "For Iteration 7688 the Loss is 0.2608.\n",
      "For Iteration 7689 the Loss is 0.2608.\n",
      "For Iteration 7690 the Loss is 0.2608.\n",
      "For Iteration 7691 the Loss is 0.2608.\n",
      "For Iteration 7692 the Loss is 0.2608.\n",
      "For Iteration 7693 the Loss is 0.2608.\n",
      "For Iteration 7694 the Loss is 0.2608.\n",
      "For Iteration 7695 the Loss is 0.2608.\n",
      "For Iteration 7696 the Loss is 0.2608.\n",
      "For Iteration 7697 the Loss is 0.2608.\n",
      "For Iteration 7698 the Loss is 0.2608.\n",
      "For Iteration 7699 the Loss is 0.2608.\n",
      "For Iteration 7700 the Loss is 0.2608.\n",
      "For Iteration 7701 the Loss is 0.2608.\n",
      "For Iteration 7702 the Loss is 0.2608.\n",
      "For Iteration 7703 the Loss is 0.2608.\n",
      "For Iteration 7704 the Loss is 0.2608.\n",
      "For Iteration 7705 the Loss is 0.2608.\n",
      "For Iteration 7706 the Loss is 0.2608.\n",
      "For Iteration 7707 the Loss is 0.2608.\n",
      "For Iteration 7708 the Loss is 0.2608.\n",
      "For Iteration 7709 the Loss is 0.2608.\n",
      "For Iteration 7710 the Loss is 0.2608.\n",
      "For Iteration 7711 the Loss is 0.2608.\n",
      "For Iteration 7712 the Loss is 0.2608.\n",
      "For Iteration 7713 the Loss is 0.2608.\n",
      "For Iteration 7714 the Loss is 0.2608.\n",
      "For Iteration 7715 the Loss is 0.2608.\n",
      "For Iteration 7716 the Loss is 0.2608.\n",
      "For Iteration 7717 the Loss is 0.2608.\n",
      "For Iteration 7718 the Loss is 0.2608.\n",
      "For Iteration 7719 the Loss is 0.2608.\n",
      "For Iteration 7720 the Loss is 0.2608.\n",
      "For Iteration 7721 the Loss is 0.2608.\n",
      "For Iteration 7722 the Loss is 0.2608.\n",
      "For Iteration 7723 the Loss is 0.2608.\n",
      "For Iteration 7724 the Loss is 0.2608.\n",
      "For Iteration 7725 the Loss is 0.2608.\n",
      "For Iteration 7726 the Loss is 0.2608.\n",
      "For Iteration 7727 the Loss is 0.2608.\n",
      "For Iteration 7728 the Loss is 0.2608.\n",
      "For Iteration 7729 the Loss is 0.2608.\n",
      "For Iteration 7730 the Loss is 0.2608.\n",
      "For Iteration 7731 the Loss is 0.2608.\n",
      "For Iteration 7732 the Loss is 0.2608.\n",
      "For Iteration 7733 the Loss is 0.2608.\n",
      "For Iteration 7734 the Loss is 0.2608.\n",
      "For Iteration 7735 the Loss is 0.2608.\n",
      "For Iteration 7736 the Loss is 0.2608.\n",
      "For Iteration 7737 the Loss is 0.2608.\n",
      "For Iteration 7738 the Loss is 0.2608.\n",
      "For Iteration 7739 the Loss is 0.2608.\n",
      "For Iteration 7740 the Loss is 0.2608.\n",
      "For Iteration 7741 the Loss is 0.2608.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 7742 the Loss is 0.2608.\n",
      "For Iteration 7743 the Loss is 0.2608.\n",
      "For Iteration 7744 the Loss is 0.2608.\n",
      "For Iteration 7745 the Loss is 0.2608.\n",
      "For Iteration 7746 the Loss is 0.2608.\n",
      "For Iteration 7747 the Loss is 0.2608.\n",
      "For Iteration 7748 the Loss is 0.2608.\n",
      "For Iteration 7749 the Loss is 0.2608.\n",
      "For Iteration 7750 the Loss is 0.2608.\n",
      "For Iteration 7751 the Loss is 0.2608.\n",
      "For Iteration 7752 the Loss is 0.2608.\n",
      "For Iteration 7753 the Loss is 0.2608.\n",
      "For Iteration 7754 the Loss is 0.2608.\n",
      "For Iteration 7755 the Loss is 0.2608.\n",
      "For Iteration 7756 the Loss is 0.2608.\n",
      "For Iteration 7757 the Loss is 0.2608.\n",
      "For Iteration 7758 the Loss is 0.2608.\n",
      "For Iteration 7759 the Loss is 0.2608.\n",
      "For Iteration 7760 the Loss is 0.2608.\n",
      "For Iteration 7761 the Loss is 0.2608.\n",
      "For Iteration 7762 the Loss is 0.2608.\n",
      "For Iteration 7763 the Loss is 0.2608.\n",
      "For Iteration 7764 the Loss is 0.2608.\n",
      "For Iteration 7765 the Loss is 0.2608.\n",
      "For Iteration 7766 the Loss is 0.2608.\n",
      "For Iteration 7767 the Loss is 0.2608.\n",
      "For Iteration 7768 the Loss is 0.2608.\n",
      "For Iteration 7769 the Loss is 0.2608.\n",
      "For Iteration 7770 the Loss is 0.2608.\n",
      "For Iteration 7771 the Loss is 0.2608.\n",
      "For Iteration 7772 the Loss is 0.2608.\n",
      "For Iteration 7773 the Loss is 0.2608.\n",
      "For Iteration 7774 the Loss is 0.2608.\n",
      "For Iteration 7775 the Loss is 0.2608.\n",
      "For Iteration 7776 the Loss is 0.2608.\n",
      "For Iteration 7777 the Loss is 0.2608.\n",
      "For Iteration 7778 the Loss is 0.2608.\n",
      "For Iteration 7779 the Loss is 0.2608.\n",
      "For Iteration 7780 the Loss is 0.2608.\n",
      "For Iteration 7781 the Loss is 0.2608.\n",
      "For Iteration 7782 the Loss is 0.2608.\n",
      "For Iteration 7783 the Loss is 0.2608.\n",
      "For Iteration 7784 the Loss is 0.2608.\n",
      "For Iteration 7785 the Loss is 0.2608.\n",
      "For Iteration 7786 the Loss is 0.2608.\n",
      "For Iteration 7787 the Loss is 0.2608.\n",
      "For Iteration 7788 the Loss is 0.2608.\n",
      "For Iteration 7789 the Loss is 0.2608.\n",
      "For Iteration 7790 the Loss is 0.2608.\n",
      "For Iteration 7791 the Loss is 0.2608.\n",
      "For Iteration 7792 the Loss is 0.2608.\n",
      "For Iteration 7793 the Loss is 0.2608.\n",
      "For Iteration 7794 the Loss is 0.2608.\n",
      "For Iteration 7795 the Loss is 0.2608.\n",
      "For Iteration 7796 the Loss is 0.2608.\n",
      "For Iteration 7797 the Loss is 0.2608.\n",
      "For Iteration 7798 the Loss is 0.2608.\n",
      "For Iteration 7799 the Loss is 0.2608.\n",
      "For Iteration 7800 the Loss is 0.2608.\n",
      "For Iteration 7801 the Loss is 0.2608.\n",
      "For Iteration 7802 the Loss is 0.2608.\n",
      "For Iteration 7803 the Loss is 0.2608.\n",
      "For Iteration 7804 the Loss is 0.2608.\n",
      "For Iteration 7805 the Loss is 0.2608.\n",
      "For Iteration 7806 the Loss is 0.2608.\n",
      "For Iteration 7807 the Loss is 0.2608.\n",
      "For Iteration 7808 the Loss is 0.2608.\n",
      "For Iteration 7809 the Loss is 0.2608.\n",
      "For Iteration 7810 the Loss is 0.2608.\n",
      "For Iteration 7811 the Loss is 0.2608.\n",
      "For Iteration 7812 the Loss is 0.2608.\n",
      "For Iteration 7813 the Loss is 0.2608.\n",
      "For Iteration 7814 the Loss is 0.2608.\n",
      "For Iteration 7815 the Loss is 0.2608.\n",
      "For Iteration 7816 the Loss is 0.2608.\n",
      "For Iteration 7817 the Loss is 0.2608.\n",
      "For Iteration 7818 the Loss is 0.2608.\n",
      "For Iteration 7819 the Loss is 0.2608.\n",
      "For Iteration 7820 the Loss is 0.2608.\n",
      "For Iteration 7821 the Loss is 0.2608.\n",
      "For Iteration 7822 the Loss is 0.2608.\n",
      "For Iteration 7823 the Loss is 0.2608.\n",
      "For Iteration 7824 the Loss is 0.2608.\n",
      "For Iteration 7825 the Loss is 0.2608.\n",
      "For Iteration 7826 the Loss is 0.2608.\n",
      "For Iteration 7827 the Loss is 0.2608.\n",
      "For Iteration 7828 the Loss is 0.2608.\n",
      "For Iteration 7829 the Loss is 0.2608.\n",
      "For Iteration 7830 the Loss is 0.2608.\n",
      "For Iteration 7831 the Loss is 0.2608.\n",
      "For Iteration 7832 the Loss is 0.2608.\n",
      "For Iteration 7833 the Loss is 0.2608.\n",
      "For Iteration 7834 the Loss is 0.2608.\n",
      "For Iteration 7835 the Loss is 0.2608.\n",
      "For Iteration 7836 the Loss is 0.2608.\n",
      "For Iteration 7837 the Loss is 0.2608.\n",
      "For Iteration 7838 the Loss is 0.2608.\n",
      "For Iteration 7839 the Loss is 0.2608.\n",
      "For Iteration 7840 the Loss is 0.2608.\n",
      "For Iteration 7841 the Loss is 0.2608.\n",
      "For Iteration 7842 the Loss is 0.2608.\n",
      "For Iteration 7843 the Loss is 0.2608.\n",
      "For Iteration 7844 the Loss is 0.2608.\n",
      "For Iteration 7845 the Loss is 0.2608.\n",
      "For Iteration 7846 the Loss is 0.2608.\n",
      "For Iteration 7847 the Loss is 0.2608.\n",
      "For Iteration 7848 the Loss is 0.2608.\n",
      "For Iteration 7849 the Loss is 0.2608.\n",
      "For Iteration 7850 the Loss is 0.2608.\n",
      "For Iteration 7851 the Loss is 0.2608.\n",
      "For Iteration 7852 the Loss is 0.2608.\n",
      "For Iteration 7853 the Loss is 0.2608.\n",
      "For Iteration 7854 the Loss is 0.2608.\n",
      "For Iteration 7855 the Loss is 0.2608.\n",
      "For Iteration 7856 the Loss is 0.2608.\n",
      "For Iteration 7857 the Loss is 0.2608.\n",
      "For Iteration 7858 the Loss is 0.2608.\n",
      "For Iteration 7859 the Loss is 0.2608.\n",
      "For Iteration 7860 the Loss is 0.2608.\n",
      "For Iteration 7861 the Loss is 0.2608.\n",
      "For Iteration 7862 the Loss is 0.2608.\n",
      "For Iteration 7863 the Loss is 0.2608.\n",
      "For Iteration 7864 the Loss is 0.2608.\n",
      "For Iteration 7865 the Loss is 0.2608.\n",
      "For Iteration 7866 the Loss is 0.2608.\n",
      "For Iteration 7867 the Loss is 0.2608.\n",
      "For Iteration 7868 the Loss is 0.2608.\n",
      "For Iteration 7869 the Loss is 0.2608.\n",
      "For Iteration 7870 the Loss is 0.2608.\n",
      "For Iteration 7871 the Loss is 0.2608.\n",
      "For Iteration 7872 the Loss is 0.2608.\n",
      "For Iteration 7873 the Loss is 0.2608.\n",
      "For Iteration 7874 the Loss is 0.2608.\n",
      "For Iteration 7875 the Loss is 0.2608.\n",
      "For Iteration 7876 the Loss is 0.2608.\n",
      "For Iteration 7877 the Loss is 0.2608.\n",
      "For Iteration 7878 the Loss is 0.2608.\n",
      "For Iteration 7879 the Loss is 0.2608.\n",
      "For Iteration 7880 the Loss is 0.2608.\n",
      "For Iteration 7881 the Loss is 0.2608.\n",
      "For Iteration 7882 the Loss is 0.2608.\n",
      "For Iteration 7883 the Loss is 0.2608.\n",
      "For Iteration 7884 the Loss is 0.2608.\n",
      "For Iteration 7885 the Loss is 0.2608.\n",
      "For Iteration 7886 the Loss is 0.2608.\n",
      "For Iteration 7887 the Loss is 0.2608.\n",
      "For Iteration 7888 the Loss is 0.2608.\n",
      "For Iteration 7889 the Loss is 0.2608.\n",
      "For Iteration 7890 the Loss is 0.2608.\n",
      "For Iteration 7891 the Loss is 0.2608.\n",
      "For Iteration 7892 the Loss is 0.2608.\n",
      "For Iteration 7893 the Loss is 0.2608.\n",
      "For Iteration 7894 the Loss is 0.2608.\n",
      "For Iteration 7895 the Loss is 0.2608.\n",
      "For Iteration 7896 the Loss is 0.2608.\n",
      "For Iteration 7897 the Loss is 0.2608.\n",
      "For Iteration 7898 the Loss is 0.2608.\n",
      "For Iteration 7899 the Loss is 0.2608.\n",
      "For Iteration 7900 the Loss is 0.2608.\n",
      "For Iteration 7901 the Loss is 0.2608.\n",
      "For Iteration 7902 the Loss is 0.2608.\n",
      "For Iteration 7903 the Loss is 0.2608.\n",
      "For Iteration 7904 the Loss is 0.2608.\n",
      "For Iteration 7905 the Loss is 0.2608.\n",
      "For Iteration 7906 the Loss is 0.2608.\n",
      "For Iteration 7907 the Loss is 0.2608.\n",
      "For Iteration 7908 the Loss is 0.2608.\n",
      "For Iteration 7909 the Loss is 0.2608.\n",
      "For Iteration 7910 the Loss is 0.2608.\n",
      "For Iteration 7911 the Loss is 0.2608.\n",
      "For Iteration 7912 the Loss is 0.2608.\n",
      "For Iteration 7913 the Loss is 0.2608.\n",
      "For Iteration 7914 the Loss is 0.2608.\n",
      "For Iteration 7915 the Loss is 0.2608.\n",
      "For Iteration 7916 the Loss is 0.2608.\n",
      "For Iteration 7917 the Loss is 0.2608.\n",
      "For Iteration 7918 the Loss is 0.2608.\n",
      "For Iteration 7919 the Loss is 0.2608.\n",
      "For Iteration 7920 the Loss is 0.2608.\n",
      "For Iteration 7921 the Loss is 0.2608.\n",
      "For Iteration 7922 the Loss is 0.2608.\n",
      "For Iteration 7923 the Loss is 0.2608.\n",
      "For Iteration 7924 the Loss is 0.2608.\n",
      "For Iteration 7925 the Loss is 0.2608.\n",
      "For Iteration 7926 the Loss is 0.2608.\n",
      "For Iteration 7927 the Loss is 0.2608.\n",
      "For Iteration 7928 the Loss is 0.2608.\n",
      "For Iteration 7929 the Loss is 0.2608.\n",
      "For Iteration 7930 the Loss is 0.2608.\n",
      "For Iteration 7931 the Loss is 0.2608.\n",
      "For Iteration 7932 the Loss is 0.2608.\n",
      "For Iteration 7933 the Loss is 0.2608.\n",
      "For Iteration 7934 the Loss is 0.2608.\n",
      "For Iteration 7935 the Loss is 0.2608.\n",
      "For Iteration 7936 the Loss is 0.2608.\n",
      "For Iteration 7937 the Loss is 0.2608.\n",
      "For Iteration 7938 the Loss is 0.2608.\n",
      "For Iteration 7939 the Loss is 0.2608.\n",
      "For Iteration 7940 the Loss is 0.2608.\n",
      "For Iteration 7941 the Loss is 0.2608.\n",
      "For Iteration 7942 the Loss is 0.2608.\n",
      "For Iteration 7943 the Loss is 0.2608.\n",
      "For Iteration 7944 the Loss is 0.2608.\n",
      "For Iteration 7945 the Loss is 0.2608.\n",
      "For Iteration 7946 the Loss is 0.2608.\n",
      "For Iteration 7947 the Loss is 0.2608.\n",
      "For Iteration 7948 the Loss is 0.2608.\n",
      "For Iteration 7949 the Loss is 0.2608.\n",
      "For Iteration 7950 the Loss is 0.2608.\n",
      "For Iteration 7951 the Loss is 0.2608.\n",
      "For Iteration 7952 the Loss is 0.2608.\n",
      "For Iteration 7953 the Loss is 0.2608.\n",
      "For Iteration 7954 the Loss is 0.2608.\n",
      "For Iteration 7955 the Loss is 0.2608.\n",
      "For Iteration 7956 the Loss is 0.2608.\n",
      "For Iteration 7957 the Loss is 0.2608.\n",
      "For Iteration 7958 the Loss is 0.2608.\n",
      "For Iteration 7959 the Loss is 0.2608.\n",
      "For Iteration 7960 the Loss is 0.2608.\n",
      "For Iteration 7961 the Loss is 0.2608.\n",
      "For Iteration 7962 the Loss is 0.2608.\n",
      "For Iteration 7963 the Loss is 0.2608.\n",
      "For Iteration 7964 the Loss is 0.2608.\n",
      "For Iteration 7965 the Loss is 0.2608.\n",
      "For Iteration 7966 the Loss is 0.2608.\n",
      "For Iteration 7967 the Loss is 0.2608.\n",
      "For Iteration 7968 the Loss is 0.2608.\n",
      "For Iteration 7969 the Loss is 0.2608.\n",
      "For Iteration 7970 the Loss is 0.2608.\n",
      "For Iteration 7971 the Loss is 0.2608.\n",
      "For Iteration 7972 the Loss is 0.2608.\n",
      "For Iteration 7973 the Loss is 0.2608.\n",
      "For Iteration 7974 the Loss is 0.2608.\n",
      "For Iteration 7975 the Loss is 0.2608.\n",
      "For Iteration 7976 the Loss is 0.2608.\n",
      "For Iteration 7977 the Loss is 0.2608.\n",
      "For Iteration 7978 the Loss is 0.2608.\n",
      "For Iteration 7979 the Loss is 0.2608.\n",
      "For Iteration 7980 the Loss is 0.2608.\n",
      "For Iteration 7981 the Loss is 0.2608.\n",
      "For Iteration 7982 the Loss is 0.2608.\n",
      "For Iteration 7983 the Loss is 0.2608.\n",
      "For Iteration 7984 the Loss is 0.2608.\n",
      "For Iteration 7985 the Loss is 0.2608.\n",
      "For Iteration 7986 the Loss is 0.2608.\n",
      "For Iteration 7987 the Loss is 0.2608.\n",
      "For Iteration 7988 the Loss is 0.2608.\n",
      "For Iteration 7989 the Loss is 0.2608.\n",
      "For Iteration 7990 the Loss is 0.2608.\n",
      "For Iteration 7991 the Loss is 0.2608.\n",
      "For Iteration 7992 the Loss is 0.2608.\n",
      "For Iteration 7993 the Loss is 0.2608.\n",
      "For Iteration 7994 the Loss is 0.2608.\n",
      "For Iteration 7995 the Loss is 0.2608.\n",
      "For Iteration 7996 the Loss is 0.2608.\n",
      "For Iteration 7997 the Loss is 0.2608.\n",
      "For Iteration 7998 the Loss is 0.2608.\n",
      "For Iteration 7999 the Loss is 0.2608.\n",
      "For Iteration 8000 the Loss is 0.2608.\n",
      "For Iteration 8001 the Loss is 0.2608.\n",
      "For Iteration 8002 the Loss is 0.2608.\n",
      "For Iteration 8003 the Loss is 0.2608.\n",
      "For Iteration 8004 the Loss is 0.2608.\n",
      "For Iteration 8005 the Loss is 0.2608.\n",
      "For Iteration 8006 the Loss is 0.2608.\n",
      "For Iteration 8007 the Loss is 0.2608.\n",
      "For Iteration 8008 the Loss is 0.2608.\n",
      "For Iteration 8009 the Loss is 0.2608.\n",
      "For Iteration 8010 the Loss is 0.2608.\n",
      "For Iteration 8011 the Loss is 0.2608.\n",
      "For Iteration 8012 the Loss is 0.2608.\n",
      "For Iteration 8013 the Loss is 0.2608.\n",
      "For Iteration 8014 the Loss is 0.2608.\n",
      "For Iteration 8015 the Loss is 0.2608.\n",
      "For Iteration 8016 the Loss is 0.2608.\n",
      "For Iteration 8017 the Loss is 0.2608.\n",
      "For Iteration 8018 the Loss is 0.2608.\n",
      "For Iteration 8019 the Loss is 0.2608.\n",
      "For Iteration 8020 the Loss is 0.2608.\n",
      "For Iteration 8021 the Loss is 0.2608.\n",
      "For Iteration 8022 the Loss is 0.2608.\n",
      "For Iteration 8023 the Loss is 0.2608.\n",
      "For Iteration 8024 the Loss is 0.2608.\n",
      "For Iteration 8025 the Loss is 0.2608.\n",
      "For Iteration 8026 the Loss is 0.2608.\n",
      "For Iteration 8027 the Loss is 0.2608.\n",
      "For Iteration 8028 the Loss is 0.2608.\n",
      "For Iteration 8029 the Loss is 0.2608.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 8030 the Loss is 0.2607.\n",
      "For Iteration 8031 the Loss is 0.2607.\n",
      "For Iteration 8032 the Loss is 0.2607.\n",
      "For Iteration 8033 the Loss is 0.2607.\n",
      "For Iteration 8034 the Loss is 0.2607.\n",
      "For Iteration 8035 the Loss is 0.2607.\n",
      "For Iteration 8036 the Loss is 0.2607.\n",
      "For Iteration 8037 the Loss is 0.2607.\n",
      "For Iteration 8038 the Loss is 0.2607.\n",
      "For Iteration 8039 the Loss is 0.2607.\n",
      "For Iteration 8040 the Loss is 0.2607.\n",
      "For Iteration 8041 the Loss is 0.2607.\n",
      "For Iteration 8042 the Loss is 0.2607.\n",
      "For Iteration 8043 the Loss is 0.2607.\n",
      "For Iteration 8044 the Loss is 0.2607.\n",
      "For Iteration 8045 the Loss is 0.2607.\n",
      "For Iteration 8046 the Loss is 0.2607.\n",
      "For Iteration 8047 the Loss is 0.2607.\n",
      "For Iteration 8048 the Loss is 0.2607.\n",
      "For Iteration 8049 the Loss is 0.2607.\n",
      "For Iteration 8050 the Loss is 0.2607.\n",
      "For Iteration 8051 the Loss is 0.2607.\n",
      "For Iteration 8052 the Loss is 0.2607.\n",
      "For Iteration 8053 the Loss is 0.2607.\n",
      "For Iteration 8054 the Loss is 0.2607.\n",
      "For Iteration 8055 the Loss is 0.2607.\n",
      "For Iteration 8056 the Loss is 0.2607.\n",
      "For Iteration 8057 the Loss is 0.2607.\n",
      "For Iteration 8058 the Loss is 0.2607.\n",
      "For Iteration 8059 the Loss is 0.2607.\n",
      "For Iteration 8060 the Loss is 0.2607.\n",
      "For Iteration 8061 the Loss is 0.2607.\n",
      "For Iteration 8062 the Loss is 0.2607.\n",
      "For Iteration 8063 the Loss is 0.2607.\n",
      "For Iteration 8064 the Loss is 0.2607.\n",
      "For Iteration 8065 the Loss is 0.2607.\n",
      "For Iteration 8066 the Loss is 0.2607.\n",
      "For Iteration 8067 the Loss is 0.2607.\n",
      "For Iteration 8068 the Loss is 0.2607.\n",
      "For Iteration 8069 the Loss is 0.2607.\n",
      "For Iteration 8070 the Loss is 0.2607.\n",
      "For Iteration 8071 the Loss is 0.2607.\n",
      "For Iteration 8072 the Loss is 0.2607.\n",
      "For Iteration 8073 the Loss is 0.2607.\n",
      "For Iteration 8074 the Loss is 0.2607.\n",
      "For Iteration 8075 the Loss is 0.2607.\n",
      "For Iteration 8076 the Loss is 0.2607.\n",
      "For Iteration 8077 the Loss is 0.2607.\n",
      "For Iteration 8078 the Loss is 0.2607.\n",
      "For Iteration 8079 the Loss is 0.2607.\n",
      "For Iteration 8080 the Loss is 0.2607.\n",
      "For Iteration 8081 the Loss is 0.2607.\n",
      "For Iteration 8082 the Loss is 0.2607.\n",
      "For Iteration 8083 the Loss is 0.2607.\n",
      "For Iteration 8084 the Loss is 0.2607.\n",
      "For Iteration 8085 the Loss is 0.2607.\n",
      "For Iteration 8086 the Loss is 0.2607.\n",
      "For Iteration 8087 the Loss is 0.2607.\n",
      "For Iteration 8088 the Loss is 0.2607.\n",
      "For Iteration 8089 the Loss is 0.2607.\n",
      "For Iteration 8090 the Loss is 0.2607.\n",
      "For Iteration 8091 the Loss is 0.2607.\n",
      "For Iteration 8092 the Loss is 0.2607.\n",
      "For Iteration 8093 the Loss is 0.2607.\n",
      "For Iteration 8094 the Loss is 0.2607.\n",
      "For Iteration 8095 the Loss is 0.2607.\n",
      "For Iteration 8096 the Loss is 0.2607.\n",
      "For Iteration 8097 the Loss is 0.2607.\n",
      "For Iteration 8098 the Loss is 0.2607.\n",
      "For Iteration 8099 the Loss is 0.2607.\n",
      "For Iteration 8100 the Loss is 0.2607.\n",
      "For Iteration 8101 the Loss is 0.2607.\n",
      "For Iteration 8102 the Loss is 0.2607.\n",
      "For Iteration 8103 the Loss is 0.2607.\n",
      "For Iteration 8104 the Loss is 0.2607.\n",
      "For Iteration 8105 the Loss is 0.2607.\n",
      "For Iteration 8106 the Loss is 0.2607.\n",
      "For Iteration 8107 the Loss is 0.2607.\n",
      "For Iteration 8108 the Loss is 0.2607.\n",
      "For Iteration 8109 the Loss is 0.2607.\n",
      "For Iteration 8110 the Loss is 0.2607.\n",
      "For Iteration 8111 the Loss is 0.2607.\n",
      "For Iteration 8112 the Loss is 0.2607.\n",
      "For Iteration 8113 the Loss is 0.2607.\n",
      "For Iteration 8114 the Loss is 0.2607.\n",
      "For Iteration 8115 the Loss is 0.2607.\n",
      "For Iteration 8116 the Loss is 0.2607.\n",
      "For Iteration 8117 the Loss is 0.2607.\n",
      "For Iteration 8118 the Loss is 0.2607.\n",
      "For Iteration 8119 the Loss is 0.2607.\n",
      "For Iteration 8120 the Loss is 0.2607.\n",
      "For Iteration 8121 the Loss is 0.2607.\n",
      "For Iteration 8122 the Loss is 0.2607.\n",
      "For Iteration 8123 the Loss is 0.2607.\n",
      "For Iteration 8124 the Loss is 0.2607.\n",
      "For Iteration 8125 the Loss is 0.2607.\n",
      "For Iteration 8126 the Loss is 0.2607.\n",
      "For Iteration 8127 the Loss is 0.2607.\n",
      "For Iteration 8128 the Loss is 0.2607.\n",
      "For Iteration 8129 the Loss is 0.2607.\n",
      "For Iteration 8130 the Loss is 0.2607.\n",
      "For Iteration 8131 the Loss is 0.2607.\n",
      "For Iteration 8132 the Loss is 0.2607.\n",
      "For Iteration 8133 the Loss is 0.2607.\n",
      "For Iteration 8134 the Loss is 0.2607.\n",
      "For Iteration 8135 the Loss is 0.2607.\n",
      "For Iteration 8136 the Loss is 0.2607.\n",
      "For Iteration 8137 the Loss is 0.2607.\n",
      "For Iteration 8138 the Loss is 0.2607.\n",
      "For Iteration 8139 the Loss is 0.2607.\n",
      "For Iteration 8140 the Loss is 0.2607.\n",
      "For Iteration 8141 the Loss is 0.2607.\n",
      "For Iteration 8142 the Loss is 0.2607.\n",
      "For Iteration 8143 the Loss is 0.2607.\n",
      "For Iteration 8144 the Loss is 0.2607.\n",
      "For Iteration 8145 the Loss is 0.2607.\n",
      "For Iteration 8146 the Loss is 0.2607.\n",
      "For Iteration 8147 the Loss is 0.2607.\n",
      "For Iteration 8148 the Loss is 0.2607.\n",
      "For Iteration 8149 the Loss is 0.2607.\n",
      "For Iteration 8150 the Loss is 0.2607.\n",
      "For Iteration 8151 the Loss is 0.2607.\n",
      "For Iteration 8152 the Loss is 0.2607.\n",
      "For Iteration 8153 the Loss is 0.2607.\n",
      "For Iteration 8154 the Loss is 0.2607.\n",
      "For Iteration 8155 the Loss is 0.2607.\n",
      "For Iteration 8156 the Loss is 0.2607.\n",
      "For Iteration 8157 the Loss is 0.2607.\n",
      "For Iteration 8158 the Loss is 0.2607.\n",
      "For Iteration 8159 the Loss is 0.2607.\n",
      "For Iteration 8160 the Loss is 0.2607.\n",
      "For Iteration 8161 the Loss is 0.2607.\n",
      "For Iteration 8162 the Loss is 0.2607.\n",
      "For Iteration 8163 the Loss is 0.2607.\n",
      "For Iteration 8164 the Loss is 0.2607.\n",
      "For Iteration 8165 the Loss is 0.2607.\n",
      "For Iteration 8166 the Loss is 0.2607.\n",
      "For Iteration 8167 the Loss is 0.2607.\n",
      "For Iteration 8168 the Loss is 0.2607.\n",
      "For Iteration 8169 the Loss is 0.2607.\n",
      "For Iteration 8170 the Loss is 0.2607.\n",
      "For Iteration 8171 the Loss is 0.2607.\n",
      "For Iteration 8172 the Loss is 0.2607.\n",
      "For Iteration 8173 the Loss is 0.2607.\n",
      "For Iteration 8174 the Loss is 0.2607.\n",
      "For Iteration 8175 the Loss is 0.2607.\n",
      "For Iteration 8176 the Loss is 0.2607.\n",
      "For Iteration 8177 the Loss is 0.2607.\n",
      "For Iteration 8178 the Loss is 0.2607.\n",
      "For Iteration 8179 the Loss is 0.2607.\n",
      "For Iteration 8180 the Loss is 0.2607.\n",
      "For Iteration 8181 the Loss is 0.2607.\n",
      "For Iteration 8182 the Loss is 0.2607.\n",
      "For Iteration 8183 the Loss is 0.2607.\n",
      "For Iteration 8184 the Loss is 0.2607.\n",
      "For Iteration 8185 the Loss is 0.2607.\n",
      "For Iteration 8186 the Loss is 0.2607.\n",
      "For Iteration 8187 the Loss is 0.2607.\n",
      "For Iteration 8188 the Loss is 0.2607.\n",
      "For Iteration 8189 the Loss is 0.2607.\n",
      "For Iteration 8190 the Loss is 0.2607.\n",
      "For Iteration 8191 the Loss is 0.2607.\n",
      "For Iteration 8192 the Loss is 0.2607.\n",
      "For Iteration 8193 the Loss is 0.2607.\n",
      "For Iteration 8194 the Loss is 0.2607.\n",
      "For Iteration 8195 the Loss is 0.2607.\n",
      "For Iteration 8196 the Loss is 0.2607.\n",
      "For Iteration 8197 the Loss is 0.2607.\n",
      "For Iteration 8198 the Loss is 0.2607.\n",
      "For Iteration 8199 the Loss is 0.2607.\n",
      "For Iteration 8200 the Loss is 0.2607.\n",
      "For Iteration 8201 the Loss is 0.2607.\n",
      "For Iteration 8202 the Loss is 0.2607.\n",
      "For Iteration 8203 the Loss is 0.2607.\n",
      "For Iteration 8204 the Loss is 0.2607.\n",
      "For Iteration 8205 the Loss is 0.2607.\n",
      "For Iteration 8206 the Loss is 0.2607.\n",
      "For Iteration 8207 the Loss is 0.2607.\n",
      "For Iteration 8208 the Loss is 0.2607.\n",
      "For Iteration 8209 the Loss is 0.2607.\n",
      "For Iteration 8210 the Loss is 0.2607.\n",
      "For Iteration 8211 the Loss is 0.2607.\n",
      "For Iteration 8212 the Loss is 0.2607.\n",
      "For Iteration 8213 the Loss is 0.2607.\n",
      "For Iteration 8214 the Loss is 0.2607.\n",
      "For Iteration 8215 the Loss is 0.2607.\n",
      "For Iteration 8216 the Loss is 0.2607.\n",
      "For Iteration 8217 the Loss is 0.2607.\n",
      "For Iteration 8218 the Loss is 0.2607.\n",
      "For Iteration 8219 the Loss is 0.2607.\n",
      "For Iteration 8220 the Loss is 0.2607.\n",
      "For Iteration 8221 the Loss is 0.2607.\n",
      "For Iteration 8222 the Loss is 0.2607.\n",
      "For Iteration 8223 the Loss is 0.2607.\n",
      "For Iteration 8224 the Loss is 0.2607.\n",
      "For Iteration 8225 the Loss is 0.2607.\n",
      "For Iteration 8226 the Loss is 0.2607.\n",
      "For Iteration 8227 the Loss is 0.2607.\n",
      "For Iteration 8228 the Loss is 0.2607.\n",
      "For Iteration 8229 the Loss is 0.2607.\n",
      "For Iteration 8230 the Loss is 0.2607.\n",
      "For Iteration 8231 the Loss is 0.2607.\n",
      "For Iteration 8232 the Loss is 0.2607.\n",
      "For Iteration 8233 the Loss is 0.2607.\n",
      "For Iteration 8234 the Loss is 0.2607.\n",
      "For Iteration 8235 the Loss is 0.2607.\n",
      "For Iteration 8236 the Loss is 0.2607.\n",
      "For Iteration 8237 the Loss is 0.2607.\n",
      "For Iteration 8238 the Loss is 0.2607.\n",
      "For Iteration 8239 the Loss is 0.2607.\n",
      "For Iteration 8240 the Loss is 0.2607.\n",
      "For Iteration 8241 the Loss is 0.2607.\n",
      "For Iteration 8242 the Loss is 0.2607.\n",
      "For Iteration 8243 the Loss is 0.2607.\n",
      "For Iteration 8244 the Loss is 0.2607.\n",
      "For Iteration 8245 the Loss is 0.2607.\n",
      "For Iteration 8246 the Loss is 0.2607.\n",
      "For Iteration 8247 the Loss is 0.2607.\n",
      "For Iteration 8248 the Loss is 0.2607.\n",
      "For Iteration 8249 the Loss is 0.2607.\n",
      "For Iteration 8250 the Loss is 0.2607.\n",
      "For Iteration 8251 the Loss is 0.2607.\n",
      "For Iteration 8252 the Loss is 0.2607.\n",
      "For Iteration 8253 the Loss is 0.2607.\n",
      "For Iteration 8254 the Loss is 0.2607.\n",
      "For Iteration 8255 the Loss is 0.2607.\n",
      "For Iteration 8256 the Loss is 0.2607.\n",
      "For Iteration 8257 the Loss is 0.2607.\n",
      "For Iteration 8258 the Loss is 0.2607.\n",
      "For Iteration 8259 the Loss is 0.2607.\n",
      "For Iteration 8260 the Loss is 0.2607.\n",
      "For Iteration 8261 the Loss is 0.2607.\n",
      "For Iteration 8262 the Loss is 0.2607.\n",
      "For Iteration 8263 the Loss is 0.2607.\n",
      "For Iteration 8264 the Loss is 0.2607.\n",
      "For Iteration 8265 the Loss is 0.2607.\n",
      "For Iteration 8266 the Loss is 0.2607.\n",
      "For Iteration 8267 the Loss is 0.2607.\n",
      "For Iteration 8268 the Loss is 0.2607.\n",
      "For Iteration 8269 the Loss is 0.2607.\n",
      "For Iteration 8270 the Loss is 0.2607.\n",
      "For Iteration 8271 the Loss is 0.2607.\n",
      "For Iteration 8272 the Loss is 0.2607.\n",
      "For Iteration 8273 the Loss is 0.2607.\n",
      "For Iteration 8274 the Loss is 0.2607.\n",
      "For Iteration 8275 the Loss is 0.2607.\n",
      "For Iteration 8276 the Loss is 0.2607.\n",
      "For Iteration 8277 the Loss is 0.2607.\n",
      "For Iteration 8278 the Loss is 0.2607.\n",
      "For Iteration 8279 the Loss is 0.2607.\n",
      "For Iteration 8280 the Loss is 0.2607.\n",
      "For Iteration 8281 the Loss is 0.2607.\n",
      "For Iteration 8282 the Loss is 0.2607.\n",
      "For Iteration 8283 the Loss is 0.2607.\n",
      "For Iteration 8284 the Loss is 0.2607.\n",
      "For Iteration 8285 the Loss is 0.2607.\n",
      "For Iteration 8286 the Loss is 0.2607.\n",
      "For Iteration 8287 the Loss is 0.2607.\n",
      "For Iteration 8288 the Loss is 0.2607.\n",
      "For Iteration 8289 the Loss is 0.2607.\n",
      "For Iteration 8290 the Loss is 0.2607.\n",
      "For Iteration 8291 the Loss is 0.2607.\n",
      "For Iteration 8292 the Loss is 0.2607.\n",
      "For Iteration 8293 the Loss is 0.2607.\n",
      "For Iteration 8294 the Loss is 0.2607.\n",
      "For Iteration 8295 the Loss is 0.2607.\n",
      "For Iteration 8296 the Loss is 0.2607.\n",
      "For Iteration 8297 the Loss is 0.2607.\n",
      "For Iteration 8298 the Loss is 0.2607.\n",
      "For Iteration 8299 the Loss is 0.2607.\n",
      "For Iteration 8300 the Loss is 0.2607.\n",
      "For Iteration 8301 the Loss is 0.2607.\n",
      "For Iteration 8302 the Loss is 0.2607.\n",
      "For Iteration 8303 the Loss is 0.2607.\n",
      "For Iteration 8304 the Loss is 0.2607.\n",
      "For Iteration 8305 the Loss is 0.2607.\n",
      "For Iteration 8306 the Loss is 0.2607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 8307 the Loss is 0.2607.\n",
      "For Iteration 8308 the Loss is 0.2607.\n",
      "For Iteration 8309 the Loss is 0.2607.\n",
      "For Iteration 8310 the Loss is 0.2607.\n",
      "For Iteration 8311 the Loss is 0.2607.\n",
      "For Iteration 8312 the Loss is 0.2607.\n",
      "For Iteration 8313 the Loss is 0.2607.\n",
      "For Iteration 8314 the Loss is 0.2607.\n",
      "For Iteration 8315 the Loss is 0.2607.\n",
      "For Iteration 8316 the Loss is 0.2607.\n",
      "For Iteration 8317 the Loss is 0.2607.\n",
      "For Iteration 8318 the Loss is 0.2607.\n",
      "For Iteration 8319 the Loss is 0.2607.\n",
      "For Iteration 8320 the Loss is 0.2607.\n",
      "For Iteration 8321 the Loss is 0.2607.\n",
      "For Iteration 8322 the Loss is 0.2607.\n",
      "For Iteration 8323 the Loss is 0.2607.\n",
      "For Iteration 8324 the Loss is 0.2607.\n",
      "For Iteration 8325 the Loss is 0.2607.\n",
      "For Iteration 8326 the Loss is 0.2607.\n",
      "For Iteration 8327 the Loss is 0.2607.\n",
      "For Iteration 8328 the Loss is 0.2607.\n",
      "For Iteration 8329 the Loss is 0.2607.\n",
      "For Iteration 8330 the Loss is 0.2607.\n",
      "For Iteration 8331 the Loss is 0.2607.\n",
      "For Iteration 8332 the Loss is 0.2607.\n",
      "For Iteration 8333 the Loss is 0.2607.\n",
      "For Iteration 8334 the Loss is 0.2607.\n",
      "For Iteration 8335 the Loss is 0.2607.\n",
      "For Iteration 8336 the Loss is 0.2607.\n",
      "For Iteration 8337 the Loss is 0.2607.\n",
      "For Iteration 8338 the Loss is 0.2607.\n",
      "For Iteration 8339 the Loss is 0.2607.\n",
      "For Iteration 8340 the Loss is 0.2607.\n",
      "For Iteration 8341 the Loss is 0.2607.\n",
      "For Iteration 8342 the Loss is 0.2607.\n",
      "For Iteration 8343 the Loss is 0.2607.\n",
      "For Iteration 8344 the Loss is 0.2607.\n",
      "For Iteration 8345 the Loss is 0.2607.\n",
      "For Iteration 8346 the Loss is 0.2607.\n",
      "For Iteration 8347 the Loss is 0.2607.\n",
      "For Iteration 8348 the Loss is 0.2607.\n",
      "For Iteration 8349 the Loss is 0.2607.\n",
      "For Iteration 8350 the Loss is 0.2607.\n",
      "For Iteration 8351 the Loss is 0.2607.\n",
      "For Iteration 8352 the Loss is 0.2607.\n",
      "For Iteration 8353 the Loss is 0.2607.\n",
      "For Iteration 8354 the Loss is 0.2607.\n",
      "For Iteration 8355 the Loss is 0.2607.\n",
      "For Iteration 8356 the Loss is 0.2607.\n",
      "For Iteration 8357 the Loss is 0.2607.\n",
      "For Iteration 8358 the Loss is 0.2607.\n",
      "For Iteration 8359 the Loss is 0.2607.\n",
      "For Iteration 8360 the Loss is 0.2607.\n",
      "For Iteration 8361 the Loss is 0.2607.\n",
      "For Iteration 8362 the Loss is 0.2607.\n",
      "For Iteration 8363 the Loss is 0.2607.\n",
      "For Iteration 8364 the Loss is 0.2607.\n",
      "For Iteration 8365 the Loss is 0.2607.\n",
      "For Iteration 8366 the Loss is 0.2607.\n",
      "For Iteration 8367 the Loss is 0.2607.\n",
      "For Iteration 8368 the Loss is 0.2607.\n",
      "For Iteration 8369 the Loss is 0.2607.\n",
      "For Iteration 8370 the Loss is 0.2607.\n",
      "For Iteration 8371 the Loss is 0.2607.\n",
      "For Iteration 8372 the Loss is 0.2607.\n",
      "For Iteration 8373 the Loss is 0.2607.\n",
      "For Iteration 8374 the Loss is 0.2607.\n",
      "For Iteration 8375 the Loss is 0.2607.\n",
      "For Iteration 8376 the Loss is 0.2607.\n",
      "For Iteration 8377 the Loss is 0.2607.\n",
      "For Iteration 8378 the Loss is 0.2607.\n",
      "For Iteration 8379 the Loss is 0.2607.\n",
      "For Iteration 8380 the Loss is 0.2607.\n",
      "For Iteration 8381 the Loss is 0.2607.\n",
      "For Iteration 8382 the Loss is 0.2607.\n",
      "For Iteration 8383 the Loss is 0.2607.\n",
      "For Iteration 8384 the Loss is 0.2607.\n",
      "For Iteration 8385 the Loss is 0.2607.\n",
      "For Iteration 8386 the Loss is 0.2607.\n",
      "For Iteration 8387 the Loss is 0.2607.\n",
      "For Iteration 8388 the Loss is 0.2607.\n",
      "For Iteration 8389 the Loss is 0.2607.\n",
      "For Iteration 8390 the Loss is 0.2607.\n",
      "For Iteration 8391 the Loss is 0.2607.\n",
      "For Iteration 8392 the Loss is 0.2607.\n",
      "For Iteration 8393 the Loss is 0.2607.\n",
      "For Iteration 8394 the Loss is 0.2607.\n",
      "For Iteration 8395 the Loss is 0.2607.\n",
      "For Iteration 8396 the Loss is 0.2607.\n",
      "For Iteration 8397 the Loss is 0.2607.\n",
      "For Iteration 8398 the Loss is 0.2607.\n",
      "For Iteration 8399 the Loss is 0.2607.\n",
      "For Iteration 8400 the Loss is 0.2607.\n",
      "For Iteration 8401 the Loss is 0.2607.\n",
      "For Iteration 8402 the Loss is 0.2607.\n",
      "For Iteration 8403 the Loss is 0.2607.\n",
      "For Iteration 8404 the Loss is 0.2607.\n",
      "For Iteration 8405 the Loss is 0.2607.\n",
      "For Iteration 8406 the Loss is 0.2607.\n",
      "For Iteration 8407 the Loss is 0.2607.\n",
      "For Iteration 8408 the Loss is 0.2607.\n",
      "For Iteration 8409 the Loss is 0.2607.\n",
      "For Iteration 8410 the Loss is 0.2607.\n",
      "For Iteration 8411 the Loss is 0.2607.\n",
      "For Iteration 8412 the Loss is 0.2607.\n",
      "For Iteration 8413 the Loss is 0.2607.\n",
      "For Iteration 8414 the Loss is 0.2607.\n",
      "For Iteration 8415 the Loss is 0.2607.\n",
      "For Iteration 8416 the Loss is 0.2607.\n",
      "For Iteration 8417 the Loss is 0.2607.\n",
      "For Iteration 8418 the Loss is 0.2607.\n",
      "For Iteration 8419 the Loss is 0.2607.\n",
      "For Iteration 8420 the Loss is 0.2607.\n",
      "For Iteration 8421 the Loss is 0.2607.\n",
      "For Iteration 8422 the Loss is 0.2607.\n",
      "For Iteration 8423 the Loss is 0.2607.\n",
      "For Iteration 8424 the Loss is 0.2607.\n",
      "For Iteration 8425 the Loss is 0.2607.\n",
      "For Iteration 8426 the Loss is 0.2607.\n",
      "For Iteration 8427 the Loss is 0.2607.\n",
      "For Iteration 8428 the Loss is 0.2607.\n",
      "For Iteration 8429 the Loss is 0.2607.\n",
      "For Iteration 8430 the Loss is 0.2607.\n",
      "For Iteration 8431 the Loss is 0.2607.\n",
      "For Iteration 8432 the Loss is 0.2607.\n",
      "For Iteration 8433 the Loss is 0.2607.\n",
      "For Iteration 8434 the Loss is 0.2607.\n",
      "For Iteration 8435 the Loss is 0.2607.\n",
      "For Iteration 8436 the Loss is 0.2607.\n",
      "For Iteration 8437 the Loss is 0.2607.\n",
      "For Iteration 8438 the Loss is 0.2607.\n",
      "For Iteration 8439 the Loss is 0.2607.\n",
      "For Iteration 8440 the Loss is 0.2607.\n",
      "For Iteration 8441 the Loss is 0.2607.\n",
      "For Iteration 8442 the Loss is 0.2607.\n",
      "For Iteration 8443 the Loss is 0.2607.\n",
      "For Iteration 8444 the Loss is 0.2607.\n",
      "For Iteration 8445 the Loss is 0.2607.\n",
      "For Iteration 8446 the Loss is 0.2607.\n",
      "For Iteration 8447 the Loss is 0.2607.\n",
      "For Iteration 8448 the Loss is 0.2607.\n",
      "For Iteration 8449 the Loss is 0.2607.\n",
      "For Iteration 8450 the Loss is 0.2607.\n",
      "For Iteration 8451 the Loss is 0.2607.\n",
      "For Iteration 8452 the Loss is 0.2607.\n",
      "For Iteration 8453 the Loss is 0.2607.\n",
      "For Iteration 8454 the Loss is 0.2607.\n",
      "For Iteration 8455 the Loss is 0.2607.\n",
      "For Iteration 8456 the Loss is 0.2607.\n",
      "For Iteration 8457 the Loss is 0.2607.\n",
      "For Iteration 8458 the Loss is 0.2607.\n",
      "For Iteration 8459 the Loss is 0.2607.\n",
      "For Iteration 8460 the Loss is 0.2607.\n",
      "For Iteration 8461 the Loss is 0.2607.\n",
      "For Iteration 8462 the Loss is 0.2607.\n",
      "For Iteration 8463 the Loss is 0.2607.\n",
      "For Iteration 8464 the Loss is 0.2607.\n",
      "For Iteration 8465 the Loss is 0.2607.\n",
      "For Iteration 8466 the Loss is 0.2607.\n",
      "For Iteration 8467 the Loss is 0.2607.\n",
      "For Iteration 8468 the Loss is 0.2607.\n",
      "For Iteration 8469 the Loss is 0.2607.\n",
      "For Iteration 8470 the Loss is 0.2607.\n",
      "For Iteration 8471 the Loss is 0.2607.\n",
      "For Iteration 8472 the Loss is 0.2607.\n",
      "For Iteration 8473 the Loss is 0.2607.\n",
      "For Iteration 8474 the Loss is 0.2607.\n",
      "For Iteration 8475 the Loss is 0.2607.\n",
      "For Iteration 8476 the Loss is 0.2607.\n",
      "For Iteration 8477 the Loss is 0.2607.\n",
      "For Iteration 8478 the Loss is 0.2607.\n",
      "For Iteration 8479 the Loss is 0.2607.\n",
      "For Iteration 8480 the Loss is 0.2607.\n",
      "For Iteration 8481 the Loss is 0.2607.\n",
      "For Iteration 8482 the Loss is 0.2607.\n",
      "For Iteration 8483 the Loss is 0.2607.\n",
      "For Iteration 8484 the Loss is 0.2607.\n",
      "For Iteration 8485 the Loss is 0.2607.\n",
      "For Iteration 8486 the Loss is 0.2607.\n",
      "For Iteration 8487 the Loss is 0.2607.\n",
      "For Iteration 8488 the Loss is 0.2607.\n",
      "For Iteration 8489 the Loss is 0.2607.\n",
      "For Iteration 8490 the Loss is 0.2607.\n",
      "For Iteration 8491 the Loss is 0.2607.\n",
      "For Iteration 8492 the Loss is 0.2607.\n",
      "For Iteration 8493 the Loss is 0.2607.\n",
      "For Iteration 8494 the Loss is 0.2607.\n",
      "For Iteration 8495 the Loss is 0.2607.\n",
      "For Iteration 8496 the Loss is 0.2607.\n",
      "For Iteration 8497 the Loss is 0.2607.\n",
      "For Iteration 8498 the Loss is 0.2607.\n",
      "For Iteration 8499 the Loss is 0.2607.\n",
      "For Iteration 8500 the Loss is 0.2607.\n",
      "For Iteration 8501 the Loss is 0.2607.\n",
      "For Iteration 8502 the Loss is 0.2607.\n",
      "For Iteration 8503 the Loss is 0.2607.\n",
      "For Iteration 8504 the Loss is 0.2607.\n",
      "For Iteration 8505 the Loss is 0.2607.\n",
      "For Iteration 8506 the Loss is 0.2607.\n",
      "For Iteration 8507 the Loss is 0.2607.\n",
      "For Iteration 8508 the Loss is 0.2607.\n",
      "For Iteration 8509 the Loss is 0.2607.\n",
      "For Iteration 8510 the Loss is 0.2607.\n",
      "For Iteration 8511 the Loss is 0.2607.\n",
      "For Iteration 8512 the Loss is 0.2607.\n",
      "For Iteration 8513 the Loss is 0.2607.\n",
      "For Iteration 8514 the Loss is 0.2607.\n",
      "For Iteration 8515 the Loss is 0.2607.\n",
      "For Iteration 8516 the Loss is 0.2607.\n",
      "For Iteration 8517 the Loss is 0.2607.\n",
      "For Iteration 8518 the Loss is 0.2607.\n",
      "For Iteration 8519 the Loss is 0.2607.\n",
      "For Iteration 8520 the Loss is 0.2607.\n",
      "For Iteration 8521 the Loss is 0.2607.\n",
      "For Iteration 8522 the Loss is 0.2607.\n",
      "For Iteration 8523 the Loss is 0.2607.\n",
      "For Iteration 8524 the Loss is 0.2607.\n",
      "For Iteration 8525 the Loss is 0.2607.\n",
      "For Iteration 8526 the Loss is 0.2607.\n",
      "For Iteration 8527 the Loss is 0.2607.\n",
      "For Iteration 8528 the Loss is 0.2607.\n",
      "For Iteration 8529 the Loss is 0.2607.\n",
      "For Iteration 8530 the Loss is 0.2607.\n",
      "For Iteration 8531 the Loss is 0.2607.\n",
      "For Iteration 8532 the Loss is 0.2607.\n",
      "For Iteration 8533 the Loss is 0.2607.\n",
      "For Iteration 8534 the Loss is 0.2607.\n",
      "For Iteration 8535 the Loss is 0.2607.\n",
      "For Iteration 8536 the Loss is 0.2607.\n",
      "For Iteration 8537 the Loss is 0.2607.\n",
      "For Iteration 8538 the Loss is 0.2607.\n",
      "For Iteration 8539 the Loss is 0.2607.\n",
      "For Iteration 8540 the Loss is 0.2607.\n",
      "For Iteration 8541 the Loss is 0.2607.\n",
      "For Iteration 8542 the Loss is 0.2607.\n",
      "For Iteration 8543 the Loss is 0.2607.\n",
      "For Iteration 8544 the Loss is 0.2607.\n",
      "For Iteration 8545 the Loss is 0.2607.\n",
      "For Iteration 8546 the Loss is 0.2607.\n",
      "For Iteration 8547 the Loss is 0.2607.\n",
      "For Iteration 8548 the Loss is 0.2607.\n",
      "For Iteration 8549 the Loss is 0.2607.\n",
      "For Iteration 8550 the Loss is 0.2607.\n",
      "For Iteration 8551 the Loss is 0.2607.\n",
      "For Iteration 8552 the Loss is 0.2607.\n",
      "For Iteration 8553 the Loss is 0.2607.\n",
      "For Iteration 8554 the Loss is 0.2607.\n",
      "For Iteration 8555 the Loss is 0.2606.\n",
      "For Iteration 8556 the Loss is 0.2606.\n",
      "For Iteration 8557 the Loss is 0.2606.\n",
      "For Iteration 8558 the Loss is 0.2606.\n",
      "For Iteration 8559 the Loss is 0.2606.\n",
      "For Iteration 8560 the Loss is 0.2606.\n",
      "For Iteration 8561 the Loss is 0.2606.\n",
      "For Iteration 8562 the Loss is 0.2606.\n",
      "For Iteration 8563 the Loss is 0.2606.\n",
      "For Iteration 8564 the Loss is 0.2606.\n",
      "For Iteration 8565 the Loss is 0.2606.\n",
      "For Iteration 8566 the Loss is 0.2606.\n",
      "For Iteration 8567 the Loss is 0.2606.\n",
      "For Iteration 8568 the Loss is 0.2606.\n",
      "For Iteration 8569 the Loss is 0.2606.\n",
      "For Iteration 8570 the Loss is 0.2606.\n",
      "For Iteration 8571 the Loss is 0.2606.\n",
      "For Iteration 8572 the Loss is 0.2606.\n",
      "For Iteration 8573 the Loss is 0.2606.\n",
      "For Iteration 8574 the Loss is 0.2606.\n",
      "For Iteration 8575 the Loss is 0.2606.\n",
      "For Iteration 8576 the Loss is 0.2606.\n",
      "For Iteration 8577 the Loss is 0.2606.\n",
      "For Iteration 8578 the Loss is 0.2606.\n",
      "For Iteration 8579 the Loss is 0.2606.\n",
      "For Iteration 8580 the Loss is 0.2606.\n",
      "For Iteration 8581 the Loss is 0.2606.\n",
      "For Iteration 8582 the Loss is 0.2606.\n",
      "For Iteration 8583 the Loss is 0.2606.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 8584 the Loss is 0.2606.\n",
      "For Iteration 8585 the Loss is 0.2606.\n",
      "For Iteration 8586 the Loss is 0.2606.\n",
      "For Iteration 8587 the Loss is 0.2606.\n",
      "For Iteration 8588 the Loss is 0.2606.\n",
      "For Iteration 8589 the Loss is 0.2606.\n",
      "For Iteration 8590 the Loss is 0.2606.\n",
      "For Iteration 8591 the Loss is 0.2606.\n",
      "For Iteration 8592 the Loss is 0.2606.\n",
      "For Iteration 8593 the Loss is 0.2606.\n",
      "For Iteration 8594 the Loss is 0.2606.\n",
      "For Iteration 8595 the Loss is 0.2606.\n",
      "For Iteration 8596 the Loss is 0.2606.\n",
      "For Iteration 8597 the Loss is 0.2606.\n",
      "For Iteration 8598 the Loss is 0.2606.\n",
      "For Iteration 8599 the Loss is 0.2606.\n",
      "For Iteration 8600 the Loss is 0.2606.\n",
      "For Iteration 8601 the Loss is 0.2606.\n",
      "For Iteration 8602 the Loss is 0.2606.\n",
      "For Iteration 8603 the Loss is 0.2606.\n",
      "For Iteration 8604 the Loss is 0.2606.\n",
      "For Iteration 8605 the Loss is 0.2606.\n",
      "For Iteration 8606 the Loss is 0.2606.\n",
      "For Iteration 8607 the Loss is 0.2606.\n",
      "For Iteration 8608 the Loss is 0.2606.\n",
      "For Iteration 8609 the Loss is 0.2606.\n",
      "For Iteration 8610 the Loss is 0.2606.\n",
      "For Iteration 8611 the Loss is 0.2606.\n",
      "For Iteration 8612 the Loss is 0.2606.\n",
      "For Iteration 8613 the Loss is 0.2606.\n",
      "For Iteration 8614 the Loss is 0.2606.\n",
      "For Iteration 8615 the Loss is 0.2606.\n",
      "For Iteration 8616 the Loss is 0.2606.\n",
      "For Iteration 8617 the Loss is 0.2606.\n",
      "For Iteration 8618 the Loss is 0.2606.\n",
      "For Iteration 8619 the Loss is 0.2606.\n",
      "For Iteration 8620 the Loss is 0.2606.\n",
      "For Iteration 8621 the Loss is 0.2606.\n",
      "For Iteration 8622 the Loss is 0.2606.\n",
      "For Iteration 8623 the Loss is 0.2606.\n",
      "For Iteration 8624 the Loss is 0.2606.\n",
      "For Iteration 8625 the Loss is 0.2606.\n",
      "For Iteration 8626 the Loss is 0.2606.\n",
      "For Iteration 8627 the Loss is 0.2606.\n",
      "For Iteration 8628 the Loss is 0.2606.\n",
      "For Iteration 8629 the Loss is 0.2606.\n",
      "For Iteration 8630 the Loss is 0.2606.\n",
      "For Iteration 8631 the Loss is 0.2606.\n",
      "For Iteration 8632 the Loss is 0.2606.\n",
      "For Iteration 8633 the Loss is 0.2606.\n",
      "For Iteration 8634 the Loss is 0.2606.\n",
      "For Iteration 8635 the Loss is 0.2606.\n",
      "For Iteration 8636 the Loss is 0.2606.\n",
      "For Iteration 8637 the Loss is 0.2606.\n",
      "For Iteration 8638 the Loss is 0.2606.\n",
      "For Iteration 8639 the Loss is 0.2606.\n",
      "For Iteration 8640 the Loss is 0.2606.\n",
      "For Iteration 8641 the Loss is 0.2606.\n",
      "For Iteration 8642 the Loss is 0.2606.\n",
      "For Iteration 8643 the Loss is 0.2606.\n",
      "For Iteration 8644 the Loss is 0.2606.\n",
      "For Iteration 8645 the Loss is 0.2606.\n",
      "For Iteration 8646 the Loss is 0.2606.\n",
      "For Iteration 8647 the Loss is 0.2606.\n",
      "For Iteration 8648 the Loss is 0.2606.\n",
      "For Iteration 8649 the Loss is 0.2606.\n",
      "For Iteration 8650 the Loss is 0.2606.\n",
      "For Iteration 8651 the Loss is 0.2606.\n",
      "For Iteration 8652 the Loss is 0.2606.\n",
      "For Iteration 8653 the Loss is 0.2606.\n",
      "For Iteration 8654 the Loss is 0.2606.\n",
      "For Iteration 8655 the Loss is 0.2606.\n",
      "For Iteration 8656 the Loss is 0.2606.\n",
      "For Iteration 8657 the Loss is 0.2606.\n",
      "For Iteration 8658 the Loss is 0.2606.\n",
      "For Iteration 8659 the Loss is 0.2606.\n",
      "For Iteration 8660 the Loss is 0.2606.\n",
      "For Iteration 8661 the Loss is 0.2606.\n",
      "For Iteration 8662 the Loss is 0.2606.\n",
      "For Iteration 8663 the Loss is 0.2606.\n",
      "For Iteration 8664 the Loss is 0.2606.\n",
      "For Iteration 8665 the Loss is 0.2606.\n",
      "For Iteration 8666 the Loss is 0.2606.\n",
      "For Iteration 8667 the Loss is 0.2606.\n",
      "For Iteration 8668 the Loss is 0.2606.\n",
      "For Iteration 8669 the Loss is 0.2606.\n",
      "For Iteration 8670 the Loss is 0.2606.\n",
      "For Iteration 8671 the Loss is 0.2606.\n",
      "For Iteration 8672 the Loss is 0.2606.\n",
      "For Iteration 8673 the Loss is 0.2606.\n",
      "For Iteration 8674 the Loss is 0.2606.\n",
      "For Iteration 8675 the Loss is 0.2606.\n",
      "For Iteration 8676 the Loss is 0.2606.\n",
      "For Iteration 8677 the Loss is 0.2606.\n",
      "For Iteration 8678 the Loss is 0.2606.\n",
      "For Iteration 8679 the Loss is 0.2606.\n",
      "For Iteration 8680 the Loss is 0.2606.\n",
      "For Iteration 8681 the Loss is 0.2606.\n",
      "For Iteration 8682 the Loss is 0.2606.\n",
      "For Iteration 8683 the Loss is 0.2606.\n",
      "For Iteration 8684 the Loss is 0.2606.\n",
      "For Iteration 8685 the Loss is 0.2606.\n",
      "For Iteration 8686 the Loss is 0.2606.\n",
      "For Iteration 8687 the Loss is 0.2606.\n",
      "For Iteration 8688 the Loss is 0.2606.\n",
      "For Iteration 8689 the Loss is 0.2606.\n",
      "For Iteration 8690 the Loss is 0.2606.\n",
      "For Iteration 8691 the Loss is 0.2606.\n",
      "For Iteration 8692 the Loss is 0.2606.\n",
      "For Iteration 8693 the Loss is 0.2606.\n",
      "For Iteration 8694 the Loss is 0.2606.\n",
      "For Iteration 8695 the Loss is 0.2606.\n",
      "For Iteration 8696 the Loss is 0.2606.\n",
      "For Iteration 8697 the Loss is 0.2606.\n",
      "For Iteration 8698 the Loss is 0.2606.\n",
      "For Iteration 8699 the Loss is 0.2606.\n",
      "For Iteration 8700 the Loss is 0.2606.\n",
      "For Iteration 8701 the Loss is 0.2606.\n",
      "For Iteration 8702 the Loss is 0.2606.\n",
      "For Iteration 8703 the Loss is 0.2606.\n",
      "For Iteration 8704 the Loss is 0.2606.\n",
      "For Iteration 8705 the Loss is 0.2606.\n",
      "For Iteration 8706 the Loss is 0.2606.\n",
      "For Iteration 8707 the Loss is 0.2606.\n",
      "For Iteration 8708 the Loss is 0.2606.\n",
      "For Iteration 8709 the Loss is 0.2606.\n",
      "For Iteration 8710 the Loss is 0.2606.\n",
      "For Iteration 8711 the Loss is 0.2606.\n",
      "For Iteration 8712 the Loss is 0.2606.\n",
      "For Iteration 8713 the Loss is 0.2606.\n",
      "For Iteration 8714 the Loss is 0.2606.\n",
      "For Iteration 8715 the Loss is 0.2606.\n",
      "For Iteration 8716 the Loss is 0.2606.\n",
      "For Iteration 8717 the Loss is 0.2606.\n",
      "For Iteration 8718 the Loss is 0.2606.\n",
      "For Iteration 8719 the Loss is 0.2606.\n",
      "For Iteration 8720 the Loss is 0.2606.\n",
      "For Iteration 8721 the Loss is 0.2606.\n",
      "For Iteration 8722 the Loss is 0.2606.\n",
      "For Iteration 8723 the Loss is 0.2606.\n",
      "For Iteration 8724 the Loss is 0.2606.\n",
      "For Iteration 8725 the Loss is 0.2606.\n",
      "For Iteration 8726 the Loss is 0.2606.\n",
      "For Iteration 8727 the Loss is 0.2606.\n",
      "For Iteration 8728 the Loss is 0.2606.\n",
      "For Iteration 8729 the Loss is 0.2606.\n",
      "For Iteration 8730 the Loss is 0.2606.\n",
      "For Iteration 8731 the Loss is 0.2606.\n",
      "For Iteration 8732 the Loss is 0.2606.\n",
      "For Iteration 8733 the Loss is 0.2606.\n",
      "For Iteration 8734 the Loss is 0.2606.\n",
      "For Iteration 8735 the Loss is 0.2606.\n",
      "For Iteration 8736 the Loss is 0.2606.\n",
      "For Iteration 8737 the Loss is 0.2606.\n",
      "For Iteration 8738 the Loss is 0.2606.\n",
      "For Iteration 8739 the Loss is 0.2606.\n",
      "For Iteration 8740 the Loss is 0.2606.\n",
      "For Iteration 8741 the Loss is 0.2606.\n",
      "For Iteration 8742 the Loss is 0.2606.\n",
      "For Iteration 8743 the Loss is 0.2606.\n",
      "For Iteration 8744 the Loss is 0.2606.\n",
      "For Iteration 8745 the Loss is 0.2606.\n",
      "For Iteration 8746 the Loss is 0.2606.\n",
      "For Iteration 8747 the Loss is 0.2606.\n",
      "For Iteration 8748 the Loss is 0.2606.\n",
      "For Iteration 8749 the Loss is 0.2606.\n",
      "For Iteration 8750 the Loss is 0.2606.\n",
      "For Iteration 8751 the Loss is 0.2606.\n",
      "For Iteration 8752 the Loss is 0.2606.\n",
      "For Iteration 8753 the Loss is 0.2606.\n",
      "For Iteration 8754 the Loss is 0.2606.\n",
      "For Iteration 8755 the Loss is 0.2606.\n",
      "For Iteration 8756 the Loss is 0.2606.\n",
      "For Iteration 8757 the Loss is 0.2606.\n",
      "For Iteration 8758 the Loss is 0.2606.\n",
      "For Iteration 8759 the Loss is 0.2606.\n",
      "For Iteration 8760 the Loss is 0.2606.\n",
      "For Iteration 8761 the Loss is 0.2606.\n",
      "For Iteration 8762 the Loss is 0.2606.\n",
      "For Iteration 8763 the Loss is 0.2606.\n",
      "For Iteration 8764 the Loss is 0.2606.\n",
      "For Iteration 8765 the Loss is 0.2606.\n",
      "For Iteration 8766 the Loss is 0.2606.\n",
      "For Iteration 8767 the Loss is 0.2606.\n",
      "For Iteration 8768 the Loss is 0.2606.\n",
      "For Iteration 8769 the Loss is 0.2606.\n",
      "For Iteration 8770 the Loss is 0.2606.\n",
      "For Iteration 8771 the Loss is 0.2606.\n",
      "For Iteration 8772 the Loss is 0.2606.\n",
      "For Iteration 8773 the Loss is 0.2606.\n",
      "For Iteration 8774 the Loss is 0.2606.\n",
      "For Iteration 8775 the Loss is 0.2606.\n",
      "For Iteration 8776 the Loss is 0.2606.\n",
      "For Iteration 8777 the Loss is 0.2606.\n",
      "For Iteration 8778 the Loss is 0.2606.\n",
      "For Iteration 8779 the Loss is 0.2606.\n",
      "For Iteration 8780 the Loss is 0.2606.\n",
      "For Iteration 8781 the Loss is 0.2606.\n",
      "For Iteration 8782 the Loss is 0.2606.\n",
      "For Iteration 8783 the Loss is 0.2606.\n",
      "For Iteration 8784 the Loss is 0.2606.\n",
      "For Iteration 8785 the Loss is 0.2606.\n",
      "For Iteration 8786 the Loss is 0.2606.\n",
      "For Iteration 8787 the Loss is 0.2606.\n",
      "For Iteration 8788 the Loss is 0.2606.\n",
      "For Iteration 8789 the Loss is 0.2606.\n",
      "For Iteration 8790 the Loss is 0.2606.\n",
      "For Iteration 8791 the Loss is 0.2606.\n",
      "For Iteration 8792 the Loss is 0.2606.\n",
      "For Iteration 8793 the Loss is 0.2606.\n",
      "For Iteration 8794 the Loss is 0.2606.\n",
      "For Iteration 8795 the Loss is 0.2606.\n",
      "For Iteration 8796 the Loss is 0.2606.\n",
      "For Iteration 8797 the Loss is 0.2606.\n",
      "For Iteration 8798 the Loss is 0.2606.\n",
      "For Iteration 8799 the Loss is 0.2606.\n",
      "For Iteration 8800 the Loss is 0.2606.\n",
      "For Iteration 8801 the Loss is 0.2606.\n",
      "For Iteration 8802 the Loss is 0.2606.\n",
      "For Iteration 8803 the Loss is 0.2606.\n",
      "For Iteration 8804 the Loss is 0.2606.\n",
      "For Iteration 8805 the Loss is 0.2606.\n",
      "For Iteration 8806 the Loss is 0.2606.\n",
      "For Iteration 8807 the Loss is 0.2606.\n",
      "For Iteration 8808 the Loss is 0.2606.\n",
      "For Iteration 8809 the Loss is 0.2606.\n",
      "For Iteration 8810 the Loss is 0.2606.\n",
      "For Iteration 8811 the Loss is 0.2606.\n",
      "For Iteration 8812 the Loss is 0.2606.\n",
      "For Iteration 8813 the Loss is 0.2606.\n",
      "For Iteration 8814 the Loss is 0.2606.\n",
      "For Iteration 8815 the Loss is 0.2606.\n",
      "For Iteration 8816 the Loss is 0.2606.\n",
      "For Iteration 8817 the Loss is 0.2606.\n",
      "For Iteration 8818 the Loss is 0.2606.\n",
      "For Iteration 8819 the Loss is 0.2606.\n",
      "For Iteration 8820 the Loss is 0.2606.\n",
      "For Iteration 8821 the Loss is 0.2606.\n",
      "For Iteration 8822 the Loss is 0.2606.\n",
      "For Iteration 8823 the Loss is 0.2606.\n",
      "For Iteration 8824 the Loss is 0.2606.\n",
      "For Iteration 8825 the Loss is 0.2606.\n",
      "For Iteration 8826 the Loss is 0.2606.\n",
      "For Iteration 8827 the Loss is 0.2606.\n",
      "For Iteration 8828 the Loss is 0.2606.\n",
      "For Iteration 8829 the Loss is 0.2606.\n",
      "For Iteration 8830 the Loss is 0.2606.\n",
      "For Iteration 8831 the Loss is 0.2606.\n",
      "For Iteration 8832 the Loss is 0.2606.\n",
      "For Iteration 8833 the Loss is 0.2606.\n",
      "For Iteration 8834 the Loss is 0.2606.\n",
      "For Iteration 8835 the Loss is 0.2606.\n",
      "For Iteration 8836 the Loss is 0.2606.\n",
      "For Iteration 8837 the Loss is 0.2606.\n",
      "For Iteration 8838 the Loss is 0.2606.\n",
      "For Iteration 8839 the Loss is 0.2606.\n",
      "For Iteration 8840 the Loss is 0.2606.\n",
      "For Iteration 8841 the Loss is 0.2606.\n",
      "For Iteration 8842 the Loss is 0.2606.\n",
      "For Iteration 8843 the Loss is 0.2606.\n",
      "For Iteration 8844 the Loss is 0.2606.\n",
      "For Iteration 8845 the Loss is 0.2606.\n",
      "For Iteration 8846 the Loss is 0.2606.\n",
      "For Iteration 8847 the Loss is 0.2606.\n",
      "For Iteration 8848 the Loss is 0.2606.\n",
      "For Iteration 8849 the Loss is 0.2606.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 8850 the Loss is 0.2606.\n",
      "For Iteration 8851 the Loss is 0.2606.\n",
      "For Iteration 8852 the Loss is 0.2606.\n",
      "For Iteration 8853 the Loss is 0.2606.\n",
      "For Iteration 8854 the Loss is 0.2606.\n",
      "For Iteration 8855 the Loss is 0.2606.\n",
      "For Iteration 8856 the Loss is 0.2606.\n",
      "For Iteration 8857 the Loss is 0.2606.\n",
      "For Iteration 8858 the Loss is 0.2606.\n",
      "For Iteration 8859 the Loss is 0.2606.\n",
      "For Iteration 8860 the Loss is 0.2606.\n",
      "For Iteration 8861 the Loss is 0.2606.\n",
      "For Iteration 8862 the Loss is 0.2606.\n",
      "For Iteration 8863 the Loss is 0.2606.\n",
      "For Iteration 8864 the Loss is 0.2606.\n",
      "For Iteration 8865 the Loss is 0.2606.\n",
      "For Iteration 8866 the Loss is 0.2606.\n",
      "For Iteration 8867 the Loss is 0.2606.\n",
      "For Iteration 8868 the Loss is 0.2606.\n",
      "For Iteration 8869 the Loss is 0.2606.\n",
      "For Iteration 8870 the Loss is 0.2606.\n",
      "For Iteration 8871 the Loss is 0.2606.\n",
      "For Iteration 8872 the Loss is 0.2606.\n",
      "For Iteration 8873 the Loss is 0.2606.\n",
      "For Iteration 8874 the Loss is 0.2606.\n",
      "For Iteration 8875 the Loss is 0.2606.\n",
      "For Iteration 8876 the Loss is 0.2606.\n",
      "For Iteration 8877 the Loss is 0.2606.\n",
      "For Iteration 8878 the Loss is 0.2606.\n",
      "For Iteration 8879 the Loss is 0.2606.\n",
      "For Iteration 8880 the Loss is 0.2606.\n",
      "For Iteration 8881 the Loss is 0.2606.\n",
      "For Iteration 8882 the Loss is 0.2606.\n",
      "For Iteration 8883 the Loss is 0.2606.\n",
      "For Iteration 8884 the Loss is 0.2606.\n",
      "For Iteration 8885 the Loss is 0.2606.\n",
      "For Iteration 8886 the Loss is 0.2606.\n",
      "For Iteration 8887 the Loss is 0.2606.\n",
      "For Iteration 8888 the Loss is 0.2606.\n",
      "For Iteration 8889 the Loss is 0.2606.\n",
      "For Iteration 8890 the Loss is 0.2606.\n",
      "For Iteration 8891 the Loss is 0.2606.\n",
      "For Iteration 8892 the Loss is 0.2606.\n",
      "For Iteration 8893 the Loss is 0.2606.\n",
      "For Iteration 8894 the Loss is 0.2606.\n",
      "For Iteration 8895 the Loss is 0.2606.\n",
      "For Iteration 8896 the Loss is 0.2606.\n",
      "For Iteration 8897 the Loss is 0.2606.\n",
      "For Iteration 8898 the Loss is 0.2606.\n",
      "For Iteration 8899 the Loss is 0.2606.\n",
      "For Iteration 8900 the Loss is 0.2606.\n",
      "For Iteration 8901 the Loss is 0.2606.\n",
      "For Iteration 8902 the Loss is 0.2606.\n",
      "For Iteration 8903 the Loss is 0.2606.\n",
      "For Iteration 8904 the Loss is 0.2606.\n",
      "For Iteration 8905 the Loss is 0.2606.\n",
      "For Iteration 8906 the Loss is 0.2606.\n",
      "For Iteration 8907 the Loss is 0.2606.\n",
      "For Iteration 8908 the Loss is 0.2606.\n",
      "For Iteration 8909 the Loss is 0.2606.\n",
      "For Iteration 8910 the Loss is 0.2606.\n",
      "For Iteration 8911 the Loss is 0.2606.\n",
      "For Iteration 8912 the Loss is 0.2606.\n",
      "For Iteration 8913 the Loss is 0.2606.\n",
      "For Iteration 8914 the Loss is 0.2606.\n",
      "For Iteration 8915 the Loss is 0.2606.\n",
      "For Iteration 8916 the Loss is 0.2606.\n",
      "For Iteration 8917 the Loss is 0.2606.\n",
      "For Iteration 8918 the Loss is 0.2606.\n",
      "For Iteration 8919 the Loss is 0.2606.\n",
      "For Iteration 8920 the Loss is 0.2606.\n",
      "For Iteration 8921 the Loss is 0.2606.\n",
      "For Iteration 8922 the Loss is 0.2606.\n",
      "For Iteration 8923 the Loss is 0.2606.\n",
      "For Iteration 8924 the Loss is 0.2606.\n",
      "For Iteration 8925 the Loss is 0.2606.\n",
      "For Iteration 8926 the Loss is 0.2606.\n",
      "For Iteration 8927 the Loss is 0.2606.\n",
      "For Iteration 8928 the Loss is 0.2606.\n",
      "For Iteration 8929 the Loss is 0.2606.\n",
      "For Iteration 8930 the Loss is 0.2606.\n",
      "For Iteration 8931 the Loss is 0.2606.\n",
      "For Iteration 8932 the Loss is 0.2606.\n",
      "For Iteration 8933 the Loss is 0.2606.\n",
      "For Iteration 8934 the Loss is 0.2606.\n",
      "For Iteration 8935 the Loss is 0.2606.\n",
      "For Iteration 8936 the Loss is 0.2606.\n",
      "For Iteration 8937 the Loss is 0.2606.\n",
      "For Iteration 8938 the Loss is 0.2606.\n",
      "For Iteration 8939 the Loss is 0.2606.\n",
      "For Iteration 8940 the Loss is 0.2606.\n",
      "For Iteration 8941 the Loss is 0.2606.\n",
      "For Iteration 8942 the Loss is 0.2606.\n",
      "For Iteration 8943 the Loss is 0.2606.\n",
      "For Iteration 8944 the Loss is 0.2606.\n",
      "For Iteration 8945 the Loss is 0.2606.\n",
      "For Iteration 8946 the Loss is 0.2606.\n",
      "For Iteration 8947 the Loss is 0.2606.\n",
      "For Iteration 8948 the Loss is 0.2606.\n",
      "For Iteration 8949 the Loss is 0.2606.\n",
      "For Iteration 8950 the Loss is 0.2606.\n",
      "For Iteration 8951 the Loss is 0.2606.\n",
      "For Iteration 8952 the Loss is 0.2606.\n",
      "For Iteration 8953 the Loss is 0.2606.\n",
      "For Iteration 8954 the Loss is 0.2606.\n",
      "For Iteration 8955 the Loss is 0.2606.\n",
      "For Iteration 8956 the Loss is 0.2606.\n",
      "For Iteration 8957 the Loss is 0.2606.\n",
      "For Iteration 8958 the Loss is 0.2606.\n",
      "For Iteration 8959 the Loss is 0.2606.\n",
      "For Iteration 8960 the Loss is 0.2606.\n",
      "For Iteration 8961 the Loss is 0.2606.\n",
      "For Iteration 8962 the Loss is 0.2606.\n",
      "For Iteration 8963 the Loss is 0.2606.\n",
      "For Iteration 8964 the Loss is 0.2606.\n",
      "For Iteration 8965 the Loss is 0.2606.\n",
      "For Iteration 8966 the Loss is 0.2606.\n",
      "For Iteration 8967 the Loss is 0.2606.\n",
      "For Iteration 8968 the Loss is 0.2606.\n",
      "For Iteration 8969 the Loss is 0.2606.\n",
      "For Iteration 8970 the Loss is 0.2606.\n",
      "For Iteration 8971 the Loss is 0.2606.\n",
      "For Iteration 8972 the Loss is 0.2606.\n",
      "For Iteration 8973 the Loss is 0.2606.\n",
      "For Iteration 8974 the Loss is 0.2606.\n",
      "For Iteration 8975 the Loss is 0.2606.\n",
      "For Iteration 8976 the Loss is 0.2606.\n",
      "For Iteration 8977 the Loss is 0.2606.\n",
      "For Iteration 8978 the Loss is 0.2606.\n",
      "For Iteration 8979 the Loss is 0.2606.\n",
      "For Iteration 8980 the Loss is 0.2606.\n",
      "For Iteration 8981 the Loss is 0.2606.\n",
      "For Iteration 8982 the Loss is 0.2606.\n",
      "For Iteration 8983 the Loss is 0.2606.\n",
      "For Iteration 8984 the Loss is 0.2606.\n",
      "For Iteration 8985 the Loss is 0.2606.\n",
      "For Iteration 8986 the Loss is 0.2606.\n",
      "For Iteration 8987 the Loss is 0.2606.\n",
      "For Iteration 8988 the Loss is 0.2606.\n",
      "For Iteration 8989 the Loss is 0.2606.\n",
      "For Iteration 8990 the Loss is 0.2606.\n",
      "For Iteration 8991 the Loss is 0.2606.\n",
      "For Iteration 8992 the Loss is 0.2606.\n",
      "For Iteration 8993 the Loss is 0.2606.\n",
      "For Iteration 8994 the Loss is 0.2606.\n",
      "For Iteration 8995 the Loss is 0.2606.\n",
      "For Iteration 8996 the Loss is 0.2606.\n",
      "For Iteration 8997 the Loss is 0.2606.\n",
      "For Iteration 8998 the Loss is 0.2606.\n",
      "For Iteration 8999 the Loss is 0.2606.\n",
      "For Iteration 9000 the Loss is 0.2606.\n",
      "For Iteration 9001 the Loss is 0.2606.\n",
      "For Iteration 9002 the Loss is 0.2606.\n",
      "For Iteration 9003 the Loss is 0.2606.\n",
      "For Iteration 9004 the Loss is 0.2606.\n",
      "For Iteration 9005 the Loss is 0.2606.\n",
      "For Iteration 9006 the Loss is 0.2606.\n",
      "For Iteration 9007 the Loss is 0.2606.\n",
      "For Iteration 9008 the Loss is 0.2606.\n",
      "For Iteration 9009 the Loss is 0.2606.\n",
      "For Iteration 9010 the Loss is 0.2606.\n",
      "For Iteration 9011 the Loss is 0.2606.\n",
      "For Iteration 9012 the Loss is 0.2606.\n",
      "For Iteration 9013 the Loss is 0.2606.\n",
      "For Iteration 9014 the Loss is 0.2606.\n",
      "For Iteration 9015 the Loss is 0.2606.\n",
      "For Iteration 9016 the Loss is 0.2606.\n",
      "For Iteration 9017 the Loss is 0.2606.\n",
      "For Iteration 9018 the Loss is 0.2606.\n",
      "For Iteration 9019 the Loss is 0.2606.\n",
      "For Iteration 9020 the Loss is 0.2606.\n",
      "For Iteration 9021 the Loss is 0.2606.\n",
      "For Iteration 9022 the Loss is 0.2606.\n",
      "For Iteration 9023 the Loss is 0.2606.\n",
      "For Iteration 9024 the Loss is 0.2606.\n",
      "For Iteration 9025 the Loss is 0.2606.\n",
      "For Iteration 9026 the Loss is 0.2606.\n",
      "For Iteration 9027 the Loss is 0.2606.\n",
      "For Iteration 9028 the Loss is 0.2606.\n",
      "For Iteration 9029 the Loss is 0.2606.\n",
      "For Iteration 9030 the Loss is 0.2606.\n",
      "For Iteration 9031 the Loss is 0.2606.\n",
      "For Iteration 9032 the Loss is 0.2606.\n",
      "For Iteration 9033 the Loss is 0.2606.\n",
      "For Iteration 9034 the Loss is 0.2606.\n",
      "For Iteration 9035 the Loss is 0.2606.\n",
      "For Iteration 9036 the Loss is 0.2606.\n",
      "For Iteration 9037 the Loss is 0.2606.\n",
      "For Iteration 9038 the Loss is 0.2606.\n",
      "For Iteration 9039 the Loss is 0.2606.\n",
      "For Iteration 9040 the Loss is 0.2606.\n",
      "For Iteration 9041 the Loss is 0.2606.\n",
      "For Iteration 9042 the Loss is 0.2606.\n",
      "For Iteration 9043 the Loss is 0.2606.\n",
      "For Iteration 9044 the Loss is 0.2606.\n",
      "For Iteration 9045 the Loss is 0.2606.\n",
      "For Iteration 9046 the Loss is 0.2606.\n",
      "For Iteration 9047 the Loss is 0.2606.\n",
      "For Iteration 9048 the Loss is 0.2606.\n",
      "For Iteration 9049 the Loss is 0.2606.\n",
      "For Iteration 9050 the Loss is 0.2606.\n",
      "For Iteration 9051 the Loss is 0.2606.\n",
      "For Iteration 9052 the Loss is 0.2606.\n",
      "For Iteration 9053 the Loss is 0.2606.\n",
      "For Iteration 9054 the Loss is 0.2606.\n",
      "For Iteration 9055 the Loss is 0.2606.\n",
      "For Iteration 9056 the Loss is 0.2606.\n",
      "For Iteration 9057 the Loss is 0.2606.\n",
      "For Iteration 9058 the Loss is 0.2606.\n",
      "For Iteration 9059 the Loss is 0.2606.\n",
      "For Iteration 9060 the Loss is 0.2606.\n",
      "For Iteration 9061 the Loss is 0.2606.\n",
      "For Iteration 9062 the Loss is 0.2606.\n",
      "For Iteration 9063 the Loss is 0.2606.\n",
      "For Iteration 9064 the Loss is 0.2606.\n",
      "For Iteration 9065 the Loss is 0.2606.\n",
      "For Iteration 9066 the Loss is 0.2606.\n",
      "For Iteration 9067 the Loss is 0.2606.\n",
      "For Iteration 9068 the Loss is 0.2606.\n",
      "For Iteration 9069 the Loss is 0.2606.\n",
      "For Iteration 9070 the Loss is 0.2606.\n",
      "For Iteration 9071 the Loss is 0.2606.\n",
      "For Iteration 9072 the Loss is 0.2606.\n",
      "For Iteration 9073 the Loss is 0.2606.\n",
      "For Iteration 9074 the Loss is 0.2606.\n",
      "For Iteration 9075 the Loss is 0.2606.\n",
      "For Iteration 9076 the Loss is 0.2606.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 9077 the Loss is 0.2606.\n",
      "For Iteration 9078 the Loss is 0.2606.\n",
      "For Iteration 9079 the Loss is 0.2606.\n",
      "For Iteration 9080 the Loss is 0.2606.\n",
      "For Iteration 9081 the Loss is 0.2606.\n",
      "For Iteration 9082 the Loss is 0.2606.\n",
      "For Iteration 9083 the Loss is 0.2606.\n",
      "For Iteration 9084 the Loss is 0.2606.\n",
      "For Iteration 9085 the Loss is 0.2606.\n",
      "For Iteration 9086 the Loss is 0.2606.\n",
      "For Iteration 9087 the Loss is 0.2606.\n",
      "For Iteration 9088 the Loss is 0.2606.\n",
      "For Iteration 9089 the Loss is 0.2606.\n",
      "For Iteration 9090 the Loss is 0.2606.\n",
      "For Iteration 9091 the Loss is 0.2606.\n",
      "For Iteration 9092 the Loss is 0.2606.\n",
      "For Iteration 9093 the Loss is 0.2606.\n",
      "For Iteration 9094 the Loss is 0.2606.\n",
      "For Iteration 9095 the Loss is 0.2606.\n",
      "For Iteration 9096 the Loss is 0.2606.\n",
      "For Iteration 9097 the Loss is 0.2606.\n",
      "For Iteration 9098 the Loss is 0.2606.\n",
      "For Iteration 9099 the Loss is 0.2606.\n",
      "For Iteration 9100 the Loss is 0.2606.\n",
      "For Iteration 9101 the Loss is 0.2606.\n",
      "For Iteration 9102 the Loss is 0.2606.\n",
      "For Iteration 9103 the Loss is 0.2606.\n",
      "For Iteration 9104 the Loss is 0.2606.\n",
      "For Iteration 9105 the Loss is 0.2606.\n",
      "For Iteration 9106 the Loss is 0.2606.\n",
      "For Iteration 9107 the Loss is 0.2606.\n",
      "For Iteration 9108 the Loss is 0.2606.\n",
      "For Iteration 9109 the Loss is 0.2606.\n",
      "For Iteration 9110 the Loss is 0.2606.\n",
      "For Iteration 9111 the Loss is 0.2606.\n",
      "For Iteration 9112 the Loss is 0.2606.\n",
      "For Iteration 9113 the Loss is 0.2606.\n",
      "For Iteration 9114 the Loss is 0.2606.\n",
      "For Iteration 9115 the Loss is 0.2606.\n",
      "For Iteration 9116 the Loss is 0.2606.\n",
      "For Iteration 9117 the Loss is 0.2606.\n",
      "For Iteration 9118 the Loss is 0.2606.\n",
      "For Iteration 9119 the Loss is 0.2606.\n",
      "For Iteration 9120 the Loss is 0.2606.\n",
      "For Iteration 9121 the Loss is 0.2606.\n",
      "For Iteration 9122 the Loss is 0.2606.\n",
      "For Iteration 9123 the Loss is 0.2606.\n",
      "For Iteration 9124 the Loss is 0.2606.\n",
      "For Iteration 9125 the Loss is 0.2606.\n",
      "For Iteration 9126 the Loss is 0.2606.\n",
      "For Iteration 9127 the Loss is 0.2606.\n",
      "For Iteration 9128 the Loss is 0.2606.\n",
      "For Iteration 9129 the Loss is 0.2606.\n",
      "For Iteration 9130 the Loss is 0.2606.\n",
      "For Iteration 9131 the Loss is 0.2606.\n",
      "For Iteration 9132 the Loss is 0.2606.\n",
      "For Iteration 9133 the Loss is 0.2606.\n",
      "For Iteration 9134 the Loss is 0.2606.\n",
      "For Iteration 9135 the Loss is 0.2606.\n",
      "For Iteration 9136 the Loss is 0.2606.\n",
      "For Iteration 9137 the Loss is 0.2606.\n",
      "For Iteration 9138 the Loss is 0.2606.\n",
      "For Iteration 9139 the Loss is 0.2606.\n",
      "For Iteration 9140 the Loss is 0.2606.\n",
      "For Iteration 9141 the Loss is 0.2606.\n",
      "For Iteration 9142 the Loss is 0.2606.\n",
      "For Iteration 9143 the Loss is 0.2606.\n",
      "For Iteration 9144 the Loss is 0.2606.\n",
      "For Iteration 9145 the Loss is 0.2606.\n",
      "For Iteration 9146 the Loss is 0.2606.\n",
      "For Iteration 9147 the Loss is 0.2606.\n",
      "For Iteration 9148 the Loss is 0.2606.\n",
      "For Iteration 9149 the Loss is 0.2606.\n",
      "For Iteration 9150 the Loss is 0.2606.\n",
      "For Iteration 9151 the Loss is 0.2606.\n",
      "For Iteration 9152 the Loss is 0.2606.\n",
      "For Iteration 9153 the Loss is 0.2606.\n",
      "For Iteration 9154 the Loss is 0.2606.\n",
      "For Iteration 9155 the Loss is 0.2606.\n",
      "For Iteration 9156 the Loss is 0.2606.\n",
      "For Iteration 9157 the Loss is 0.2606.\n",
      "For Iteration 9158 the Loss is 0.2606.\n",
      "For Iteration 9159 the Loss is 0.2606.\n",
      "For Iteration 9160 the Loss is 0.2606.\n",
      "For Iteration 9161 the Loss is 0.2606.\n",
      "For Iteration 9162 the Loss is 0.2606.\n",
      "For Iteration 9163 the Loss is 0.2606.\n",
      "For Iteration 9164 the Loss is 0.2606.\n",
      "For Iteration 9165 the Loss is 0.2606.\n",
      "For Iteration 9166 the Loss is 0.2606.\n",
      "For Iteration 9167 the Loss is 0.2606.\n",
      "For Iteration 9168 the Loss is 0.2606.\n",
      "For Iteration 9169 the Loss is 0.2606.\n",
      "For Iteration 9170 the Loss is 0.2606.\n",
      "For Iteration 9171 the Loss is 0.2606.\n",
      "For Iteration 9172 the Loss is 0.2606.\n",
      "For Iteration 9173 the Loss is 0.2606.\n",
      "For Iteration 9174 the Loss is 0.2606.\n",
      "For Iteration 9175 the Loss is 0.2605.\n",
      "For Iteration 9176 the Loss is 0.2605.\n",
      "For Iteration 9177 the Loss is 0.2605.\n",
      "For Iteration 9178 the Loss is 0.2605.\n",
      "For Iteration 9179 the Loss is 0.2605.\n",
      "For Iteration 9180 the Loss is 0.2605.\n",
      "For Iteration 9181 the Loss is 0.2605.\n",
      "For Iteration 9182 the Loss is 0.2605.\n",
      "For Iteration 9183 the Loss is 0.2605.\n",
      "For Iteration 9184 the Loss is 0.2605.\n",
      "For Iteration 9185 the Loss is 0.2605.\n",
      "For Iteration 9186 the Loss is 0.2605.\n",
      "For Iteration 9187 the Loss is 0.2605.\n",
      "For Iteration 9188 the Loss is 0.2605.\n",
      "For Iteration 9189 the Loss is 0.2605.\n",
      "For Iteration 9190 the Loss is 0.2605.\n",
      "For Iteration 9191 the Loss is 0.2605.\n",
      "For Iteration 9192 the Loss is 0.2605.\n",
      "For Iteration 9193 the Loss is 0.2605.\n",
      "For Iteration 9194 the Loss is 0.2605.\n",
      "For Iteration 9195 the Loss is 0.2605.\n",
      "For Iteration 9196 the Loss is 0.2605.\n",
      "For Iteration 9197 the Loss is 0.2605.\n",
      "For Iteration 9198 the Loss is 0.2605.\n",
      "For Iteration 9199 the Loss is 0.2605.\n",
      "For Iteration 9200 the Loss is 0.2605.\n",
      "For Iteration 9201 the Loss is 0.2605.\n",
      "For Iteration 9202 the Loss is 0.2605.\n",
      "For Iteration 9203 the Loss is 0.2605.\n",
      "For Iteration 9204 the Loss is 0.2605.\n",
      "For Iteration 9205 the Loss is 0.2605.\n",
      "For Iteration 9206 the Loss is 0.2605.\n",
      "For Iteration 9207 the Loss is 0.2605.\n",
      "For Iteration 9208 the Loss is 0.2605.\n",
      "For Iteration 9209 the Loss is 0.2605.\n",
      "For Iteration 9210 the Loss is 0.2605.\n",
      "For Iteration 9211 the Loss is 0.2605.\n",
      "For Iteration 9212 the Loss is 0.2605.\n",
      "For Iteration 9213 the Loss is 0.2605.\n",
      "For Iteration 9214 the Loss is 0.2605.\n",
      "For Iteration 9215 the Loss is 0.2605.\n",
      "For Iteration 9216 the Loss is 0.2605.\n",
      "For Iteration 9217 the Loss is 0.2605.\n",
      "For Iteration 9218 the Loss is 0.2605.\n",
      "For Iteration 9219 the Loss is 0.2605.\n",
      "For Iteration 9220 the Loss is 0.2605.\n",
      "For Iteration 9221 the Loss is 0.2605.\n",
      "For Iteration 9222 the Loss is 0.2605.\n",
      "For Iteration 9223 the Loss is 0.2605.\n",
      "For Iteration 9224 the Loss is 0.2605.\n",
      "For Iteration 9225 the Loss is 0.2605.\n",
      "For Iteration 9226 the Loss is 0.2605.\n",
      "For Iteration 9227 the Loss is 0.2605.\n",
      "For Iteration 9228 the Loss is 0.2605.\n",
      "For Iteration 9229 the Loss is 0.2605.\n",
      "For Iteration 9230 the Loss is 0.2605.\n",
      "For Iteration 9231 the Loss is 0.2605.\n",
      "For Iteration 9232 the Loss is 0.2605.\n",
      "For Iteration 9233 the Loss is 0.2605.\n",
      "For Iteration 9234 the Loss is 0.2605.\n",
      "For Iteration 9235 the Loss is 0.2605.\n",
      "For Iteration 9236 the Loss is 0.2605.\n",
      "For Iteration 9237 the Loss is 0.2605.\n",
      "For Iteration 9238 the Loss is 0.2605.\n",
      "For Iteration 9239 the Loss is 0.2605.\n",
      "For Iteration 9240 the Loss is 0.2605.\n",
      "For Iteration 9241 the Loss is 0.2605.\n",
      "For Iteration 9242 the Loss is 0.2605.\n",
      "For Iteration 9243 the Loss is 0.2605.\n",
      "For Iteration 9244 the Loss is 0.2605.\n",
      "For Iteration 9245 the Loss is 0.2605.\n",
      "For Iteration 9246 the Loss is 0.2605.\n",
      "For Iteration 9247 the Loss is 0.2605.\n",
      "For Iteration 9248 the Loss is 0.2605.\n",
      "For Iteration 9249 the Loss is 0.2605.\n",
      "For Iteration 9250 the Loss is 0.2605.\n",
      "For Iteration 9251 the Loss is 0.2605.\n",
      "For Iteration 9252 the Loss is 0.2605.\n",
      "For Iteration 9253 the Loss is 0.2605.\n",
      "For Iteration 9254 the Loss is 0.2605.\n",
      "For Iteration 9255 the Loss is 0.2605.\n",
      "For Iteration 9256 the Loss is 0.2605.\n",
      "For Iteration 9257 the Loss is 0.2605.\n",
      "For Iteration 9258 the Loss is 0.2605.\n",
      "For Iteration 9259 the Loss is 0.2605.\n",
      "For Iteration 9260 the Loss is 0.2605.\n",
      "For Iteration 9261 the Loss is 0.2605.\n",
      "For Iteration 9262 the Loss is 0.2605.\n",
      "For Iteration 9263 the Loss is 0.2605.\n",
      "For Iteration 9264 the Loss is 0.2605.\n",
      "For Iteration 9265 the Loss is 0.2605.\n",
      "For Iteration 9266 the Loss is 0.2605.\n",
      "For Iteration 9267 the Loss is 0.2605.\n",
      "For Iteration 9268 the Loss is 0.2605.\n",
      "For Iteration 9269 the Loss is 0.2605.\n",
      "For Iteration 9270 the Loss is 0.2605.\n",
      "For Iteration 9271 the Loss is 0.2605.\n",
      "For Iteration 9272 the Loss is 0.2605.\n",
      "For Iteration 9273 the Loss is 0.2605.\n",
      "For Iteration 9274 the Loss is 0.2605.\n",
      "For Iteration 9275 the Loss is 0.2605.\n",
      "For Iteration 9276 the Loss is 0.2605.\n",
      "For Iteration 9277 the Loss is 0.2605.\n",
      "For Iteration 9278 the Loss is 0.2605.\n",
      "For Iteration 9279 the Loss is 0.2605.\n",
      "For Iteration 9280 the Loss is 0.2605.\n",
      "For Iteration 9281 the Loss is 0.2605.\n",
      "For Iteration 9282 the Loss is 0.2605.\n",
      "For Iteration 9283 the Loss is 0.2605.\n",
      "For Iteration 9284 the Loss is 0.2605.\n",
      "For Iteration 9285 the Loss is 0.2605.\n",
      "For Iteration 9286 the Loss is 0.2605.\n",
      "For Iteration 9287 the Loss is 0.2605.\n",
      "For Iteration 9288 the Loss is 0.2605.\n",
      "For Iteration 9289 the Loss is 0.2605.\n",
      "For Iteration 9290 the Loss is 0.2605.\n",
      "For Iteration 9291 the Loss is 0.2605.\n",
      "For Iteration 9292 the Loss is 0.2605.\n",
      "For Iteration 9293 the Loss is 0.2605.\n",
      "For Iteration 9294 the Loss is 0.2605.\n",
      "For Iteration 9295 the Loss is 0.2605.\n",
      "For Iteration 9296 the Loss is 0.2605.\n",
      "For Iteration 9297 the Loss is 0.2605.\n",
      "For Iteration 9298 the Loss is 0.2605.\n",
      "For Iteration 9299 the Loss is 0.2605.\n",
      "For Iteration 9300 the Loss is 0.2605.\n",
      "For Iteration 9301 the Loss is 0.2605.\n",
      "For Iteration 9302 the Loss is 0.2605.\n",
      "For Iteration 9303 the Loss is 0.2605.\n",
      "For Iteration 9304 the Loss is 0.2605.\n",
      "For Iteration 9305 the Loss is 0.2605.\n",
      "For Iteration 9306 the Loss is 0.2605.\n",
      "For Iteration 9307 the Loss is 0.2605.\n",
      "For Iteration 9308 the Loss is 0.2605.\n",
      "For Iteration 9309 the Loss is 0.2605.\n",
      "For Iteration 9310 the Loss is 0.2605.\n",
      "For Iteration 9311 the Loss is 0.2605.\n",
      "For Iteration 9312 the Loss is 0.2605.\n",
      "For Iteration 9313 the Loss is 0.2605.\n",
      "For Iteration 9314 the Loss is 0.2605.\n",
      "For Iteration 9315 the Loss is 0.2605.\n",
      "For Iteration 9316 the Loss is 0.2605.\n",
      "For Iteration 9317 the Loss is 0.2605.\n",
      "For Iteration 9318 the Loss is 0.2605.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 9319 the Loss is 0.2605.\n",
      "For Iteration 9320 the Loss is 0.2605.\n",
      "For Iteration 9321 the Loss is 0.2605.\n",
      "For Iteration 9322 the Loss is 0.2605.\n",
      "For Iteration 9323 the Loss is 0.2605.\n",
      "For Iteration 9324 the Loss is 0.2605.\n",
      "For Iteration 9325 the Loss is 0.2605.\n",
      "For Iteration 9326 the Loss is 0.2605.\n",
      "For Iteration 9327 the Loss is 0.2605.\n",
      "For Iteration 9328 the Loss is 0.2605.\n",
      "For Iteration 9329 the Loss is 0.2605.\n",
      "For Iteration 9330 the Loss is 0.2605.\n",
      "For Iteration 9331 the Loss is 0.2605.\n",
      "For Iteration 9332 the Loss is 0.2605.\n",
      "For Iteration 9333 the Loss is 0.2605.\n",
      "For Iteration 9334 the Loss is 0.2605.\n",
      "For Iteration 9335 the Loss is 0.2605.\n",
      "For Iteration 9336 the Loss is 0.2605.\n",
      "For Iteration 9337 the Loss is 0.2605.\n",
      "For Iteration 9338 the Loss is 0.2605.\n",
      "For Iteration 9339 the Loss is 0.2605.\n",
      "For Iteration 9340 the Loss is 0.2605.\n",
      "For Iteration 9341 the Loss is 0.2605.\n",
      "For Iteration 9342 the Loss is 0.2605.\n",
      "For Iteration 9343 the Loss is 0.2605.\n",
      "For Iteration 9344 the Loss is 0.2605.\n",
      "For Iteration 9345 the Loss is 0.2605.\n",
      "For Iteration 9346 the Loss is 0.2605.\n",
      "For Iteration 9347 the Loss is 0.2605.\n",
      "For Iteration 9348 the Loss is 0.2605.\n",
      "For Iteration 9349 the Loss is 0.2605.\n",
      "For Iteration 9350 the Loss is 0.2605.\n",
      "For Iteration 9351 the Loss is 0.2605.\n",
      "For Iteration 9352 the Loss is 0.2605.\n",
      "For Iteration 9353 the Loss is 0.2605.\n",
      "For Iteration 9354 the Loss is 0.2605.\n",
      "For Iteration 9355 the Loss is 0.2605.\n",
      "For Iteration 9356 the Loss is 0.2605.\n",
      "For Iteration 9357 the Loss is 0.2605.\n",
      "For Iteration 9358 the Loss is 0.2605.\n",
      "For Iteration 9359 the Loss is 0.2605.\n",
      "For Iteration 9360 the Loss is 0.2605.\n",
      "For Iteration 9361 the Loss is 0.2605.\n",
      "For Iteration 9362 the Loss is 0.2605.\n",
      "For Iteration 9363 the Loss is 0.2605.\n",
      "For Iteration 9364 the Loss is 0.2605.\n",
      "For Iteration 9365 the Loss is 0.2605.\n",
      "For Iteration 9366 the Loss is 0.2605.\n",
      "For Iteration 9367 the Loss is 0.2605.\n",
      "For Iteration 9368 the Loss is 0.2605.\n",
      "For Iteration 9369 the Loss is 0.2605.\n",
      "For Iteration 9370 the Loss is 0.2605.\n",
      "For Iteration 9371 the Loss is 0.2605.\n",
      "For Iteration 9372 the Loss is 0.2605.\n",
      "For Iteration 9373 the Loss is 0.2605.\n",
      "For Iteration 9374 the Loss is 0.2605.\n",
      "For Iteration 9375 the Loss is 0.2605.\n",
      "For Iteration 9376 the Loss is 0.2605.\n",
      "For Iteration 9377 the Loss is 0.2605.\n",
      "For Iteration 9378 the Loss is 0.2605.\n",
      "For Iteration 9379 the Loss is 0.2605.\n",
      "For Iteration 9380 the Loss is 0.2605.\n",
      "For Iteration 9381 the Loss is 0.2605.\n",
      "For Iteration 9382 the Loss is 0.2605.\n",
      "For Iteration 9383 the Loss is 0.2605.\n",
      "For Iteration 9384 the Loss is 0.2605.\n",
      "For Iteration 9385 the Loss is 0.2605.\n",
      "For Iteration 9386 the Loss is 0.2605.\n",
      "For Iteration 9387 the Loss is 0.2605.\n",
      "For Iteration 9388 the Loss is 0.2605.\n",
      "For Iteration 9389 the Loss is 0.2605.\n",
      "For Iteration 9390 the Loss is 0.2605.\n",
      "For Iteration 9391 the Loss is 0.2605.\n",
      "For Iteration 9392 the Loss is 0.2605.\n",
      "For Iteration 9393 the Loss is 0.2605.\n",
      "For Iteration 9394 the Loss is 0.2605.\n",
      "For Iteration 9395 the Loss is 0.2605.\n",
      "For Iteration 9396 the Loss is 0.2605.\n",
      "For Iteration 9397 the Loss is 0.2605.\n",
      "For Iteration 9398 the Loss is 0.2605.\n",
      "For Iteration 9399 the Loss is 0.2605.\n",
      "For Iteration 9400 the Loss is 0.2605.\n",
      "For Iteration 9401 the Loss is 0.2605.\n",
      "For Iteration 9402 the Loss is 0.2605.\n",
      "For Iteration 9403 the Loss is 0.2605.\n",
      "For Iteration 9404 the Loss is 0.2605.\n",
      "For Iteration 9405 the Loss is 0.2605.\n",
      "For Iteration 9406 the Loss is 0.2605.\n",
      "For Iteration 9407 the Loss is 0.2605.\n",
      "For Iteration 9408 the Loss is 0.2605.\n",
      "For Iteration 9409 the Loss is 0.2605.\n",
      "For Iteration 9410 the Loss is 0.2605.\n",
      "For Iteration 9411 the Loss is 0.2605.\n",
      "For Iteration 9412 the Loss is 0.2605.\n",
      "For Iteration 9413 the Loss is 0.2605.\n",
      "For Iteration 9414 the Loss is 0.2605.\n",
      "For Iteration 9415 the Loss is 0.2605.\n",
      "For Iteration 9416 the Loss is 0.2605.\n",
      "For Iteration 9417 the Loss is 0.2605.\n",
      "For Iteration 9418 the Loss is 0.2605.\n",
      "For Iteration 9419 the Loss is 0.2605.\n",
      "For Iteration 9420 the Loss is 0.2605.\n",
      "For Iteration 9421 the Loss is 0.2605.\n",
      "For Iteration 9422 the Loss is 0.2605.\n",
      "For Iteration 9423 the Loss is 0.2605.\n",
      "For Iteration 9424 the Loss is 0.2605.\n",
      "For Iteration 9425 the Loss is 0.2605.\n",
      "For Iteration 9426 the Loss is 0.2605.\n",
      "For Iteration 9427 the Loss is 0.2605.\n",
      "For Iteration 9428 the Loss is 0.2605.\n",
      "For Iteration 9429 the Loss is 0.2605.\n",
      "For Iteration 9430 the Loss is 0.2605.\n",
      "For Iteration 9431 the Loss is 0.2605.\n",
      "For Iteration 9432 the Loss is 0.2605.\n",
      "For Iteration 9433 the Loss is 0.2605.\n",
      "For Iteration 9434 the Loss is 0.2605.\n",
      "For Iteration 9435 the Loss is 0.2605.\n",
      "For Iteration 9436 the Loss is 0.2605.\n",
      "For Iteration 9437 the Loss is 0.2605.\n",
      "For Iteration 9438 the Loss is 0.2605.\n",
      "For Iteration 9439 the Loss is 0.2605.\n",
      "For Iteration 9440 the Loss is 0.2605.\n",
      "For Iteration 9441 the Loss is 0.2605.\n",
      "For Iteration 9442 the Loss is 0.2605.\n",
      "For Iteration 9443 the Loss is 0.2605.\n",
      "For Iteration 9444 the Loss is 0.2605.\n",
      "For Iteration 9445 the Loss is 0.2605.\n",
      "For Iteration 9446 the Loss is 0.2605.\n",
      "For Iteration 9447 the Loss is 0.2605.\n",
      "For Iteration 9448 the Loss is 0.2605.\n",
      "For Iteration 9449 the Loss is 0.2605.\n",
      "For Iteration 9450 the Loss is 0.2605.\n",
      "For Iteration 9451 the Loss is 0.2605.\n",
      "For Iteration 9452 the Loss is 0.2605.\n",
      "For Iteration 9453 the Loss is 0.2605.\n",
      "For Iteration 9454 the Loss is 0.2605.\n",
      "For Iteration 9455 the Loss is 0.2605.\n",
      "For Iteration 9456 the Loss is 0.2605.\n",
      "For Iteration 9457 the Loss is 0.2605.\n",
      "For Iteration 9458 the Loss is 0.2605.\n",
      "For Iteration 9459 the Loss is 0.2605.\n",
      "For Iteration 9460 the Loss is 0.2605.\n",
      "For Iteration 9461 the Loss is 0.2605.\n",
      "For Iteration 9462 the Loss is 0.2605.\n",
      "For Iteration 9463 the Loss is 0.2605.\n",
      "For Iteration 9464 the Loss is 0.2605.\n",
      "For Iteration 9465 the Loss is 0.2605.\n",
      "For Iteration 9466 the Loss is 0.2605.\n",
      "For Iteration 9467 the Loss is 0.2605.\n",
      "For Iteration 9468 the Loss is 0.2605.\n",
      "For Iteration 9469 the Loss is 0.2605.\n",
      "For Iteration 9470 the Loss is 0.2605.\n",
      "For Iteration 9471 the Loss is 0.2605.\n",
      "For Iteration 9472 the Loss is 0.2605.\n",
      "For Iteration 9473 the Loss is 0.2605.\n",
      "For Iteration 9474 the Loss is 0.2605.\n",
      "For Iteration 9475 the Loss is 0.2605.\n",
      "For Iteration 9476 the Loss is 0.2605.\n",
      "For Iteration 9477 the Loss is 0.2605.\n",
      "For Iteration 9478 the Loss is 0.2605.\n",
      "For Iteration 9479 the Loss is 0.2605.\n",
      "For Iteration 9480 the Loss is 0.2605.\n",
      "For Iteration 9481 the Loss is 0.2605.\n",
      "For Iteration 9482 the Loss is 0.2605.\n",
      "For Iteration 9483 the Loss is 0.2605.\n",
      "For Iteration 9484 the Loss is 0.2605.\n",
      "For Iteration 9485 the Loss is 0.2605.\n",
      "For Iteration 9486 the Loss is 0.2605.\n",
      "For Iteration 9487 the Loss is 0.2605.\n",
      "For Iteration 9488 the Loss is 0.2605.\n",
      "For Iteration 9489 the Loss is 0.2605.\n",
      "For Iteration 9490 the Loss is 0.2605.\n",
      "For Iteration 9491 the Loss is 0.2605.\n",
      "For Iteration 9492 the Loss is 0.2605.\n",
      "For Iteration 9493 the Loss is 0.2605.\n",
      "For Iteration 9494 the Loss is 0.2605.\n",
      "For Iteration 9495 the Loss is 0.2605.\n",
      "For Iteration 9496 the Loss is 0.2605.\n",
      "For Iteration 9497 the Loss is 0.2605.\n",
      "For Iteration 9498 the Loss is 0.2605.\n",
      "For Iteration 9499 the Loss is 0.2605.\n",
      "For Iteration 9500 the Loss is 0.2605.\n",
      "For Iteration 9501 the Loss is 0.2605.\n",
      "For Iteration 9502 the Loss is 0.2605.\n",
      "For Iteration 9503 the Loss is 0.2605.\n",
      "For Iteration 9504 the Loss is 0.2605.\n",
      "For Iteration 9505 the Loss is 0.2605.\n",
      "For Iteration 9506 the Loss is 0.2605.\n",
      "For Iteration 9507 the Loss is 0.2605.\n",
      "For Iteration 9508 the Loss is 0.2605.\n",
      "For Iteration 9509 the Loss is 0.2605.\n",
      "For Iteration 9510 the Loss is 0.2605.\n",
      "For Iteration 9511 the Loss is 0.2605.\n",
      "For Iteration 9512 the Loss is 0.2605.\n",
      "For Iteration 9513 the Loss is 0.2605.\n",
      "For Iteration 9514 the Loss is 0.2605.\n",
      "For Iteration 9515 the Loss is 0.2605.\n",
      "For Iteration 9516 the Loss is 0.2605.\n",
      "For Iteration 9517 the Loss is 0.2605.\n",
      "For Iteration 9518 the Loss is 0.2605.\n",
      "For Iteration 9519 the Loss is 0.2605.\n",
      "For Iteration 9520 the Loss is 0.2605.\n",
      "For Iteration 9521 the Loss is 0.2605.\n",
      "For Iteration 9522 the Loss is 0.2605.\n",
      "For Iteration 9523 the Loss is 0.2605.\n",
      "For Iteration 9524 the Loss is 0.2605.\n",
      "For Iteration 9525 the Loss is 0.2605.\n",
      "For Iteration 9526 the Loss is 0.2605.\n",
      "For Iteration 9527 the Loss is 0.2605.\n",
      "For Iteration 9528 the Loss is 0.2605.\n",
      "For Iteration 9529 the Loss is 0.2605.\n",
      "For Iteration 9530 the Loss is 0.2605.\n",
      "For Iteration 9531 the Loss is 0.2605.\n",
      "For Iteration 9532 the Loss is 0.2605.\n",
      "For Iteration 9533 the Loss is 0.2605.\n",
      "For Iteration 9534 the Loss is 0.2605.\n",
      "For Iteration 9535 the Loss is 0.2605.\n",
      "For Iteration 9536 the Loss is 0.2605.\n",
      "For Iteration 9537 the Loss is 0.2605.\n",
      "For Iteration 9538 the Loss is 0.2605.\n",
      "For Iteration 9539 the Loss is 0.2605.\n",
      "For Iteration 9540 the Loss is 0.2605.\n",
      "For Iteration 9541 the Loss is 0.2605.\n",
      "For Iteration 9542 the Loss is 0.2605.\n",
      "For Iteration 9543 the Loss is 0.2605.\n",
      "For Iteration 9544 the Loss is 0.2605.\n",
      "For Iteration 9545 the Loss is 0.2605.\n",
      "For Iteration 9546 the Loss is 0.2605.\n",
      "For Iteration 9547 the Loss is 0.2605.\n",
      "For Iteration 9548 the Loss is 0.2605.\n",
      "For Iteration 9549 the Loss is 0.2605.\n",
      "For Iteration 9550 the Loss is 0.2605.\n",
      "For Iteration 9551 the Loss is 0.2605.\n",
      "For Iteration 9552 the Loss is 0.2605.\n",
      "For Iteration 9553 the Loss is 0.2605.\n",
      "For Iteration 9554 the Loss is 0.2605.\n",
      "For Iteration 9555 the Loss is 0.2605.\n",
      "For Iteration 9556 the Loss is 0.2605.\n",
      "For Iteration 9557 the Loss is 0.2605.\n",
      "For Iteration 9558 the Loss is 0.2605.\n",
      "For Iteration 9559 the Loss is 0.2605.\n",
      "For Iteration 9560 the Loss is 0.2605.\n",
      "For Iteration 9561 the Loss is 0.2605.\n",
      "For Iteration 9562 the Loss is 0.2605.\n",
      "For Iteration 9563 the Loss is 0.2605.\n",
      "For Iteration 9564 the Loss is 0.2605.\n",
      "For Iteration 9565 the Loss is 0.2605.\n",
      "For Iteration 9566 the Loss is 0.2605.\n",
      "For Iteration 9567 the Loss is 0.2605.\n",
      "For Iteration 9568 the Loss is 0.2605.\n",
      "For Iteration 9569 the Loss is 0.2605.\n",
      "For Iteration 9570 the Loss is 0.2605.\n",
      "For Iteration 9571 the Loss is 0.2605.\n",
      "For Iteration 9572 the Loss is 0.2605.\n",
      "For Iteration 9573 the Loss is 0.2605.\n",
      "For Iteration 9574 the Loss is 0.2605.\n",
      "For Iteration 9575 the Loss is 0.2605.\n",
      "For Iteration 9576 the Loss is 0.2605.\n",
      "For Iteration 9577 the Loss is 0.2605.\n",
      "For Iteration 9578 the Loss is 0.2605.\n",
      "For Iteration 9579 the Loss is 0.2605.\n",
      "For Iteration 9580 the Loss is 0.2605.\n",
      "For Iteration 9581 the Loss is 0.2605.\n",
      "For Iteration 9582 the Loss is 0.2605.\n",
      "For Iteration 9583 the Loss is 0.2605.\n",
      "For Iteration 9584 the Loss is 0.2605.\n",
      "For Iteration 9585 the Loss is 0.2605.\n",
      "For Iteration 9586 the Loss is 0.2605.\n",
      "For Iteration 9587 the Loss is 0.2605.\n",
      "For Iteration 9588 the Loss is 0.2605.\n",
      "For Iteration 9589 the Loss is 0.2605.\n",
      "For Iteration 9590 the Loss is 0.2605.\n",
      "For Iteration 9591 the Loss is 0.2605.\n",
      "For Iteration 9592 the Loss is 0.2605.\n",
      "For Iteration 9593 the Loss is 0.2605.\n",
      "For Iteration 9594 the Loss is 0.2605.\n",
      "For Iteration 9595 the Loss is 0.2605.\n",
      "For Iteration 9596 the Loss is 0.2605.\n",
      "For Iteration 9597 the Loss is 0.2605.\n",
      "For Iteration 9598 the Loss is 0.2605.\n",
      "For Iteration 9599 the Loss is 0.2605.\n",
      "For Iteration 9600 the Loss is 0.2605.\n",
      "For Iteration 9601 the Loss is 0.2605.\n",
      "For Iteration 9602 the Loss is 0.2605.\n",
      "For Iteration 9603 the Loss is 0.2605.\n",
      "For Iteration 9604 the Loss is 0.2605.\n",
      "For Iteration 9605 the Loss is 0.2605.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 9606 the Loss is 0.2605.\n",
      "For Iteration 9607 the Loss is 0.2605.\n",
      "For Iteration 9608 the Loss is 0.2605.\n",
      "For Iteration 9609 the Loss is 0.2605.\n",
      "For Iteration 9610 the Loss is 0.2605.\n",
      "For Iteration 9611 the Loss is 0.2605.\n",
      "For Iteration 9612 the Loss is 0.2605.\n",
      "For Iteration 9613 the Loss is 0.2605.\n",
      "For Iteration 9614 the Loss is 0.2605.\n",
      "For Iteration 9615 the Loss is 0.2605.\n",
      "For Iteration 9616 the Loss is 0.2605.\n",
      "For Iteration 9617 the Loss is 0.2605.\n",
      "For Iteration 9618 the Loss is 0.2605.\n",
      "For Iteration 9619 the Loss is 0.2605.\n",
      "For Iteration 9620 the Loss is 0.2605.\n",
      "For Iteration 9621 the Loss is 0.2605.\n",
      "For Iteration 9622 the Loss is 0.2605.\n",
      "For Iteration 9623 the Loss is 0.2605.\n",
      "For Iteration 9624 the Loss is 0.2605.\n",
      "For Iteration 9625 the Loss is 0.2605.\n",
      "For Iteration 9626 the Loss is 0.2605.\n",
      "For Iteration 9627 the Loss is 0.2605.\n",
      "For Iteration 9628 the Loss is 0.2605.\n",
      "For Iteration 9629 the Loss is 0.2605.\n",
      "For Iteration 9630 the Loss is 0.2605.\n",
      "For Iteration 9631 the Loss is 0.2605.\n",
      "For Iteration 9632 the Loss is 0.2605.\n",
      "For Iteration 9633 the Loss is 0.2605.\n",
      "For Iteration 9634 the Loss is 0.2605.\n",
      "For Iteration 9635 the Loss is 0.2605.\n",
      "For Iteration 9636 the Loss is 0.2605.\n",
      "For Iteration 9637 the Loss is 0.2605.\n",
      "For Iteration 9638 the Loss is 0.2605.\n",
      "For Iteration 9639 the Loss is 0.2605.\n",
      "For Iteration 9640 the Loss is 0.2605.\n",
      "For Iteration 9641 the Loss is 0.2605.\n",
      "For Iteration 9642 the Loss is 0.2605.\n",
      "For Iteration 9643 the Loss is 0.2605.\n",
      "For Iteration 9644 the Loss is 0.2605.\n",
      "For Iteration 9645 the Loss is 0.2605.\n",
      "For Iteration 9646 the Loss is 0.2605.\n",
      "For Iteration 9647 the Loss is 0.2605.\n",
      "For Iteration 9648 the Loss is 0.2605.\n",
      "For Iteration 9649 the Loss is 0.2605.\n",
      "For Iteration 9650 the Loss is 0.2605.\n",
      "For Iteration 9651 the Loss is 0.2605.\n",
      "For Iteration 9652 the Loss is 0.2605.\n",
      "For Iteration 9653 the Loss is 0.2605.\n",
      "For Iteration 9654 the Loss is 0.2605.\n",
      "For Iteration 9655 the Loss is 0.2605.\n",
      "For Iteration 9656 the Loss is 0.2605.\n",
      "For Iteration 9657 the Loss is 0.2605.\n",
      "For Iteration 9658 the Loss is 0.2605.\n",
      "For Iteration 9659 the Loss is 0.2605.\n",
      "For Iteration 9660 the Loss is 0.2605.\n",
      "For Iteration 9661 the Loss is 0.2605.\n",
      "For Iteration 9662 the Loss is 0.2605.\n",
      "For Iteration 9663 the Loss is 0.2605.\n",
      "For Iteration 9664 the Loss is 0.2605.\n",
      "For Iteration 9665 the Loss is 0.2605.\n",
      "For Iteration 9666 the Loss is 0.2605.\n",
      "For Iteration 9667 the Loss is 0.2605.\n",
      "For Iteration 9668 the Loss is 0.2605.\n",
      "For Iteration 9669 the Loss is 0.2605.\n",
      "For Iteration 9670 the Loss is 0.2605.\n",
      "For Iteration 9671 the Loss is 0.2605.\n",
      "For Iteration 9672 the Loss is 0.2605.\n",
      "For Iteration 9673 the Loss is 0.2605.\n",
      "For Iteration 9674 the Loss is 0.2605.\n",
      "For Iteration 9675 the Loss is 0.2605.\n",
      "For Iteration 9676 the Loss is 0.2605.\n",
      "For Iteration 9677 the Loss is 0.2605.\n",
      "For Iteration 9678 the Loss is 0.2605.\n",
      "For Iteration 9679 the Loss is 0.2605.\n",
      "For Iteration 9680 the Loss is 0.2605.\n",
      "For Iteration 9681 the Loss is 0.2605.\n",
      "For Iteration 9682 the Loss is 0.2605.\n",
      "For Iteration 9683 the Loss is 0.2605.\n",
      "For Iteration 9684 the Loss is 0.2605.\n",
      "For Iteration 9685 the Loss is 0.2605.\n",
      "For Iteration 9686 the Loss is 0.2605.\n",
      "For Iteration 9687 the Loss is 0.2605.\n",
      "For Iteration 9688 the Loss is 0.2605.\n",
      "For Iteration 9689 the Loss is 0.2605.\n",
      "For Iteration 9690 the Loss is 0.2605.\n",
      "For Iteration 9691 the Loss is 0.2605.\n",
      "For Iteration 9692 the Loss is 0.2605.\n",
      "For Iteration 9693 the Loss is 0.2605.\n",
      "For Iteration 9694 the Loss is 0.2605.\n",
      "For Iteration 9695 the Loss is 0.2605.\n",
      "For Iteration 9696 the Loss is 0.2605.\n",
      "For Iteration 9697 the Loss is 0.2605.\n",
      "For Iteration 9698 the Loss is 0.2605.\n",
      "For Iteration 9699 the Loss is 0.2605.\n",
      "For Iteration 9700 the Loss is 0.2605.\n",
      "For Iteration 9701 the Loss is 0.2605.\n",
      "For Iteration 9702 the Loss is 0.2605.\n",
      "For Iteration 9703 the Loss is 0.2605.\n",
      "For Iteration 9704 the Loss is 0.2605.\n",
      "For Iteration 9705 the Loss is 0.2605.\n",
      "For Iteration 9706 the Loss is 0.2605.\n",
      "For Iteration 9707 the Loss is 0.2605.\n",
      "For Iteration 9708 the Loss is 0.2605.\n",
      "For Iteration 9709 the Loss is 0.2605.\n",
      "For Iteration 9710 the Loss is 0.2605.\n",
      "For Iteration 9711 the Loss is 0.2605.\n",
      "For Iteration 9712 the Loss is 0.2605.\n",
      "For Iteration 9713 the Loss is 0.2605.\n",
      "For Iteration 9714 the Loss is 0.2605.\n",
      "For Iteration 9715 the Loss is 0.2605.\n",
      "For Iteration 9716 the Loss is 0.2605.\n",
      "For Iteration 9717 the Loss is 0.2605.\n",
      "For Iteration 9718 the Loss is 0.2605.\n",
      "For Iteration 9719 the Loss is 0.2605.\n",
      "For Iteration 9720 the Loss is 0.2605.\n",
      "For Iteration 9721 the Loss is 0.2605.\n",
      "For Iteration 9722 the Loss is 0.2605.\n",
      "For Iteration 9723 the Loss is 0.2605.\n",
      "For Iteration 9724 the Loss is 0.2605.\n",
      "For Iteration 9725 the Loss is 0.2605.\n",
      "For Iteration 9726 the Loss is 0.2605.\n",
      "For Iteration 9727 the Loss is 0.2605.\n",
      "For Iteration 9728 the Loss is 0.2605.\n",
      "For Iteration 9729 the Loss is 0.2605.\n",
      "For Iteration 9730 the Loss is 0.2605.\n",
      "For Iteration 9731 the Loss is 0.2605.\n",
      "For Iteration 9732 the Loss is 0.2605.\n",
      "For Iteration 9733 the Loss is 0.2605.\n",
      "For Iteration 9734 the Loss is 0.2605.\n",
      "For Iteration 9735 the Loss is 0.2605.\n",
      "For Iteration 9736 the Loss is 0.2605.\n",
      "For Iteration 9737 the Loss is 0.2605.\n",
      "For Iteration 9738 the Loss is 0.2605.\n",
      "For Iteration 9739 the Loss is 0.2605.\n",
      "For Iteration 9740 the Loss is 0.2605.\n",
      "For Iteration 9741 the Loss is 0.2605.\n",
      "For Iteration 9742 the Loss is 0.2605.\n",
      "For Iteration 9743 the Loss is 0.2605.\n",
      "For Iteration 9744 the Loss is 0.2605.\n",
      "For Iteration 9745 the Loss is 0.2605.\n",
      "For Iteration 9746 the Loss is 0.2605.\n",
      "For Iteration 9747 the Loss is 0.2605.\n",
      "For Iteration 9748 the Loss is 0.2605.\n",
      "For Iteration 9749 the Loss is 0.2605.\n",
      "For Iteration 9750 the Loss is 0.2605.\n",
      "For Iteration 9751 the Loss is 0.2605.\n",
      "For Iteration 9752 the Loss is 0.2605.\n",
      "For Iteration 9753 the Loss is 0.2605.\n",
      "For Iteration 9754 the Loss is 0.2605.\n",
      "For Iteration 9755 the Loss is 0.2605.\n",
      "For Iteration 9756 the Loss is 0.2605.\n",
      "For Iteration 9757 the Loss is 0.2605.\n",
      "For Iteration 9758 the Loss is 0.2605.\n",
      "For Iteration 9759 the Loss is 0.2605.\n",
      "For Iteration 9760 the Loss is 0.2605.\n",
      "For Iteration 9761 the Loss is 0.2605.\n",
      "For Iteration 9762 the Loss is 0.2605.\n",
      "For Iteration 9763 the Loss is 0.2605.\n",
      "For Iteration 9764 the Loss is 0.2605.\n",
      "For Iteration 9765 the Loss is 0.2605.\n",
      "For Iteration 9766 the Loss is 0.2605.\n",
      "For Iteration 9767 the Loss is 0.2605.\n",
      "For Iteration 9768 the Loss is 0.2605.\n",
      "For Iteration 9769 the Loss is 0.2605.\n",
      "For Iteration 9770 the Loss is 0.2605.\n",
      "For Iteration 9771 the Loss is 0.2605.\n",
      "For Iteration 9772 the Loss is 0.2605.\n",
      "For Iteration 9773 the Loss is 0.2605.\n",
      "For Iteration 9774 the Loss is 0.2605.\n",
      "For Iteration 9775 the Loss is 0.2605.\n",
      "For Iteration 9776 the Loss is 0.2605.\n",
      "For Iteration 9777 the Loss is 0.2605.\n",
      "For Iteration 9778 the Loss is 0.2605.\n",
      "For Iteration 9779 the Loss is 0.2605.\n",
      "For Iteration 9780 the Loss is 0.2605.\n",
      "For Iteration 9781 the Loss is 0.2605.\n",
      "For Iteration 9782 the Loss is 0.2605.\n",
      "For Iteration 9783 the Loss is 0.2605.\n",
      "For Iteration 9784 the Loss is 0.2605.\n",
      "For Iteration 9785 the Loss is 0.2605.\n",
      "For Iteration 9786 the Loss is 0.2605.\n",
      "For Iteration 9787 the Loss is 0.2605.\n",
      "For Iteration 9788 the Loss is 0.2605.\n",
      "For Iteration 9789 the Loss is 0.2605.\n",
      "For Iteration 9790 the Loss is 0.2605.\n",
      "For Iteration 9791 the Loss is 0.2605.\n",
      "For Iteration 9792 the Loss is 0.2605.\n",
      "For Iteration 9793 the Loss is 0.2605.\n",
      "For Iteration 9794 the Loss is 0.2605.\n",
      "For Iteration 9795 the Loss is 0.2605.\n",
      "For Iteration 9796 the Loss is 0.2605.\n",
      "For Iteration 9797 the Loss is 0.2605.\n",
      "For Iteration 9798 the Loss is 0.2605.\n",
      "For Iteration 9799 the Loss is 0.2605.\n",
      "For Iteration 9800 the Loss is 0.2605.\n",
      "For Iteration 9801 the Loss is 0.2605.\n",
      "For Iteration 9802 the Loss is 0.2605.\n",
      "For Iteration 9803 the Loss is 0.2605.\n",
      "For Iteration 9804 the Loss is 0.2605.\n",
      "For Iteration 9805 the Loss is 0.2605.\n",
      "For Iteration 9806 the Loss is 0.2605.\n",
      "For Iteration 9807 the Loss is 0.2605.\n",
      "For Iteration 9808 the Loss is 0.2605.\n",
      "For Iteration 9809 the Loss is 0.2605.\n",
      "For Iteration 9810 the Loss is 0.2605.\n",
      "For Iteration 9811 the Loss is 0.2605.\n",
      "For Iteration 9812 the Loss is 0.2605.\n",
      "For Iteration 9813 the Loss is 0.2605.\n",
      "For Iteration 9814 the Loss is 0.2605.\n",
      "For Iteration 9815 the Loss is 0.2605.\n",
      "For Iteration 9816 the Loss is 0.2605.\n",
      "For Iteration 9817 the Loss is 0.2605.\n",
      "For Iteration 9818 the Loss is 0.2605.\n",
      "For Iteration 9819 the Loss is 0.2605.\n",
      "For Iteration 9820 the Loss is 0.2605.\n",
      "For Iteration 9821 the Loss is 0.2605.\n",
      "For Iteration 9822 the Loss is 0.2605.\n",
      "For Iteration 9823 the Loss is 0.2605.\n",
      "For Iteration 9824 the Loss is 0.2605.\n",
      "For Iteration 9825 the Loss is 0.2605.\n",
      "For Iteration 9826 the Loss is 0.2605.\n",
      "For Iteration 9827 the Loss is 0.2605.\n",
      "For Iteration 9828 the Loss is 0.2605.\n",
      "For Iteration 9829 the Loss is 0.2605.\n",
      "For Iteration 9830 the Loss is 0.2605.\n",
      "For Iteration 9831 the Loss is 0.2605.\n",
      "For Iteration 9832 the Loss is 0.2605.\n",
      "For Iteration 9833 the Loss is 0.2605.\n",
      "For Iteration 9834 the Loss is 0.2605.\n",
      "For Iteration 9835 the Loss is 0.2605.\n",
      "For Iteration 9836 the Loss is 0.2605.\n",
      "For Iteration 9837 the Loss is 0.2605.\n",
      "For Iteration 9838 the Loss is 0.2605.\n",
      "For Iteration 9839 the Loss is 0.2605.\n",
      "For Iteration 9840 the Loss is 0.2605.\n",
      "For Iteration 9841 the Loss is 0.2605.\n",
      "For Iteration 9842 the Loss is 0.2605.\n",
      "For Iteration 9843 the Loss is 0.2605.\n",
      "For Iteration 9844 the Loss is 0.2605.\n",
      "For Iteration 9845 the Loss is 0.2605.\n",
      "For Iteration 9846 the Loss is 0.2605.\n",
      "For Iteration 9847 the Loss is 0.2605.\n",
      "For Iteration 9848 the Loss is 0.2605.\n",
      "For Iteration 9849 the Loss is 0.2605.\n",
      "For Iteration 9850 the Loss is 0.2605.\n",
      "For Iteration 9851 the Loss is 0.2605.\n",
      "For Iteration 9852 the Loss is 0.2605.\n",
      "For Iteration 9853 the Loss is 0.2605.\n",
      "For Iteration 9854 the Loss is 0.2605.\n",
      "For Iteration 9855 the Loss is 0.2605.\n",
      "For Iteration 9856 the Loss is 0.2605.\n",
      "For Iteration 9857 the Loss is 0.2605.\n",
      "For Iteration 9858 the Loss is 0.2605.\n",
      "For Iteration 9859 the Loss is 0.2605.\n",
      "For Iteration 9860 the Loss is 0.2605.\n",
      "For Iteration 9861 the Loss is 0.2605.\n",
      "For Iteration 9862 the Loss is 0.2605.\n",
      "For Iteration 9863 the Loss is 0.2605.\n",
      "For Iteration 9864 the Loss is 0.2605.\n",
      "For Iteration 9865 the Loss is 0.2605.\n",
      "For Iteration 9866 the Loss is 0.2605.\n",
      "For Iteration 9867 the Loss is 0.2605.\n",
      "For Iteration 9868 the Loss is 0.2605.\n",
      "For Iteration 9869 the Loss is 0.2605.\n",
      "For Iteration 9870 the Loss is 0.2605.\n",
      "For Iteration 9871 the Loss is 0.2605.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 9872 the Loss is 0.2605.\n",
      "For Iteration 9873 the Loss is 0.2605.\n",
      "For Iteration 9874 the Loss is 0.2605.\n",
      "For Iteration 9875 the Loss is 0.2605.\n",
      "For Iteration 9876 the Loss is 0.2605.\n",
      "For Iteration 9877 the Loss is 0.2605.\n",
      "For Iteration 9878 the Loss is 0.2605.\n",
      "For Iteration 9879 the Loss is 0.2605.\n",
      "For Iteration 9880 the Loss is 0.2605.\n",
      "For Iteration 9881 the Loss is 0.2605.\n",
      "For Iteration 9882 the Loss is 0.2605.\n",
      "For Iteration 9883 the Loss is 0.2605.\n",
      "For Iteration 9884 the Loss is 0.2605.\n",
      "For Iteration 9885 the Loss is 0.2605.\n",
      "For Iteration 9886 the Loss is 0.2605.\n",
      "For Iteration 9887 the Loss is 0.2605.\n",
      "For Iteration 9888 the Loss is 0.2605.\n",
      "For Iteration 9889 the Loss is 0.2605.\n",
      "For Iteration 9890 the Loss is 0.2605.\n",
      "For Iteration 9891 the Loss is 0.2605.\n",
      "For Iteration 9892 the Loss is 0.2605.\n",
      "For Iteration 9893 the Loss is 0.2605.\n",
      "For Iteration 9894 the Loss is 0.2605.\n",
      "For Iteration 9895 the Loss is 0.2605.\n",
      "For Iteration 9896 the Loss is 0.2605.\n",
      "For Iteration 9897 the Loss is 0.2605.\n",
      "For Iteration 9898 the Loss is 0.2605.\n",
      "For Iteration 9899 the Loss is 0.2605.\n",
      "For Iteration 9900 the Loss is 0.2605.\n",
      "For Iteration 9901 the Loss is 0.2605.\n",
      "For Iteration 9902 the Loss is 0.2605.\n",
      "For Iteration 9903 the Loss is 0.2605.\n",
      "For Iteration 9904 the Loss is 0.2605.\n",
      "For Iteration 9905 the Loss is 0.2605.\n",
      "For Iteration 9906 the Loss is 0.2605.\n",
      "For Iteration 9907 the Loss is 0.2605.\n",
      "For Iteration 9908 the Loss is 0.2605.\n",
      "For Iteration 9909 the Loss is 0.2605.\n",
      "For Iteration 9910 the Loss is 0.2605.\n",
      "For Iteration 9911 the Loss is 0.2605.\n",
      "For Iteration 9912 the Loss is 0.2605.\n",
      "For Iteration 9913 the Loss is 0.2605.\n",
      "For Iteration 9914 the Loss is 0.2605.\n",
      "For Iteration 9915 the Loss is 0.2605.\n",
      "For Iteration 9916 the Loss is 0.2605.\n",
      "For Iteration 9917 the Loss is 0.2605.\n",
      "For Iteration 9918 the Loss is 0.2605.\n",
      "For Iteration 9919 the Loss is 0.2605.\n",
      "For Iteration 9920 the Loss is 0.2605.\n",
      "For Iteration 9921 the Loss is 0.2605.\n",
      "For Iteration 9922 the Loss is 0.2604.\n",
      "For Iteration 9923 the Loss is 0.2604.\n",
      "For Iteration 9924 the Loss is 0.2604.\n",
      "For Iteration 9925 the Loss is 0.2604.\n",
      "For Iteration 9926 the Loss is 0.2604.\n",
      "For Iteration 9927 the Loss is 0.2604.\n",
      "For Iteration 9928 the Loss is 0.2604.\n",
      "For Iteration 9929 the Loss is 0.2604.\n",
      "For Iteration 9930 the Loss is 0.2604.\n",
      "For Iteration 9931 the Loss is 0.2604.\n",
      "For Iteration 9932 the Loss is 0.2604.\n",
      "For Iteration 9933 the Loss is 0.2604.\n",
      "For Iteration 9934 the Loss is 0.2604.\n",
      "For Iteration 9935 the Loss is 0.2604.\n",
      "For Iteration 9936 the Loss is 0.2604.\n",
      "For Iteration 9937 the Loss is 0.2604.\n",
      "For Iteration 9938 the Loss is 0.2604.\n",
      "For Iteration 9939 the Loss is 0.2604.\n",
      "For Iteration 9940 the Loss is 0.2604.\n",
      "For Iteration 9941 the Loss is 0.2604.\n",
      "For Iteration 9942 the Loss is 0.2604.\n",
      "For Iteration 9943 the Loss is 0.2604.\n",
      "For Iteration 9944 the Loss is 0.2604.\n",
      "For Iteration 9945 the Loss is 0.2604.\n",
      "For Iteration 9946 the Loss is 0.2604.\n",
      "For Iteration 9947 the Loss is 0.2604.\n",
      "For Iteration 9948 the Loss is 0.2604.\n",
      "For Iteration 9949 the Loss is 0.2604.\n",
      "For Iteration 9950 the Loss is 0.2604.\n",
      "For Iteration 9951 the Loss is 0.2604.\n",
      "For Iteration 9952 the Loss is 0.2604.\n",
      "For Iteration 9953 the Loss is 0.2604.\n",
      "For Iteration 9954 the Loss is 0.2604.\n",
      "For Iteration 9955 the Loss is 0.2604.\n",
      "For Iteration 9956 the Loss is 0.2604.\n",
      "For Iteration 9957 the Loss is 0.2604.\n",
      "For Iteration 9958 the Loss is 0.2604.\n",
      "For Iteration 9959 the Loss is 0.2604.\n",
      "For Iteration 9960 the Loss is 0.2604.\n",
      "For Iteration 9961 the Loss is 0.2604.\n",
      "For Iteration 9962 the Loss is 0.2604.\n",
      "For Iteration 9963 the Loss is 0.2604.\n",
      "For Iteration 9964 the Loss is 0.2604.\n",
      "For Iteration 9965 the Loss is 0.2604.\n",
      "For Iteration 9966 the Loss is 0.2604.\n",
      "For Iteration 9967 the Loss is 0.2604.\n",
      "For Iteration 9968 the Loss is 0.2604.\n",
      "For Iteration 9969 the Loss is 0.2604.\n",
      "For Iteration 9970 the Loss is 0.2604.\n",
      "For Iteration 9971 the Loss is 0.2604.\n",
      "For Iteration 9972 the Loss is 0.2604.\n",
      "For Iteration 9973 the Loss is 0.2604.\n",
      "For Iteration 9974 the Loss is 0.2604.\n",
      "For Iteration 9975 the Loss is 0.2604.\n",
      "For Iteration 9976 the Loss is 0.2604.\n",
      "For Iteration 9977 the Loss is 0.2604.\n",
      "For Iteration 9978 the Loss is 0.2604.\n",
      "For Iteration 9979 the Loss is 0.2604.\n",
      "For Iteration 9980 the Loss is 0.2604.\n",
      "For Iteration 9981 the Loss is 0.2604.\n",
      "For Iteration 9982 the Loss is 0.2604.\n",
      "For Iteration 9983 the Loss is 0.2604.\n",
      "For Iteration 9984 the Loss is 0.2604.\n",
      "For Iteration 9985 the Loss is 0.2604.\n",
      "For Iteration 9986 the Loss is 0.2604.\n",
      "For Iteration 9987 the Loss is 0.2604.\n",
      "For Iteration 9988 the Loss is 0.2604.\n",
      "For Iteration 9989 the Loss is 0.2604.\n",
      "For Iteration 9990 the Loss is 0.2604.\n",
      "For Iteration 9991 the Loss is 0.2604.\n",
      "For Iteration 9992 the Loss is 0.2604.\n",
      "For Iteration 9993 the Loss is 0.2604.\n",
      "For Iteration 9994 the Loss is 0.2604.\n",
      "For Iteration 9995 the Loss is 0.2604.\n",
      "For Iteration 9996 the Loss is 0.2604.\n",
      "For Iteration 9997 the Loss is 0.2604.\n",
      "For Iteration 9998 the Loss is 0.2604.\n",
      "For Iteration 9999 the Loss is 0.2604.\n"
     ]
    }
   ],
   "source": [
    "model3=LogisticRegression(learning_rate=.4,itr=10000)\n",
    "model3.fit(train_x,train_y)\n",
    "#model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "45dbb0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(test_y,model3.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "25c33c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x261026225b0>]"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model3.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "5148b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pkl_file3', 'wb') as files:\n",
    "    pickle.dump(model3, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "3f93186d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('pkl_file3' , 'rb') as f:\n",
    "    lr3 = pickle.load(f)\n",
    "\n",
    "accuracy(test_y,lr3.predict(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b289da5f",
   "metadata": {},
   "source": [
    "# Best Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "8f5133df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.002542204096210659,\n",
       " -0.005083935650023809,\n",
       " -0.007625187317765646,\n",
       " -0.010165951641590665,\n",
       " -0.012706221047763838,\n",
       " -0.015245987844919933,\n",
       " -0.01778524422229979,\n",
       " -0.020323982247963363,\n",
       " -0.02286219386697939,\n",
       " -0.025399870899591483,\n",
       " -0.027937005039360488,\n",
       " -0.030473587851282992,\n",
       " -0.03300961076988576,\n",
       " -0.035545065097296026,\n",
       " -0.03807994200128742,\n",
       " -0.04061423251330147,\n",
       " -0.04314792752644453,\n",
       " -0.04568101779345996,\n",
       " -0.04821349392467555,\n",
       " -0.05074534638592595,\n",
       " -0.05327656549645017,\n",
       " -0.055807141426763934,\n",
       " -0.058337064196506846,\n",
       " -0.060866323672264355,\n",
       " -0.06339490956536437,\n",
       " -0.06592281142964855,\n",
       " -0.06845001865921828,\n",
       " -0.07097652048615513,\n",
       " -0.07350230597821598,\n",
       " -0.07602736403650276,\n",
       " -0.07855168339310671,\n",
       " -0.08107525260872744,\n",
       " -0.08359806007026654,\n",
       " -0.08612009398839604,\n",
       " -0.08864134239510169,\n",
       " -0.09116179314120117,\n",
       " -0.0936814338938374,\n",
       " -0.096200252133947,\n",
       " -0.09871823515370412,\n",
       " -0.1012353700539398,\n",
       " -0.10375164374153706,\n",
       " -0.1062670429268019,\n",
       " -0.10878155412081054,\n",
       " -0.11129516363273308,\n",
       " -0.11380785756713384,\n",
       " -0.11631962182124891,\n",
       " -0.11883044208224097,\n",
       " -0.12134030382443192,\n",
       " -0.12384919230651374,\n",
       " -0.12635709256873787,\n",
       " -0.1288639894300837,\n",
       " -0.1313698674854067,\n",
       " -0.13387471110256638,\n",
       " -0.13637850441953525,\n",
       " -0.13888123134148878,\n",
       " -0.14138287553787726,\n",
       " -0.1438834204394803,\n",
       " -0.14638284923544456,\n",
       " -0.1488811448703054,\n",
       " -0.15137829004099349,\n",
       " -0.15387426719382688,\n",
       " -0.15636905852148977,\n",
       " -0.15886264595999858,\n",
       " -0.16135501118565648,\n",
       " -0.16384613561199748,\n",
       " -0.16633600038672083,\n",
       " -0.16882458638861722,\n",
       " -0.17131187422448774,\n",
       " -0.17379784422605693,\n",
       " -0.17628247644688105,\n",
       " -0.1787657506592533,\n",
       " -0.18124764635110674,\n",
       " -0.18372814272291718,\n",
       " -0.18620721868460696,\n",
       " -0.18868485285245132,\n",
       " -0.19116102354598938,\n",
       " -0.19363570878494096,\n",
       " -0.19610888628613143,\n",
       " -0.19858053346042617,\n",
       " -0.20105062740967683,\n",
       " -0.2035191449236812,\n",
       " -0.2059860624771589,\n",
       " -0.20845135622674504,\n",
       " -0.2109150020080041,\n",
       " -0.2133769753324661,\n",
       " -0.21583725138468818,\n",
       " -0.2182958050193431,\n",
       " -0.220752610758338,\n",
       " -0.22320764278796587,\n",
       " -0.2256608749560923,\n",
       " -0.2281122807693806,\n",
       " -0.2305618333905579,\n",
       " -0.23300950563572595,\n",
       " -0.23545526997171856,\n",
       " -0.2378990985135102,\n",
       " -0.2403409630216782,\n",
       " -0.24278083489992214,\n",
       " -0.24521868519264411,\n",
       " -0.2476544845825934,\n",
       " -0.250088203388579,\n",
       " -0.2525198115632543,\n",
       " -0.2549492786909773,\n",
       " -0.25737657398575053,\n",
       " -0.2598016662892453,\n",
       " -0.2622245240689132,\n",
       " -0.2646451154161909,\n",
       " -0.26706340804480094,\n",
       " -0.2694793692891543,\n",
       " -0.27189296610285846,\n",
       " -0.2743041650573365,\n",
       " -0.27671293234056116,\n",
       " -0.27911923375590925,\n",
       " -0.28152303472114154,\n",
       " -0.2839243002675128,\n",
       " -0.2863229950390173,\n",
       " -0.2887190832917754,\n",
       " -0.2911125288935659,\n",
       " -0.2935032953235102,\n",
       " -0.29589134567191333,\n",
       " -0.2982766426402679,\n",
       " -0.3006591485414261,\n",
       " -0.303038825299946,\n",
       " -0.3054156344526174,\n",
       " -0.3077895371491735,\n",
       " -0.3101604941531945,\n",
       " -0.31252846584320804,\n",
       " -0.31489341221399364,\n",
       " -0.3172552928780968,\n",
       " -0.3196140670675582,\n",
       " -0.3219696936358651,\n",
       " -0.32432213106013025,\n",
       " -0.3266713374435047,\n",
       " -0.32901727051783036,\n",
       " -0.33135988764653884,\n",
       " -0.3336991458278021,\n",
       " -0.33603500169794115,\n",
       " -0.33836741153509897,\n",
       " -0.3406963312631829,\n",
       " -0.3430217164560829,\n",
       " -0.3453435223421716,\n",
       " -0.34766170380909056,\n",
       " -0.34997621540883017,\n",
       " -0.3522870113631069,\n",
       " -0.354594045569044,\n",
       " -0.3568972716051607,\n",
       " -0.35919664273767477,\n",
       " -0.36149211192712294,\n",
       " -0.36378363183530443,\n",
       " -0.36607115483255126,\n",
       " -0.36835463300532956,\n",
       " -0.37063401816417624,\n",
       " -0.372909261851974,\n",
       " -0.3751803153525681,\n",
       " -0.37744712969972827,\n",
       " -0.3797096556864582,\n",
       " -0.38196784387465454,\n",
       " -0.3842216446051179,\n",
       " -0.38647100800791734,\n",
       " -0.38871588401310947,\n",
       " -0.39095622236181266,\n",
       " -0.39319197261763705,\n",
       " -0.39542308417847033,\n",
       " -0.39764950628861795,\n",
       " -0.39987118805129773,\n",
       " -0.4020880784414864,\n",
       " -0.40430012631911627,\n",
       " -0.4065072804426195,\n",
       " -0.4087094894828166,\n",
       " -0.4109067020371451,\n",
       " -0.4130988666442246,\n",
       " -0.415285931798753,\n",
       " -0.41746784596672765,\n",
       " -0.4196445576009863,\n",
       " -0.4218160151570604,\n",
       " -0.4239821671093332,\n",
       " -0.4261429619674946,\n",
       " -0.42829834829328456,\n",
       " -0.4304482747175142,\n",
       " -0.4325926899573563,\n",
       " -0.43473154283389287,\n",
       " -0.4368647822899094,\n",
       " -0.4389923574079229,\n",
       " -0.44111421742843193,\n",
       " -0.443230311768374,\n",
       " -0.4453405900397778,\n",
       " -0.44744500206859394,\n",
       " -0.4495434979136906,\n",
       " -0.4516360278859967,\n",
       " -0.4537225425677773,\n",
       " -0.45580299283202313,\n",
       " -0.45787732986193813,\n",
       " -0.4599455051705049,\n",
       " -0.4620074706201115,\n",
       " -0.46406317844221884,\n",
       " -0.4661125812570505,\n",
       " -0.46815563209328404,\n",
       " -0.4701922844077237,\n",
       " -0.4722224921049347,\n",
       " -0.4742462095568169,\n",
       " -0.47626339162209735,\n",
       " -0.4782739936657205,\n",
       " -0.4802779715781136,\n",
       " -0.4822752817943064,\n",
       " -0.48426588131288284,\n",
       " -0.48624972771474284,\n",
       " -0.4882267791816525,\n",
       " -0.4901969945145608,\n",
       " -0.4921603331516606,\n",
       " -0.4941167551861732,\n",
       " -0.4960662213838343,\n",
       " -0.49800869320006036,\n",
       " -0.49994413279677463,\n",
       " -0.5018725030588724,\n",
       " -0.503793767610305,\n",
       " -0.5057078908297632,\n",
       " -0.5076148378659402,\n",
       " -0.5095145746523566,\n",
       " -0.5114070679217282,\n",
       " -0.5132922852198596,\n",
       " -0.515170194919047,\n",
       " -0.5170407662309737,\n",
       " -0.5189039692190832,\n",
       " -0.5207597748104151,\n",
       " -0.5226081548068909,\n",
       " -0.5244490818960352,\n",
       " -0.5262825296611223,\n",
       " -0.528108472590735,\n",
       " -0.5299268860877275,\n",
       " -0.5317377464775817,\n",
       " -0.5335410310161488,\n",
       " -0.5353367178967707,\n",
       " -0.5371247862567722,\n",
       " -0.5389052161833204,\n",
       " -0.5406779887186484,\n",
       " -0.5424430858646363,\n",
       " -0.5442004905867524,\n",
       " -0.5459501868173482,\n",
       " -0.5476921594583122,\n",
       " -0.5494263943830792,\n",
       " -0.5511528784379996,\n",
       " -0.5528715994430714,\n",
       " -0.5545825461920368,\n",
       " -0.5562857084518514,\n",
       " -0.5579810769615293,\n",
       " -0.5596686434303715,\n",
       " -0.5613484005355858,\n",
       " -0.5630203419193062,\n",
       " -0.5646844621850206,\n",
       " -0.5663407568934178,\n",
       " -0.567989222557664,\n",
       " -0.5696298566381208,\n",
       " -0.5712626575365157,\n",
       " -0.5728876245895805,\n",
       " -0.5745047580621684,\n",
       " -0.576114059139865,\n",
       " -0.5777155299211091,\n",
       " -0.5793091734088356,\n",
       " -0.5808949935016585,\n",
       " -0.5824729949846089,\n",
       " -0.5840431835194444,\n",
       " -0.5856055656345466,\n",
       " -0.5871601487144236,\n",
       " -0.5887069409888346,\n",
       " -0.5902459515215545,\n",
       " -0.5917771901987947,\n",
       " -0.5933006677172997,\n",
       " -0.594816395572135,\n",
       " -0.5963243860441856,\n",
       " -0.5978246521873829,\n",
       " -0.5993172078156767,\n",
       " -0.6008020674897703,\n",
       " -0.6022792465036372,\n",
       " -0.6037487608708355,\n",
       " -0.6052106273106385,\n",
       " -0.606664863233997,\n",
       " -0.608111486729353,\n",
       " -0.609550516548317,\n",
       " -0.6109819720912307,\n",
       " -0.6124058733926259,\n",
       " -0.6138222411065996,\n",
       " -0.6152310964921179,\n",
       " -0.6166324613982657,\n",
       " -0.6180263582494548,\n",
       " -0.6194128100306079,\n",
       " -0.6207918402723285,\n",
       " -0.6221634730360739,\n",
       " -0.6235277328993422,\n",
       " -0.6248846449408867,\n",
       " -0.6262342347259701,\n",
       " -0.6275765282916698,\n",
       " -0.6289115521322467,\n",
       " -0.6302393331845874,\n",
       " -0.6315598988137304,\n",
       " -0.6328732767984876,\n",
       " -0.6341794953171687,\n",
       " -0.6354785829334193,\n",
       " -0.6367705685821811,\n",
       " -0.6380554815557808,\n",
       " -0.6393333514901589,\n",
       " -0.6406042083512414,\n",
       " -0.6418680824214656,\n",
       " -0.6431250042864635,\n",
       " -0.64437500482191,\n",
       " -0.6456181151805421,\n",
       " -0.646854366779353,\n",
       " -0.6480837912869676,\n",
       " -0.6493064206112019,\n",
       " -0.6505222868868132,\n",
       " -0.6517314224634417,\n",
       " -0.6529338598937487,\n",
       " -0.654129631921755,\n",
       " -0.65531877147138,\n",
       " -0.656501311635186,\n",
       " -0.657677285663329,\n",
       " -0.6588467269527165,\n",
       " -0.6600096690363766,\n",
       " -0.6611661455730363,\n",
       " -0.6623161903369124,\n",
       " -0.6634598372077138,\n",
       " -0.6645971201608577,\n",
       " -0.6657280732578967,\n",
       " -0.6668527306371606,\n",
       " -0.6679711265046085,\n",
       " -0.6690832951248946,\n",
       " -0.6701892708126432,\n",
       " -0.6712890879239354,\n",
       " -0.6723827808480036,\n",
       " -0.673470383999135,\n",
       " -0.6745519318087798,\n",
       " -0.6756274587178662,\n",
       " -0.6766969991693166,\n",
       " -0.6777605876007659,\n",
       " -0.6788182584374788,\n",
       " -0.6798700460854648,\n",
       " -0.6809159849247868,\n",
       " -0.6819561093030639,\n",
       " -0.6829904535291638,\n",
       " -0.684019051867082,\n",
       " -0.6850419385300086,\n",
       " -0.6860591476745755,\n",
       " -0.6870707133952849,\n",
       " -0.6880766697191143,\n",
       " -0.6890770506002966,\n",
       " -0.6900718899152714,\n",
       " -0.6910612214578057,\n",
       " -0.6920450789342796,\n",
       " -0.6930234959591357,\n",
       " -0.6939965060504883,\n",
       " -0.6949641426258898,\n",
       " -0.6959264389982508,\n",
       " -0.696883428371912,\n",
       " -0.6978351438388637,\n",
       " -0.6987816183751105,\n",
       " -0.6997228848371786,\n",
       " -0.7006589759587614,\n",
       " -0.7015899243475022,\n",
       " -0.7025157624819098,\n",
       " -0.7034365227084041,\n",
       " -0.7043522372384892,\n",
       " -0.7052629381460512,\n",
       " -0.7061686573647775,\n",
       " -0.7070694266856943,\n",
       " -0.7079652777548202,\n",
       " -0.7088562420709332,\n",
       " -0.7097423509834476,\n",
       " -0.7106236356903982,\n",
       " -0.7115001272365296,\n",
       " -0.7123718565114872,\n",
       " -0.7132388542481084,\n",
       " -0.7141011510208097,\n",
       " -0.7149587772440686,\n",
       " -0.715811763170997,\n",
       " -0.7166601388920039,\n",
       " -0.7175039343335443,\n",
       " -0.7183431792569539,\n",
       " -0.7191779032573636,\n",
       " -0.7200081357626954,\n",
       " -0.7208339060327346,\n",
       " -0.7216552431582774,\n",
       " -0.7224721760603515,\n",
       " -0.7232847334895065,\n",
       " -0.7240929440251738,\n",
       " -0.7248968360750924,\n",
       " -0.7256964378747992,\n",
       " -0.7264917774871819,\n",
       " -0.7272828828020912,\n",
       " -0.7280697815360133,\n",
       " -0.7288525012317975,\n",
       " -0.7296310692584386,\n",
       " -0.7304055128109134,\n",
       " -0.7311758589100673,\n",
       " -0.7319421344025507,\n",
       " -0.7327043659608031,\n",
       " -0.7334625800830834,\n",
       " -0.734216803093545,\n",
       " -0.734967061142353,\n",
       " -0.735713380205844,\n",
       " -0.7364557860867248,\n",
       " -0.7371943044143102,\n",
       " -0.7379289606447977,\n",
       " -0.7386597800615778,\n",
       " -0.7393867877755788,\n",
       " -0.7401100087256441,\n",
       " -0.7408294676789424,\n",
       " -0.7415451892314068,\n",
       " -0.7422571978082045,\n",
       " -0.7429655176642342,\n",
       " -0.7436701728846501,\n",
       " -0.7443711873854121,\n",
       " -0.7450685849138606,\n",
       " -0.7457623890493155,\n",
       " -0.7464526232036964,\n",
       " -0.7471393106221667,\n",
       " -0.7478224743837959,\n",
       " -0.7485021374022441,\n",
       " -0.7491783224264635,\n",
       " -0.7498510520414184,\n",
       " -0.7505203486688227,\n",
       " -0.7511862345678928,\n",
       " -0.7518487318361173,\n",
       " -0.7525078624100398,\n",
       " -0.7531636480660562,\n",
       " -0.7538161104212256,\n",
       " -0.7544652709340932,\n",
       " -0.7551111509055244,\n",
       " -0.755753771479551,\n",
       " -0.756393153644227,\n",
       " -0.7570293182324942,\n",
       " -0.7576622859230577,\n",
       " -0.758292077241269,\n",
       " -0.7589187125600173,\n",
       " -0.7595422121006287,\n",
       " -0.7601625959337717,\n",
       " -0.7607798839803693,\n",
       " -0.7613940960125173,\n",
       " -0.7620052516544069,\n",
       " -0.7626133703832536,\n",
       " -0.7632184715302293,\n",
       " -0.763820574281399,\n",
       " -0.7644196976786615,\n",
       " -0.7650158606206919,\n",
       " -0.7656090818638885,\n",
       " -0.7661993800233209,\n",
       " -0.7667867735736805,\n",
       " -0.7673712808502332,\n",
       " -0.7679529200497722,\n",
       " -0.768531709231573,\n",
       " -0.7691076663183485,\n",
       " -0.7696808090972045,\n",
       " -0.7702511552205954,\n",
       " -0.7708187222072798,\n",
       " -0.7713835274432752,\n",
       " -0.7719455881828123,\n",
       " -0.7725049215492885,\n",
       " -0.7730615445362196,\n",
       " -0.7736154740081902,\n",
       " -0.7741667267018034,\n",
       " -0.7747153192266268,\n",
       " -0.775261268066138,\n",
       " -0.7758045895786668,\n",
       " -0.7763452999983353,\n",
       " -0.7768834154359958,\n",
       " -0.7774189518801647,\n",
       " -0.7779519251979545,\n",
       " -0.778482351136002,\n",
       " -0.7790102453213933,\n",
       " -0.7795356232625859,\n",
       " -0.7800585003503261,\n",
       " -0.7805788918585638,\n",
       " -0.7810968129453626,\n",
       " -0.7816122786538066,\n",
       " -0.7821253039129025,\n",
       " -0.782635903538478,\n",
       " -0.7831440922340758,\n",
       " -0.7836498845918428,\n",
       " -0.7841532950934156,\n",
       " -0.7846543381108005,\n",
       " -0.7851530279072501,\n",
       " -0.7856493786381333,\n",
       " -0.7861434043518029,\n",
       " -0.7866351189904558,\n",
       " -0.7871245363909908,\n",
       " -0.7876116702858591,\n",
       " -0.7880965343039114,\n",
       " -0.788579141971239,\n",
       " -0.7890595067120105,\n",
       " -0.7895376418493024,\n",
       " -0.7900135606059252,\n",
       " -0.7904872761052437,\n",
       " -0.7909588013719928,\n",
       " -0.791428149333087,\n",
       " -0.7918953328184251,\n",
       " -0.7923603645616892,\n",
       " -0.7928232572011389,\n",
       " -0.7932840232803992,\n",
       " -0.7937426752492435,\n",
       " -0.7941992254643712,\n",
       " -0.7946536861901791,\n",
       " -0.7951060695995285,\n",
       " -0.7955563877745053,\n",
       " -0.7960046527071762,\n",
       " -0.7964508763003381,\n",
       " -0.7968950703682622,\n",
       " -0.7973372466374337,\n",
       " -0.797777416747284,\n",
       " -0.7982155922509194,\n",
       " -0.7986517846158429,\n",
       " -0.7990860052246707,\n",
       " -0.7995182653758441,\n",
       " -0.799948576284335,\n",
       " -0.8003769490823456,\n",
       " -0.8008033948200037,\n",
       " -0.8012279244660523,\n",
       " -0.8016505489085326,\n",
       " -0.8020712789554632,\n",
       " -0.8024901253355124,\n",
       " -0.8029070986986666,\n",
       " -0.8033222096168918,\n",
       " -0.8037354685847907,\n",
       " -0.8041468860202543,\n",
       " -0.8045564722651078,\n",
       " -0.8049642375857519,\n",
       " -0.8053701921737976,\n",
       " -0.8057743461466973,\n",
       " -0.8061767095483695,\n",
       " -0.8065772923498188,\n",
       " -0.8069761044497502,\n",
       " -0.8073731556751793,\n",
       " -0.8077684557820359,\n",
       " -0.808162014455764,\n",
       " -0.8085538413119152,\n",
       " -0.8089439458967387,\n",
       " -0.8093323376877647,\n",
       " -0.809719026094384,\n",
       " -0.8101040204584217,\n",
       " -0.8104873300547069,\n",
       " -0.8108689640916367,\n",
       " -0.8112489317117357,\n",
       " -0.8116272419922108,\n",
       " -0.8120039039455005,\n",
       " -0.8123789265198201,\n",
       " -0.8127523185997023,\n",
       " -0.8131240890065322,\n",
       " -0.8134942464990784,\n",
       " -0.8138627997740192,\n",
       " -0.8142297574664641,\n",
       " -0.8145951281504705,\n",
       " -0.8149589203395566,\n",
       " -0.815321142487209,\n",
       " -0.8156818029873859,\n",
       " -0.8160409101750163,\n",
       " -0.8163984723264947,\n",
       " -0.8167544976601703,\n",
       " -0.8171089943368339,\n",
       " -0.8174619704601986,\n",
       " -0.8178134340773772,\n",
       " -0.8181633931793545,\n",
       " -0.8185118557014568,\n",
       " -0.8188588295238158,\n",
       " -0.819204322471829,\n",
       " -0.8195483423166153,\n",
       " -0.8198908967754683,\n",
       " -0.820231993512303,\n",
       " -0.8205716401381005,\n",
       " -0.8209098442113474,\n",
       " -0.8212466132384725,\n",
       " -0.8215819546742781,\n",
       " -0.8219158759223686,\n",
       " -0.8222483843355748,\n",
       " -0.8225794872163741,\n",
       " -0.8229091918173073,\n",
       " -0.8232375053413914,\n",
       " -0.823564434942529,\n",
       " -0.8238899877259133,\n",
       " -0.8242141707484301,\n",
       " -0.824536991019056,\n",
       " -0.8248584554992527,\n",
       " -0.8251785711033582,\n",
       " -0.8254973446989738,\n",
       " -0.8258147831073483,\n",
       " -0.8261308931037582,\n",
       " -0.8264456814178845,\n",
       " -0.8267591547341866,\n",
       " -0.8270713196922719,\n",
       " -0.8273821828872628,\n",
       " -0.8276917508701602,\n",
       " -0.8280000301482033,\n",
       " -0.8283070271852265,\n",
       " -0.8286127484020133,\n",
       " -0.828917200176646,\n",
       " -0.8292203888448536,\n",
       " -0.8295223207003553,\n",
       " -0.8298230019952013,\n",
       " -0.8301224389401113,\n",
       " -0.8304206377048083,\n",
       " -0.8307176044183509,\n",
       " -0.8310133451694615,\n",
       " -0.8313078660068522,\n",
       " -0.8316011729395478,\n",
       " -0.8318932719372049,\n",
       " -0.832184168930429,\n",
       " -0.8324738698110887,\n",
       " -0.8327623804326267,\n",
       " -0.833049706610368,\n",
       " -0.8333358541218255,\n",
       " -0.8336208287070026,\n",
       " -0.8339046360686931,\n",
       " -0.834187281872779,\n",
       " -0.8344687717485243,\n",
       " -0.8347491112888674,\n",
       " -0.8350283060507102,\n",
       " -0.8353063615552045,\n",
       " -0.8355832832880365,\n",
       " -0.8358590766997077,\n",
       " -0.8361337472058142,\n",
       " -0.8364073001873229,\n",
       " -0.8366797409908454,\n",
       " -0.8369510749289095,\n",
       " -0.837221307280228,\n",
       " -0.8374904432899654,\n",
       " -0.8377584881700015,\n",
       " -0.8380254470991942,\n",
       " -0.8382913252236378,\n",
       " -0.8385561276569204,\n",
       " -0.8388198594803791,\n",
       " -0.8390825257433514,\n",
       " -0.8393441314634265,\n",
       " -0.8396046816266922,\n",
       " -0.8398641811879816,\n",
       " -0.8401226350711155,\n",
       " -0.8403800481691445,\n",
       " -0.8406364253445876,\n",
       " -0.8408917714296698,\n",
       " -0.841146091226556,\n",
       " -0.841399389507585,\n",
       " -0.8416516710154988,\n",
       " -0.8419029404636723,\n",
       " -0.8421532025363395,\n",
       " -0.8424024618888178,\n",
       " -0.842650723147731,\n",
       " -0.8428979909112294,\n",
       " -0.8431442697492085,\n",
       " -0.8433895642035261,\n",
       " -0.8436338787882163,\n",
       " -0.8438772179897027,\n",
       " -0.8441195862670097,\n",
       " -0.8443609880519706,\n",
       " -0.8446014277494356,\n",
       " -0.8448409097374768,\n",
       " -0.8450794383675919,\n",
       " -0.8453170179649057,\n",
       " -0.8455536528283703,\n",
       " -0.8457893472309631,\n",
       " -0.8460241054198835,\n",
       " -0.8462579316167472,\n",
       " -0.8464908300177796,\n",
       " -0.8467228047940067,\n",
       " -0.8469538600914451,\n",
       " -0.8471840000312897,\n",
       " -0.8474132287100998,\n",
       " -0.8476415501999844,\n",
       " -0.8478689685487846,\n",
       " -0.8480954877802551,\n",
       " -0.8483211118942444,\n",
       " -0.8485458448668729,\n",
       " -0.8487696906507094,\n",
       " -0.8489926531749464,\n",
       " -0.8492147363455742,\n",
       " -0.8494359440455521,\n",
       " -0.8496562801349798,\n",
       " -0.8498757484512665,\n",
       " -0.8500943528092979,\n",
       " -0.8503120970016034,\n",
       " -0.8505289847985199,\n",
       " -0.850745019948356,\n",
       " -0.8509602061775535,\n",
       " -0.8511745471908478,\n",
       " -0.8513880466714276,\n",
       " -0.851600708281092,\n",
       " -0.8518125356604075,\n",
       " -0.8520235324288626,\n",
       " -0.852233702185022,\n",
       " -0.8524430485066786,\n",
       " -0.852651574951005,\n",
       " -0.8528592850547027,\n",
       " -0.8530661823341513,\n",
       " -0.8532722702855555,\n",
       " -0.853477552385091,\n",
       " -0.8536820320890491,\n",
       " -0.8538857128339807,\n",
       " -0.8540885980368381,\n",
       " -0.8542906910951162,\n",
       " -0.8544919953869927,\n",
       " -0.8546925142714663,\n",
       " -0.8548922510884945,\n",
       " -0.8550912091591298,\n",
       " -0.8552893917856551,\n",
       " -0.8554868022517172,\n",
       " -0.8556834438224605,\n",
       " -0.8558793197446581,\n",
       " -0.856074433246843,\n",
       " -0.8562687875394369,\n",
       " -0.8564623858148795,\n",
       " -0.8566552312477557,\n",
       " -0.8568473269949214,\n",
       " -0.8570386761956293,\n",
       " -0.857229281971653,\n",
       " -0.8574191474274101,\n",
       " -0.8576082756500842,\n",
       " -0.8577966697097466,\n",
       " -0.8579843326594753,\n",
       " -0.8581712675354753,\n",
       " -0.8583574773571961,\n",
       " -0.8585429651274489,\n",
       " -0.8587277338325225,\n",
       " -0.8589117864422993,\n",
       " -0.8590951259103684,\n",
       " -0.8592777551741398,\n",
       " -0.8594596771549565,\n",
       " -0.8596408947582057,\n",
       " -0.8598214108734293,\n",
       " -0.860001228374434,\n",
       " -0.8601803501193993,\n",
       " -0.8603587789509857,\n",
       " -0.8605365176964415,\n",
       " -0.8607135691677088,\n",
       " -0.8608899361615286,\n",
       " -0.8610656214595455,\n",
       " -0.8612406278284103,\n",
       " -0.8614149580198834,\n",
       " -0.8615886147709363,\n",
       " -0.8617616008038521,\n",
       " -0.8619339188263264,\n",
       " -0.8621055715315659,\n",
       " -0.8622765615983871,\n",
       " -0.862446891691314,\n",
       " -0.8626165644606753,\n",
       " -0.8627855825426995,\n",
       " -0.8629539485596117,\n",
       " -0.8631216651197268,\n",
       " -0.8632887348175439,\n",
       " -0.8634551602338394,\n",
       " -0.8636209439357592,\n",
       " -0.8637860884769099,\n",
       " -0.8639505963974501,\n",
       " -0.8641144702241801,\n",
       " -0.8642777124706315,\n",
       " -0.8644403256371556,\n",
       " -0.8646023122110115,\n",
       " -0.8647636746664532,\n",
       " -0.8649244154648164,\n",
       " -0.8650845370546038,\n",
       " -0.8652440418715706,\n",
       " -0.865402932338809,\n",
       " -0.8655612108668322,\n",
       " -0.8657188798536568,\n",
       " -0.8658759416848859,\n",
       " -0.8660323987337907,\n",
       " -0.8661882533613918,\n",
       " -0.8663435079165397,\n",
       " -0.8664981647359944,\n",
       " -0.8666522261445052,\n",
       " -0.8668056944548889,\n",
       " -0.8669585719681078,\n",
       " -0.8671108609733476,\n",
       " -0.8672625637480935,\n",
       " -0.8674136825582072,\n",
       " -0.8675642196580017,\n",
       " -0.8677141772903169,\n",
       " -0.8678635576865937,\n",
       " -0.8680123630669481,\n",
       " -0.868160595640244,\n",
       " -0.8683082576041664,\n",
       " -0.8684553511452935,\n",
       " -0.8686018784391678,\n",
       " -0.8687478416503677,\n",
       " -0.8688932429325775,\n",
       " -0.8690380844286576,\n",
       " -0.8691823682707137,\n",
       " -0.8693260965801659,\n",
       " -0.8694692714678167,\n",
       " -0.869611895033919,\n",
       " -0.8697539693682432,\n",
       " -0.8698954965501442,\n",
       " -0.8700364786486272,\n",
       " -0.870176917722414,\n",
       " -0.8703168158200079,\n",
       " -0.8704561749797581,\n",
       " -0.8705949972299248,\n",
       " -0.8707332845887421,\n",
       " -0.8708710390644815,\n",
       " -0.8710082626555149,\n",
       " -0.8711449573503767,\n",
       " -0.8712811251278255,\n",
       " -0.8714167679569059,\n",
       " -0.871551887797009,\n",
       " -0.8716864865979327,\n",
       " -0.871820566299942,\n",
       " -0.8719541288338285,\n",
       " -0.8720871761209694,\n",
       " -0.872219710073386,\n",
       " -0.8723517325938017,\n",
       " -0.8724832455757006,\n",
       " -0.8726142509033836,\n",
       " -0.8727447504520265,\n",
       " -0.8728747460877353,\n",
       " -0.8730042396676031,\n",
       " -0.8731332330397654,\n",
       " -0.8732617280434553,\n",
       " -0.8733897265090579,\n",
       " -0.8735172302581656,\n",
       " -0.8736442411036313,\n",
       " -0.8737707608496221,\n",
       " -0.8738967912916727,\n",
       " -0.8740223342167381,\n",
       " -0.8741473914032458,\n",
       " -0.8742719646211481,\n",
       " -0.8743960556319733,\n",
       " -0.8745196661888776,\n",
       " -0.8746427980366952,\n",
       " -0.8747654529119895,\n",
       " -0.8748876325431025,\n",
       " -0.8750093386502052,\n",
       " -0.8751305729453466,\n",
       " -0.8752513371325026,\n",
       " -0.8753716329076252,\n",
       " -0.8754914619586903,\n",
       " -0.8756108259657459,\n",
       " -0.8757297266009596,\n",
       " -0.875848165528666,\n",
       " -0.8759661444054132,\n",
       " -0.87608366488001,\n",
       " -0.8762007285935716,\n",
       " -0.8763173371795658,\n",
       " -0.8764334922638586,\n",
       " -0.876549195464759,\n",
       " -0.8766644483930645,\n",
       " -0.8767792526521052,\n",
       " -0.8768936098377885,\n",
       " -0.8770075215386427,\n",
       " -0.8771209893358607,\n",
       " -0.8772340148033434,\n",
       " -0.8773465995077427,\n",
       " -0.8774587450085043,\n",
       " -0.8775704528579097,\n",
       " -0.8776817246011187,\n",
       " -0.877792561776211,\n",
       " -0.8779029659142278,\n",
       " -0.8780129385392128,\n",
       " -0.8781224811682535,\n",
       " -0.8782315953115213,\n",
       " -0.8783402824723121,\n",
       " -0.8784485441470864,\n",
       " -0.8785563818255089,\n",
       " -0.878663796990488,\n",
       " -0.8787707911182152,\n",
       " -0.8788773656782036,\n",
       " -0.8789835221333271,\n",
       " -0.8790892619398581,\n",
       " -0.8791945865475063,\n",
       " -0.8792994973994559,\n",
       " -0.8794039959324038,\n",
       " -0.8795080835765964,\n",
       " -0.8796117617558668,\n",
       " -0.8797150318876715,\n",
       " -0.8798178953831274,\n",
       " -0.8799203536470472,\n",
       " -0.8800224080779765,\n",
       " -0.8801240600682285,\n",
       " -0.8802253110039204,\n",
       " -0.880326162265008,\n",
       " -0.8804266152253213,\n",
       " -0.8805266712525988,\n",
       " -0.8806263317085223,\n",
       " -0.8807255979487514,\n",
       " -0.880824471322957,\n",
       " -0.8809229531748558,\n",
       " -0.8810210448422434,\n",
       " -0.8811187476570279,\n",
       " -0.8812160629452631,\n",
       " -0.8813129920271812,\n",
       " -0.8814095362172256,\n",
       " -0.8815056968240835,\n",
       " -0.8816014751507181,\n",
       " -0.8816968724944002,\n",
       " -0.8817918901467408,\n",
       " -0.8818865293937218,\n",
       " -0.8819807915157283,\n",
       " -0.8820746777875789,\n",
       " -0.8821681894785571,\n",
       " -0.8822613278524423,\n",
       " -0.8823540941675395,\n",
       " -0.8824464896767106,\n",
       " -0.8825385156274037,\n",
       " -0.8826301732616837,\n",
       " -0.8827214638162613,\n",
       " -0.8828123885225231,\n",
       " -0.8829029486065608,\n",
       " -0.8829931452892001,\n",
       " -0.8830829797860298,\n",
       " -0.8831724533074304,\n",
       " -0.8832615670586029,\n",
       " -0.8833503222395969,\n",
       " -0.8834387200453385,\n",
       " -0.8835267616656592,\n",
       " -0.8836144482853228,\n",
       " -0.883701781084053,\n",
       " -0.8837887612365615,\n",
       " -0.8838753899125745,\n",
       " -0.8839616682768604,\n",
       " -0.8840475974892561,\n",
       " -0.884133178704694,\n",
       " -0.8842184130732285,\n",
       " -0.8843033017400622,\n",
       " -0.8843878458455723,\n",
       " -0.8844720465253363,\n",
       " -0.884555904910158,\n",
       " -0.8846394221260934,\n",
       " -0.8847225992944755,\n",
       " -0.8848054375319404,\n",
       " -0.884887937950452,\n",
       " -0.8849701016573269,\n",
       " -0.8850519297552597,\n",
       " -0.8851334233423472,\n",
       " -0.8852145835121131,\n",
       " -0.8852954113535324,\n",
       " -0.8853759079510553,\n",
       " -0.8854560743846315,\n",
       " -0.8855359117297342,\n",
       " -0.8856154210573834,\n",
       " -0.8856946034341697,\n",
       " -0.8857734599222777,\n",
       " -0.8858519915795094,\n",
       " -0.8859301994593068,\n",
       " -0.8860080846107756,\n",
       " -0.8860856480787073,\n",
       " -0.8861628909036022,\n",
       " -0.8862398141216923,\n",
       " -0.8863164187649627,\n",
       " -0.8863927058611748,\n",
       " -0.8864686764338878,\n",
       " -0.886544331502481,\n",
       " -0.8866196720821752,\n",
       " -0.8866946991840549,\n",
       " -0.8867694138150892,\n",
       " -0.8868438169781538,\n",
       " -0.886917909672052,\n",
       " -0.8869916928915355,\n",
       " -0.8870651676273259,\n",
       " -0.8871383348661355,\n",
       " -0.8872111955906875,\n",
       " -0.8872837507797372,\n",
       " -0.8873560014080921,\n",
       " -0.8874279484466323,\n",
       " -0.8874995928623309,\n",
       " -0.8875709356182737,\n",
       " -0.8876419776736796,\n",
       " -0.88771271998392,\n",
       " -0.8877831635005387,\n",
       " -0.8878533091712714,\n",
       " -0.8879231579400654,\n",
       " -0.8879927107470986,\n",
       " -0.8880619685287989,\n",
       " -0.8881309322178633,\n",
       " -0.8881996027432766,\n",
       " -0.8882679810303304,\n",
       " -0.8883360680006424,\n",
       " -0.8884038645721738,\n",
       " -0.888471371659249,\n",
       " -0.8885385901725734,\n",
       " -0.8886055210192516,\n",
       " -0.888672165102806,\n",
       " -0.8887385233231944,\n",
       " -0.888804596576828,\n",
       " -0.8888703857565895,\n",
       " -0.8889358917518507,\n",
       " -0.88900111544849,\n",
       " -0.88906605772891,\n",
       " -0.8891307194720547,\n",
       " -0.8891951015534271,\n",
       " -0.8892592048451061,\n",
       " -0.8893230302157638,\n",
       " -0.8893865785306821,\n",
       " -0.8894498506517702,\n",
       " -0.8895128474375806,\n",
       " -0.8895755697433264,\n",
       " -0.8896380184208975,\n",
       " -0.8897001943188773,\n",
       " -0.8897620982825585,\n",
       " -0.8898237311539602,\n",
       " -0.8898850937718435,\n",
       " -0.8899461869717277,\n",
       " -0.8900070115859061,\n",
       " -0.8900675684434622,\n",
       " -0.8901278583702853,\n",
       " -0.890187882189086,\n",
       " -0.8902476407194121,\n",
       " -0.8903071347776639,\n",
       " -0.8903663651771097,\n",
       " -0.8904253327279009,\n",
       " ...]"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "e4238082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.99521622, 0.99882567, 0.99885875, 0.99868819, 0.99887888,\n",
       "        0.99613319]),\n",
       " array([0.99043303, 0.99765149, 0.99771776, 0.99737656, 0.99775792,\n",
       "        0.99226707]),\n",
       " array([0.98565045, 0.99647746, 0.99657703, 0.9960651 , 0.99663713,\n",
       "        0.98840164]),\n",
       " array([0.98086849, 0.99530358, 0.99543658, 0.99475383, 0.9955165 ,\n",
       "        0.98453693]),\n",
       " array([0.97608716, 0.99412986, 0.9942964 , 0.99344274, 0.99439603,\n",
       "        0.98067295]),\n",
       " array([0.97130646, 0.9929563 , 0.99315649, 0.99213184, 0.99327574,\n",
       "        0.97680969]),\n",
       " array([0.96652641, 0.99178289, 0.99201687, 0.99082114, 0.99215562,\n",
       "        0.97294719]),\n",
       " array([0.96174702, 0.99060966, 0.99087753, 0.98951062, 0.99103567,\n",
       "        0.96908544]),\n",
       " array([0.95696829, 0.98943658, 0.98973848, 0.98820031, 0.9899159 ,\n",
       "        0.96522446]),\n",
       " array([0.95219025, 0.98826368, 0.98859973, 0.9868902 , 0.98879631,\n",
       "        0.96136427]),\n",
       " array([0.94741291, 0.98709095, 0.98746127, 0.98558029, 0.98767691,\n",
       "        0.95750487]),\n",
       " array([0.94263627, 0.9859184 , 0.98632312, 0.98427059, 0.98655769,\n",
       "        0.95364628]),\n",
       " array([0.93786034, 0.98474602, 0.98518528, 0.9829611 , 0.98543866,\n",
       "        0.94978851]),\n",
       " array([0.93308515, 0.98357382, 0.98404774, 0.98165182, 0.98431983,\n",
       "        0.94593158]),\n",
       " array([0.92831069, 0.98240181, 0.98291053, 0.98034277, 0.98320119,\n",
       "        0.94207549]),\n",
       " array([0.92353699, 0.98122999, 0.98177363, 0.97903393, 0.98208275,\n",
       "        0.93822027]),\n",
       " array([0.91876406, 0.98005835, 0.98063706, 0.97772532, 0.98096451,\n",
       "        0.93436592]),\n",
       " array([0.9139919 , 0.97888691, 0.97950082, 0.97641694, 0.97984648,\n",
       "        0.93051247]),\n",
       " array([0.90922054, 0.97771566, 0.97836492, 0.9751088 , 0.97872866,\n",
       "        0.92665991]),\n",
       " array([0.90444999, 0.97654462, 0.97722936, 0.97380089, 0.97761105,\n",
       "        0.92280828]),\n",
       " array([0.89968025, 0.97537377, 0.97609414, 0.97249322, 0.97649365,\n",
       "        0.91895758]),\n",
       " array([0.89491135, 0.97420314, 0.97495927, 0.97118579, 0.97537647,\n",
       "        0.91510783]),\n",
       " array([0.8901433 , 0.97303271, 0.97382476, 0.96987862, 0.97425952,\n",
       "        0.91125904]),\n",
       " array([0.88537612, 0.9718625 , 0.97269061, 0.96857169, 0.97314279,\n",
       "        0.90741123]),\n",
       " array([0.8806098 , 0.9706925 , 0.97155683, 0.96726503, 0.97202629,\n",
       "        0.90356442]),\n",
       " array([0.87584439, 0.96952273, 0.97042341, 0.96595863, 0.97091002,\n",
       "        0.89971862]),\n",
       " array([0.87107987, 0.96835317, 0.96929038, 0.96465249, 0.969794  ,\n",
       "        0.89587384]),\n",
       " array([0.86631629, 0.96718385, 0.96815773, 0.96334662, 0.96867821,\n",
       "        0.89203011]),\n",
       " array([0.86155364, 0.96601476, 0.96702546, 0.96204102, 0.96756267,\n",
       "        0.88818744]),\n",
       " array([0.85679194, 0.9648459 , 0.96589359, 0.96073571, 0.96644737,\n",
       "        0.88434585]),\n",
       " array([0.85203122, 0.96367729, 0.96476212, 0.95943068, 0.96533233,\n",
       "        0.88050535]),\n",
       " array([0.84727149, 0.96250891, 0.96363106, 0.95812594, 0.96421754,\n",
       "        0.87666597]),\n",
       " array([0.84251276, 0.96134079, 0.9625004 , 0.95682149, 0.96310302,\n",
       "        0.87282771]),\n",
       " array([0.83775505, 0.96017291, 0.96137017, 0.95551733, 0.96198876,\n",
       "        0.86899061]),\n",
       " array([0.83299838, 0.9590053 , 0.96024036, 0.95421349, 0.96087477,\n",
       "        0.86515467]),\n",
       " array([0.82824276, 0.95783794, 0.95911098, 0.95290994, 0.95976105,\n",
       "        0.86131991]),\n",
       " array([0.82348823, 0.95667085, 0.95798203, 0.95160672, 0.95864762,\n",
       "        0.85748637]),\n",
       " array([0.81873478, 0.95550402, 0.95685353, 0.95030381, 0.95753446,\n",
       "        0.85365404]),\n",
       " array([0.81398245, 0.95433747, 0.95572547, 0.94900122, 0.95642159,\n",
       "        0.84982296]),\n",
       " array([0.80923124, 0.9531712 , 0.95459787, 0.94769896, 0.95530901,\n",
       "        0.84599314]),\n",
       " array([0.80448119, 0.95200521, 0.95347074, 0.94639704, 0.95419673,\n",
       "        0.84216461]),\n",
       " array([0.79973231, 0.95083951, 0.95234407, 0.94509545, 0.95308475,\n",
       "        0.83833738]),\n",
       " array([0.79498461, 0.9496741 , 0.95121788, 0.94379421, 0.95197308,\n",
       "        0.83451148]),\n",
       " array([0.79023813, 0.94850899, 0.95009217, 0.94249332, 0.95086171,\n",
       "        0.83068692]),\n",
       " array([0.78549288, 0.94734418, 0.94896695, 0.94119279, 0.94975066,\n",
       "        0.82686373]),\n",
       " array([0.78074888, 0.94617968, 0.94784223, 0.93989262, 0.94863993,\n",
       "        0.82304193]),\n",
       " array([0.77600615, 0.94501549, 0.94671801, 0.93859282, 0.94752953,\n",
       "        0.81922154]),\n",
       " array([0.77126471, 0.94385161, 0.94559431, 0.9372934 , 0.94641946,\n",
       "        0.81540259]),\n",
       " array([0.76652459, 0.94268806, 0.94447112, 0.93599435, 0.94530972,\n",
       "        0.81158509]),\n",
       " array([0.76178582, 0.94152484, 0.94334846, 0.93469569, 0.94420033,\n",
       "        0.80776907]),\n",
       " array([0.7570484 , 0.94036195, 0.94222634, 0.93339743, 0.94309128,\n",
       "        0.80395456]),\n",
       " array([0.75231237, 0.93919941, 0.94110475, 0.93209956, 0.94198259,\n",
       "        0.80014157]),\n",
       " array([0.74757775, 0.9380372 , 0.93998372, 0.9308021 , 0.94087425,\n",
       "        0.79633014]),\n",
       " array([0.74284456, 0.93687535, 0.93886324, 0.92950505, 0.93976628,\n",
       "        0.79252028]),\n",
       " array([0.73811282, 0.93571385, 0.93774333, 0.92820843, 0.93865868,\n",
       "        0.78871203]),\n",
       " array([0.73338257, 0.93455272, 0.93662399, 0.92691223, 0.93755145,\n",
       "        0.7849054 ]),\n",
       " array([0.72865383, 0.93339195, 0.93550524, 0.92561646, 0.9364446 ,\n",
       "        0.78110042]),\n",
       " array([0.72392662, 0.93223156, 0.93438707, 0.92432113, 0.93533815,\n",
       "        0.77729713]),\n",
       " array([0.71920097, 0.93107155, 0.93326951, 0.92302626, 0.93423208,\n",
       "        0.77349554]),\n",
       " array([0.7144769 , 0.92991193, 0.93215255, 0.92173183, 0.93312642,\n",
       "        0.76969568]),\n",
       " array([0.70975445, 0.9287527 , 0.93103621, 0.92043787, 0.93202116,\n",
       "        0.76589758]),\n",
       " array([0.70503364, 0.92759388, 0.92992049, 0.91914438, 0.93091631,\n",
       "        0.76210127]),\n",
       " array([0.70031449, 0.92643546, 0.92880541, 0.91785137, 0.92981189,\n",
       "        0.75830678]),\n",
       " array([0.69559704, 0.92527745, 0.92769097, 0.91655885, 0.92870789,\n",
       "        0.75451413]),\n",
       " array([0.69088132, 0.92411986, 0.92657719, 0.91526682, 0.92760432,\n",
       "        0.75072336]),\n",
       " array([0.68616735, 0.9229627 , 0.92546406, 0.91397529, 0.9265012 ,\n",
       "        0.74693448]),\n",
       " array([0.68145516, 0.92180598, 0.92435161, 0.91268427, 0.92539852,\n",
       "        0.74314754]),\n",
       " array([0.67674479, 0.9206497 , 0.92323984, 0.91139376, 0.92429629,\n",
       "        0.73936257]),\n",
       " array([0.67203627, 0.91949387, 0.92212876, 0.91010379, 0.92319452,\n",
       "        0.73557959]),\n",
       " array([0.66732962, 0.91833849, 0.92101838, 0.90881435, 0.92209323,\n",
       "        0.73179863]),\n",
       " array([0.66262488, 0.91718358, 0.91990871, 0.90752545, 0.9209924 ,\n",
       "        0.72801973]),\n",
       " array([0.65792208, 0.91602914, 0.91879976, 0.90623711, 0.91989206,\n",
       "        0.72424292]),\n",
       " array([0.65322125, 0.91487518, 0.91769154, 0.90494933, 0.91879222,\n",
       "        0.72046823]),\n",
       " array([0.64852243, 0.91372171, 0.91658407, 0.90366212, 0.91769287,\n",
       "        0.7166957 ]),\n",
       " array([0.64382565, 0.91256874, 0.91547734, 0.90237548, 0.91659402,\n",
       "        0.71292535]),\n",
       " array([0.63913095, 0.91141626, 0.91437138, 0.90108944, 0.91549569,\n",
       "        0.70915723]),\n",
       " array([0.63443835, 0.91026431, 0.91326619, 0.899804  , 0.91439788,\n",
       "        0.70539137]),\n",
       " array([0.62974791, 0.90911287, 0.91216179, 0.89851917, 0.91330061,\n",
       "        0.70162779]),\n",
       " array([0.62505964, 0.90796196, 0.91105818, 0.89723495, 0.91220387,\n",
       "        0.69786655]),\n",
       " array([0.62037359, 0.90681159, 0.90995538, 0.89595136, 0.91110768,\n",
       "        0.69410767]),\n",
       " array([0.6156898 , 0.90566177, 0.90885339, 0.89466842, 0.91001204,\n",
       "        0.69035119]),\n",
       " array([0.61100831, 0.9045125 , 0.90775224, 0.89338612, 0.90891697,\n",
       "        0.68659714]),\n",
       " array([0.60632914, 0.9033638 , 0.90665192, 0.89210448, 0.90782247,\n",
       "        0.68284557]),\n",
       " array([0.60165235, 0.90221567, 0.90555246, 0.8908235 , 0.90672855,\n",
       "        0.67909652]),\n",
       " array([0.59697797, 0.90106813, 0.90445386, 0.88954321, 0.90563523,\n",
       "        0.67535001]),\n",
       " array([0.59230604, 0.89992118, 0.90335614, 0.88826361, 0.9045425 ,\n",
       "        0.6716061 ]),\n",
       " array([0.5876366 , 0.89877484, 0.9022593 , 0.88698472, 0.90345039,\n",
       "        0.66786482]),\n",
       " array([0.5829697 , 0.8976291 , 0.90116337, 0.88570654, 0.90235889,\n",
       "        0.66412621]),\n",
       " array([0.57830538, 0.89648399, 0.90006835, 0.88442908, 0.90126802,\n",
       "        0.66039031]),\n",
       " array([0.57364367, 0.89533952, 0.89897426, 0.88315236, 0.90017779,\n",
       "        0.65665716]),\n",
       " array([0.56898463, 0.89419569, 0.89788111, 0.88187638, 0.89908821,\n",
       "        0.6529268 ]),\n",
       " array([0.5643283 , 0.89305251, 0.89678891, 0.88060117, 0.89799929,\n",
       "        0.64919929]),\n",
       " array([0.55967472, 0.89190999, 0.89569767, 0.87932673, 0.89691104,\n",
       "        0.64547465]),\n",
       " array([0.55502394, 0.89076816, 0.89460741, 0.87805307, 0.89582346,\n",
       "        0.64175294]),\n",
       " array([0.550376  , 0.88962701, 0.89351815, 0.87678021, 0.89473657,\n",
       "        0.6380342 ]),\n",
       " array([0.54573096, 0.88848655, 0.89242988, 0.87550816, 0.89365038,\n",
       "        0.63431846]),\n",
       " array([0.54108885, 0.88734681, 0.89134264, 0.87423693, 0.89256491,\n",
       "        0.63060579]),\n",
       " array([0.53644974, 0.88620778, 0.89025643, 0.87296653, 0.89148015,\n",
       "        0.62689622]),\n",
       " array([0.53181366, 0.88506949, 0.88917127, 0.87169699, 0.89039612,\n",
       "        0.6231898 ]),\n",
       " array([0.52718067, 0.88393194, 0.88808717, 0.8704283 , 0.88931284,\n",
       "        0.61948658]),\n",
       " array([0.52255082, 0.88279515, 0.88700414, 0.86916049, 0.88823032,\n",
       "        0.61578661]),\n",
       " array([0.51792416, 0.88165913, 0.88592221, 0.86789357, 0.88714855,\n",
       "        0.61208992]),\n",
       " array([0.51330074, 0.88052388, 0.88484138, 0.86662755, 0.88606757,\n",
       "        0.60839658]),\n",
       " array([0.50868061, 0.87938943, 0.88376167, 0.86536244, 0.88498738,\n",
       "        0.60470664]),\n",
       " array([0.50406384, 0.87825578, 0.88268309, 0.86409827, 0.88390798,\n",
       "        0.60102013]),\n",
       " array([0.49945047, 0.87712296, 0.88160567, 0.86283504, 0.8828294 ,\n",
       "        0.59733712]),\n",
       " array([0.49484055, 0.87599096, 0.88052941, 0.86157277, 0.88175164,\n",
       "        0.59365765]),\n",
       " array([0.49023415, 0.87485981, 0.87945433, 0.86031147, 0.88067473,\n",
       "        0.58998178]),\n",
       " array([0.48563133, 0.87372951, 0.87838044, 0.85905116, 0.87959866,\n",
       "        0.58630956]),\n",
       " array([0.48103213, 0.87260009, 0.87730777, 0.85779186, 0.87852345,\n",
       "        0.58264104]),\n",
       " array([0.47643662, 0.87147155, 0.87623633, 0.85653358, 0.87744912,\n",
       "        0.57897628]),\n",
       " array([0.47184486, 0.87034391, 0.87516613, 0.85527633, 0.87637568,\n",
       "        0.57531532]),\n",
       " array([0.46725691, 0.86921719, 0.87409719, 0.85402013, 0.87530314,\n",
       "        0.57165824]),\n",
       " array([0.46267283, 0.86809139, 0.87302953, 0.852765  , 0.87423152,\n",
       "        0.56800507]),\n",
       " array([0.45809267, 0.86696653, 0.87196316, 0.85151095, 0.87316082,\n",
       "        0.56435589]),\n",
       " array([0.45351651, 0.86584263, 0.87089811, 0.850258  , 0.87209106,\n",
       "        0.56071074]),\n",
       " array([0.44894441, 0.8647197 , 0.86983438, 0.84900616, 0.87102227,\n",
       "        0.55706968]),\n",
       " array([0.44437643, 0.86359776, 0.868772  , 0.84775546, 0.86995444,\n",
       "        0.55343277]),\n",
       " array([0.43981263, 0.86247682, 0.86771098, 0.84650591, 0.86888759,\n",
       "        0.54980008]),\n",
       " array([0.43525309, 0.8613569 , 0.86665134, 0.84525752, 0.86782174,\n",
       "        0.54617165]),\n",
       " array([0.43069786, 0.86023801, 0.86559309, 0.84401032, 0.86675691,\n",
       "        0.54254756]),\n",
       " array([0.42614703, 0.85912017, 0.86453627, 0.84276431, 0.8656931 ,\n",
       "        0.53892786]),\n",
       " array([0.42160065, 0.85800339, 0.86348087, 0.84151953, 0.86463033,\n",
       "        0.53531261]),\n",
       " array([0.41705879, 0.8568877 , 0.86242693, 0.84027599, 0.86356862,\n",
       "        0.53170187]),\n",
       " array([0.41252153, 0.8557731 , 0.86137446, 0.8390337 , 0.86250798,\n",
       "        0.52809572]),\n",
       " array([0.40798894, 0.85465962, 0.86032347, 0.83779268, 0.86144843,\n",
       "        0.5244942 ]),\n",
       " array([0.40346108, 0.85354726, 0.859274  , 0.83655295, 0.86038998,\n",
       "        0.52089739]),\n",
       " array([0.39893804, 0.85243606, 0.85822604, 0.83531454, 0.85933265,\n",
       "        0.51730536]),\n",
       " array([0.39441988, 0.85132602, 0.85717964, 0.83407745, 0.85827645,\n",
       "        0.51371815]),\n",
       " array([0.38990669, 0.85021717, 0.85613479, 0.83284172, 0.85722141,\n",
       "        0.51013586]),\n",
       " array([0.38539853, 0.84910951, 0.85509154, 0.83160735, 0.85616752,\n",
       "        0.50655853]),\n",
       " array([0.38089549, 0.84800307, 0.85404988, 0.83037437, 0.85511482,\n",
       "        0.50298623]),\n",
       " array([0.37639763, 0.84689787, 0.85300985, 0.8291428 , 0.85406332,\n",
       "        0.49941904]),\n",
       " array([0.37190504, 0.84579393, 0.85197145, 0.82791265, 0.85301303,\n",
       "        0.49585702]),\n",
       " array([0.36741781, 0.84469125, 0.85093472, 0.82668395, 0.85196398,\n",
       "        0.49230025]),\n",
       " array([0.36293599, 0.84358987, 0.84989968, 0.82545673, 0.85091617,\n",
       "        0.48874878]),\n",
       " array([0.35845969, 0.8424898 , 0.84886633, 0.82423099, 0.84986963,\n",
       "        0.4852027 ]),\n",
       " array([0.35398898, 0.84139106, 0.84783471, 0.82300676, 0.84882437,\n",
       "        0.48166207]),\n",
       " array([0.34952394, 0.84029367, 0.84680483, 0.82178406, 0.84778041,\n",
       "        0.47812697]),\n",
       " array([0.34506466, 0.83919764, 0.84577671, 0.82056291, 0.84673776,\n",
       "        0.47459746]),\n",
       " array([0.34061122, 0.83810301, 0.84475038, 0.81934333, 0.84569645,\n",
       "        0.47107362]),\n",
       " array([0.33616371, 0.83700978, 0.84372585, 0.81812535, 0.8446565 ,\n",
       "        0.46755552]),\n",
       " array([0.33172221, 0.83591798, 0.84270315, 0.81690899, 0.84361791,\n",
       "        0.46404324]),\n",
       " array([0.3272868 , 0.83482762, 0.8416823 , 0.81569426, 0.84258072,\n",
       "        0.46053685]),\n",
       " array([0.32285759, 0.83373874, 0.84066331, 0.81448119, 0.84154493,\n",
       "        0.45703642]),\n",
       " array([0.31843465, 0.83265134, 0.83964622, 0.81326981, 0.84051056,\n",
       "        0.45354204]),\n",
       " array([0.31401807, 0.83156545, 0.83863104, 0.81206013, 0.83947764,\n",
       "        0.45005377]),\n",
       " array([0.30960795, 0.8304811 , 0.83761779, 0.81085218, 0.83844618,\n",
       "        0.44657169]),\n",
       " array([0.30520437, 0.82939829, 0.8366065 , 0.80964598, 0.83741621,\n",
       "        0.44309588]),\n",
       " array([0.30080743, 0.82831706, 0.83559718, 0.80844155, 0.83638773,\n",
       "        0.43962642]),\n",
       " array([0.29641722, 0.82723742, 0.83458987, 0.80723892, 0.83536077,\n",
       "        0.43616339]),\n",
       " array([0.29203384, 0.82615939, 0.83358457, 0.80603811, 0.83433535,\n",
       "        0.43270686]),\n",
       " array([0.28765737, 0.82508301, 0.83258132, 0.80483914, 0.83331149,\n",
       "        0.42925691]),\n",
       " array([0.28328791, 0.82400828, 0.83158013, 0.80364204, 0.8322892 ,\n",
       "        0.42581362]),\n",
       " array([0.27892556, 0.82293524, 0.83058103, 0.80244683, 0.83126852,\n",
       "        0.42237707]),\n",
       " array([0.27457041, 0.82186389, 0.82958404, 0.80125353, 0.83024944,\n",
       "        0.41894734]),\n",
       " array([0.27022256, 0.82079428, 0.82858919, 0.80006218, 0.82923201,\n",
       "        0.41552451]),\n",
       " array([0.26588211, 0.81972641, 0.82759649, 0.79887278, 0.82821623,\n",
       "        0.41210866]),\n",
       " array([0.26154915, 0.81866032, 0.82660597, 0.79768537, 0.82720212,\n",
       "        0.40869988]),\n",
       " array([0.25722379, 0.81759602, 0.82561765, 0.79649998, 0.82618971,\n",
       "        0.40529824]),\n",
       " array([0.25290612, 0.81653353, 0.82463156, 0.79531662, 0.82517902,\n",
       "        0.40190382]),\n",
       " array([0.24859624, 0.81547289, 0.82364771, 0.79413532, 0.82417007,\n",
       "        0.39851671]),\n",
       " array([0.24429425, 0.81441411, 0.82266613, 0.79295611, 0.82316287,\n",
       "        0.39513699]),\n",
       " array([0.24000026, 0.81335721, 0.82168684, 0.79177901, 0.82215744,\n",
       "        0.39176474]),\n",
       " array([0.23571437, 0.81230223, 0.82070987, 0.79060404, 0.82115382,\n",
       "        0.38840005]),\n",
       " array([0.23143667, 0.81124918, 0.81973524, 0.78943123, 0.82015201,\n",
       "        0.38504299]),\n",
       " array([0.22716727, 0.81019809, 0.81876297, 0.78826062, 0.81915205,\n",
       "        0.38169365]),\n",
       " array([0.22290627, 0.80914898, 0.81779309, 0.78709221, 0.81815394,\n",
       "        0.37835212]),\n",
       " array([0.21865378, 0.80810187, 0.81682561, 0.78592604, 0.81715771,\n",
       "        0.37501847]),\n",
       " array([0.2144099 , 0.80705679, 0.81586056, 0.78476213, 0.81616339,\n",
       "        0.37169279]),\n",
       " array([0.21017473, 0.80601377, 0.81489797, 0.78360051, 0.81517098,\n",
       "        0.36837516]),\n",
       " array([0.20594838, 0.80497282, 0.81393786, 0.7824412 , 0.81418052,\n",
       "        0.36506567]),\n",
       " array([0.20173095, 0.80393398, 0.81298025, 0.78128423, 0.81319202,\n",
       "        0.3617644 ]),\n",
       " array([0.19752254, 0.80289726, 0.81202516, 0.78012962, 0.81220551,\n",
       "        0.35847144]),\n",
       " array([0.19332327, 0.80186269, 0.81107262, 0.77897741, 0.81122101,\n",
       "        0.35518686]),\n",
       " array([0.18913324, 0.80083029, 0.81012264, 0.77782761, 0.81023853,\n",
       "        0.35191075]),\n",
       " array([0.18495254, 0.7998001 , 0.80917526, 0.77668025, 0.8092581 ,\n",
       "        0.34864319]),\n",
       " array([0.1807813 , 0.79877212, 0.8082305 , 0.77553536, 0.80827974,\n",
       "        0.34538427]),\n",
       " array([0.17661961, 0.79774639, 0.80728837, 0.77439296, 0.80730347,\n",
       "        0.34213407]),\n",
       " array([0.17246758, 0.79672294, 0.80634891, 0.77325309, 0.80632931,\n",
       "        0.33889268]),\n",
       " array([0.16832531, 0.79570178, 0.80541213, 0.77211575, 0.80535728,\n",
       "        0.33566017]),\n",
       " array([0.16419291, 0.79468294, 0.80447806, 0.77098099, 0.80438741,\n",
       "        0.33243662]),\n",
       " array([0.16007049, 0.79366645, 0.80354671, 0.76984882, 0.80341971,\n",
       "        0.32922213]),\n",
       " array([0.15595815, 0.79265232, 0.80261812, 0.76871928, 0.8024542 ,\n",
       "        0.32601677]),\n",
       " array([0.151856  , 0.79164059, 0.8016923 , 0.76759238, 0.80149091,\n",
       "        0.32282063]),\n",
       " array([0.14776414, 0.79063128, 0.80076928, 0.76646815, 0.80052986,\n",
       "        0.31963378]),\n",
       " array([0.14368267, 0.78962441, 0.79984908, 0.76534663, 0.79957107,\n",
       "        0.3164563 ]),\n",
       " array([0.13961171, 0.78862   , 0.79893171, 0.76422782, 0.79861455,\n",
       "        0.31328829]),\n",
       " array([0.13555135, 0.78761809, 0.79801721, 0.76311177, 0.79766034,\n",
       "        0.31012981]),\n",
       " array([0.13150171, 0.78661869, 0.7971056 , 0.76199849, 0.79670844,\n",
       "        0.30698094]),\n",
       " array([0.12746287, 0.78562183, 0.79619689, 0.76088801, 0.79575888,\n",
       "        0.30384177]),\n",
       " array([0.12343495, 0.78462753, 0.7952911 , 0.75978035, 0.79481169,\n",
       "        0.30071238]),\n",
       " array([0.11941806, 0.78363581, 0.79438827, 0.75867554, 0.79386687,\n",
       "        0.29759283]),\n",
       " array([0.11541228, 0.78264671, 0.7934884 , 0.7575736 , 0.79292446,\n",
       "        0.29448322]),\n",
       " array([0.11141773, 0.78166024, 0.79259153, 0.75647457, 0.79198446,\n",
       "        0.29138361]),\n",
       " array([0.1074345 , 0.78067643, 0.79169766, 0.75537845, 0.79104691,\n",
       "        0.28829408]),\n",
       " array([0.10346269, 0.7796953 , 0.79080683, 0.75428529, 0.79011182,\n",
       "        0.28521471]),\n",
       " array([0.09950241, 0.77871687, 0.78991905, 0.75319509, 0.78917921,\n",
       "        0.28214557]),\n",
       " array([0.09555376, 0.77774117, 0.78903434, 0.75210789, 0.7882491 ,\n",
       "        0.27908674]),\n",
       " array([0.09161682, 0.77676821, 0.78815272, 0.75102371, 0.7873215 ,\n",
       "        0.27603828]),\n",
       " array([0.0876917 , 0.77579803, 0.78727421, 0.74994257, 0.78639645,\n",
       "        0.27300028]),\n",
       " array([0.0837785 , 0.77483064, 0.78639883, 0.74886449, 0.78547395,\n",
       "        0.26997279]),\n",
       " array([0.07987731, 0.77386607, 0.78552661, 0.74778951, 0.78455403,\n",
       "        0.2669559 ]),\n",
       " array([0.07598823, 0.77290433, 0.78465755, 0.74671763, 0.7836367 ,\n",
       "        0.26394967]),\n",
       " array([0.07211134, 0.77194546, 0.78379167, 0.74564889, 0.78272199,\n",
       "        0.26095417]),\n",
       " array([0.06824674, 0.77098947, 0.782929  , 0.74458331, 0.78180991,\n",
       "        0.25796946]),\n",
       " array([0.06439453, 0.77003637, 0.78206956, 0.7435209 , 0.78090047,\n",
       "        0.25499562]),\n",
       " array([0.06055479, 0.76908621, 0.78121335, 0.7424617 , 0.77999371,\n",
       "        0.25203271]),\n",
       " array([0.05672761, 0.76813898, 0.7803604 , 0.74140571, 0.77908963,\n",
       "        0.24908079]),\n",
       " array([0.05291309, 0.76719473, 0.77951073, 0.74035297, 0.77818825,\n",
       "        0.24613992]),\n",
       " array([0.04911129, 0.76625345, 0.77866434, 0.73930349, 0.77728959,\n",
       "        0.24321017]),\n",
       " array([0.04532232, 0.76531519, 0.77782127, 0.7382573 , 0.77639366,\n",
       "        0.24029161]),\n",
       " array([0.04154626, 0.76437994, 0.77698152, 0.73721441, 0.77550049,\n",
       "        0.23738428]),\n",
       " array([0.03778318, 0.76344774, 0.7761451 , 0.73617485, 0.77461009,\n",
       "        0.23448824]),\n",
       " array([0.03403318, 0.76251861, 0.77531204, 0.73513863, 0.77372247,\n",
       "        0.23160356]),\n",
       " array([0.03029632, 0.76159255, 0.77448236, 0.73410578, 0.77283765,\n",
       "        0.22873029]),\n",
       " array([0.02657269, 0.7606696 , 0.77365605, 0.7330763 , 0.77195565,\n",
       "        0.22586849]),\n",
       " array([0.02286236, 0.75974976, 0.77283315, 0.73205023, 0.77107647,\n",
       "        0.2230182 ]),\n",
       " array([0.01916542, 0.75883306, 0.77201365, 0.73102758, 0.77020015,\n",
       "        0.22017948]),\n",
       " array([0.01548192, 0.75791951, 0.77119758, 0.73000837, 0.76932668,\n",
       "        0.21735238]),\n",
       " array([0.01181195, 0.75700914, 0.77038496, 0.72899261, 0.76845609,\n",
       "        0.21453695]),\n",
       " array([0.00815558, 0.75610195, 0.76957578, 0.72798032, 0.76758839,\n",
       "        0.21173323]),\n",
       " array([0.00451287, 0.75519796, 0.76877006, 0.72697152, 0.7667236 ,\n",
       "        0.20894128]),\n",
       " array([0.00088388, 0.75429719, 0.76796783, 0.72596623, 0.76586171,\n",
       "        0.20616113]),\n",
       " array([-0.0027313 ,  0.75339965,  0.76716908,  0.72496446,  0.76500276,\n",
       "         0.20339283]),\n",
       " array([-0.00633263,  0.75250537,  0.76637382,  0.72396622,  0.76414675,\n",
       "         0.20063643]),\n",
       " array([-0.00992004,  0.75161434,  0.76558208,  0.72297154,  0.76329369,\n",
       "         0.19789196]),\n",
       " array([-0.01349348,  0.7507266 ,  0.76479386,  0.72198042,  0.76244359,\n",
       "         0.19515945]),\n",
       " array([-0.01705287,  0.74984214,  0.76400917,  0.72099288,  0.76159648,\n",
       "         0.19243896]),\n",
       " array([-0.02059817,  0.748961  ,  0.76322801,  0.72000894,  0.76075235,\n",
       "         0.18973051]),\n",
       " array([-0.02412933,  0.74808317,  0.7624504 ,  0.71902861,  0.75991123,\n",
       "         0.18703414]),\n",
       " array([-0.02764629,  0.74720867,  0.76167635,  0.71805189,  0.75907311,\n",
       "         0.18434988]),\n",
       " array([-0.031149  ,  0.74633751,  0.76090587,  0.71707881,  0.75823801,\n",
       "         0.18167776]),\n",
       " array([-0.03463741,  0.74546971,  0.76013895,  0.71610937,  0.75740595,\n",
       "         0.17901781]),\n",
       " array([-0.03811148,  0.74460527,  0.75937562,  0.71514359,  0.75657692,\n",
       "         0.17637007]),\n",
       " array([-0.04157116,  0.74374421,  0.75861587,  0.71418148,  0.75575094,\n",
       "         0.17373454]),\n",
       " array([-0.04501642,  0.74288653,  0.75785971,  0.71322305,  0.75492802,\n",
       "         0.17111127]),\n",
       " array([-0.0484472 ,  0.74203225,  0.75710716,  0.7122683 ,  0.75410817,\n",
       "         0.16850027]),\n",
       " array([-0.05186348,  0.74118138,  0.7563582 ,  0.71131725,  0.75329139,\n",
       "         0.16590156]),\n",
       " array([-0.05526522,  0.74033393,  0.75561286,  0.71036991,  0.75247769,\n",
       "         0.16331516]),\n",
       " array([-0.05865237,  0.73948989,  0.75487113,  0.70942629,  0.75166708,\n",
       "         0.1607411 ]),\n",
       " array([-0.06202492,  0.73864929,  0.75413302,  0.70848639,  0.75085957,\n",
       "         0.15817939]),\n",
       " array([-0.06538283,  0.73781213,  0.75339853,  0.70755022,  0.75005516,\n",
       "         0.15563004]),\n",
       " array([-0.06872608,  0.73697841,  0.75266767,  0.7066178 ,  0.74925386,\n",
       "         0.15309306]),\n",
       " array([-0.07205462,  0.73614814,  0.75194043,  0.70568912,  0.74845567,\n",
       "         0.15056848]),\n",
       " array([-0.07536846,  0.73532134,  0.75121682,  0.70476419,  0.7476606 ,\n",
       "         0.14805629]),\n",
       " array([-0.07866755,  0.734498  ,  0.75049685,  0.70384302,  0.74686866,\n",
       "         0.14555651]),\n",
       " array([-0.08195189,  0.73367813,  0.74978051,  0.70292562,  0.74607984,\n",
       "         0.14306914]),\n",
       " array([-0.08522144,  0.73286173,  0.74906781,  0.70201199,  0.74529416,\n",
       "         0.1405942 ]),\n",
       " array([-0.08847621,  0.73204882,  0.74835874,  0.70110213,  0.74451162,\n",
       "         0.13813167]),\n",
       " array([-0.09171617,  0.73123939,  0.74765331,  0.70019605,  0.74373222,\n",
       "         0.13568158]),\n",
       " array([-0.09494132,  0.73043344,  0.74695152,  0.69929375,  0.74295596,\n",
       "         0.1332439 ]),\n",
       " array([-0.09815163,  0.72963099,  0.74625336,  0.69839524,  0.74218284,\n",
       "         0.13081866]),\n",
       " array([-0.10134711,  0.72883202,  0.74555884,  0.69750051,  0.74141288,\n",
       "         0.12840583]),\n",
       " array([-0.10452775,  0.72803655,  0.74486795,  0.69660958,  0.74064606,\n",
       "         0.12600543]),\n",
       " array([-0.10769355,  0.72724458,  0.74418069,  0.69572244,  0.7398824 ,\n",
       "         0.12361744]),\n",
       " array([-0.1108445 ,  0.72645611,  0.74349707,  0.69483909,  0.7391219 ,\n",
       "         0.12124185]),\n",
       " array([-0.1139806 ,  0.72567113,  0.74281707,  0.69395953,  0.73836454,\n",
       "         0.11887867]),\n",
       " array([-0.11710186,  0.72488965,  0.74214069,  0.69308377,  0.73761034,\n",
       "         0.11652787]),\n",
       " array([-0.12020828,  0.72411168,  0.74146793,  0.69221181,  0.7368593 ,\n",
       "         0.11418945]),\n",
       " array([-0.12329987,  0.72333719,  0.74079879,  0.69134364,  0.73611141,\n",
       "         0.11186339]),\n",
       " array([-0.12637663,  0.72256621,  0.74013327,  0.69047926,  0.73536668,\n",
       "         0.10954969]),\n",
       " array([-0.12943857,  0.72179872,  0.73947135,  0.68961868,  0.7346251 ,\n",
       "         0.10724832]),\n",
       " array([-0.1324857 ,  0.72103473,  0.73881303,  0.68876188,  0.73388667,\n",
       "         0.10495927]),\n",
       " array([-0.13551804,  0.72027423,  0.73815832,  0.68790888,  0.7331514 ,\n",
       "         0.10268252]),\n",
       " array([-0.13853561,  0.71951722,  0.73750719,  0.68705966,  0.73241927,\n",
       "         0.10041805]),\n",
       " array([-0.14153841,  0.71876369,  0.73685966,  0.68621422,  0.73169029,\n",
       "         0.09816584]),\n",
       " array([-0.14452647,  0.71801365,  0.7362157 ,  0.68537256,  0.73096445,\n",
       "         0.09592587]),\n",
       " array([-0.1474998 ,  0.71726709,  0.73557532,  0.68453468,  0.73024175,\n",
       "         0.09369811]),\n",
       " array([-0.15045843,  0.716524  ,  0.7349385 ,  0.68370057,  0.7295222 ,\n",
       "         0.09148254]),\n",
       " array([-0.15340239,  0.71578439,  0.73430524,  0.68287023,  0.72880577,\n",
       "         0.08927914]),\n",
       " array([-0.15633169,  0.71504824,  0.73367554,  0.68204365,  0.72809248,\n",
       "         0.08708788]),\n",
       " array([-0.15924636,  0.71431555,  0.73304937,  0.68122083,  0.72738231,\n",
       "         0.08490873]),\n",
       " array([-0.16214643,  0.71358631,  0.73242675,  0.68040176,  0.72667527,\n",
       "         0.08274165]),\n",
       " array([-0.16503193,  0.71286053,  0.73180765,  0.67958643,  0.72597134,\n",
       "         0.08058663]),\n",
       " array([-0.1679029 ,  0.71213819,  0.73119207,  0.67877484,  0.72527052,\n",
       "         0.07844362]),\n",
       " array([-0.17075936,  0.71141928,  0.73057999,  0.67796699,  0.72457281,\n",
       "         0.07631261]),\n",
       " array([-0.17360134,  0.7107038 ,  0.72997142,  0.67716286,  0.72387821,\n",
       "         0.07419354]),\n",
       " array([-0.17642889,  0.70999175,  0.72936633,  0.67636245,  0.7231867 ,\n",
       "         0.07208639]),\n",
       " array([-0.17924203,  0.70928311,  0.72876473,  0.67556574,  0.72249827,\n",
       "         0.06999113]),\n",
       " array([-0.18204082,  0.70857788,  0.72816659,  0.67477275,  0.72181293,\n",
       "         0.06790771]),\n",
       " array([-0.18482527,  0.70787604,  0.72757192,  0.67398344,  0.72113067,\n",
       "         0.0658361 ]),\n",
       " array([-0.18759545,  0.7071776 ,  0.72698069,  0.67319782,  0.72045147,\n",
       "         0.06377626]),\n",
       " array([-0.19035139,  0.70648254,  0.72639289,  0.67241587,  0.71977534,\n",
       "         0.06172815]),\n",
       " array([-0.19309312,  0.70579084,  0.72580853,  0.6716376 ,  0.71910226,\n",
       "         0.05969173]),\n",
       " array([-0.19582071,  0.70510252,  0.72522757,  0.67086297,  0.71843223,\n",
       "         0.05766697]),\n",
       " array([-0.19853418,  0.70441754,  0.72465002,  0.670092  ,  0.71776524,\n",
       "         0.05565381]),\n",
       " array([-0.20123359,  0.70373591,  0.72407586,  0.66932466,  0.71710128,\n",
       "         0.05365221]),\n",
       " array([-0.20391899,  0.70305761,  0.72350508,  0.66856096,  0.71644034,\n",
       "         0.05166214]),\n",
       " array([-0.20659042,  0.70238263,  0.72293766,  0.66780086,  0.71578242,\n",
       "         0.04968354]),\n",
       " array([-0.20924793,  0.70171096,  0.7223736 ,  0.66704437,  0.7151275 ,\n",
       "         0.04771638]),\n",
       " array([-0.21189158,  0.7010426 ,  0.72181287,  0.66629148,  0.71447558,\n",
       "         0.04576061]),\n",
       " array([-0.21452141,  0.70037752,  0.72125548,  0.66554217,  0.71382664,\n",
       "         0.04381618]),\n",
       " array([-0.21713749,  0.69971572,  0.72070139,  0.66479643,  0.71318069,\n",
       "         0.04188304]),\n",
       " array([-0.21973985,  0.69905719,  0.72015061,  0.66405424,  0.7125377 ,\n",
       "         0.03996115]),\n",
       " array([-0.22232856,  0.69840192,  0.71960312,  0.66331561,  0.71189767,\n",
       "         0.03805046]),\n",
       " array([-0.22490367,  0.69774989,  0.7190589 ,  0.66258051,  0.71126059,\n",
       "         0.03615092]),\n",
       " array([-0.22746523,  0.69710109,  0.71851794,  0.66184893,  0.71062645,\n",
       "         0.03426248]),\n",
       " array([-0.23001331,  0.6964555 ,  0.71798022,  0.66112086,  0.70999523,\n",
       "         0.03238509]),\n",
       " array([-0.23254796,  0.69581313,  0.71744574,  0.66039629,  0.70936694,\n",
       "         0.0305187 ]),\n",
       " array([-0.23506924,  0.69517394,  0.71691447,  0.6596752 ,  0.70874155,\n",
       "         0.02866327]),\n",
       " array([-0.2375772 ,  0.69453794,  0.71638641,  0.65895759,  0.70811906,\n",
       "         0.02681873]),\n",
       " array([-0.24007191,  0.6939051 ,  0.71586153,  0.65824342,  0.70749945,\n",
       "         0.02498504]),\n",
       " array([-0.24255343,  0.69327542,  0.71533983,  0.65753271,  0.70688272,\n",
       "         0.02316215]),\n",
       " array([-0.24502182,  0.69264888,  0.71482129,  0.65682542,  0.70626885,\n",
       "         0.02135   ]),\n",
       " array([-0.24747713,  0.69202546,  0.7143059 ,  0.65612154,  0.70565784,\n",
       "         0.01954854]),\n",
       " array([-0.24991944,  0.69140516,  0.71379363,  0.65542107,  0.70504967,\n",
       "         0.01775772]),\n",
       " array([-0.2523488 ,  0.69078796,  0.71328448,  0.65472399,  0.70444432,\n",
       "         0.01597749]),\n",
       " array([-0.25476527,  0.69017384,  0.71277843,  0.65403027,  0.7038418 ,\n",
       "         0.01420778]),\n",
       " array([-0.25716893,  0.6895628 ,  0.71227546,  0.65333992,  0.70324208,\n",
       "         0.01244855]),\n",
       " array([-0.25955982,  0.68895481,  0.71177556,  0.65265291,  0.70264516,\n",
       "         0.01069974]),\n",
       " array([-0.26193803,  0.68834987,  0.71127871,  0.65196923,  0.70205101,\n",
       "         0.0089613 ]),\n",
       " array([-0.2643036 ,  0.68774796,  0.7107849 ,  0.65128886,  0.70145964,\n",
       "         0.00723317]),\n",
       " array([-0.26665661,  0.68714906,  0.71029412,  0.65061179,  0.70087103,\n",
       "         0.0055153 ]),\n",
       " array([-0.26899712,  0.68655316,  0.70980634,  0.64993801,  0.70028516,\n",
       "         0.00380762]),\n",
       " array([-0.2713252 ,  0.68596025,  0.70932155,  0.64926749,  0.69970203,\n",
       "         0.0021101 ]),\n",
       " array([-2.73640914e-01,  6.85370302e-01,  7.08839741e-01,  6.48600231e-01,\n",
       "         6.99121620e-01,  4.22665658e-04]),\n",
       " array([-0.27594433,  0.68478331,  0.70836089,  0.64793621,  0.69854392,\n",
       "        -0.00125473]),\n",
       " array([-0.2782355 ,  0.68419927,  0.70788498,  0.64727541,  0.69796891,\n",
       "        -0.00292215]),\n",
       " array([-0.28051451,  0.68361815,  0.70741201,  0.64661781,  0.69739659,\n",
       "        -0.00457965]),\n",
       " array([-0.28278142,  0.68303994,  0.70694194,  0.64596341,  0.69682694,\n",
       "        -0.00622728]),\n",
       " array([-0.28503629,  0.68246462,  0.70647477,  0.64531218,  0.69625995,\n",
       "        -0.00786511]),\n",
       " array([-0.2872792 ,  0.68189219,  0.70601048,  0.64466412,  0.69569561,\n",
       "        -0.00949317]),\n",
       " array([-0.28951021,  0.68132263,  0.70554906,  0.6440192 ,  0.69513389,\n",
       "        -0.01111154]),\n",
       " array([-0.29172938,  0.68075591,  0.70509049,  0.6433774 ,  0.6945748 ,\n",
       "        -0.01272027]),\n",
       " array([-0.29393679,  0.68019203,  0.70463475,  0.64273872,  0.69401832,\n",
       "        -0.01431942]),\n",
       " array([-0.29613251,  0.67963097,  0.70418183,  0.64210314,  0.69346443,\n",
       "        -0.01590903]),\n",
       " array([-0.29831659,  0.67907271,  0.70373171,  0.64147064,  0.69291312,\n",
       "        -0.01748918]),\n",
       " array([-0.30048912,  0.67851725,  0.70328437,  0.6408412 ,  0.69236438,\n",
       "        -0.01905991]),\n",
       " array([-0.30265015,  0.67796455,  0.70283981,  0.64021482,  0.69181819,\n",
       "        -0.02062128]),\n",
       " array([-0.30479975,  0.67741462,  0.70239799,  0.63959146,  0.69127455,\n",
       "        -0.02217335]),\n",
       " array([-0.306938  ,  0.67686743,  0.70195892,  0.63897113,  0.69073344,\n",
       "        -0.02371617]),\n",
       " array([-0.30906496,  0.67632297,  0.70152257,  0.63835379,  0.69019484,\n",
       "        -0.02524981]),\n",
       " array([-0.3111807 ,  0.67578122,  0.70108892,  0.63773944,  0.68965874,\n",
       "        -0.02677431]),\n",
       " array([-0.31328528,  0.67524216,  0.70065796,  0.63712805,  0.68912514,\n",
       "        -0.02828974]),\n",
       " array([-0.31537878,  0.67470579,  0.70022968,  0.63651962,  0.68859401,\n",
       "        -0.02979614]),\n",
       " array([-0.31746126,  0.67417209,  0.69980406,  0.63591413,  0.68806535,\n",
       "        -0.03129358]),\n",
       " array([-0.31953278,  0.67364103,  0.69938108,  0.63531155,  0.68753913,\n",
       "        -0.03278211]),\n",
       " array([-0.32159343,  0.67311261,  0.69896072,  0.63471188,  0.68701536,\n",
       "        -0.03426179]),\n",
       " array([-0.32364326,  0.67258682,  0.69854298,  0.6341151 ,  0.686494  ,\n",
       "        -0.03573267]),\n",
       " array([-0.32568234,  0.67206362,  0.69812783,  0.63352119,  0.68597506,\n",
       "        -0.03719481]),\n",
       " array([-0.32771074,  0.67154302,  0.69771526,  0.63293014,  0.68545852,\n",
       "        -0.03864827]),\n",
       " array([-0.32972853,  0.67102499,  0.69730526,  0.63234192,  0.68494436,\n",
       "        -0.04009309]),\n",
       " array([-0.33173577,  0.67050952,  0.6968978 ,  0.63175653,  0.68443257,\n",
       "        -0.04152934]),\n",
       " array([-0.33373253,  0.66999659,  0.69649288,  0.63117395,  0.68392314,\n",
       "        -0.04295707]),\n",
       " array([-0.33571888,  0.66948619,  0.69609047,  0.63059415,  0.68341605,\n",
       "        -0.04437633]),\n",
       " array([-0.33769488,  0.6689783 ,  0.69569056,  0.63001714,  0.6829113 ,\n",
       "        -0.04578718]),\n",
       " array([-0.33966059,  0.66847291,  0.69529314,  0.62944288,  0.68240887,\n",
       "        -0.04718968]),\n",
       " array([-0.3416161 ,  0.66797001,  0.6948982 ,  0.62887136,  0.68190874,\n",
       "        -0.04858387]),\n",
       " array([-0.34356145,  0.66746957,  0.6945057 ,  0.62830258,  0.6814109 ,\n",
       "        -0.04996981]),\n",
       " array([-0.34549673,  0.66697158,  0.69411565,  0.6277365 ,  0.68091535,\n",
       "        -0.05134756]),\n",
       " array([-0.34742198,  0.66647603,  0.69372802,  0.62717312,  0.68042206,\n",
       "        -0.05271717]),\n",
       " array([-0.34933728,  0.6659829 ,  0.69334281,  0.62661242,  0.67993103,\n",
       "        -0.05407869]),\n",
       " array([-0.3512427 ,  0.66549219,  0.69295998,  0.62605439,  0.67944223,\n",
       "        -0.05543217]),\n",
       " array([-0.35313829,  0.66500386,  0.69257954,  0.625499  ,  0.67895567,\n",
       "        -0.05677768]),\n",
       " array([-0.35502413,  0.66451791,  0.69220146,  0.62494625,  0.67847132,\n",
       "        -0.05811526]),\n",
       " array([-0.35690027,  0.66403432,  0.69182574,  0.62439611,  0.67798917,\n",
       "        -0.05944496]),\n",
       " array([-0.35876678,  0.66355308,  0.69145234,  0.62384857,  0.67750921,\n",
       "        -0.06076683]),\n",
       " array([-0.36062372,  0.66307418,  0.69108127,  0.62330362,  0.67703143,\n",
       "        -0.06208094]),\n",
       " array([-0.36247116,  0.66259759,  0.6907125 ,  0.62276124,  0.67655581,\n",
       "        -0.06338732]),\n",
       " array([-0.36430916,  0.66212331,  0.69034603,  0.62222142,  0.67608235,\n",
       "        -0.06468604]),\n",
       " array([-0.36613778,  0.66165132,  0.68998183,  0.62168413,  0.67561102,\n",
       "        -0.06597714]),\n",
       " array([-0.36795709,  0.6611816 ,  0.68961989,  0.62114937,  0.67514182,\n",
       "        -0.06726067]),\n",
       " array([-0.36976715,  0.66071415,  0.6892602 ,  0.62061712,  0.67467473,\n",
       "        -0.06853669]),\n",
       " array([-0.37156801,  0.66024894,  0.68890274,  0.62008736,  0.67420974,\n",
       "        -0.06980524]),\n",
       " array([-0.37335975,  0.65978596,  0.6885475 ,  0.61956008,  0.67374685,\n",
       "        -0.07106638]),\n",
       " array([-0.37514242,  0.6593252 ,  0.68819446,  0.61903527,  0.67328602,\n",
       "        -0.07232015]),\n",
       " array([-0.37691608,  0.65886664,  0.68784361,  0.6185129 ,  0.67282727,\n",
       "        -0.07356661]),\n",
       " array([-0.3786808 ,  0.65841027,  0.68749494,  0.61799296,  0.67237056,\n",
       "        -0.0748058 ]),\n",
       " array([-0.38043663,  0.65795608,  0.68714844,  0.61747544,  0.67191589,\n",
       "        -0.07603778]),\n",
       " array([-0.38218364,  0.65750405,  0.68680408,  0.61696033,  0.67146325,\n",
       "        -0.07726259]),\n",
       " array([-0.38392188,  0.65705417,  0.68646185,  0.61644761,  0.67101262,\n",
       "        -0.07848028]),\n",
       " array([-0.38565141,  0.65660642,  0.68612174,  0.61593726,  0.670564  ,\n",
       "        -0.07969089]),\n",
       " array([-0.3873723 ,  0.65616079,  0.68578374,  0.61542927,  0.67011737,\n",
       "        -0.08089449]),\n",
       " array([-0.38908459,  0.65571727,  0.68544784,  0.61492362,  0.66967271,\n",
       "        -0.08209111]),\n",
       " array([-0.39078836,  0.65527584,  0.68511401,  0.6144203 ,  0.66923003,\n",
       "        -0.0832808 ]),\n",
       " array([-0.39248366,  0.65483649,  0.68478225,  0.6139193 ,  0.66878929,\n",
       "        -0.08446362]),\n",
       " array([-0.39417054,  0.65439921,  0.68445254,  0.6134206 ,  0.6683505 ,\n",
       "        -0.08563959]),\n",
       " array([-0.39584906,  0.65396398,  0.68412487,  0.61292419,  0.66791365,\n",
       "        -0.08680878]),\n",
       " array([-0.39751928,  0.65353079,  0.68379923,  0.61243005,  0.66747871,\n",
       "        -0.08797124]),\n",
       " array([-0.39918126,  0.65309962,  0.6834756 ,  0.61193817,  0.66704567,\n",
       "        -0.08912699]),\n",
       " array([-0.40083505,  0.65267047,  0.68315397,  0.61144854,  0.66661454,\n",
       "        -0.0902761 ]),\n",
       " array([-0.40248071,  0.65224331,  0.68283433,  0.61096113,  0.66618529,\n",
       "        -0.0914186 ]),\n",
       " array([-0.4041183 ,  0.65181814,  0.68251666,  0.61047594,  0.66575791,\n",
       "        -0.09255454]),\n",
       " array([-0.40574786,  0.65139495,  0.68220095,  0.60999296,  0.66533239,\n",
       "        -0.09368397]),\n",
       " array([-0.40736946,  0.65097371,  0.68188719,  0.60951216,  0.66490873,\n",
       "        -0.09480693]),\n",
       " array([-0.40898314,  0.65055443,  0.68157536,  0.60903354,  0.6644869 ,\n",
       "        -0.09592347]),\n",
       " array([-0.41058897,  0.65013708,  0.68126546,  0.60855708,  0.6640669 ,\n",
       "        -0.09703362]),\n",
       " array([-0.412187  ,  0.64972165,  0.68095746,  0.60808277,  0.66364871,\n",
       "        -0.09813744]),\n",
       " array([-0.41377728,  0.64930813,  0.68065137,  0.60761059,  0.66323233,\n",
       "        -0.09923496]),\n",
       " array([-0.41535986,  0.64889651,  0.68034716,  0.60714054,  0.66281775,\n",
       "        -0.10032623]),\n",
       " array([-0.41693479,  0.64848678,  0.68004482,  0.60667259,  0.66240495,\n",
       "        -0.10141129]),\n",
       " array([-0.41850213,  0.64807891,  0.67974435,  0.60620674,  0.66199392,\n",
       "        -0.10249019]),\n",
       " array([-0.42006194,  0.64767291,  0.67944572,  0.60574297,  0.66158465,\n",
       "        -0.10356296]),\n",
       " array([-0.42161425,  0.64726876,  0.67914893,  0.60528127,  0.66117713,\n",
       "        -0.10462965]),\n",
       " array([-0.42315913,  0.64686644,  0.67885397,  0.60482162,  0.66077136,\n",
       "        -0.1056903 ]),\n",
       " array([-0.42469663,  0.64646594,  0.67856082,  0.60436401,  0.66036731,\n",
       "        -0.10674494]),\n",
       " array([-0.42622679,  0.64606726,  0.67826947,  0.60390844,  0.65996498,\n",
       "        -0.10779363]),\n",
       " array([-0.42774966,  0.64567038,  0.67797991,  0.60345488,  0.65956435,\n",
       "        -0.10883641]),\n",
       " array([-0.4292653 ,  0.64527529,  0.67769213,  0.60300333,  0.65916543,\n",
       "        -0.1098733 ]),\n",
       " array([-0.43077375,  0.64488197,  0.67740611,  0.60255376,  0.65876819,\n",
       "        -0.11090436]),\n",
       " array([-0.43227507,  0.64449042,  0.67712185,  0.60210618,  0.65837263,\n",
       "        -0.11192962]),\n",
       " array([-0.4337693 ,  0.64410063,  0.67683933,  0.60166056,  0.65797873,\n",
       "        -0.11294912]),\n",
       " array([-0.4352565 ,  0.64371257,  0.67655854,  0.6012169 ,  0.65758649,\n",
       "        -0.1139629 ]),\n",
       " array([-0.4367367 ,  0.64332625,  0.67627948,  0.60077518,  0.65719589,\n",
       "        -0.11497101]),\n",
       " array([-0.43820996,  0.64294165,  0.67600212,  0.60033538,  0.65680693,\n",
       "        -0.11597347]),\n",
       " array([-0.43967632,  0.64255875,  0.67572647,  0.59989751,  0.6564196 ,\n",
       "        -0.11697033]),\n",
       " array([-0.44113583,  0.64217755,  0.6754525 ,  0.59946154,  0.65603388,\n",
       "        -0.11796163]),\n",
       " array([-0.44258855,  0.64179804,  0.6751802 ,  0.59902747,  0.65564977,\n",
       "        -0.11894739]),\n",
       " array([-0.4440345 ,  0.6414202 ,  0.67490958,  0.59859527,  0.65526725,\n",
       "        -0.11992767]),\n",
       " array([-0.44547375,  0.64104403,  0.6746406 ,  0.59816495,  0.65488632,\n",
       "        -0.1209025 ]),\n",
       " array([-0.44690634,  0.64066951,  0.67437328,  0.59773648,  0.65450696,\n",
       "        -0.12187191]),\n",
       " array([-0.44833231,  0.64029663,  0.67410758,  0.59730986,  0.65412917,\n",
       "        -0.12283594]),\n",
       " array([-0.4497517 ,  0.63992538,  0.67384351,  0.59688507,  0.65375294,\n",
       "        -0.12379463]),\n",
       " array([-0.45116457,  0.63955576,  0.67358105,  0.59646211,  0.65337825,\n",
       "        -0.12474802]),\n",
       " array([-0.45257095,  0.63918774,  0.6733202 ,  0.59604096,  0.6530051 ,\n",
       "        -0.12569613]),\n",
       " array([-0.4539709 ,  0.63882133,  0.67306093,  0.59562161,  0.65263348,\n",
       "        -0.12663901]),\n",
       " array([-0.45536445,  0.6384565 ,  0.67280325,  0.59520405,  0.65226338,\n",
       "        -0.12757669]),\n",
       " array([-0.45675164,  0.63809325,  0.67254714,  0.59478826,  0.65189479,\n",
       "        -0.12850921]),\n",
       " array([-0.45813253,  0.63773158,  0.67229259,  0.59437424,  0.65152769,\n",
       "        -0.1294366 ]),\n",
       " array([-0.45950715,  0.63737146,  0.67203959,  0.59396198,  0.65116209,\n",
       "        -0.13035889]),\n",
       " array([-0.46087554,  0.63701289,  0.67178813,  0.59355146,  0.65079797,\n",
       "        -0.13127612]),\n",
       " array([-0.46223775,  0.63665586,  0.67153821,  0.59314268,  0.65043533,\n",
       "        -0.13218833]),\n",
       " array([-0.46359382,  0.63630035,  0.67128981,  0.59273562,  0.65007414,\n",
       "        -0.13309554]),\n",
       " array([-0.4649438 ,  0.63594637,  0.67104292,  0.59233027,  0.64971441,\n",
       "        -0.1339978 ]),\n",
       " array([-0.46628771,  0.63559389,  0.67079753,  0.59192662,  0.64935613,\n",
       "        -0.13489512]),\n",
       " array([-0.46762561,  0.63524292,  0.67055363,  0.59152466,  0.64899928,\n",
       "        -0.13578756]),\n",
       " array([-0.46895753,  0.63489343,  0.67031122,  0.59112438,  0.64864386,\n",
       "        -0.13667513]),\n",
       " array([-0.47028352,  0.63454542,  0.67007028,  0.59072577,  0.64828986,\n",
       "        -0.13755788]),\n",
       " array([-0.4716036 ,  0.63419889,  0.6698308 ,  0.59032882,  0.64793728,\n",
       "        -0.13843583]),\n",
       " array([-0.47291783,  0.63385381,  0.66959279,  0.58993352,  0.64758609,\n",
       "        -0.13930902]),\n",
       " array([-0.47422625,  0.63351019,  0.66935621,  0.58953986,  0.6472363 ,\n",
       "        -0.14017747]),\n",
       " array([-0.47552888,  0.63316801,  0.66912108,  0.58914783,  0.64688789,\n",
       "        -0.14104123]),\n",
       " array([-0.47682578,  0.63282726,  0.66888737,  0.58875741,  0.64654086,\n",
       "        -0.14190031]),\n",
       " array([-0.47811697,  0.63248793,  0.66865508,  0.58836861,  0.6461952 ,\n",
       "        -0.14275476]),\n",
       " array([-0.4794025 ,  0.63215002,  0.66842419,  0.5879814 ,  0.6458509 ,\n",
       "        -0.1436046 ]),\n",
       " array([-0.4806824 ,  0.63181351,  0.66819471,  0.58759578,  0.64550794,\n",
       "        -0.14444986]),\n",
       " array([-0.48195671,  0.63147841,  0.66796663,  0.58721174,  0.64516634,\n",
       "        -0.14529057]),\n",
       " array([-0.48322547,  0.63114468,  0.66773992,  0.58682926,  0.64482607,\n",
       "        -0.14612677]),\n",
       " array([-0.48448872,  0.63081234,  0.66751459,  0.58644835,  0.64448712,\n",
       "        -0.14695848]),\n",
       " array([-0.48574649,  0.63048137,  0.66729063,  0.58606899,  0.6441495 ,\n",
       "        -0.14778573]),\n",
       " array([-0.48699882,  0.63015176,  0.66706802,  0.58569116,  0.64381318,\n",
       "        -0.14860855]),\n",
       " array([-0.48824574,  0.6298235 ,  0.66684676,  0.58531487,  0.64347817,\n",
       "        -0.14942698]),\n",
       " array([-0.48948729,  0.62949658,  0.66662684,  0.5849401 ,  0.64314446,\n",
       "        -0.15024103]),\n",
       " array([-0.49072351,  0.629171  ,  0.66640826,  0.58456684,  0.64281203,\n",
       "        -0.15105074]),\n",
       " array([-0.49195443,  0.62884675,  0.66619099,  0.58419508,  0.64248088,\n",
       "        -0.15185614]),\n",
       " array([-0.49318008,  0.62852381,  0.66597505,  0.58382481,  0.64215101,\n",
       "        -0.15265726]),\n",
       " array([-0.49440051,  0.62820219,  0.66576041,  0.58345603,  0.6418224 ,\n",
       "        -0.15345411]),\n",
       " array([-0.49561574,  0.62788186,  0.66554706,  0.58308873,  0.64149504,\n",
       "        -0.15424674]),\n",
       " array([-0.4968258 ,  0.62756283,  0.66533501,  0.58272289,  0.64116894,\n",
       "        -0.15503516]),\n",
       " array([-0.49803074,  0.62724509,  0.66512425,  0.5823585 ,  0.64084408,\n",
       "        -0.15581941]),\n",
       " array([-0.49923059,  0.62692862,  0.66491475,  0.58199557,  0.64052045,\n",
       "        -0.15659951]),\n",
       " array([-0.50042537,  0.62661342,  0.66470653,  0.58163407,  0.64019805,\n",
       "        -0.1573755 ]),\n",
       " array([-0.50161513,  0.62629949,  0.66449956,  0.58127401,  0.63987687,\n",
       "        -0.15814738]),\n",
       " array([-0.50279989,  0.62598681,  0.66429385,  0.58091537,  0.6395569 ,\n",
       "        -0.1589152 ]),\n",
       " array([-0.50397968,  0.62567537,  0.66408938,  0.58055814,  0.63923814,\n",
       "        -0.15967898]),\n",
       " array([-0.50515455,  0.62536517,  0.66388615,  0.58020231,  0.63892058,\n",
       "        -0.16043874]),\n",
       " array([-0.50632452,  0.6250562 ,  0.66368414,  0.57984788,  0.63860421,\n",
       "        -0.16119451]),\n",
       " array([-0.50748962,  0.62474845,  0.66348336,  0.57949484,  0.63828902,\n",
       "        -0.16194632]),\n",
       " array([-0.50864989,  0.62444192,  0.66328378,  0.57914318,  0.63797501,\n",
       "        -0.16269419]),\n",
       " array([-0.50980535,  0.6241366 ,  0.66308542,  0.57879289,  0.63766216,\n",
       "        -0.16343815]),\n",
       " array([-0.51095604,  0.62383247,  0.66288825,  0.57844396,  0.63735049,\n",
       "        -0.16417821]),\n",
       " array([-0.51210199,  0.62352954,  0.66269228,  0.57809639,  0.63703996,\n",
       "        -0.16491442]),\n",
       " array([-0.51324323,  0.62322779,  0.66249749,  0.57775016,  0.63673059,\n",
       "        -0.16564678]),\n",
       " array([-0.51437978,  0.62292722,  0.66230388,  0.57740527,  0.63642236,\n",
       "        -0.16637533]),\n",
       " array([-0.51551168,  0.62262782,  0.66211144,  0.57706172,  0.63611527,\n",
       "        -0.16710009]),\n",
       " array([-0.51663896,  0.62232958,  0.66192016,  0.57671948,  0.6358093 ,\n",
       "        -0.16782109]),\n",
       " array([-0.51776165,  0.6220325 ,  0.66173003,  0.57637856,  0.63550446,\n",
       "        -0.16853834]),\n",
       " array([-0.51887977,  0.62173657,  0.66154106,  0.57603894,  0.63520073,\n",
       "        -0.16925187]),\n",
       " array([-0.51999336,  0.62144178,  0.66135322,  0.57570063,  0.63489812,\n",
       "        -0.16996171]),\n",
       " array([-0.52110245,  0.62114812,  0.66116653,  0.5753636 ,  0.6345966 ,\n",
       "        -0.17066787]),\n",
       " array([-0.52220706,  0.6208556 ,  0.66098096,  0.57502786,  0.63429619,\n",
       "        -0.17137039]),\n",
       " array([-0.52330722,  0.62056419,  0.66079651,  0.5746934 ,  0.63399686,\n",
       "        -0.17206928]),\n",
       " array([-0.52440295,  0.62027389,  0.66061317,  0.5743602 ,  0.63369862,\n",
       "        -0.17276457]),\n",
       " array([-0.5254943 ,  0.61998471,  0.66043095,  0.57402826,  0.63340145,\n",
       "        -0.17345627]),\n",
       " array([-0.52658129,  0.61969662,  0.66024982,  0.57369758,  0.63310536,\n",
       "        -0.17414442]),\n",
       " array([-0.52766393,  0.61940963,  0.66006979,  0.57336814,  0.63281033,\n",
       "        -0.17482903]),\n",
       " array([-0.52874227,  0.61912373,  0.65989085,  0.57303994,  0.63251636,\n",
       "        -0.17551013]),\n",
       " array([-0.52981632,  0.6188389 ,  0.65971299,  0.57271297,  0.63222344,\n",
       "        -0.17618774]),\n",
       " array([-0.53088612,  0.61855515,  0.65953621,  0.57238723,  0.63193157,\n",
       "        -0.17686187]),\n",
       " array([-0.53195169,  0.61827247,  0.6593605 ,  0.5720627 ,  0.63164074,\n",
       "        -0.17753256]),\n",
       " array([-0.53301305,  0.61799085,  0.65918584,  0.57173938,  0.63135094,\n",
       "        -0.17819982]),\n",
       " array([-0.53407024,  0.61771028,  0.65901225,  0.57141727,  0.63106218,\n",
       "        -0.17886367]),\n",
       " array([-0.53512328,  0.61743076,  0.6588397 ,  0.57109635,  0.63077443,\n",
       "        -0.17952414]),\n",
       " array([-0.53617219,  0.61715228,  0.6586682 ,  0.57077661,  0.6304877 ,\n",
       "        -0.18018124]),\n",
       " array([-0.53721701,  0.61687484,  0.65849774,  0.57045806,  0.63020199,\n",
       "        -0.180835  ]),\n",
       " array([-0.53825775,  0.61659843,  0.65832831,  0.57014069,  0.62991728,\n",
       "        -0.18148544]),\n",
       " array([-0.53929444,  0.61632304,  0.6581599 ,  0.56982448,  0.62963357,\n",
       "        -0.18213257]),\n",
       " array([-0.54032711,  0.61604867,  0.65799252,  0.56950943,  0.62935085,\n",
       "        -0.18277642]),\n",
       " array([-0.54135578,  0.61577531,  0.65782615,  0.56919554,  0.62906912,\n",
       "        -0.18341701]),\n",
       " array([-0.54238048,  0.61550296,  0.65766078,  0.5688828 ,  0.62878837,\n",
       "        -0.18405436]),\n",
       " array([-0.54340122,  0.6152316 ,  0.65749642,  0.56857119,  0.6285086 ,\n",
       "        -0.18468848]),\n",
       " array([-0.54441804,  0.61496124,  0.65733306,  0.56826073,  0.62822981,\n",
       "        -0.1853194 ]),\n",
       " array([-0.54543096,  0.61469187,  0.65717068,  0.56795139,  0.62795197,\n",
       "        -0.18594713]),\n",
       " array([-0.54644   ,  0.61442347,  0.65700929,  0.56764317,  0.6276751 ,\n",
       "        -0.18657171]),\n",
       " array([-0.54744519,  0.61415606,  0.65684888,  0.56733606,  0.62739918,\n",
       "        -0.18719313]),\n",
       " array([-0.54844655,  0.61388961,  0.65668944,  0.56703007,  0.62712422,\n",
       "        -0.18781143]),\n",
       " array([-0.5494441 ,  0.61362413,  0.65653097,  0.56672517,  0.6268502 ,\n",
       "        -0.18842662]),\n",
       " array([-0.55043786,  0.6133596 ,  0.65637346,  0.56642137,  0.62657711,\n",
       "        -0.18903872]),\n",
       " array([-0.55142787,  0.61309603,  0.65621691,  0.56611867,  0.62630496,\n",
       "        -0.18964776]),\n",
       " array([-0.55241414,  0.61283341,  0.65606131,  0.56581704,  0.62603374,\n",
       "        -0.19025374]),\n",
       " array([-0.55339669,  0.61257173,  0.65590666,  0.56551649,  0.62576344,\n",
       "        -0.19085668]),\n",
       " array([-0.55437554,  0.61231098,  0.65575294,  0.56521702,  0.62549406,\n",
       "        -0.19145662]),\n",
       " array([-0.55535073,  0.61205117,  0.65560016,  0.56491861,  0.6252256 ,\n",
       "        -0.19205355]),\n",
       " array([-0.55632227,  0.61179228,  0.65544831,  0.56462125,  0.62495804,\n",
       "        -0.19264751]),\n",
       " array([-0.55729018,  0.61153431,  0.65529739,  0.56432496,  0.62469138,\n",
       "        -0.1932385 ]),\n",
       " array([-0.55825448,  0.61127726,  0.65514738,  0.5640297 ,  0.62442562,\n",
       "        -0.19382655]),\n",
       " array([-0.5592152 ,  0.61102111,  0.65499829,  0.56373549,  0.62416076,\n",
       "        -0.19441167]),\n",
       " array([-0.56017236,  0.61076587,  0.65485011,  0.56344232,  0.62389678,\n",
       "        -0.19499388]),\n",
       " array([-0.56112598,  0.61051153,  0.65470283,  0.56315017,  0.62363369,\n",
       "        -0.1955732 ]),\n",
       " array([-0.56207607,  0.61025809,  0.65455644,  0.56285905,  0.62337147,\n",
       "        -0.19614964]),\n",
       " array([-0.56302267,  0.61000553,  0.65441096,  0.56256895,  0.62311013,\n",
       "        -0.19672323]),\n",
       " array([-0.56396579,  0.60975385,  0.65426636,  0.56227985,  0.62284965,\n",
       "        -0.19729397]),\n",
       " array([-0.56490545,  0.60950306,  0.65412264,  0.56199177,  0.62259004,\n",
       "        -0.19786189]),\n",
       " array([-0.56584167,  0.60925314,  0.6539798 ,  0.56170468,  0.62233129,\n",
       "        -0.19842701]),\n",
       " array([-0.56677448,  0.60900408,  0.65383784,  0.5614186 ,  0.62207339,\n",
       "        -0.19898933]),\n",
       " array([-0.56770389,  0.60875589,  0.65369674,  0.5611335 ,  0.62181635,\n",
       "        -0.19954887]),\n",
       " array([-0.56862992,  0.60850856,  0.65355651,  0.56084938,  0.62156014,\n",
       "        -0.20010565]),\n",
       " array([-0.56955259,  0.60826208,  0.65341714,  0.56056625,  0.62130478,\n",
       "        -0.20065969]),\n",
       " array([-0.57047193,  0.60801645,  0.65327862,  0.56028409,  0.62105025,\n",
       "        -0.20121101]),\n",
       " array([-0.57138794,  0.60777167,  0.65314095,  0.56000289,  0.62079656,\n",
       "        -0.20175961]),\n",
       " array([-0.57230066,  0.60752772,  0.65300412,  0.55972266,  0.62054369,\n",
       "        -0.20230551]),\n",
       " array([-0.5732101 ,  0.60728461,  0.65286814,  0.55944339,  0.62029165,\n",
       "        -0.20284874]),\n",
       " array([-0.57411628,  0.60704233,  0.65273299,  0.55916507,  0.62004042,\n",
       "        -0.2033893 ]),\n",
       " array([-0.57501922,  0.60680088,  0.65259867,  0.55888769,  0.61979   ,\n",
       "        -0.20392721]),\n",
       " array([-0.57591893,  0.60656024,  0.65246518,  0.55861126,  0.6195404 ,\n",
       "        -0.20446249]),\n",
       " array([-0.57681544,  0.60632042,  0.65233251,  0.55833577,  0.6192916 ,\n",
       "        -0.20499514]),\n",
       " array([-0.57770877,  0.60608142,  0.65220066,  0.5580612 ,  0.61904359,\n",
       "        -0.2055252 ]),\n",
       " array([-0.57859892,  0.60584321,  0.65206962,  0.55778756,  0.61879639,\n",
       "        -0.20605266]),\n",
       " array([-0.57948593,  0.60560582,  0.65193939,  0.55751485,  0.61854998,\n",
       "        -0.20657755]),\n",
       " array([-0.5803698 ,  0.60536922,  0.65180996,  0.55724305,  0.61830435,\n",
       "        -0.20709988]),\n",
       " array([-0.58125057,  0.60513341,  0.65168133,  0.55697216,  0.61805951,\n",
       "        -0.20761966]),\n",
       " array([-0.58212823,  0.60489839,  0.6515535 ,  0.55670218,  0.61781545,\n",
       "        -0.20813691]),\n",
       " array([-0.58300282,  0.60466416,  0.65142646,  0.5564331 ,  0.61757216,\n",
       "        -0.20865165]),\n",
       " array([-0.58387435,  0.60443071,  0.65130021,  0.55616492,  0.61732964,\n",
       "        -0.20916388]),\n",
       " array([-0.58474284,  0.60419803,  0.65117473,  0.55589762,  0.61708789,\n",
       "        -0.20967363]),\n",
       " array([-0.5856083 ,  0.60396613,  0.65105004,  0.55563122,  0.6168469 ,\n",
       "        -0.2101809 ]),\n",
       " array([-0.58647075,  0.60373499,  0.65092612,  0.5553657 ,  0.61660667,\n",
       "        -0.21068571]),\n",
       " array([-0.58733022,  0.60350462,  0.65080297,  0.55510105,  0.6163672 ,\n",
       "        -0.21118807]),\n",
       " array([-0.5881867 ,  0.603275  ,  0.65068058,  0.55483728,  0.61612848,\n",
       "        -0.211688  ]),\n",
       " array([-0.58904023,  0.60304614,  0.65055896,  0.55457438,  0.6158905 ,\n",
       "        -0.21218551]),\n",
       " array([-0.58989082,  0.60281803,  0.65043809,  0.55431234,  0.61565326,\n",
       "        -0.21268062]),\n",
       " array([-0.59073848,  0.60259067,  0.65031798,  0.55405116,  0.61541677,\n",
       "        -0.21317333]),\n",
       " array([-0.59158324,  0.60236405,  0.65019862,  0.55379083,  0.61518101,\n",
       "        -0.21366367]),\n",
       " array([-0.5924251 ,  0.60213817,  0.65008   ,  0.55353136,  0.61494598,\n",
       "        -0.21415163]),\n",
       " array([-0.59326409,  0.60191302,  0.64996213,  0.55327273,  0.61471168,\n",
       "        -0.21463725]),\n",
       " array([-0.59410022,  0.60168861,  0.64984499,  0.55301494,  0.6144781 ,\n",
       "        -0.21512053]),\n",
       " array([-0.59493351,  0.60146492,  0.64972858,  0.55275798,  0.61424524,\n",
       "        -0.21560148]),\n",
       " array([-0.59576397,  0.60124196,  0.64961291,  0.55250186,  0.6140131 ,\n",
       "        -0.21608011]),\n",
       " array([-0.59659161,  0.60101971,  0.64949796,  0.55224657,  0.61378167,\n",
       "        -0.21655645]),\n",
       " array([-0.59741646,  0.60079818,  0.64938373,  0.5519921 ,  0.61355095,\n",
       "        -0.21703049]),\n",
       " array([-0.59823853,  0.60057736,  0.64927022,  0.55173844,  0.61332093,\n",
       "        -0.21750227]),\n",
       " array([-0.59905783,  0.60035724,  0.64915743,  0.55148561,  0.61309162,\n",
       "        -0.21797177]),\n",
       " array([-0.59987438,  0.60013783,  0.64904535,  0.55123358,  0.612863  ,\n",
       "        -0.21843903]),\n",
       " array([-0.6006882 ,  0.59991912,  0.64893397,  0.55098236,  0.61263508,\n",
       "        -0.21890405]),\n",
       " array([-0.6014993 ,  0.59970111,  0.6488233 ,  0.55073194,  0.61240785,\n",
       "        -0.21936684]),\n",
       " array([-0.60230769,  0.59948379,  0.64871332,  0.55048232,  0.6121813 ,\n",
       "        -0.21982741]),\n",
       " array([-0.60311339,  0.59926715,  0.64860405,  0.55023349,  0.61195544,\n",
       "        -0.22028579]),\n",
       " array([-0.60391641,  0.59905121,  0.64849546,  0.54998546,  0.61173026,\n",
       "        -0.22074197]),\n",
       " array([-0.60471678,  0.59883594,  0.64838756,  0.5497382 ,  0.61150575,\n",
       "        -0.22119598]),\n",
       " array([-0.60551449,  0.59862135,  0.64828035,  0.54949173,  0.61128192,\n",
       "        -0.22164782]),\n",
       " array([-0.60630958,  0.59840744,  0.64817383,  0.54924604,  0.61105876,\n",
       "        -0.2220975 ]),\n",
       " array([-0.60710204,  0.59819419,  0.64806797,  0.54900112,  0.61083626,\n",
       "        -0.22254504]),\n",
       " array([-0.60789191,  0.59798162,  0.6479628 ,  0.54875697,  0.61061443,\n",
       "        -0.22299045]),\n",
       " array([-0.60867918,  0.5977697 ,  0.64785829,  0.54851358,  0.61039326,\n",
       "        -0.22343374]),\n",
       " array([-0.60946387,  0.59755845,  0.64775446,  0.54827096,  0.61017274,\n",
       "        -0.22387492]),\n",
       " array([-0.61024601,  0.59734785,  0.64765128,  0.54802909,  0.60995287,\n",
       "        -0.224314  ]),\n",
       " array([-0.61102559,  0.59713791,  0.64754877,  0.54778798,  0.60973366,\n",
       "        -0.22475099]),\n",
       " array([-0.61180264,  0.59692862,  0.64744692,  0.54754761,  0.60951509,\n",
       "        -0.22518591]),\n",
       " array([-0.61257717,  0.59671997,  0.64734572,  0.547308  ,  0.60929716,\n",
       "        -0.22561876]),\n",
       " array([-0.6133492 ,  0.59651197,  0.64724517,  0.54706912,  0.60907988,\n",
       "        -0.22604956]),\n",
       " array([-0.61411873,  0.5963046 ,  0.64714527,  0.54683098,  0.60886323,\n",
       "        -0.22647832]),\n",
       " array([-0.61488577,  0.59609787,  0.64704601,  0.54659358,  0.60864721,\n",
       "        -0.22690505]),\n",
       " array([-0.61565035,  0.59589178,  0.64694739,  0.54635691,  0.60843183,\n",
       "        -0.22732975]),\n",
       " array([-0.61641248,  0.59568632,  0.64684942,  0.54612097,  0.60821707,\n",
       "        -0.22775244]),\n",
       " array([-0.61717216,  0.59548148,  0.64675207,  0.54588574,  0.60800293,\n",
       "        -0.22817314]),\n",
       " array([-0.61792942,  0.59527726,  0.64665536,  0.54565124,  0.60778942,\n",
       "        -0.22859184]),\n",
       " array([-0.61868426,  0.59507367,  0.64655928,  0.54541746,  0.60757652,\n",
       "        -0.22900857]),\n",
       " array([-0.61943669,  0.59487069,  0.64646382,  0.54518439,  0.60736424,\n",
       "        -0.22942333]),\n",
       " array([-0.62018674,  0.59466833,  0.64636898,  0.54495203,  0.60715257,\n",
       "        -0.22983613]),\n",
       " array([-0.62093441,  0.59446657,  0.64627476,  0.54472037,  0.60694151,\n",
       "        -0.23024698]),\n",
       " array([-0.62167971,  0.59426543,  0.64618116,  0.54448941,  0.60673106,\n",
       "        -0.23065589]),\n",
       " array([-0.62242266,  0.59406489,  0.64608817,  0.54425916,  0.60652121,\n",
       "        -0.23106288]),\n",
       " array([-0.62316327,  0.59386495,  0.64599579,  0.54402959,  0.60631195,\n",
       "        -0.23146795]),\n",
       " array([-0.62390155,  0.59366561,  0.64590401,  0.54380072,  0.6061033 ,\n",
       "        -0.23187111]),\n",
       " array([-0.62463751,  0.59346686,  0.64581284,  0.54357254,  0.60589524,\n",
       "        -0.23227237]),\n",
       " array([-0.62537117,  0.59326871,  0.64572227,  0.54334504,  0.60568776,\n",
       "        -0.23267174]),\n",
       " array([-0.62610254,  0.59307114,  0.6456323 ,  0.54311823,  0.60548088,\n",
       "        -0.23306924]),\n",
       " array([-0.62683163,  0.59287417,  0.64554292,  0.54289209,  0.60527458,\n",
       "        -0.23346487]),\n",
       " array([-0.62755845,  0.59267777,  0.64545413,  0.54266663,  0.60506887,\n",
       "        -0.23385864]),\n",
       " array([-0.62828301,  0.59248196,  0.64536593,  0.54244183,  0.60486373,\n",
       "        -0.23425055]),\n",
       " array([-0.62900532,  0.59228672,  0.64527832,  0.54221771,  0.60465917,\n",
       "        -0.23464063]),\n",
       " array([-0.62972541,  0.59209206,  0.64519129,  0.54199425,  0.60445519,\n",
       "        -0.23502888]),\n",
       " array([-0.63044327,  0.59189797,  0.64510484,  0.54177145,  0.60425177,\n",
       "        -0.23541531]),\n",
       " array([-0.63115892,  0.59170445,  0.64501897,  0.54154931,  0.60404893,\n",
       "        -0.23579992]),\n",
       " array([-0.63187237,  0.59151149,  0.64493367,  0.54132783,  0.60384665,\n",
       "        -0.23618273]),\n",
       " array([-0.63258363,  0.5913191 ,  0.64484895,  0.54110699,  0.60364493,\n",
       "        -0.23656375]),\n",
       " array([-0.63329271,  0.59112727,  0.64476479,  0.54088681,  0.60344378,\n",
       "        -0.23694299]),\n",
       " array([-0.63399963,  0.59093599,  0.6446812 ,  0.54066727,  0.60324318,\n",
       "        -0.23732044]),\n",
       " array([-0.63470439,  0.59074528,  0.64459817,  0.54044837,  0.60304313,\n",
       "        -0.23769614]),\n",
       " array([-0.63540701,  0.59055511,  0.6445157 ,  0.54023012,  0.60284364,\n",
       "        -0.23807007]),\n",
       " array([-0.6361075 ,  0.59036549,  0.6444338 ,  0.54001249,  0.6026447 ,\n",
       "        -0.23844226]),\n",
       " array([-0.63680586,  0.59017642,  0.64435244,  0.53979551,  0.6024463 ,\n",
       "        -0.2388127 ]),\n",
       " array([-0.63750211,  0.58998789,  0.64427164,  0.53957915,  0.60224845,\n",
       "        -0.23918142]),\n",
       " array([-0.63819626,  0.58979991,  0.64419139,  0.53936342,  0.60205114,\n",
       "        -0.23954841]),\n",
       " array([-0.63888832,  0.58961246,  0.64411168,  0.53914831,  0.60185437,\n",
       "        -0.23991369]),\n",
       " array([-0.6395783 ,  0.58942555,  0.64403252,  0.53893382,  0.60165814,\n",
       "        -0.24027726]),\n",
       " array([-0.64026621,  0.58923918,  0.6439539 ,  0.53871995,  0.60146244,\n",
       "        -0.24063913]),\n",
       " array([-0.64095206,  0.58905333,  0.64387583,  0.5385067 ,  0.60126727,\n",
       "        -0.24099932]),\n",
       " array([-0.64163586,  0.58886801,  0.64379828,  0.53829406,  0.60107263,\n",
       "        -0.24135783]),\n",
       " array([-0.64231762,  0.58868322,  0.64372128,  0.53808202,  0.60087852,\n",
       "        -0.24171467]),\n",
       " array([-0.64299736,  0.58849895,  0.6436448 ,  0.5378706 ,  0.60068493,\n",
       "        -0.24206984]),\n",
       " array([-0.64367507,  0.5883152 ,  0.64356885,  0.53765977,  0.60049187,\n",
       "        -0.24242336]),\n",
       " array([-0.64435078,  0.58813197,  0.64349343,  0.53744955,  0.60029932,\n",
       "        -0.24277523]),\n",
       " array([-0.64502449,  0.58794926,  0.64341853,  0.53723992,  0.60010729,\n",
       "        -0.24312546]),\n",
       " array([-0.64569621,  0.58776706,  0.64334416,  0.53703089,  0.59991577,\n",
       "        -0.24347406]),\n",
       " array([-0.64636595,  0.58758536,  0.6432703 ,  0.53682245,  0.59972477,\n",
       "        -0.24382104]),\n",
       " array([-0.64703372,  0.58740418,  0.64319696,  0.53661461,  0.59953427,\n",
       "        -0.2441664 ]),\n",
       " array([-0.64769953,  0.5872235 ,  0.64312414,  0.53640734,  0.59934429,\n",
       "        -0.24451016]),\n",
       " array([-0.64836339,  0.58704333,  0.64305183,  0.53620066,  0.5991548 ,\n",
       "        -0.24485232]),\n",
       " array([-0.64902532,  0.58686366,  0.64298002,  0.53599456,  0.59896582,\n",
       "        -0.24519288]),\n",
       " array([-0.64968531,  0.58668448,  0.64290873,  0.53578904,  0.59877734,\n",
       "        -0.24553187]),\n",
       " array([-0.65034338,  0.5865058 ,  0.64283794,  0.5355841 ,  0.59858936,\n",
       "        -0.24586928]),\n",
       " array([-0.65099954,  0.58632762,  0.64276765,  0.53537973,  0.59840187,\n",
       "        -0.24620512]),\n",
       " array([-0.65165379,  0.58614993,  0.64269786,  0.53517592,  0.59821488,\n",
       "        -0.2465394 ]),\n",
       " array([-0.65230615,  0.58597272,  0.64262857,  0.53497269,  0.59802838,\n",
       "        -0.24687212]),\n",
       " array([-0.65295663,  0.58579601,  0.64255977,  0.53477002,  0.59784236,\n",
       "        -0.2472033 ]),\n",
       " array([-0.65360524,  0.58561977,  0.64249147,  0.53456791,  0.59765684,\n",
       "        -0.24753295]),\n",
       " array([-0.65425197,  0.58544402,  0.64242365,  0.53436636,  0.59747179,\n",
       "        -0.24786106]),\n",
       " array([-0.65489685,  0.58526875,  0.64235633,  0.53416537,  0.59728723,\n",
       "        -0.24818765]),\n",
       " array([-0.65553989,  0.58509396,  0.64228949,  0.53396494,  0.59710315,\n",
       "        -0.24851272]),\n",
       " array([-0.65618108,  0.58491964,  0.64222314,  0.53376505,  0.59691955,\n",
       "        -0.24883629]),\n",
       " array([-0.65682044,  0.5847458 ,  0.64215726,  0.53356572,  0.59673642,\n",
       "        -0.24915836]),\n",
       " array([-0.65745798,  0.58457242,  0.64209187,  0.53336693,  0.59655377,\n",
       "        -0.24947893]),\n",
       " array([-0.65809371,  0.58439952,  0.64202696,  0.53316868,  0.59637159,\n",
       "        -0.24979801]),\n",
       " array([-0.65872764,  0.58422708,  0.64196252,  0.53297098,  0.59618987,\n",
       "        -0.25011561]),\n",
       " array([-0.65935977,  0.58405511,  0.64189855,  0.53277382,  0.59600862,\n",
       "        -0.25043174]),\n",
       " array([-0.65999011,  0.5838836 ,  0.64183505,  0.53257719,  0.59582784,\n",
       "        -0.25074641]),\n",
       " array([-0.66061868,  0.58371254,  0.64177202,  0.5323811 ,  0.59564752,\n",
       "        -0.25105962]),\n",
       " array([-0.66124547,  0.58354195,  0.64170946,  0.53218554,  0.59546766,\n",
       "        -0.25137137]),\n",
       " array([-0.6618705 ,  0.58337181,  0.64164737,  0.53199051,  0.59528826,\n",
       "        -0.25168168]),\n",
       " array([-0.66249378,  0.58320213,  0.64158573,  0.531796  ,  0.59510932,\n",
       "        -0.25199055]),\n",
       " array([-0.66311532,  0.5830329 ,  0.64152456,  0.53160202,  0.59493083,\n",
       "        -0.25229799]),\n",
       " array([-0.66373512,  0.58286411,  0.64146384,  0.53140857,  0.5947528 ,\n",
       "        -0.25260401]),\n",
       " array([-0.66435319,  0.58269578,  0.64140358,  0.53121563,  0.59457521,\n",
       "        -0.2529086 ]),\n",
       " array([-0.66496953,  0.58252789,  0.64134377,  0.53102321,  0.59439807,\n",
       "        -0.25321179]),\n",
       " array([-0.66558417,  0.58236044,  0.64128442,  0.5308313 ,  0.59422138,\n",
       "        -0.25351357]),\n",
       " array([-0.6661971 ,  0.58219344,  0.64122551,  0.53063991,  0.59404513,\n",
       "        -0.25381395]),\n",
       " array([-0.66680834,  0.58202687,  0.64116706,  0.53044903,  0.59386933,\n",
       "        -0.25411294]),\n",
       " array([-0.66741789,  0.58186074,  0.64110904,  0.53025866,  0.59369396,\n",
       "        -0.25441055]),\n",
       " array([-0.66802576,  0.58169505,  0.64105148,  0.53006879,  0.59351904,\n",
       "        -0.25470678]),\n",
       " array([-0.66863195,  0.58152979,  0.64099435,  0.52987943,  0.59334455,\n",
       "        -0.25500163]),\n",
       " array([-0.66923648,  0.58136496,  0.64093766,  0.52969057,  0.5931705 ,\n",
       "        -0.25529512]),\n",
       " array([-0.66983936,  0.58120056,  0.64088142,  0.52950221,  0.59299687,\n",
       "        -0.25558725]),\n",
       " array([-0.67044058,  0.58103659,  0.6408256 ,  0.52931435,  0.59282368,\n",
       "        -0.25587803]),\n",
       " array([-0.67104016,  0.58087305,  0.64077023,  0.52912698,  0.59265092,\n",
       "        -0.25616746]),\n",
       " array([-0.67163811,  0.58070992,  0.64071528,  0.5289401 ,  0.59247859,\n",
       "        -0.25645555]),\n",
       " array([-0.67223443,  0.58054722,  0.64066076,  0.52875371,  0.59230668,\n",
       "        -0.2567423 ]),\n",
       " array([-0.67282914,  0.58038494,  0.64060668,  0.52856781,  0.59213519,\n",
       "        -0.25702773]),\n",
       " array([-0.67342223,  0.58022308,  0.64055301,  0.5283824 ,  0.59196413,\n",
       "        -0.25731183]),\n",
       " array([-0.67401371,  0.58006163,  0.64049978,  0.52819747,  0.59179348,\n",
       "        -0.25759462]),\n",
       " array([-0.6746036 ,  0.5799006 ,  0.64044696,  0.52801303,  0.59162326,\n",
       "        -0.2578761 ]),\n",
       " array([-0.6751919 ,  0.57973998,  0.64039457,  0.52782906,  0.59145345,\n",
       "        -0.25815627]),\n",
       " array([-0.67577862,  0.57957977,  0.6403426 ,  0.52764557,  0.59128405,\n",
       "        -0.25843514]),\n",
       " array([-0.67636376,  0.57941997,  0.64029104,  0.52746256,  0.59111507,\n",
       "        -0.25871273]),\n",
       " array([-0.67694734,  0.57926057,  0.64023989,  0.52728002,  0.59094649,\n",
       "        -0.25898902]),\n",
       " array([-0.67752935,  0.57910158,  0.64018917,  0.52709795,  0.59077833,\n",
       "        -0.25926404]),\n",
       " array([-0.67810981,  0.578943  ,  0.64013885,  0.52691635,  0.59061057,\n",
       "        -0.25953778]),\n",
       " array([-0.67868872,  0.57878481,  0.64008894,  0.52673522,  0.59044322,\n",
       "        -0.25981025]),\n",
       " array([-0.6792661 ,  0.57862703,  0.64003944,  0.52655455,  0.59027627,\n",
       "        -0.26008146]),\n",
       " array([-0.67984194,  0.57846964,  0.63999035,  0.52637435,  0.59010972,\n",
       "        -0.26035141]),\n",
       " array([-0.68041625,  0.57831265,  0.63994166,  0.52619461,  0.58994357,\n",
       "        -0.2606201 ]),\n",
       " array([-0.68098905,  0.57815605,  0.63989337,  0.52601533,  0.58977783,\n",
       "        -0.26088756]),\n",
       " array([-0.68156033,  0.57799985,  0.63984548,  0.5258365 ,  0.58961247,\n",
       "        -0.26115377]),\n",
       " array([-0.68213011,  0.57784404,  0.639798  ,  0.52565813,  0.58944752,\n",
       "        -0.26141874]),\n",
       " array([-0.68269839,  0.57768861,  0.63975091,  0.52548021,  0.58928296,\n",
       "        -0.26168249]),\n",
       " array([-0.68326518,  0.57753358,  0.63970421,  0.52530275,  0.58911878,\n",
       "        -0.26194501]),\n",
       " array([-0.68383048,  0.57737893,  0.63965791,  0.52512573,  0.588955  ,\n",
       "        -0.26220632]),\n",
       " array([-0.68439431,  0.57722466,  0.63961201,  0.52494916,  0.58879161,\n",
       "        -0.26246641]),\n",
       " array([-0.68495666,  0.57707078,  0.63956649,  0.52477304,  0.5886286 ,\n",
       "        -0.26272529]),\n",
       " array([-0.68551755,  0.57691728,  0.63952136,  0.52459736,  0.58846598,\n",
       "        -0.26298297]),\n",
       " array([-0.68607697,  0.57676415,  0.63947662,  0.52442212,  0.58830374,\n",
       "        -0.26323945]),\n",
       " array([-0.68663495,  0.57661141,  0.63943227,  0.52424732,  0.58814189,\n",
       "        -0.26349475]),\n",
       " array([-0.68719148,  0.57645904,  0.6393883 ,  0.52407296,  0.58798041,\n",
       "        -0.26374885]),\n",
       " array([-0.68774657,  0.57630704,  0.63934471,  0.52389904,  0.58781931,\n",
       "        -0.26400178]),\n",
       " array([-0.68830023,  0.57615542,  0.6393015 ,  0.52372555,  0.58765859,\n",
       "        -0.26425352]),\n",
       " array([-0.68885246,  0.57600416,  0.63925867,  0.52355249,  0.58749825,\n",
       "        -0.2645041 ]),\n",
       " array([-0.68940327,  0.57585328,  0.63921622,  0.52337986,  0.58733828,\n",
       "        -0.26475351]),\n",
       " array([-0.68995266,  0.57570277,  0.63917415,  0.52320767,  0.58717868,\n",
       "        -0.26500177]),\n",
       " array([-0.69050065,  0.57555262,  0.63913245,  0.52303589,  0.58701945,\n",
       "        -0.26524886]),\n",
       " array([-0.69104723,  0.57540283,  0.63909112,  0.52286455,  0.58686059,\n",
       "        -0.26549481]),\n",
       " array([-0.69159242,  0.57525341,  0.63905016,  0.52269363,  0.5867021 ,\n",
       "        -0.26573961]),\n",
       " array([-0.69213622,  0.57510435,  0.63900957,  0.52252312,  0.58654397,\n",
       "        -0.26598327]),\n",
       " array([-0.69267864,  0.57495565,  0.63896935,  0.52235304,  0.58638621,\n",
       "        -0.26622579]),\n",
       " array([-0.69321967,  0.57480731,  0.6389295 ,  0.52218338,  0.58622882,\n",
       "        -0.26646719]),\n",
       " array([-0.69375934,  0.57465933,  0.63889001,  0.52201413,  0.58607178,\n",
       "        -0.26670746]),\n",
       " array([-0.69429764,  0.5745117 ,  0.63885089,  0.5218453 ,  0.5859151 ,\n",
       "        -0.26694661]),\n",
       " array([-0.69483458,  0.57436443,  0.63881213,  0.52167688,  0.58575879,\n",
       "        -0.26718464]),\n",
       " array([-0.69537017,  0.5742175 ,  0.63877372,  0.52150887,  0.58560283,\n",
       "        -0.26742156]),\n",
       " array([-0.69590441,  0.57407093,  0.63873568,  0.52134127,  0.58544722,\n",
       "        -0.26765738]),\n",
       " array([-0.6964373 ,  0.57392471,  0.63869799,  0.52117408,  0.58529197,\n",
       "        -0.2678921 ]),\n",
       " array([-0.69696886,  0.57377884,  0.63866066,  0.52100729,  0.58513707,\n",
       "        -0.26812571]),\n",
       " array([-0.69749909,  0.57363331,  0.63862369,  0.52084091,  0.58498253,\n",
       "        -0.26835824]),\n",
       " array([-0.698028  ,  0.57348813,  0.63858706,  0.52067493,  0.58482833,\n",
       "        -0.26858968]),\n",
       " array([-0.69855558,  0.57334329,  0.63855079,  0.52050935,  0.58467448,\n",
       "        -0.26882004]),\n",
       " array([-0.69908185,  0.5731988 ,  0.63851487,  0.52034417,  0.58452098,\n",
       "        -0.26904932]),\n",
       " array([-0.69960682,  0.57305464,  0.63847929,  0.52017939,  0.58436782,\n",
       "        -0.26927753]),\n",
       " array([-0.70013048,  0.57291082,  0.63844407,  0.52001501,  0.58421501,\n",
       "        -0.26950467]),\n",
       " array([-0.70065284,  0.57276735,  0.63840919,  0.51985101,  0.58406254,\n",
       "        -0.26973074]),\n",
       " array([-0.70117391,  0.5726242 ,  0.63837465,  0.51968742,  0.58391042,\n",
       "        -0.26995576]),\n",
       " array([-0.7016937 ,  0.5724814 ,  0.63834046,  0.51952421,  0.58375863,\n",
       "        -0.27017972]),\n",
       " array([-0.7022122 ,  0.57233893,  0.6383066 ,  0.51936139,  0.58360718,\n",
       "        -0.27040263]),\n",
       " array([-0.70272944,  0.57219679,  0.63827309,  0.51919896,  0.58345607,\n",
       "        -0.27062449]),\n",
       " array([-0.7032454 ,  0.57205498,  0.63823992,  0.51903692,  0.58330529,\n",
       "        -0.27084532]),\n",
       " array([-0.70376009,  0.5719135 ,  0.63820708,  0.51887526,  0.58315485,\n",
       "        -0.2710651 ]),\n",
       " array([-0.70427353,  0.57177234,  0.63817458,  0.51871398,  0.58300474,\n",
       "        -0.27128386]),\n",
       " array([-0.70478572,  0.57163152,  0.63814241,  0.51855309,  0.58285497,\n",
       "        -0.27150158]),\n",
       " array([-0.70529665,  0.57149102,  0.63811058,  0.51839257,  0.58270552,\n",
       "        -0.27171829]),\n",
       " array([-0.70580635,  0.57135085,  0.63807908,  0.51823244,  0.5825564 ,\n",
       "        -0.27193397]),\n",
       " array([-0.7063148 ,  0.57121099,  0.6380479 ,  0.51807268,  0.58240761,\n",
       "        -0.27214863]),\n",
       " array([-0.70682202,  0.57107146,  0.63801706,  0.51791329,  0.58225915,\n",
       "        -0.27236229]),\n",
       " array([-0.70732802,  0.57093225,  0.63798655,  0.51775429,  0.58211101,\n",
       "        -0.27257494]),\n",
       " array([-0.70783279,  0.57079336,  0.63795636,  0.51759565,  0.5819632 ,\n",
       "        -0.27278659]),\n",
       " array([-0.70833635,  0.57065479,  0.63792649,  0.51743738,  0.58181571,\n",
       "        -0.27299723]),\n",
       " array([-0.70883869,  0.57051653,  0.63789695,  0.51727949,  0.58166854,\n",
       "        -0.27320689]),\n",
       " array([-0.70933983,  0.57037859,  0.63786774,  0.51712196,  0.58152169,\n",
       "        -0.27341555]),\n",
       " array([-0.70983976,  0.57024096,  0.63783884,  0.51696479,  0.58137516,\n",
       "        -0.27362323]),\n",
       " array([-0.7103385 ,  0.57010364,  0.63781026,  0.516808  ,  0.58122895,\n",
       "        -0.27382993]),\n",
       " array([-0.71083604,  0.56996664,  0.637782  ,  0.51665156,  0.58108305,\n",
       "        -0.27403565]),\n",
       " array([-0.7113324 ,  0.56982994,  0.63775406,  0.51649549,  0.58093747,\n",
       "        -0.27424039]),\n",
       " array([-0.71182757,  0.56969356,  0.63772644,  0.51633978,  0.5807922 ,\n",
       "        -0.27444417]),\n",
       " array([-0.71232157,  0.56955748,  0.63769913,  0.51618443,  0.58064725,\n",
       "        -0.27464698]),\n",
       " array([-0.7128144 ,  0.5694217 ,  0.63767213,  0.51602944,  0.5805026 ,\n",
       "        -0.27484883]),\n",
       " array([-0.71330605,  0.56928624,  0.63764545,  0.5158748 ,  0.58035827,\n",
       "        -0.27504972]),\n",
       " array([-0.71379655,  0.56915107,  0.63761907,  0.51572052,  0.58021424,\n",
       "        -0.27524965]),\n",
       " array([-0.71428588,  0.56901621,  0.63759301,  0.51556659,  0.58007053,\n",
       "        -0.27544864]),\n",
       " array([-0.71477407,  0.56888165,  0.63756725,  0.51541301,  0.57992711,\n",
       "        -0.27564668]),\n",
       " array([-0.7152611 ,  0.56874739,  0.6375418 ,  0.51525978,  0.57978401,\n",
       "        -0.27584378]),\n",
       " array([-0.71574699,  0.56861343,  0.63751666,  0.51510691,  0.57964121,\n",
       "        -0.27603995]),\n",
       " array([-0.71623174,  0.56847977,  0.63749182,  0.51495438,  0.57949871,\n",
       "        -0.27623518]),\n",
       " array([-0.71671535,  0.5683464 ,  0.63746729,  0.5148022 ,  0.57935651,\n",
       "        -0.27642947]),\n",
       " array([-0.71719784,  0.56821333,  0.63744306,  0.51465036,  0.57921461,\n",
       "        -0.27662284]),\n",
       " array([-0.7176792 ,  0.56808055,  0.63741913,  0.51449887,  0.57907302,\n",
       "        -0.27681529]),\n",
       " array([-0.71815943,  0.56794807,  0.6373955 ,  0.51434771,  0.57893172,\n",
       "        -0.27700682]),\n",
       " array([-0.71863856,  0.56781587,  0.63737216,  0.51419691,  0.57879071,\n",
       "        -0.27719744]),\n",
       " array([-0.71911657,  0.56768397,  0.63734913,  0.51404644,  0.57865001,\n",
       "        -0.27738714]),\n",
       " array([-0.71959347,  0.56755236,  0.63732639,  0.5138963 ,  0.5785096 ,\n",
       "        -0.27757593]),\n",
       " array([-0.72006927,  0.56742103,  0.63730395,  0.51374651,  0.57836948,\n",
       "        -0.27776382]),\n",
       " array([-0.72054397,  0.56729   ,  0.6372818 ,  0.51359705,  0.57822965,\n",
       "        -0.27795082]),\n",
       " array([-0.72101757,  0.56715925,  0.63725994,  0.51344793,  0.57809012,\n",
       "        -0.27813691]),\n",
       " array([-0.72149009,  0.56702878,  0.63723838,  0.51329914,  0.57795088,\n",
       "        -0.27832211]),\n",
       " array([-0.72196152,  0.5668986 ,  0.63721711,  0.51315068,  0.57781192,\n",
       "        -0.27850642]),\n",
       " array([-0.72243187,  0.5667687 ,  0.63719612,  0.51300255,  0.57767325,\n",
       "        -0.27868985]),\n",
       " array([-0.72290115,  0.56663908,  0.63717543,  0.51285476,  0.57753487,\n",
       "        -0.27887239]),\n",
       " array([-0.72336935,  0.56650975,  0.63715502,  0.51270729,  0.57739678,\n",
       "        -0.27905405]),\n",
       " array([-0.72383648,  0.56638069,  0.6371349 ,  0.51256014,  0.57725897,\n",
       "        -0.27923484]),\n",
       " array([-0.72430255,  0.56625191,  0.63711506,  0.51241333,  0.57712144,\n",
       "        -0.27941476]),\n",
       " array([-0.72476756,  0.56612341,  0.63709551,  0.51226683,  0.5769842 ,\n",
       "        -0.2795938 ]),\n",
       " array([-0.72523151,  0.56599518,  0.63707624,  0.51212066,  0.57684724,\n",
       "        -0.27977199]),\n",
       " array([-0.72569441,  0.56586723,  0.63705725,  0.51197482,  0.57671056,\n",
       "        -0.27994931]),\n",
       " array([-0.72615627,  0.56573955,  0.63703854,  0.51182929,  0.57657416,\n",
       "        -0.28012577]),\n",
       " array([-0.72661708,  0.56561215,  0.63702012,  0.51168408,  0.57643803,\n",
       "        -0.28030138]),\n",
       " array([-0.72707685,  0.56548502,  0.63700197,  0.51153919,  0.57630218,\n",
       "        -0.28047614]),\n",
       " array([-0.72753559,  0.56535816,  0.63698409,  0.51139462,  0.57616661,\n",
       "        -0.28065005]),\n",
       " array([-0.7279933 ,  0.56523157,  0.6369665 ,  0.51125036,  0.57603132,\n",
       "        -0.28082312]),\n",
       " array([-0.72844998,  0.56510525,  0.63694918,  0.51110642,  0.5758963 ,\n",
       "        -0.28099535]),\n",
       " array([-0.72890564,  0.56497919,  0.63693213,  0.51096279,  0.57576155,\n",
       "        -0.28116673]),\n",
       " array([-0.72936028,  0.5648534 ,  0.63691536,  0.51081947,  0.57562707,\n",
       "        -0.28133729]),\n",
       " array([-0.72981391,  0.56472788,  0.63689886,  0.51067647,  0.57549287,\n",
       "        -0.28150701]),\n",
       " array([-0.73026652,  0.56460263,  0.63688263,  0.51053377,  0.57535893,\n",
       "        -0.2816759 ]),\n",
       " array([-0.73071813,  0.56447763,  0.63686667,  0.51039138,  0.57522527,\n",
       "        -0.28184397]),\n",
       " array([-0.73116873,  0.5643529 ,  0.63685098,  0.5102493 ,  0.57509187,\n",
       "        -0.28201122]),\n",
       " array([-0.73161834,  0.56422843,  0.63683556,  0.51010753,  0.57495874,\n",
       "        -0.28217765]),\n",
       " array([-0.73206695,  0.56410422,  0.6368204 ,  0.50996606,  0.57482587,\n",
       "        -0.28234327]),\n",
       " array([-0.73251457,  0.56398027,  0.63680551,  0.5098249 ,  0.57469327,\n",
       "        -0.28250807]),\n",
       " array([-0.73296121,  0.56385658,  0.63679089,  0.50968404,  0.57456094,\n",
       "        -0.28267207]),\n",
       " array([-0.73340686,  0.56373315,  0.63677652,  0.50954348,  0.57442886,\n",
       "        -0.28283526]),\n",
       " array([-0.73385153,  0.56360998,  0.63676243,  0.50940322,  0.57429705,\n",
       "        -0.28299765]),\n",
       " array([-0.73429522,  0.56348706,  0.63674859,  0.50926326,  0.5741655 ,\n",
       "        -0.28315924]),\n",
       " array([-0.73473794,  0.56336439,  0.63673502,  0.5091236 ,  0.57403421,\n",
       "        -0.28332003]),\n",
       " array([-0.7351797 ,  0.56324198,  0.6367217 ,  0.50898423,  0.57390318,\n",
       "        -0.28348003]),\n",
       " array([-0.73562049,  0.56311982,  0.63670864,  0.50884516,  0.57377241,\n",
       "        -0.28363924]),\n",
       " array([-0.73606032,  0.56299792,  0.63669585,  0.50870639,  0.57364189,\n",
       "        -0.28379767]),\n",
       " array([-0.73649919,  0.56287626,  0.6366833 ,  0.50856791,  0.57351163,\n",
       "        -0.28395531]),\n",
       " array([-0.7369371 ,  0.56275485,  0.63667102,  0.50842973,  0.57338163,\n",
       "        -0.28411217]),\n",
       " array([-0.73737407,  0.5626337 ,  0.63665899,  0.50829183,  0.57325188,\n",
       "        -0.28426825]),\n",
       " array([-0.73781009,  0.56251279,  0.63664721,  0.50815423,  0.57312238,\n",
       "        -0.28442356]),\n",
       " array([-0.73824517,  0.56239213,  0.63663569,  0.50801691,  0.57299314,\n",
       "        -0.2845781 ]),\n",
       " array([-0.73867931,  0.56227171,  0.63662442,  0.50787989,  0.57286415,\n",
       "        -0.28473186]),\n",
       " array([-0.73911252,  0.56215154,  0.6366134 ,  0.50774315,  0.5727354 ,\n",
       "        -0.28488487]),\n",
       " array([-0.73954479,  0.56203162,  0.63660263,  0.5076067 ,  0.57260691,\n",
       "        -0.28503711]),\n",
       " array([-0.73997614,  0.56191194,  0.63659211,  0.50747053,  0.57247867,\n",
       "        -0.28518859]),\n",
       " array([-0.74040656,  0.5617925 ,  0.63658184,  0.50733465,  0.57235068,\n",
       "        -0.28533931]),\n",
       " array([-0.74083605,  0.5616733 ,  0.63657182,  0.50719905,  0.57222293,\n",
       "        -0.28548928]),\n",
       " array([-0.74126463,  0.56155435,  0.63656204,  0.50706373,  0.57209543,\n",
       "        -0.2856385 ]),\n",
       " array([-0.7416923 ,  0.56143563,  0.63655251,  0.5069287 ,  0.57196817,\n",
       "        -0.28578697]),\n",
       " array([-0.74211906,  0.56131715,  0.63654323,  0.50679394,  0.57184116,\n",
       "        -0.2859347 ]),\n",
       " array([-0.7425449 ,  0.56119891,  0.63653418,  0.50665947,  0.57171439,\n",
       "        -0.28608168]),\n",
       " array([-0.74296985,  0.56108091,  0.63652538,  0.50652527,  0.57158786,\n",
       "        -0.28622792]),\n",
       " array([-0.74339389,  0.56096315,  0.63651683,  0.50639135,  0.57146158,\n",
       "        -0.28637343]),\n",
       " array([-0.74381704,  0.56084562,  0.63650851,  0.5062577 ,  0.57133554,\n",
       "        -0.2865182 ]),\n",
       " array([-0.74423929,  0.56072832,  0.63650044,  0.50612433,  0.57120973,\n",
       "        -0.28666225]),\n",
       " array([-0.74466065,  0.56061126,  0.6364926 ,  0.50599123,  0.57108417,\n",
       "        -0.28680556]),\n",
       " array([-0.74508112,  0.56049443,  0.636485  ,  0.50585841,  0.57095884,\n",
       "        -0.28694815]),\n",
       " array([-0.74550071,  0.56037784,  0.63647764,  0.50572586,  0.57083375,\n",
       "        -0.28709001]),\n",
       " array([-0.74591942,  0.56026147,  0.63647052,  0.50559358,  0.5707089 ,\n",
       "        -0.28723116]),\n",
       " array([-0.74633726,  0.56014534,  0.63646363,  0.50546157,  0.57058428,\n",
       "        -0.28737158]),\n",
       " array([-0.74675421,  0.56002943,  0.63645698,  0.50532983,  0.5704599 ,\n",
       "        -0.2875113 ]),\n",
       " array([-0.7471703 ,  0.55991376,  0.63645056,  0.50519836,  0.57033576,\n",
       "        -0.2876503 ]),\n",
       " array([-0.74758552,  0.55979831,  0.63644438,  0.50506715,  0.57021184,\n",
       "        -0.28778859]),\n",
       " array([-0.74799988,  0.55968309,  0.63643842,  0.50493621,  0.57008816,\n",
       "        -0.28792617]),\n",
       " array([-0.74841337,  0.55956809,  0.6364327 ,  0.50480554,  0.56996471,\n",
       "        -0.28806305]),\n",
       " array([-0.74882601,  0.55945332,  0.63642721,  0.50467513,  0.56984149,\n",
       "        -0.28819923]),\n",
       " array([-0.74923779,  0.55933878,  0.63642195,  0.50454498,  0.5697185 ,\n",
       "        -0.28833471]),\n",
       " array([-0.74964872,  0.55922446,  0.63641692,  0.5044151 ,  0.56959574,\n",
       "        -0.2884695 ]),\n",
       " array([-0.7500588 ,  0.55911036,  0.63641212,  0.50428548,  0.56947321,\n",
       "        -0.28860359]),\n",
       " array([-0.75046804,  0.55899648,  0.63640755,  0.50415612,  0.5693509 ,\n",
       "        -0.28873699]),\n",
       " array([-0.75087643,  0.55888283,  0.6364032 ,  0.50402701,  0.56922882,\n",
       "        -0.2888697 ]),\n",
       " array([-0.75128398,  0.55876939,  0.63639908,  0.50389817,  0.56910697,\n",
       "        -0.28900173]),\n",
       " array([-0.7516907 ,  0.55865618,  0.63639518,  0.50376958,  0.56898534,\n",
       "        -0.28913307]),\n",
       " array([-0.75209659,  0.55854319,  0.6363915 ,  0.50364125,  0.56886394,\n",
       "        -0.28926373]),\n",
       " array([-0.75250164,  0.55843041,  0.63638806,  0.50351318,  0.56874276,\n",
       "        -0.28939372]),\n",
       " array([-0.75290587,  0.55831785,  0.63638483,  0.50338536,  0.5686218 ,\n",
       "        -0.28952303]),\n",
       " array([-0.75330928,  0.55820551,  0.63638182,  0.5032578 ,  0.56850106,\n",
       "        -0.28965166]),\n",
       " array([-0.75371186,  0.55809338,  0.63637904,  0.50313049,  0.56838055,\n",
       "        -0.28977963]),\n",
       " array([-0.75411363,  0.55798147,  0.63637647,  0.50300343,  0.56826026,\n",
       "        -0.28990692]),\n",
       " array([-0.75451458,  0.55786977,  0.63637413,  0.50287662,  0.56814018,\n",
       "        -0.29003355]),\n",
       " array([-0.75491472,  0.55775829,  0.636372  ,  0.50275007,  0.56802033,\n",
       "        -0.29015952]),\n",
       " array([-0.75531405,  0.55764701,  0.63637009,  0.50262376,  0.56790069,\n",
       "        -0.29028483]),\n",
       " array([-0.75571257,  0.55753596,  0.6363684 ,  0.5024977 ,  0.56778127,\n",
       "        -0.29040948]),\n",
       " array([-0.75611029,  0.55742511,  0.63636693,  0.50237189,  0.56766206,\n",
       "        -0.29053347]),\n",
       " array([-0.75650721,  0.55731447,  0.63636567,  0.50224633,  0.56754308,\n",
       "        -0.29065681]),\n",
       " array([-0.75690333,  0.55720404,  0.63636462,  0.50212101,  0.5674243 ,\n",
       "        -0.2907795 ]),\n",
       " array([-0.75729866,  0.55709383,  0.63636379,  0.50199594,  0.56730574,\n",
       "        -0.29090154]),\n",
       " array([-0.7576932 ,  0.55698382,  0.63636317,  0.50187111,  0.5671874 ,\n",
       "        -0.29102293]),\n",
       " array([-0.75808695,  0.55687401,  0.63636276,  0.50174653,  0.56706927,\n",
       "        -0.29114368]),\n",
       " array([-0.75847991,  0.55676442,  0.63636257,  0.50162219,  0.56695135,\n",
       "        -0.29126379]),\n",
       " array([-0.75887209,  0.55665503,  0.63636259,  0.50149809,  0.56683364,\n",
       "        -0.29138325]),\n",
       " array([-0.75926348,  0.55654584,  0.63636281,  0.50137423,  0.56671614,\n",
       "        -0.29150209]),\n",
       " array([-0.75965411,  0.55643686,  0.63636325,  0.50125062,  0.56659885,\n",
       "        -0.29162028]),\n",
       " array([-0.76004395,  0.55632809,  0.63636389,  0.50112724,  0.56648177,\n",
       "        -0.29173785]),\n",
       " array([-0.76043302,  0.55621952,  0.63636475,  0.5010041 ,  0.5663649 ,\n",
       "        -0.29185478]),\n",
       " array([-0.76082133,  0.55611115,  0.63636581,  0.5008812 ,  0.56624823,\n",
       "        -0.29197109]),\n",
       " array([-0.76120887,  0.55600298,  0.63636707,  0.50075853,  0.56613178,\n",
       "        -0.29208677]),\n",
       " array([-0.76159564,  0.55589501,  0.63636855,  0.5006361 ,  0.56601553,\n",
       "        -0.29220183]),\n",
       " array([-0.76198165,  0.55578725,  0.63637022,  0.50051391,  0.56589948,\n",
       "        -0.29231626]),\n",
       " array([-0.76236691,  0.55567968,  0.6363721 ,  0.50039195,  0.56578364,\n",
       "        -0.29243008]),\n",
       " array([-0.76275141,  0.55557231,  0.63637419,  0.50027022,  0.56566801,\n",
       "        -0.29254329]),\n",
       " array([-0.76313515,  0.55546515,  0.63637648,  0.50014873,  0.56555257,\n",
       "        -0.29265587]),\n",
       " array([-0.76351815,  0.55535817,  0.63637897,  0.50002747,  0.56543734,\n",
       "        -0.29276785]),\n",
       " array([-0.7639004 ,  0.5552514 ,  0.63638166,  0.49990644,  0.56532232,\n",
       "        -0.29287922]),\n",
       " array([-0.7642819 ,  0.55514482,  0.63638455,  0.49978564,  0.56520749,\n",
       "        -0.29298997]),\n",
       " array([-0.76466266,  0.55503844,  0.63638764,  0.49966507,  0.56509287,\n",
       "        -0.29310013]),\n",
       " array([-0.76504268,  0.55493225,  0.63639093,  0.49954473,  0.56497844,\n",
       "        -0.29320968]),\n",
       " array([-0.76542196,  0.55482626,  0.63639442,  0.49942462,  0.56486421,\n",
       "        -0.29331863]),\n",
       " array([-0.76580051,  0.55472046,  0.63639811,  0.49930473,  0.56475019,\n",
       "        -0.29342698]),\n",
       " array([-0.76617833,  0.55461485,  0.636402  ,  0.49918508,  0.56463636,\n",
       "        -0.29353473]),\n",
       " array([-0.76655542,  0.55450944,  0.63640608,  0.49906564,  0.56452273,\n",
       "        -0.29364189]),\n",
       " array([-0.76693178,  0.55440421,  0.63641036,  0.49894643,  0.56440929,\n",
       "        -0.29374845]),\n",
       " array([-0.76730741,  0.55429918,  0.63641483,  0.49882745,  0.56429605,\n",
       "        -0.29385443]),\n",
       " array([-0.76768233,  0.55419434,  0.6364195 ,  0.49870869,  0.56418301,\n",
       "        -0.29395982]),\n",
       " array([-0.76805652,  0.55408969,  0.63642436,  0.49859015,  0.56407016,\n",
       "        -0.29406462]),\n",
       " array([-0.76843   ,  0.55398523,  0.63642941,  0.49847184,  0.5639575 ,\n",
       "        -0.29416884]),\n",
       " array([-0.76880276,  0.55388095,  0.63643466,  0.49835374,  0.56384504,\n",
       "        -0.29427247]),\n",
       " array([-0.76917481,  0.55377686,  0.6364401 ,  0.49823587,  0.56373277,\n",
       "        -0.29437553]),\n",
       " array([-0.76954616,  0.55367296,  0.63644573,  0.49811821,  0.56362069,\n",
       "        -0.294478  ]),\n",
       " array([-0.76991679,  0.55356925,  0.63645155,  0.49800078,  0.56350881,\n",
       "        -0.2945799 ]),\n",
       " array([-0.77028672,  0.55346572,  0.63645756,  0.49788356,  0.56339711,\n",
       "        -0.29468123]),\n",
       " array([-0.77065595,  0.55336238,  0.63646376,  0.49776656,  0.56328561,\n",
       "        -0.29478199]),\n",
       " array([-0.77102448,  0.55325922,  0.63647014,  0.49764978,  0.56317429,\n",
       "        -0.29488217]),\n",
       " array([-0.77139231,  0.55315625,  0.63647672,  0.49753321,  0.56306317,\n",
       "        -0.29498179]),\n",
       " array([-0.77175945,  0.55305346,  0.63648348,  0.49741686,  0.56295223,\n",
       "        -0.29508085]),\n",
       " array([-0.77212589,  0.55295085,  0.63649043,  0.49730072,  0.56284148,\n",
       "        -0.29517933]),\n",
       " array([-0.77249165,  0.55284842,  0.63649757,  0.4971848 ,  0.56273091,\n",
       "        -0.29527726]),\n",
       " array([-0.77285671,  0.55274618,  0.63650489,  0.49706909,  0.56262054,\n",
       "        -0.29537463]),\n",
       " array([-0.77322109,  0.55264412,  0.63651239,  0.49695359,  0.56251035,\n",
       "        -0.29547144]),\n",
       " array([-0.77358479,  0.55254223,  0.63652008,  0.49683831,  0.56240034,\n",
       "        -0.29556769]),\n",
       " array([-0.77394781,  0.55244053,  0.63652795,  0.49672323,  0.56229052,\n",
       "        -0.29566339]),\n",
       " array([-0.77431015,  0.552339  ,  0.63653601,  0.49660837,  0.56218088,\n",
       "        -0.29575854]),\n",
       " array([-0.77467182,  0.55223766,  0.63654425,  0.49649371,  0.56207143,\n",
       "        -0.29585314]),\n",
       " array([-0.77503281,  0.55213649,  0.63655267,  0.49637927,  0.56196216,\n",
       "        -0.29594719]),\n",
       " array([-0.77539312,  0.5520355 ,  0.63656127,  0.49626503,  0.56185307,\n",
       "        -0.29604069]),\n",
       " array([-0.77575277,  0.55193468,  0.63657005,  0.496151  ,  0.56174416,\n",
       "        -0.29613365]),\n",
       " array([-0.77611176,  0.55183404,  0.63657901,  0.49603717,  0.56163543,\n",
       "        -0.29622607]),\n",
       " array([-0.77647008,  0.55173358,  0.63658814,  0.49592356,  0.56152688,\n",
       "        -0.29631794]),\n",
       " array([-0.77682773,  0.55163329,  0.63659746,  0.49581014,  0.56141852,\n",
       "        -0.29640928]),\n",
       " array([-0.77718473,  0.55153318,  0.63660696,  0.49569694,  0.56131033,\n",
       "        -0.29650008]),\n",
       " array([-0.77754107,  0.55143324,  0.63661663,  0.49558393,  0.56120232,\n",
       "        -0.29659035]),\n",
       " array([-0.77789675,  0.55133347,  0.63662648,  0.49547113,  0.56109449,\n",
       "        -0.29668008]),\n",
       " array([-0.77825178,  0.55123387,  0.63663651,  0.49535854,  0.56098684,\n",
       "        -0.29676928]),\n",
       " array([-0.77860616,  0.55113445,  0.63664671,  0.49524614,  0.56087936,\n",
       "        -0.29685796]),\n",
       " array([-0.77895989,  0.5510352 ,  0.63665708,  0.49513395,  0.56077206,\n",
       "        -0.2969461 ]),\n",
       " array([-0.77931297,  0.55093612,  0.63666763,  0.49502196,  0.56066493,\n",
       "        -0.29703372]),\n",
       " array([-0.77966541,  0.55083721,  0.63667836,  0.49491016,  0.56055798,\n",
       "        -0.29712082]),\n",
       " array([-0.7800172 ,  0.55073847,  0.63668925,  0.49479857,  0.56045121,\n",
       "        -0.29720739]),\n",
       " array([-0.78036836,  0.55063989,  0.63670032,  0.49468718,  0.56034461,\n",
       "        -0.29729344]),\n",
       " array([-0.78071888,  0.55054149,  0.63671156,  0.49457598,  0.56023818,\n",
       "        -0.29737898]),\n",
       " array([-0.78106876,  0.55044326,  0.63672298,  0.49446498,  0.56013192,\n",
       "        -0.29746399]),\n",
       " array([-0.78141801,  0.55034519,  0.63673456,  0.49435418,  0.56002584,\n",
       "        -0.2975485 ]),\n",
       " array([-0.78176662,  0.55024729,  0.63674631,  0.49424358,  0.55991993,\n",
       "        -0.29763249]),\n",
       " array([-0.78211461,  0.55014955,  0.63675824,  0.49413317,  0.55981419,\n",
       "        -0.29771597]),\n",
       " array([-0.78246197,  0.55005199,  0.63677033,  0.49402295,  0.55970862,\n",
       "        -0.29779894]),\n",
       " array([-0.7828087 ,  0.54995458,  0.63678259,  0.49391293,  0.55960322,\n",
       "        -0.2978814 ]),\n",
       " array([-0.78315481,  0.54985734,  0.63679502,  0.4938031 ,  0.55949799,\n",
       "        -0.29796335]),\n",
       " array([-0.7835003 ,  0.54976027,  0.63680761,  0.49369347,  0.55939293,\n",
       "        -0.2980448 ]),\n",
       " array([-0.78384517,  0.54966336,  0.63682038,  0.49358403,  0.55928804,\n",
       "        -0.29812575]),\n",
       " array([-0.78418942,  0.54956661,  0.63683331,  0.49347478,  0.55918331,\n",
       "        -0.2982062 ]),\n",
       " array([-0.78453306,  0.54947003,  0.6368464 ,  0.49336572,  0.55907876,\n",
       "        -0.29828614]),\n",
       " array([-0.78487609,  0.54937361,  0.63685966,  0.49325685,  0.55897437,\n",
       "        -0.29836559]),\n",
       " array([-0.7852185 ,  0.54927734,  0.63687309,  0.49314818,  0.55887014,\n",
       "        -0.29844455]),\n",
       " array([-0.7855603 ,  0.54918124,  0.63688667,  0.49303969,  0.55876609,\n",
       "        -0.29852301]),\n",
       " array([-0.7859015 ,  0.54908531,  0.63690043,  0.49293139,  0.55866219,\n",
       "        -0.29860098]),\n",
       " array([-0.78624209,  0.54898953,  0.63691434,  0.49282328,  0.55855846,\n",
       "        -0.29867845]),\n",
       " array([-0.78658208,  0.54889391,  0.63692842,  0.49271535,  0.5584549 ,\n",
       "        -0.29875544]),\n",
       " array([-0.78692147,  0.54879845,  0.63694266,  0.49260761,  0.5583515 ,\n",
       "        -0.29883194]),\n",
       " array([-0.78726026,  0.54870314,  0.63695706,  0.49250006,  0.55824826,\n",
       "        -0.29890795]),\n",
       " array([-0.78759845,  0.548608  ,  0.63697162,  0.4923927 ,  0.55814519,\n",
       "        -0.29898348]),\n",
       " array([-0.78793605,  0.54851301,  0.63698634,  0.49228552,  0.55804228,\n",
       "        -0.29905853]),\n",
       " array([-0.78827306,  0.54841818,  0.63700123,  0.49217852,  0.55793952,\n",
       "        -0.2991331 ]),\n",
       " array([-0.78860947,  0.54832351,  0.63701627,  0.49207171,  0.55783693,\n",
       "        -0.29920719]),\n",
       " array([-0.78894529,  0.54822899,  0.63703147,  0.49196508,  0.55773451,\n",
       "        -0.29928079]),\n",
       " array([-0.78928053,  0.54813463,  0.63704683,  0.49185864,  0.55763224,\n",
       "        -0.29935393]),\n",
       " array([-0.78961518,  0.54804042,  0.63706234,  0.49175237,  0.55753013,\n",
       "        -0.29942659]),\n",
       " array([-0.78994925,  0.54794637,  0.63707801,  0.49164629,  0.55742818,\n",
       "        -0.29949877]),\n",
       " array([-0.79028273,  0.54785247,  0.63709384,  0.49154039,  0.55732638,\n",
       "        -0.29957049]),\n",
       " array([-0.79061564,  0.54775872,  0.63710983,  0.49143467,  0.55722475,\n",
       "        -0.29964173]),\n",
       " array([-0.79094797,  0.54766513,  0.63712597,  0.49132913,  0.55712327,\n",
       "        -0.29971251]),\n",
       " array([-0.79127972,  0.54757169,  0.63714227,  0.49122377,  0.55702195,\n",
       "        -0.29978282]),\n",
       " array([-0.7916109 ,  0.5474784 ,  0.63715872,  0.49111859,  0.55692079,\n",
       "        -0.29985266]),\n",
       " array([-0.7919415 ,  0.54738526,  0.63717532,  0.49101359,  0.55681978,\n",
       "        -0.29992205]),\n",
       " array([-0.79227154,  0.54729228,  0.63719208,  0.49090876,  0.55671893,\n",
       "        -0.29999097]),\n",
       " array([-0.79260101,  0.54719944,  0.63720899,  0.49080411,  0.55661824,\n",
       "        -0.30005943]),\n",
       " array([-0.79292991,  0.54710676,  0.63722606,  0.49069964,  0.5565177 ,\n",
       "        -0.30012743]),\n",
       " array([-0.79325824,  0.54701422,  0.63724327,  0.49059534,  0.55641731,\n",
       "        -0.30019497]),\n",
       " array([-0.79358601,  0.54692184,  0.63726064,  0.49049122,  0.55631708,\n",
       "        -0.30026206]),\n",
       " array([-0.79391322,  0.5468296 ,  0.63727816,  0.49038727,  0.556217  ,\n",
       "        -0.30032869]),\n",
       " array([-0.79423988,  0.54673751,  0.63729583,  0.4902835 ,  0.55611707,\n",
       "        -0.30039487]),\n",
       " array([-0.79456597,  0.54664557,  0.63731365,  0.4901799 ,  0.55601729,\n",
       "        -0.3004606 ]),\n",
       " array([-0.79489151,  0.54655377,  0.63733162,  0.49007648,  0.55591767,\n",
       "        -0.30052588]),\n",
       " array([-0.79521649,  0.54646212,  0.63734974,  0.48997323,  0.5558182 ,\n",
       "        -0.30059071]),\n",
       " array([-0.79554092,  0.54637062,  0.63736801,  0.48987015,  0.55571887,\n",
       "        -0.30065509]),\n",
       " array([-0.7958648 ,  0.54627927,  0.63738642,  0.48976724,  0.5556197 ,\n",
       "        -0.30071903]),\n",
       " array([-0.79618814,  0.54618806,  0.63740499,  0.4896645 ,  0.55552068,\n",
       "        -0.30078253]),\n",
       " array([-0.79651092,  0.54609699,  0.6374237 ,  0.48956193,  0.55542181,\n",
       "        -0.30084558]),\n",
       " array([-0.79683316,  0.54600607,  0.63744255,  0.48945954,  0.55532309,\n",
       "        -0.30090819]),\n",
       " array([-0.79715486,  0.54591529,  0.63746156,  0.48935731,  0.55522451,\n",
       "        -0.30097036]),\n",
       " array([-0.79747601,  0.54582466,  0.63748071,  0.48925525,  0.55512608,\n",
       "        -0.3010321 ]),\n",
       " array([-0.79779663,  0.54573417,  0.6375    ,  0.48915336,  0.55502781,\n",
       "        -0.3010934 ]),\n",
       " array([-0.7981167 ,  0.54564382,  0.63751944,  0.48905164,  0.55492967,\n",
       "        -0.30115426]),\n",
       " array([-0.79843624,  0.54555362,  0.63753902,  0.48895008,  0.55483169,\n",
       "        -0.30121469]),\n",
       " array([-0.79875525,  0.54546355,  0.63755875,  0.48884869,  0.55473385,\n",
       "        -0.30127469]),\n",
       " array([-0.79907372,  0.54537363,  0.63757862,  0.48874747,  0.55463616,\n",
       "        -0.30133425]),\n",
       " array([-0.79939166,  0.54528385,  0.63759864,  0.48864642,  0.55453861,\n",
       "        -0.30139339]),\n",
       " array([-0.79970907,  0.54519421,  0.63761879,  0.48854552,  0.55444121,\n",
       "        -0.3014521 ]),\n",
       " array([-0.80002595,  0.54510471,  0.63763909,  0.4884448 ,  0.55434395,\n",
       "        -0.30151038]),\n",
       " array([-0.8003423 ,  0.54501535,  0.63765953,  0.48834424,  0.55424683,\n",
       "        -0.30156824]),\n",
       " array([-0.80065813,  0.54492613,  0.63768011,  0.48824384,  0.55414986,\n",
       "        -0.30162567]),\n",
       " array([-0.80097344,  0.54483704,  0.63770084,  0.48814361,  0.55405303,\n",
       "        -0.30168268]),\n",
       " array([-0.80128823,  0.5447481 ,  0.6377217 ,  0.48804353,  0.55395635,\n",
       "        -0.30173927]),\n",
       " array([-0.80160249,  0.54465929,  0.6377427 ,  0.48794362,  0.55385981,\n",
       "        -0.30179544]),\n",
       " array([-0.80191624,  0.54457062,  0.63776384,  0.48784388,  0.5537634 ,\n",
       "        -0.30185119]),\n",
       " array([-0.80222947,  0.54448209,  0.63778512,  0.48774429,  0.55366714,\n",
       "        -0.30190652]),\n",
       " array([-0.80254218,  0.54439369,  0.63780654,  0.48764486,  0.55357103,\n",
       "        -0.30196144]),\n",
       " array([-0.80285439,  0.54430543,  0.6378281 ,  0.4875456 ,  0.55347505,\n",
       "        -0.30201595]),\n",
       " array([-0.80316608,  0.54421731,  0.6378498 ,  0.48744649,  0.55337921,\n",
       "        -0.30207004]),\n",
       " array([-0.80347726,  0.54412932,  0.63787163,  0.48734755,  0.55328351,\n",
       "        -0.30212372]),\n",
       " array([-0.80378793,  0.54404147,  0.6378936 ,  0.48724876,  0.55318795,\n",
       "        -0.30217699]),\n",
       " array([-0.80409809,  0.54395375,  0.6379157 ,  0.48715013,  0.55309253,\n",
       "        -0.30222985]),\n",
       " array([-0.80440775,  0.54386617,  0.63793794,  0.48705166,  0.55299725,\n",
       "        -0.3022823 ]),\n",
       " array([-0.8047169 ,  0.54377871,  0.63796032,  0.48695335,  0.5529021 ,\n",
       "        -0.30233435]),\n",
       " array([-0.80502556,  0.5436914 ,  0.63798283,  0.48685519,  0.5528071 ,\n",
       "        -0.30238599]),\n",
       " array([-0.80533371,  0.54360421,  0.63800548,  0.48675719,  0.55271223,\n",
       "        -0.30243723]),\n",
       " array([-0.80564136,  0.54351716,  0.63802826,  0.48665935,  0.5526175 ,\n",
       "        -0.30248807]),\n",
       " array([-0.80594852,  0.54343024,  0.63805117,  0.48656166,  0.5525229 ,\n",
       "        -0.3025385 ]),\n",
       " array([-0.80625517,  0.54334345,  0.63807422,  0.48646412,  0.55242844,\n",
       "        -0.30258854]),\n",
       " array([-0.80656134,  0.54325679,  0.6380974 ,  0.48636674,  0.55233412,\n",
       "        -0.30263817]),\n",
       " array([-0.80686701,  0.54317027,  0.63812071,  0.48626952,  0.55223993,\n",
       "        -0.30268741]),\n",
       " array([-0.80717219,  0.54308387,  0.63814415,  0.48617245,  0.55214587,\n",
       "        -0.30273626]),\n",
       " array([-0.80747688,  0.54299761,  0.63816773,  0.48607553,  0.55205195,\n",
       "        -0.30278471]),\n",
       " array([-0.80778108,  0.54291147,  0.63819144,  0.48597876,  0.55195816,\n",
       "        -0.30283276]),\n",
       " array([-0.80808479,  0.54282546,  0.63821528,  0.48588215,  0.55186451,\n",
       "        -0.30288043]),\n",
       " array([-0.80838802,  0.54273959,  0.63823924,  0.48578569,  0.55177099,\n",
       "        -0.3029277 ]),\n",
       " array([-0.80869076,  0.54265384,  0.63826334,  0.48568938,  0.5516776 ,\n",
       "        -0.30297458]),\n",
       " array([-0.80899303,  0.54256822,  0.63828757,  0.48559322,  0.55158435,\n",
       "        -0.30302108]),\n",
       " array([-0.80929481,  0.54248272,  0.63831193,  0.48549721,  0.55149123,\n",
       "        -0.30306719]),\n",
       " array([-0.80959611,  0.54239736,  0.63833641,  0.48540135,  0.55139823,\n",
       "        -0.30311291]),\n",
       " array([-0.80989693,  0.54231212,  0.63836103,  0.48530564,  0.55130537,\n",
       "        -0.30315825]),\n",
       " array([-0.81019728,  0.54222701,  0.63838577,  0.48521008,  0.55121264,\n",
       "        -0.3032032 ]),\n",
       " array([-0.81049715,  0.54214202,  0.63841064,  0.48511467,  0.55112004,\n",
       "        -0.30324777]),\n",
       " array([-0.81079654,  0.54205716,  0.63843564,  0.4850194 ,  0.55102757,\n",
       "        -0.30329196]),\n",
       " array([-0.81109547,  0.54197242,  0.63846076,  0.48492429,  0.55093524,\n",
       "        -0.30333577]),\n",
       " array([-0.81139392,  0.54188781,  0.63848601,  0.48482932,  0.55084302,\n",
       "        -0.30337921]),\n",
       " array([-0.81169191,  0.54180333,  0.63851139,  0.4847345 ,  0.55075094,\n",
       "        -0.30342226]),\n",
       " array([-0.81198942,  0.54171897,  0.63853689,  0.48463982,  0.55065899,\n",
       "        -0.30346494]),\n",
       " array([-0.81228647,  0.54163473,  0.63856252,  0.48454529,  0.55056716,\n",
       "        -0.30350725]),\n",
       " array([-0.81258306,  0.54155062,  0.63858827,  0.48445091,  0.55047547,\n",
       "        -0.30354918]),\n",
       " array([-0.81287918,  0.54146663,  0.63861415,  0.48435667,  0.5503839 ,\n",
       "        -0.30359074]),\n",
       " array([-0.81317484,  0.54138276,  0.63864015,  0.48426258,  0.55029245,\n",
       "        -0.30363192]),\n",
       " array([-0.81347004,  0.54129902,  0.63866627,  0.48416863,  0.55020114,\n",
       "        -0.30367274]),\n",
       " array([-0.81376478,  0.5412154 ,  0.63869252,  0.48407482,  0.55010995,\n",
       "        -0.30371319]),\n",
       " ...]"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0c742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
